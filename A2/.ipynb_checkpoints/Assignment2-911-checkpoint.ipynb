{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessment Requirements \n",
    "Each group is required to complete the following two tasks: \n",
    "1. Generate a sparse representation for Paper Bodies (i.e. paper text without Title, Authors, Abstract and References). The sparse representation consists of two files: a. Vocabulary index file b. Sparse count vectors file \n",
    " \n",
    "2. Generate  a CSV file (stats.csv) containing three columns: a. Top 10 most frequent terms appearing in all Titles b. Top 10 most frequent Authors c. Top 10 most frequent terms appearing in all Abstracts \n",
    " \n",
    "Note: In case of ties in any of the above fields, settle the tie based on alphabetical ascending order. (example:  if the author named John appeared as many times as Mark, then John shall be selected over Mark) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Bot1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import pdfminer\n",
    "import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# segmentation first, then find capital words, then loop through and lower each word, then tokenize.\n",
    "import io\n",
    "from io import StringIO\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import os\n",
    "import sys, getopt\n",
    "\n",
    "import nltk.data\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "from nltk.collocations import BigramCollocationFinder \n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from itertools import chain\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "\n",
    "from nltk.probability import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "from nltk.collocations import BigramCollocationFinder \n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from itertools import chain\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "nltk.download('punkt')\n",
    "\n",
    "# segmentation first, then find capital words, then loop through and lower each word, then tokenize.\n",
    "import io\n",
    "from io import StringIO\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import os\n",
    "import sys, getopt\n",
    "\n",
    "import nltk.data\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfminer.six\n",
      "  Downloading https://files.pythonhosted.org/packages/3f/e7/857165590454c3c9b940af05b04d04284afca4bb45a7a2b06b7c927309f5/pdfminer.six-20181108-py2-none-any.whl (5.6MB)\n",
      "Requirement already satisfied: six in d:\\anacoda\\envs\\py2\\lib\\site-packages (from pdfminer.six) (1.12.0)\n",
      "Collecting pycryptodome (from pdfminer.six)\n",
      "  Downloading https://files.pythonhosted.org/packages/4b/e3/1829c5be39d54c3d249ce7e30936b4aca33f1bb6b87b88f3f3490227d9b3/pycryptodome-3.9.0-cp27-cp27m-win_amd64.whl (10.0MB)\n",
      "Collecting sortedcontainers (from pdfminer.six)\n",
      "  Using cached https://files.pythonhosted.org/packages/13/f3/cf85f7c3a2dbd1a515d51e1f1676d971abe41bba6f4ab5443240d9a78e5b/sortedcontainers-2.1.0-py2.py3-none-any.whl\n",
      "Installing collected packages: pycryptodome, sortedcontainers, pdfminer.six\n",
      "Successfully installed pdfminer.six-20181108 pycryptodome-3.9.0 sortedcontainers-2.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\n"
     ]
    }
   ],
   "source": [
    "#!pip install Tabula-py \n",
    "#!pip install pdfminer.six #uncomment to install Tabula-py\n",
    "#!pip install pdfminer3k #uncomment to install Tabula-py\n",
    "#!pip install tqdm\n",
    "#pdfminer3k is a Python 3 port of pdfminer\n",
    "!pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(fname, pages=None):\n",
    "    if not pages:\n",
    "        pagenums = set()\n",
    "    else:\n",
    "        pagenums = set(pages)\n",
    "\n",
    "    output = io.StringIO()\n",
    "    manager = PDFResourceManager()\n",
    "    converter = TextConverter(manager, output, laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(manager, converter)\n",
    "\n",
    "    infile = open(fname, 'rb')\n",
    "    for page in PDFPage.get_pages(infile, pagenums):\n",
    "        interpreter.process_page(page)\n",
    "    infile.close()\n",
    "    converter.close()\n",
    "    text = output.getvalue()\n",
    "    output.close\n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_input.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2656\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2657\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-1a59ae370356>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mlineList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlineList\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'P'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlineList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'filename'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.pdf'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'url'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2926\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2927\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2928\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2657\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2659\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2661\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "data = convert(\"Group102.pdf\")\n",
    "file = open('Group102.txt', \"w\")\n",
    "file.write(data)\n",
    "file.close()\n",
    "filepath = 'Group102.txt'\n",
    "with open(filepath) as f: lineList = f. readlines()\n",
    "lineList = [line for line in lineList if line[0] == 'P']\n",
    "df = pd.DataFrame(lineList)\n",
    "df[0] = df[0].apply(lambda x: x.split(\" \"))\n",
    "df['filename'] = df[0].apply(lambda x: x[0].replace('.pdf',''))\n",
    "df['url'] = df[0].apply(lambda x: x[1])\n",
    "df.drop(0,axis = 1)\n",
    "os.remove('Group102.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:39,  1.64s/it]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data') # make a dataset, to store all the pdf files downloaded\n",
    "    for each in tqdm(df.iterrows()):\n",
    "        response = requests.get(each[1][1])\n",
    "        with open('data/'+ str(each[1][0]),'wb') as f:\n",
    "            f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An empty list to store all the given stopwords\n",
    "stopwords=[]\n",
    "\n",
    "#Opening the given stopwords file and storing the words in the stopwords list\n",
    "with open('stopwords_en.txt') as f:\n",
    "    stopwords = f.read().splitlines()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_lower(sentence): #E\n",
    "    \n",
    "    aux_sentence = ''\n",
    "     #A.\n",
    "    sentence = sentence.replace('the','')\n",
    "    sentence = sentence.replace('The','')\n",
    "    sentence = sentence.replace('ﬁ','fi')\n",
    "    sentence = sentence.replace('ﬀ','ff')\n",
    "    sentence = sentence.replace('- ','')\n",
    "    sentence = sentence.replace('-\\n','')\n",
    "    sentence = sentence.replace('\\n',' ')\n",
    "    sentence = sentence.replace('coe   cient','coefficient')\n",
    "    cap_set = re.findall(r'(?!^)\\b([A-Z]\\w+)',sentence)\n",
    "    for word in sentence.split(\" \"):\n",
    "        if (len(word) > 2) and (word not in stopwords):\n",
    "            if (word not in cap_set):\n",
    "                aux_sentence += word.lower() + str(' ')\n",
    "            elif (len(word) > 2) and (word not in stopwords):\n",
    "                aux_sentence += word + str(' ')\n",
    "                \n",
    "    aux_sentence = re.sub('[^A-Za-z-/]+', ' ', aux_sentence.strip())\n",
    "    \n",
    "    return  aux_sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "body_dict={}\n",
    "\n",
    "def get_data(directory):\n",
    "    \n",
    " \n",
    "    \n",
    "    shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')    \n",
    "    body_dict={}   \n",
    "    for filename in tqdm(os.listdir(directory)):\n",
    "            filepdf = filename.replace('.pdf','')\n",
    "            raw_body = convert(str(os.path.join(directory, filename)))\n",
    "                     \n",
    "            sentence_list = sent_detector.tokenize(raw_body.strip())\n",
    "\n",
    "\n",
    "            body = []\n",
    "            start = 0\n",
    "            stop = 0\n",
    "            for i in range(len(sentence_list)):\n",
    "                if 'Paper Body' in sentence_list[i]:\n",
    "                    start = i\n",
    "                    sentence_list[i] = sentence_list[i].replace('Paper Body','')\n",
    "                if '2 Reference' in sentence_list[i]:\n",
    "                    stop = i\n",
    "                sentence_list[i] = sentence_list[i].replace('w indows','windows')\n",
    "                sentence_list[i] = sentence_list[i].replace('W indows','Windows')\n",
    "            # this is to find the start and stop of Paper body\n",
    "            for i in range(start, stop):\n",
    "                body.append(sentence_list[i])\n",
    "            for i in range(len(body)):\n",
    "                body[i] = body[i].replace('\\n',' ') #replace all the new line\n",
    "                body[i] = selective_lower(body[i]) #E:.\n",
    "                \n",
    "            for i in range(start, stop):\n",
    "                body.append(sentence_list[i])\n",
    "            for i in range(len(body)):\n",
    "                body[i] = body[i].replace('\\n',' ') #replace all the new line\n",
    "                body[i] = selective_lower(body[i]).strip() #E:.\n",
    "                body[i] = shortword.sub('',body[i]) #make sure shortword removed\n",
    "            body_dict[filepdf] = \" \".join(body)\n",
    "    return body_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|██████████████████████████████████████████▉                                      | 53/100 [00:57<00:50,  1.07s/it]"
     ]
    }
   ],
   "source": [
    "pathpdf = 'data/'\n",
    "tokenizer = RegexpTokenizer(\"[A-Za-z]\\w+(?:[-'?]\\w+)?\")\n",
    "body_dict = get_data(pathpdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PP3209': 'learning procedure generalize well, converges faster multiplicative baseline algorithms.” preliminaries problem-line learning linear-threshold functions labeled data spurred substantial amount research Machine learning. relevance task oretical practical point view widely recognized: hand, linear functions combine ﬂexiblity analytical computational tractability, hand, online algorithms provide eﬃcient methods processing massive amounts data. moreover, widespread kernel methods Machine Learning]) greatly improved scope learning technology, reby increasing furr general attention specific task incremental learning (generalized) linear functions. models/algorithms proposed literature (stochastic, adversarial, noisy, etc.) any list references justice existing work subject. paper, interested problem online learning linear-threshold functions adversarially generated examples. introduce family algorithms, collectively called higher-order Perceptron algorithm (where ?higher? means ?higher one., ?higher first-order? descent algorithms gradientdescent standard perceptron-like algorithms?). contrary higher-order algorithms, ridge regression-like algorithms considered], higher-order Perceptron ability put toger principled ﬂexible manner second-order statistics data ?logarithmic behavior? multiplicative/dual-norm algorithms]). algorithm exploits simplified form inverse data matrix, lending easily combined dual norms machinery introduced] (see]). see, computational advantages, allowing formulate eﬃcient (subquadratic) implementation. contribution twofold. first, provide initial oretical analysis suggesting algorithm standard Perceptron algorithm] operating transformed sequence examples improved margin properties. analysis suggests simple (but principled) switching higher-order first-order updates.  authors gratefully acknowledge partial support PASCAL Network Excellence grant 506778. publication reﬂects authors? views. convenient deal kernel functions, major concern sparsity computed solution. contribution paper experimental investigation algorithm artificial real-world datasets domains: compared higher-order Perceptron baseline Perceptron algorithms, second-order Perceptron algorithm defined] standard-norm) Perceptron algorithm]. found experiments higher-order Perceptron generalizes well. experimental findings following: higher-order Perceptron outperforming multiplicative-norm) baseline (thus stored data matrix beneficial terms convergence speed); When dealing Euclidean norms), comparison second-order Perceptron clear depends specific task hand. learning protocol notation. algorithm works well-known mistake bound model-line learning, introduced], furr investigated authors] references rein). prediction proceeds sequence trials. trial  prediction algorithm instance vector (for simplicity, vectors normalized euclidean norm orwise specified), guesses binary label , denote algorithm prediction ybt }. true label disclosed. case ybt algorithm made prediction mistake. call pair sequence examples sequence  paper, competing class linear-threshold predictors, parametrized normal vectors  }. case, common measuring (relative) prediction performance algorithm compare total number mistakes measure linear separability One measure]) cumulative hinge-loss soft-margin) . linear classifier margin   max,  (observe      ; vanishes separates margin mistake-driven algorithm updates internal state mistakes. refore associate run subsequence, , mistaken trials. now, standard analysis algorithms restrict behavior comparison class mistaken trialsponly and, consequence, refine ; include trials  max,   bounds performance relative sequence examples produced, actually, selected-line functioning. analysis Section step furr: number mistakes contrasted cumulative hinge loss transformed sequence ((? xi1 yi1 xi2 yi2    xim yim )), instance xik transformed mapping depending past behavior algorithm., examples trial section sequence ?more separable? original sequence, sense linearly separable margin, transformed sequence separable larger margin. higher-order Perceptron algorithm algorithm (described Figure takes input sequence nonnegative parameters ..., maintains product matrix (initialized identity matrix sum vector (initialized). indexed counter storing current number mistakes (plus one). receiving normalized instance vector algorithm computes binary prediction ybt sign product vector vector ybt matrix updates multiplicatively vector updated additively standard Perceptron rule matrix vector trial. ybt update performed (hence algorithm mistake driven). observe makes algorithm degenerate standard Perceptron algorithm]. moreover, easily that, order algorithm exploit information collected matrix (and algorithm behavior substantially perceptron) ensure sequel, standard choice, ). Sections implementing higher-order Perceptron ways. below, quickly describe merits. primal version. store update matrix-dimensional column parameters: ... ). initialization: repeat    instance  predict ybt sgn  label  ybt   figure higher-order Perceptron algorithm (for). vector matrix updated ¿ ¿ taking operations, updated Figure computing algorithm margin carried time quadratic dimension input space. dual version. implementation kernel functions]). denote matrix columns-dimensional instance vectors ..., mistake occurred far-dimensional column vector) labels. store update matrix diagonal matrix-dimensional column vector vector ones. interpret primal matrix hard show margin equal computed) extra products. now mistake, vector updated extra products updating way. emptymatrices.  diag     DIAG hand, DIAG observe trials matrix padded row column. ) This amounts matrix updated closer update mechanism conclude extra products needed compute quadratic number past mistaken trials turns important sparse version algorithm which, mistaken trial, decides wher update (see Section). implicit primal version dual norms algorithm. based simple observation vector compute unwrapping  vector calculated time). computing margin takes). maintaining implicit representation product matrix convenient eﬃcient dual version unavailable, case multiplicative, generally, dual norms) extension algorithm. recall multiplicative algorithm learning sparse target hyperplanes]). obtain dual norms algorithm introducing norm parameter gradient mapping2      figure normalize instance vectors. -norm, define gen¿ eralize matrix update  see, resulting algorithm combines multiplicative behavior-norm algorithms ?second-order? information contained matrix easily above-mentioned argument computing margin time) holds. observe that, construction, symmetric matrix. mapping]. recall setting(log yields algorithm similar Winnow]. also, notice yields identity. analysis express performance higher-order Perceptron algorithm terms hinge-loss behavior linear classifier transformed sequence )   ) trial mistake occurs matrix produced algorithm. observe feature vector) transformed matrix depending past examples only. relevant argument larger margin original sequence (see discussion end section). neat-line structure? shared competing higher-order algorithms, ?ridge regression-like? algorithms considered]. sake simplicity, state orem case general statement holds  orem Let higher-order Perceptron algorithm Figure run sequence examples    sequence parameters satisfy   mistaken instance vector, ]. total number mistakes satisfies3  ;   ??  )     holding unit norm vector   ) . proof. analysis deliberately mimics standard Perceptron convergence analysis]. fix arbitrary sequence    ,   set trials algorithm Figure made mistake. ) trial mistake occurred. study evolution mistaken trials. notice matrix¿ positive semidefinite write  (from update rule    (using rk2 set brevity   proceed upper lower bounding chain equalities. end, ensure  observe implies   hand order nonnegative, suﬃces pick  cases   implies rk2    now (combined ), conclude     set simple (and crude) upper bound term observing implies  ——, spectral norm (largest eigenvalue factor matrix form  ¿   spectral norm one   )  refore, summing   , equivalently, yields upper bound  ) find lower bound left-hand side), pick unit norm vector apply standard cauchy-schwartz inequality  observe generic trial) update rule algorithm write    max,   }), inequality holds margin  sum subscript emphasizes dependence transformed sequence choice Note special case reby recovering standard Perceptron bound nonseparable sequences (see]). slightly refined bound derived depends trace matrices  details full version paper.    exploit   rearranging terms.      ?  combining) solving claimed bound. result algorithm viewed standard Perceptron algorithm operating transformed sequence). give qualitative argument, suggestive improved margin properties assume simplicity examples original sequence correctly classified hyperplane margin ). orem parameters  small positive numbers. assume, simplicity, set small  order, matrix  ) approximated  ) extent approximation holds, write   )  )  )     now margin (first-order) Perceptron vector mistaken trial higher-order Perceptron vector vectors correlated (recall mistaken condition imply opposite. tends yield margin larger original margin mentioned Section advantageous computational standpoint, cases matrix update skipped (this equivalent setting), orem hold. starting point oretical understanding margin properties algorithm, paper prefer stop early leave furr investigation collecting experimental evidence. experiments tested empirical performance algorithm conducting number experiments collection datasets, artificial real-world diverse domains (optical Character recognition, text categorization, DNA microarrays). main goal experiments compare higher-order Perceptron (with perceptron-like algorithms, first-order] second-order Perceptron], terms training accuracy., convergence speed) test set accuracy. results contained Tables Figure task DNA microarrays artificial data. goal test convergence properties algorithms sparse target learning tasks. tested couple well-known DNA microarray datasets. dataset, generated number random training/test splits (our random splits included random permutations training set). reported results averaged random splits. dna datasets are: ? dataset]. task analyze expression profiles breast cancer classify breast tumors (estrogen receptor) status. dataset (which call ?breast? dataset) expression profiles 3389 genes. randomly split 1000 times training set size test set size. ?lymphoma? dataset]. goal separate cancerous normal tissues large-cell lymphoma problem. dataset expression profiles 4026 genes. randomly split dataset training set size test set size. again, random split performed 1000 times. datasets, tested algorithms run cycling times current training set. kernel functions used. artificially generated (moderately) sparse learning problems margin  .005 labeling noise levels  (linearly separable) , respectively. datasets generated random generating (normalized) target vectors }500 components selected independently random} remaining 450 again, similar argument holds general setting  reader notice important dependence past argument. components set  target  and, settings, randomly generated 1000 training examples 1000 test examples. instance vectors chosen random]500 normalized.    label    label \\x0cassociated labels obtained ﬂipped probability   rejected vector drawn. call datasets ?artificial ?artificial tested algorithms training increasing number epochs checking evolution test set accuracy. again, kernel functions used. task Text categorization. text categorization datasets derived,000 newswire stories Reuters Corpus Volume (rcv1]). standard IDF bag-words encoding transform news story normalized vector real attributes. built binary classification problems ?binarizing? consecutive news stories target categories, 101. 2nd, 3rd, 4th, 5th frequent6 categories, respectively,000 news stories rcv1. call datasets RCV1x, 101. dataset split training set size,000 test set size. algorithms trained single epoch. initially polynomial kernels, realized kernel functions significantly alter conclusions task. reported results refer algorithms kernel functions. task Optical character recognition (ocr). well-known OCR benchmarks: USPS dataset MNIST dataset] standard experimental setups], including one-versus-rest scheme reducing multiclass problem set binary tasks. algorithm standard Gaussian polynomial kernels, parameters chosen-fold cross validation training set standard ranges. again, algorithms trained single epoch training set. results Table refer parameter settings kernel. algorithms. implemented standard Perceptron algorithm (with kernels), second-order Perceptron algorithm] (with kernels), Higherorder Perceptron algorithm. implementation algorithm (for ?implicit primal? tested sparse learning tasks, dual variables tasks. second-order perceptron, set parameter (see] details) testing generous range values. brevity, settings achieving results reported. sparse learning tasks higher-order Perceptron norm, tasks set case, set7. corresponds standard-norm Perceptron algorithm] emphasize comparison. finally, kernels OCR tasks, compared sparse dual version higher-order perceptron. mistaken round), algorithm sets orwise (thus matrix updated). sake brevity, standard Perceptron algorithm called (?first order? second-order algorithm denoted (?second order? higher-order algorithm norm parameter abbreviated HOp). thus, instance, HO2). results conclusions. higher-order Perceptron algorithm deliver interesting results. experiments HOp) outperforms HOp). hand, comparison HOp. depends specific task. dna datasets, HOp) superior breast. lymphoma, HOp) worse increases. good indication that, general, multiplicative algorithm suitable dataset. case, HO2 turns slightly worse. artificial datasets HOp-norm Perceptron algorithm. text categorization tasks, HO2 perform. usps, HO2 superior competitors, MNIST performs similarly combined Gaussian kernels (though turns sparser), slightly inferior polynomial kernels. sparse version HO2 cuts matrix updates roughly half, maintaining good performance. cases HO2 (eir sparse not) significantly outperforms. conclusion, higher-order Perceptron algorithm interesting tool-line binary clas6 frequent category significant overlap ones. notice setting fulfills condition stated orem table Training test error datasets ?breast? ?lymphoma?. training error average total number updates training epochs, test error average fraction misclassified patterns test set, results refer training/test splits. algorithm, setting shown (best training test setting coincided experiments). thus, instance, HO2 differs parameter. emphasized comparison HO7. HO7) tested values. Wilcoxon signed rank test, error difference% larger considered significant. bold smallest figures achieved row table. TRAIN TEST TRAIN TEST LYMPHOMA) Training updates training epochs Artificial training updates 800) 600) 400 300 2400) 700 500) 2000 1200 400 Test error rates% Test error rates (minus) training epochs) Test error rates training epochs Artificial% training epochs Test error rates training epochs Artificial% training epochs) 1600 800) \\x0ctraining updates training epochs Artificial) training updates REAST training epochs Figure Experiments artificial datasets (artificial0 left, artificial0 right). plots give training test behavior function number training epochs. notice test set artificial0 affected labelling noise rate%. hence, visual comparison plots bottom made shift-axis noisy plot%. hand, training plots (top) readily comparable. reader diﬃculty telling kinds algorithms HOp) HOp) practice, turned slightly superior performance former. sification, ability combine multiplicative nonadditive) second-order behavior single inference procedure. algorithms, HOp extended (details omitted due space limitations) ways worst-case learning technologies, large margin]), label-eﬃcient/active learning]), bounded memory]). our learning procedure generalize well, converges faster multiplicative baseline algorithms.” preliminaries problem-line learning linear-threshold functions labeled data spurred substantial amount research Machine learning. relevance task oretical practical point view widely recognized: hand, linear functions combine ﬂexiblity analytical computational tractability, hand, online algorithms provide eﬃcient methods processing massive amounts data. moreover, widespread kernel methods Machine Learning]) greatly improved scope learning technology, reby increasing furr general attention specific task incremental learning (generalized) linear functions. many models/algorithms proposed literature (stochastic, adversarial, noisy, etc.) Any list references justice existing work subject. paper, interested problem online learning linear-threshold functions adversarially generated examples. introduce family algorithms, collectively called higher-order Perceptron algorithm (where ?higher? means ?higher one., ?higher first-order? descent algorithms gradientdescent standard perceptron-like algorithms?). contrary higher-order algorithms, ridge regression-like algorithms considered], higher-order Perceptron ability put toger principled ﬂexible manner second-order statistics data ?logarithmic behavior? multiplicative/dual-norm algorithms]). our algorithm exploits simplified form inverse data matrix, lending easily combined dual norms machinery introduced] (see]). see, computational advantages, allowing formulate eﬃcient (subquadratic) implementation. our contribution twofold. first, provide initial oretical analysis suggesting algorithm standard Perceptron algorithm] operating transformed sequence examples improved margin properties. analysis suggests simple (but principled) switching higher-order first-order updates. this authors gratefully acknowledge partial support PASCAL Network Excellence grant 506778. this publication reﬂects authors? views. convenient deal kernel functions, major concern sparsity computed solution. contribution paper experimental investigation algorithm artificial real-world datasets domains: compared higher-order Perceptron baseline Perceptron algorithms, second-order Perceptron algorithm defined] standard-norm) Perceptron algorithm]. found experiments higher-order Perceptron generalizes well. among experimental findings following: higher-order Perceptron outperforming multiplicative-norm) baseline (thus stored data matrix beneficial terms convergence speed); When dealing Euclidean norms), comparison second-order Perceptron clear depends specific task hand. learning protocol notation. our algorithm works well-known mistake bound model-line learning, introduced], furr investigated authors] references rein). prediction proceeds sequence trials. trial  prediction algorithm instance vector (for simplicity, vectors normalized Euclidean norm orwise specified), guesses binary label , denote algorithm prediction ybt }. true label disclosed. case ybt algorithm made prediction mistake. call pair sequence examples sequence  paper, competing class linear-threshold predictors, parametrized normal vectors  }. case, common measuring (relative) prediction performance algorithm compare total number mistakes measure linear separability One measure]) cumulative hinge-loss soft-margin) . linear classifier margin   max,  (observe      ; vanishes separates margin mistake-driven algorithm updates internal state mistakes. one refore associate run subsequence, , mistaken trials. now, standard analysis algorithms restrict behavior comparison class mistaken trialsponly and, consequence, refine ; include trials  max,   this bounds performance relative sequence examples produced, actually, selected-line functioning. our analysis Section step furr: number mistakes contrasted cumulative hinge loss transformed sequence ((? xi1 yi1 xi2 yi2    xim yim )), instance xik transformed mapping depending past behavior algorithm., examples trial Section sequence ?more separable? original sequence, sense linearly separable margin, transformed sequence separable larger margin. higher-order Perceptron algorithm algorithm (described Figure takes input sequence nonnegative parameters ..., maintains product matrix (initialized identity matrix sum vector (initialized). both indexed counter storing current number mistakes (plus one). upon receiving normalized instance vector algorithm computes binary prediction ybt sign product vector vector ybt matrix updates multiplicatively vector updated additively standard Perceptron rule matrix vector trial. ybt update performed (hence algorithm mistake driven). observe makes algorithm degenerate standard Perceptron algorithm]. moreover, easily that, order algorithm exploit information collected matrix (and algorithm behavior substantially perceptron) ensure sequel, standard choice, ). see Sections implementing higher-order Perceptron ways. below, quickly describe merits. Primal version. store update matrix-dimensional column parameters: ... ). initialization: repeat    get instance  predict ybt sgn  get label  ybt   figure higher-order Perceptron algorithm (for). vector matrix updated ¿ ¿ taking operations, updated Figure computing algorithm margin carried time quadratic dimension input space. Dual version. this implementation kernel functions]). let denote matrix columns-dimensional instance vectors ..., mistake occurred far-dimensional column vector) labels. store update matrix diagonal matrix-dimensional column vector vector ones. interpret primal matrix hard show margin equal computed) extra products. now mistake, vector updated extra products updating way. emptymatrices.  diag     DIAG hand, DIAG observe trials matrix padded row column. ) This amounts matrix updated closer update mechanism conclude extra products needed compute quadratic number past mistaken trials this turns important sparse version algorithm which, mistaken trial, decides wher update (see Section). Implicit primal version dual norms algorithm. this based simple observation vector compute unwrapping  vector calculated time). thus computing margin takes). maintaining implicit representation product matrix convenient eﬃcient dual version unavailable, case multiplicative, generally, dual norms) extension algorithm. recall multiplicative algorithm learning sparse target hyperplanes]). obtain dual norms algorithm introducing norm parameter gradient mapping2      Figure normalize instance vectors. -norm, define gen¿ eralize matrix update  see, resulting algorithm combines multiplicative behavior-norm algorithms ?second-order? information contained matrix one easily above-mentioned argument computing margin time) holds. Observe that, construction, symmetric matrix. this mapping]. recall setting(log yields algorithm similar Winnow]. also, notice yields identity. Analysis express performance higher-order Perceptron algorithm terms hinge-loss behavior linear classifier transformed sequence )   ) trial mistake occurs matrix produced algorithm. observe feature vector) transformed matrix depending past examples only. this relevant argument larger margin original sequence (see discussion end section). this neat-line structure? shared competing higher-order algorithms, ?ridge regression-like? algorithms considered]. for sake simplicity, state orem case general statement holds  orem Let higher-order Perceptron algorithm Figure run sequence examples    let sequence parameters satisfy   mistaken instance vector, ]. total number mistakes satisfies3  ;   ??  )     holding unit norm vector   ) . proof. analysis deliberately mimics standard Perceptron convergence analysis]. fix arbitrary sequence    ,   set trials algorithm Figure made mistake. let) trial mistake occurred. study evolution mistaken trials. notice matrix¿ positive semidefinite write  (from update rule    (using rk2 set brevity   proceed upper lower bounding chain equalities. end, ensure  observe implies   hand order nonnegative, suﬃces pick  cases   implies rk2    now (combined ), conclude     set simple (and crude) upper bound term observing implies  ——, spectral norm (largest eigenvalue since factor matrix form  ¿   spectral norm one   )  refore, summing   , equivalently, yields upper bound  ) find lower bound left-hand side), pick unit norm vector apply standard cauchy-schwartz inequality  observe generic trial) update rule algorithm write    max,   }), inequality holds margin  sum subscript emphasizes dependence transformed sequence choice Note special case reby recovering standard Perceptron bound nonseparable sequences (see]). slightly refined bound derived depends trace matrices  details full version paper.    exploit   rearranging terms. this     ?  combining) solving claimed bound. from result algorithm viewed standard Perceptron algorithm operating transformed sequence). give qualitative argument, suggestive improved margin properties assume simplicity examples original sequence correctly classified hyperplane margin ). according orem parameters  small positive numbers. assume, simplicity, set small  order, matrix  ) approximated  ) extent approximation holds, write   )  )  )     now margin (first-order) Perceptron vector mistaken trial higher-order Perceptron vector since vectors correlated (recall mistaken condition imply opposite. this \\x0ctends yield margin larger original margin mentioned Section advantageous computational standpoint, cases matrix update skipped (this equivalent setting), orem hold. though starting point oretical understanding margin properties algorithm, paper prefer stop early leave furr investigation collecting experimental evidence. Experiments tested empirical performance algorithm conducting number experiments collection datasets, artificial real-world diverse domains (optical Character recognition, text categorization, DNA microarrays). main goal experiments compare higher-order Perceptron (with perceptron-like algorithms, first-order] second-order Perceptron], terms training accuracy., convergence speed) test set accuracy. results contained Tables Figure task DNA microarrays artificial data. goal test convergence properties algorithms sparse target learning tasks. tested couple well-known DNA microarray datasets. for dataset, generated number random training/test splits (our random splits included random permutations training set). reported results averaged random splits. DNA datasets are: ? dataset]. here task analyze expression profiles breast cancer classify breast tumors (estrogen receptor) status. this dataset (which call ?breast? dataset) expression profiles 3389 genes. randomly split 1000 times training set size test set size. ?lymphoma? dataset]. here goal separate cancerous normal tissues large-cell lymphoma problem. dataset expression profiles 4026 genes. randomly split dataset training set size test set size. again, random split performed 1000 times. datasets, tested algorithms run cycling times current training set. kernel functions used. artificially generated (moderately) sparse learning problems margin  .005 labeling noise levels  (linearly separable) , respectively. datasets generated random generating (normalized) target vectors }500 components selected independently random} remaining 450 again, similar argument holds general setting  reader notice important dependence past argument. components set  target  and, settings, randomly generated 1000 training examples 1000 test examples. instance vectors chosen random]500 normalized.    label    label \\x0cassociated labels obtained ﬂipped probability   rejected vector drawn. call datasets ?artificial ?artificial tested algorithms training increasing number epochs checking evolution test set accuracy. again, kernel functions used. task Text categorization. text categorization datasets derived,000 newswire stories Reuters Corpus Volume (rcv1]). standard IDF bag-words encoding transform news story normalized vector real attributes. built binary classification problems ?binarizing? consecutive news stories target categories, 101. 2nd, 3rd, 4th, 5th frequent6 categories, respectively,000 news stories rcv1. call datasets RCV1x, 101. each dataset split training set size,000 test set size. all algorithms trained single epoch. initially polynomial kernels, realized kernel functions significantly alter conclusions task. thus reported results refer algorithms kernel functions. task Optical character recognition (ocr). well-known OCR benchmarks: USPS dataset MNIST dataset] standard experimental setups], including one-versus-rest scheme reducing multiclass problem set binary tasks. algorithm standard Gaussian polynomial kernels, parameters chosen-fold cross validation training set standard ranges. again, algorithms trained single epoch training set. results Table refer parameter settings kernel. algorithms. implemented standard Perceptron algorithm (with kernels), second-order Perceptron algorithm] (with kernels), Higherorder Perceptron algorithm. implementation algorithm (for ?implicit primal? tested sparse learning tasks, dual variables tasks. when second-order perceptron, set parameter (see] details) testing generous range values. for brevity, settings achieving results reported. sparse learning tasks higher-order Perceptron norm, tasks set case, set7. since corresponds standard-norm Perceptron algorithm] emphasize comparison. finally, kernels OCR tasks, compared sparse dual version higher-order perceptron. mistaken round), algorithm sets orwise (thus matrix updated). for sake brevity, standard Perceptron algorithm called (?first order? second-order algorithm denoted (?second order? higher-order algorithm norm parameter abbreviated HOp). thus, instance, HO2). results conclusions. our higher-order Perceptron algorithm deliver interesting results. our experiments HOp) outperforms HOp). hand, comparison HOp. depends specific task. DNA datasets, HOp) superior breast. lymphoma, HOp) worse increases. this good indication that, general, multiplicative algorithm suitable dataset. case, HO2 turns slightly worse. artificial datasets HOp-norm Perceptron algorithm. text categorization tasks, HO2 perform. usps, HO2 superior competitors, MNIST performs similarly combined Gaussian kernels (though turns sparser), slightly inferior polynomial kernels. sparse version HO2 cuts matrix updates roughly half, maintaining good performance. cases HO2 (eir sparse not) significantly outperforms. conclusion, higher-order Perceptron algorithm interesting tool-line binary clas6 frequent category significant overlap ones. notice setting fulfills condition stated orem table Training test error datasets ?breast? ?lymphoma?. training error average total number updates training epochs, test error average fraction misclassified patterns test set, results refer training/test splits. for algorithm, setting shown (best training test setting coincided experiments). thus, instance, HO2 differs parameter. emphasized comparison HO7. HO7) tested values. according Wilcoxon signed rank test, error difference% larger considered significant. bold smallest figures achieved row table. TRAIN TEST TRAIN TEST LYMPHOMA) Training updates training epochs Artificial training updates 800) 600) 400 300 2400) 700 500) 2000 1200 400 Test error rates% Test error rates (minus) training epochs) Test error rates training epochs Artificial% training epochs Test error rates training epochs Artificial% training epochs) 1600 800) \\x0ctraining updates training epochs Artificial) training updates REAST training epochs Figure Experiments artificial datasets (artificial0 left, artificial0 right). plots give training test behavior function number training epochs. notice test set artificial0 affected labelling noise rate%. hence, visual comparison plots bottom made shift-axis noisy plot%. hand, training plots (top) readily comparable. reader diﬃculty telling kinds algorithms HOp) HOp) practice, turned slightly superior performance former. sification, ability combine multiplicative nonadditive) second-order behavior single inference procedure. like algorithms, HOp extended (details omitted due space limitations) ways worst-case learning technologies, large margin]), label-eﬃcient/active learning]), bounded memory]).',\n",
       " 'PP3316': 'neuromorphic analog, VLSI devices] derive organizational computational principles biologically plausible models neural systems, aiming providing long run electronic substrate innovative, bioinspired computational paradigms. line standard assumptions computational neuroscience, neuromorphic devices endowed adaptive capabilities forms plasticity synapses connect neural elements. widely adopted framework Hebbian learning, eﬃcacy synapse potentiated post-synaptic effect spike enhanced) preand post-synaptic neurons simultaneously active suitable time scale. mechanisms proposed, relying average firing rates preand post-synaptic neurons, (rate-based Hebbian learning), ors based tight constraints time lags preand post-synaptic spikes (?spike-timing-dependent-plasticity?). synaptic circuits implement stochastic version rate-based Hebbian learning. decade, realized general constraints plausibly met concrete implementation synaptic device neural network, bear profound consequences http://neural.iss.infn/ capacity network memory system. specifically, accepts synaptic element neir unlimited dynamic range. synaptic eﬃcacy bounded), undergo arbitrarily small. synaptic eﬃcacy finite analog depth), proven]) deterministic learning prescription implies extremely low memory capacity, severe ?palimpsest? property: memories quickly erase trace older ones. turns stochastic mechanism general, logically appealing eﬃcient solution: preand post-synaptic neural activities, synapse made eligible changing eﬃcacy Hebbian prescription, state probability. stochastic element learning dynamics imply hoc elements, fact spike-driven implementation synapse, noisy activity neurons network provide needed ?noise generator? ]. refore, eﬃcient learning electronic network, implementation neuron spiking element requirement ?biological plausibility?, compelling computational requirement. learning networks spiking neurons stochastic plastic synapses studied oretically], stochastic-stable synaptic models implemented silicon]. limitations far, oretical implementation level, artificially simple statistics stimuli learnt., overlap neural representations). recently] modification stochastic-stable synaptic model proposed, endowed regulatory mechanism termed ?stop learning? synaptic downregulation depends average activity postsynaptic neuron recent past; synapse pointing neuron found highly active, poorly active, furr potentiated depressed, respectively. reason prescription essentially correlated patterns learnt network, successful strategy-emphasize coherent synaptic Hebbian potentiation result overlapping part synaptic matrix, ultimately spoil ability distinguish patterns. detailed learning strategy line proven] linearly separable patterns perceptron-like network; extension spiking recurrent networks studied. section give overview chip architecture implemented synaptic model. section show measures effectuated chip characterize synaptic neuronal parameters. section report characterization results compared oretical prediction obtained chip-oriented simulation. paragraph describes chip performances simple classification task, illustrate improvement brought stop-learning mechanism. chip architecture main features chip] implements recurrent network integrate-and-fire neurons spikefrequency adaptation-stable, stochastic, Hebbian synapses. completely reconfigurable synaptic matrix supports all-all recurrent connectivity, aer-based external connectivity. establishing arbitrary synaptic connectivity, excitatory/inhibitory nature synapse set. implemented neuron neuron constant leakage term lower bound membrane potential) introduced] studied oretically]. circuit borrowed low-power design], refer reader details. neurons directly probed., ?membrane potential? sampled), emitted spikes monitored AER]. dendritic tree neuron composed activated recurrent synapses activated external, AER ones. recurrent synapses, impinging spike triggers short-time (and possibly long-term) state synapse, detailed below. spikes neurons chip form AER events, targeted correct AER synapse decoder. synapses set excitatory, eir AER recurrent plastic; inhibitory synapses fixed. spikes generated neurons chip arbitrated access AER bus monitoring and mapping external targets. synaptic circuit] implements model proposed] brieﬂy motivated introduction. synapse possesses states eﬃcacy-stable device): internal synaptic dynamics internal variable eﬃcacy set potentiated, orwise set depressed. subjected short-term, spike-driven dynamics: arrival impinging spike, candidate upward downward jump, depending instantaneous post-synaptic potential Vpost threshold jump performed depending furr variable explained below. absence intervening spikes forced drift ?high? ?low? depending wher jump left preserves synaptic eﬃcacy long time scale. furr variable post-synaptic neuron dynamics, essentially measures average firing activity. ], analogy role played intracellular concentration calcium ions spike emission, call ?calcium variable? ). ) undergoes upward jump postsynaptic neuron emits spike, linearly decays spikes. refore integrates spikes sequence and, compared suitable thresholds detailed below, determines candidate synaptic jumps allowed occur; example, constrain synapse stop-regulating post-synaptic neuron active. ) acts regulatory element synaptic dynamics. resulting short-term dynamics internal synaptic variable conditions) ) Jup Vpost) ) jdw Vpost) ) Jup Jdw positive \\x0cconstants. detailed description circuits implementing conditions found]. figure illustrate effect calcium dynamics increasing input forces postsynaptic neuron fire increasing frequencies. long) undergoes jumps. ) jumps inhibited forced drift lower bound. vpost) VTH2) vpre) Figure Illustrative stop-learning mechanism (see text). top bottom: postsynaptic neuron potential Vpost calcium variable internal synaptic variable pre-synaptic neuron potential Vpre ltp/ltd probabilities: measurement chip-oriented simulation report synapse potentiation (ltp) depression (ltd)from chip compare experimental results simulations. synapse subset, generate pre-synaptic poisson spike train. post synaptic neuron forced fire poisson spike train applying external current poisson train inhibitory spikes aer. setting potentiated depressed eﬃcacies, activity post-synaptic neuron easily tuned varying amplitude current frequency inhibitory AER train. initialize (aer) synapses depressed (potentiated) monitor post-synaptic neuron activity stimulation trial lasting seconds. end trial read synaptic state AER protocol developed purpose. chosen post-synaptic firing rate, evaluate probability find synapses potentiated (depressed) state repeating test times. results reported figure (solid lines) represent average LTP LTD probabilities trail synapses. tests performed active inactive Calcium mechanism. calcium mechanism inactive, LTP monotonically increasing post-synaptic firing rate calcium circuit activated LTP probability max form Vpost. identical tests run simulation (dashed curves figure). purpose meaningful comparison chip behaviour relevant parameter affecting neural synaptic dynamics distributions (due inhomogenities mismatches) characterized. simulated measured data qualitative agreement. parameters chose tests classification task paragraph. fraction potentiated synapses experiment: solid line simulation: dashed line 100 150 ?post] 200 250 300 Figure Transition probabilities. red blue lines LTP probabilities calcium stop-learning mechanism respectively. gray lines LTD probabilities calcium stoplearning mechanism, case LTD mechanism shown. error bars standard deviations trials Learning overlapping patterns configured synaptic matrix perceptron network output inputs AER synapses). synapses set plastic excitatory ones, 32nd set inhibitory modulate post-synpatic neuron activity. aim teach perceptron classify patterns semi-supervised learning strategy? ?down?. expect learning perceptron respond high output frequency pattern? low output frequency pattern ?down?. regulating mechanism exploited improve performances Down patterns significant overlap. learning semi-supervised: pattern ?teacher? input output neuron steering activity high low, desired. end learning period ?teacher? turned perceptron output driven input stimuli: conditions classification ability tested. present learning performances input patterns increasing overlap, demonstrate effect stop learning mechanism (overlap ranging synapses). stimulation active pre-synaptic inputs poisson spike trains, inactive inputs poisson spike trains. trial lasts half second. down patterns randomly presented equal probability. teaching signal, combination excitatory constant cur4 rent inhibitory AER spike train, forces output firing rate. run lasts 150 trials suﬃcient stabilization output frequencies. end trial turn teaching signal, freeze synaptic dynamics read state synapse AER protocol developed purpose. conditions performed seconds test (?checking phase?) measure perceptron frequencies pattern pattern Down presented. experiment includes runs. run change: ?definition? patterns down: inputs activated pattern Down chosen randomly beginning run; initial synaptic state, constraint synapses potentiated; stimulation sequence. experiment turned stop learning mechanism chose orthogonal patterns. case perceptron correctly classify stimuli: trials, choosing suitable threshold, discriminate perceptron ouput patterns (lower left panel figure). output frequency separation slightly increases trial number 100 remaining stable point. studied case overlapped patterns active inactive Calcium mechanism. repeated experiment increasing overlap. (implying increase coding level orthogonal case overlap equal). threshold Khigh active threshold jumps inhibnited). calcium circuit parameters tuned variable passes Khigh firing rate postsynaptic neuron. show figure distributions potentiated fraction synapses runs stages run overlap inactive (upper panels) active (lower panels) calcium mechanism. divided synapses subgroups: (red) synapses pre-synaptic input activated solely pattern, Down (blue) synapses pre-synaptic inputs activated Down pattern, Overlap (green) synapses pre-synpatic inputs activated pattern down. state synapses recorded learning step. accumulating statistics runs obtain distributions reported figure fraction potentiated synapses calculated number synapses belonging subgroup. stop learning mechanism inactive, end experiment, green mechanism inactive trial trial+) trial 100 trial 150 synapses Overlap synapses synapses Down mechanism active trial trial+) trial 100 trial 150 synapses Overlap synapses synapses Down Figure Distribution fraction potentiated synapses. number inputs belonging patterns. distribution overlap synapses broad, Calcium mechanism active, synapses overlap tend depotentiated. result ?microscopic? effect stop learning mechanism: number potentiated synapses suﬃcient drive perceptron output frequency, overlap synapses tend depotentiated. overlap synapses pushed half times potentiated state half times depressed state, synapses reach earlier potentiated state. stop learning mechanism active, potentiated synapses enogh drive output neuron, furr potentiation inhibited synapses overlap synapses depressed average. condition transition probability suﬃciently small avoid trial learning completely disrupted. distribution output frequencies increasing overlap illustrated figure mechanism inactive upper panels, active lower panels). frequencies recorded ?checking phase?. blue histograms output frequency pattern, red pattern. clear figure output frequency distribution remain separated high overlap Calcium mechanism active. quantitative parameter describe distribution separation    ?2up ?2dn) values summarized table mechanism inactive Overlap Overlap Overlap Overlap Pattern Down) Pattern 100] 200 100] 200 100] 200 100] 200 mechanism active Overlap Overlap Overlap Overlap Pattern Down) Pattern 100] 200 100] 200 100] 200 100] 200 Figure Distributions perceptron frequencies learning overlapped patterns. blue bars refer pattern Down stimulation, red bars refers pattern. panel refers overlap. table Discrimination power [seconds] overlap overlap overlap lap OFF For run number potentiated synapses due random choices, Down Overlap synapses run mismatches affecting behavior synapses. failure discrimination high overlap absence stop learning mechanism due fact number potentiated synapses overcome effect teaching signal pattern. calcium mechanism, defining maximum number allowed potentiated synapses, limits problem. offer possibility establishing priori threshold discriminate perceptron outputs basis frequency maximum LTP probability curve. conclusions brieﬂy illustrate analog VLSI chip implementing network neurons,048 reconfigurable, hebbian, plastic, stop-learning synapses. circuit parameters measured dispersion \\x0cacross chip. data chip-oriented simulation set results, compared experimental ones, demonstrate circuits behavior follow oretical predictions. configured network perceptron AER synapses output neuron), classification task performed. stimuli increasing overlap used. results show ability network eﬃciently classify presented patterns improvement performances due calcium stop-learning mechanism. Neuromorphic analog, VLSI devices] derive organizational computational principles biologically plausible models neural systems, aiming providing long run electronic substrate innovative, bioinspired computational paradigms. line standard assumptions computational neuroscience, neuromorphic devices endowed adaptive capabilities forms plasticity synapses connect neural elements. widely adopted framework Hebbian learning, eﬃcacy synapse potentiated post-synaptic effect spike enhanced) preand post-synaptic neurons simultaneously active suitable time scale. different mechanisms proposed, relying average firing rates preand post-synaptic neurons, (rate-based Hebbian learning), ors based tight constraints time lags preand post-synaptic spikes (?spike-timing-dependent-plasticity?). synaptic circuits implement stochastic version rate-based Hebbian learning. decade, realized general constraints plausibly met concrete implementation synaptic device neural network, bear profound consequences http://neural.iss.infn/ capacity network memory system. specifically, accepts synaptic element neir unlimited dynamic range. synaptic eﬃcacy bounded), undergo arbitrarily small. synaptic eﬃcacy finite analog depth), proven]) deterministic learning prescription implies extremely low memory capacity, severe ?palimpsest? property: memories quickly erase trace older ones. turns stochastic mechanism general, logically appealing eﬃcient solution: preand post-synaptic neural activities, synapse made eligible changing eﬃcacy Hebbian prescription, state probability. stochastic element learning dynamics imply hoc elements, fact spike-driven implementation synapse, noisy activity neurons network provide needed ?noise generator? ]. refore, eﬃcient learning electronic network, implementation neuron spiking element requirement ?biological plausibility?, compelling computational requirement. learning networks spiking neurons stochastic plastic synapses studied oretically], stochastic-stable synaptic models implemented silicon]. one limitations far, oretical implementation level, artificially simple statistics stimuli learnt., overlap neural representations). very recently] modification stochastic-stable synaptic model proposed, endowed regulatory mechanism termed ?stop learning? synaptic downregulation depends average activity postsynaptic neuron recent past; synapse pointing neuron found highly active, poorly active, furr potentiated depressed, respectively. reason prescription essentially correlated patterns learnt network, successful strategy-emphasize coherent synaptic Hebbian potentiation result overlapping part synaptic matrix, ultimately spoil ability distinguish patterns. detailed learning strategy line proven] linearly separable patterns perceptron-like network; extension spiking recurrent networks studied. section give overview chip architecture implemented synaptic model. section show measures effectuated chip characterize synaptic neuronal parameters. section report characterization results compared oretical prediction obtained chip-oriented simulation. paragraph describes chip performances simple classification task, illustrate improvement brought stop-learning mechanism. Chip architecture main features chip] implements recurrent network integrate-and-fire neurons spikefrequency adaptation-stable, stochastic, Hebbian synapses. completely reconfigurable synaptic matrix supports all-all recurrent connectivity, aer-based external connectivity. besides establishing arbitrary synaptic connectivity, excitatory/inhibitory nature synapse set. implemented neuron neuron constant leakage term lower bound membrane potential) introduced] studied oretically]. circuit borrowed low-power design], refer reader details. only neurons directly probed., ?membrane potential? sampled), emitted spikes monitored AER]. dendritic tree neuron composed activated recurrent synapses activated external, AER ones. for recurrent synapses, impinging spike triggers short-time (and possibly long-term) state synapse, detailed below. spikes neurons chip form AER events, targeted correct AER synapse decoder. synapses set excitatory, eir AER recurrent plastic; inhibitory synapses fixed. spikes generated neurons chip arbitrated access AER bus monitoring and mapping external targets. synaptic circuit] implements model proposed] brieﬂy motivated introduction. synapse possesses states eﬃcacy-stable device): internal synaptic dynamics internal variable eﬃcacy set potentiated, orwise set depressed. subjected short-term, spike-driven dynamics: arrival impinging spike, candidate upward downward jump, depending instantaneous post-synaptic potential Vpost threshold jump performed depending furr variable explained below. absence intervening spikes forced drift ?high? ?low? depending wher jump left this preserves synaptic eﬃcacy long time scale. furr variable post-synaptic neuron dynamics, essentially measures average firing activity. following], analogy role played intracellular concentration calcium ions spike emission, call ?calcium variable? ). ) undergoes upward jump postsynaptic neuron emits spike, linearly decays spikes. refore integrates spikes sequence and, compared suitable thresholds detailed below, determines candidate synaptic jumps allowed occur; example, constrain synapse stop-regulating post-synaptic neuron active. ) acts regulatory element synaptic dynamics. resulting short-term dynamics internal synaptic variable conditions) ) Jup Vpost) ) jdw Vpost) ) Jup Jdw positive \\x0cconstants. detailed description circuits implementing conditions found]. figure illustrate effect calcium dynamics increasing input forces postsynaptic neuron fire increasing frequencies. long) undergoes jumps. when) jumps inhibited forced drift lower bound. vpost) VTH2) vpre) Figure Illustrative stop-learning mechanism (see text). top bottom: postsynaptic neuron potential Vpost calcium variable internal synaptic variable pre-synaptic neuron potential Vpre ltp/ltd probabilities: measurement chip-oriented simulation report synapse potentiation (ltp) depression (ltd)from chip compare experimental results simulations. for synapse subset, generate pre-synaptic poisson spike train. post synaptic neuron forced fire poisson spike train applying external current poisson train inhibitory spikes aer. setting potentiated depressed eﬃcacies, activity post-synaptic neuron easily tuned varying amplitude current frequency inhibitory AER train. initialize (aer) synapses depressed (potentiated) monitor post-synaptic neuron activity stimulation trial lasting seconds. end trial read synaptic state AER protocol developed purpose. for chosen post-synaptic firing rate, evaluate probability find synapses potentiated (depressed) state repeating test times. results reported figure (solid lines) represent average LTP LTD probabilities trail synapses. tests performed active inactive Calcium mechanism. when calcium mechanism inactive, LTP monotonically increasing post-synaptic firing rate calcium circuit activated LTP probability max form Vpost. identical tests run simulation (dashed curves figure). for purpose meaningful comparison chip behaviour relevant parameter affecting neural synaptic dynamics distributions (due inhomogenities mismatches) characterized. simulated measured data qualitative agreement. parameters chose tests classification task paragraph. fraction potentiated synapses experiment: solid line simulation: dashed line 100 150 ?post] 200 250 300 Figure Transition probabilities. red blue lines LTP probabilities calcium stop-learning mechanism respectively. gray lines LTD probabilities calcium stoplearning mechanism, case LTD mechanism shown. error bars standard deviations trials Learning overlapping patterns configured synaptic matrix perceptron network output inputs AER synapses). synapses set plastic excitatory ones, 32nd set inhibitory modulate post-synpatic neuron activity. our aim teach perceptron classify patterns semi-supervised learning strategy? ?down?. expect learning perceptron respond high output frequency pattern? low output frequency pattern ?down?. regulating mechanism exploited improve performances Down patterns significant overlap. learning semi-supervised: pattern ?teacher? input output neuron steering activity high low, desired. end learning period ?teacher? turned perceptron output driven input stimuli: conditions classification ability tested. present learning performances input patterns increasing overlap, demonstrate effect stop learning mechanism (overlap ranging synapses). upon stimulation active pre-synaptic inputs poisson spike trains, inactive inputs poisson spike trains. each trial lasts half second. Down patterns randomly presented equal probability. teaching signal, combination excitatory constant cur4 rent inhibitory AER spike train, forces output firing rate. one run lasts 150 trials suﬃcient stabilization output frequencies. end trial turn teaching signal, freeze synaptic dynamics read state synapse AER protocol developed purpose. conditions performed seconds test (?checking phase?) measure perceptron frequencies pattern pattern Down presented. each experiment includes runs. for run change: ?definition? patterns down: inputs activated pattern Down chosen randomly beginning run; initial synaptic state, constraint synapses potentiated; stimulation sequence. for experiment turned stop learning mechanism chose orthogonal patterns. case perceptron correctly classify stimuli: trials, choosing suitable threshold, discriminate perceptron ouput patterns (lower left panel figure). output frequency separation slightly increases trial number 100 remaining stable point. studied case overlapped patterns active inactive Calcium mechanism. repeated experiment increasing overlap. (implying increase coding level orthogonal case overlap equal). only threshold Khigh active threshold jumps inhibnited). Calcium circuit parameters tuned variable passes Khigh firing rate postsynaptic neuron. show figure distributions potentiated fraction synapses runs stages run overlap inactive (upper panels) active (lower panels) calcium mechanism. divided synapses subgroups: (red) synapses pre-synaptic input activated solely pattern, Down (blue) synapses pre-synaptic inputs activated Down pattern, Overlap (green) synapses pre-synpatic inputs activated pattern down. state synapses recorded learning step. accumulating statistics runs obtain distributions reported figure fraction potentiated synapses calculated number synapses belonging subgroup. when stop learning mechanism inactive, end experiment, green mechanism inactive trial trial+) trial 100 trial 150 synapses Overlap synapses synapses Down mechanism active trial trial+) trial 100 trial 150 synapses Overlap synapses synapses Down Figure Distribution fraction potentiated synapses. number inputs belonging patterns. distribution overlap synapses broad, Calcium mechanism active, synapses overlap tend depotentiated. this result ?microscopic? effect stop learning mechanism: number potentiated synapses suﬃcient drive perceptron output frequency, overlap synapses tend depotentiated. overlap synapses pushed half times potentiated state half times depressed state, synapses reach earlier potentiated state. when stop learning mechanism active, potentiated synapses enogh drive output neuron, furr potentiation inhibited synapses overlap synapses depressed average. this condition transition probability suﬃciently small avoid trial learning completely disrupted. distribution output frequencies increasing overlap illustrated figure mechanism inactive upper panels, active lower panels). frequencies recorded ?checking phase?. blue histograms output frequency pattern, red pattern. clear figure output frequency distribution remain separated high overlap Calcium mechanism active. quantitative parameter describe distribution separation    ?2up ?2dn) values summarized table mechanism inactive Overlap Overlap Overlap Overlap Pattern Down) Pattern 100] 200 100] 200 100] 200 100] 200 mechanism active Overlap Overlap Overlap Overlap Pattern Down) Pattern 100] 200 100] 200 100] 200 100] 200 Figure Distributions perceptron frequencies learning overlapped patterns. blue bars refer pattern Down stimulation, red bars refers pattern. each panel refers overlap. table Discrimination power [seconds] overlap overlap overlap lap OFF For run number potentiated synapses due random choices, Down Overlap synapses run mismatches affecting behavior synapses. failure discrimination high overlap absence stop learning mechanism due fact number potentiated synapses overcome effect teaching signal pattern. Calcium mechanism, defining maximum number allowed potentiated synapses, limits problem. this offer possibility establishing priori threshold discriminate perceptron outputs basis frequency maximum LTP probability curve. Conclusions brieﬂy illustrate analog VLSI chip implementing network neurons,048 reconfigurable, hebbian, plastic, stop-learning synapses. circuit parameters measured dispersion \\x0cacross chip. using data chip-oriented simulation set results, compared experimental ones, demonstrate circuits behavior follow oretical predictions. once configured network perceptron AER synapses output neuron), classification task performed. stimuli increasing overlap used. results show ability network eﬃciently classify presented patterns improvement performances due calcium stop-learning mechanism.',\n",
       " 'PP3335': 'important problem statistics machine learning consists testing wher distributions random variables identical alternative differ ways. ) precisely   xn1   xn2 independent random variables taking values input space), common distributions respectively. problem consists testing null hyposis alternative problem arises applications, ranging computational anatomy] process monitoring]. input space general, including finitedimensional Euclidean spaces sophisticated structures strings graphs (see]) arising applications bioinformatics]. traditional approaches problem based distribution functions distance empirical distributions obtained samples. popular procedures two-sample kolmogorov-smirnov tests cramer-von Mises tests, standard addressing issues dimension input space small). tests popular due simplicity, insensitive characteristics distribution, densities highfrequency components local features bumps. low-power traditional density based statistics improved test statistics based kernel density estimators] wavelet \\x0cestimators]. recent work] shown difference means RKHSs order consistently test homogeneity. paper, show taking account covariance structure RKHS obtain simple limiting distributions. paper organized follows: Section Section state main definitions construct test statistics. section give asymptotic distribution test statistic null hyposis, investigate, consistency power test fixed alternatives. section provide experimental evidence performance test statistic artificial real datasets. detailed proofs presented sections. mean covariance reproducing kernel Hilbert spaces highlight main assumptions make paper reproducing kernel, introduce operator-oretic tools working distributions infinite-dimensional spaces.  Reproducing kernel Hilbert spaces Let, separable metric space, denote ?algebra. Xvalued random variable, probability measure expectation denoted Consider Hilbert space functions Hilbert space RKHS point evaluation operator maps ) bounded linear functional. point corresponds element )  call feature map) )  ),   positive definite kernel. denote, norm. assumed remainder separable Hilbert space. note case separable metric space kernel continuous (see]). paper, make assumptions kernel) kernel bounded—? sup, ) For probability measures, RKHS(?, dense). asymptotic normality test statistics valid assumption), consistency results fixed alternatives). assumption) true translation-invariant kernels], Gaussian kernel]. note require compactness Mean element covariance operator operator-oretic tools define elements covariance operators rkhs. linear operator bounded number  operatornorm defined infimum numbers supkf (see]). recall basic facts second-order moments rkhs-valued random variables. ) element defined functions unique element satisfying, def ) furrmore) covariance operator defined unique linear operator \\x0conto satisfying def, gih   ) Note assumption) satisfied, map injective. operator self-adjoint nonnegative trace-class operator. sequel, dependence omitted risk confusion. empirical estimates element covariance operator defined empirical moments lead:   Given sample           ???  ) operator self-adjoint nonnegative trace-class operators. hence, diagonalized orthonormal basis, spectrum composed strictly decreasing sequence tending potentially null space (?) composed functions ., functions constant support null space reduced null element Gaussian kernel), infinite-dimensional. similarly, infinitely strictly positive eigenvalues (true nonparametric case) finitely (underlying finite dimensional problems). kfda-based test statistic feature space, two-sample homogeneity test procedure formulated follows.    xn1   xn2 distributions independent identically distributed samples covariance operators test null hyposis alternative hyposis paper, tackle problem (regularized) kernelized version Fisher disdef criminant analysis. denote pooled covariance operator, def within-class covariance matrix finite-dimensional setting def (see]. denote   between-class covariance oper? empirical estimates element ator. denote   def )? covariance operator, defined previously stated). denote )?  def empirical pooled covariance estimator,  )(?       empirical between-class covariance operator.  sequence strictly positive numbers. maximum Fisher discriminant ratioeserves basis test statistics  max)?    denotes identity operator. note input space euclidean. kernel linear, quantity matches-called hotelling statistic two-sample case]. moreover, practice computed kernel trick, adapted kernel Fisher discriminant analysis outlined, Chapter]. make assumptions ) For eigenvalues satisfy ) For infinitely strictly positive eigenvalues statistical analysis conducted Section demonstrate, rate, recenter rescale standard statistical transformation studentization) maximum Fisher discriminant ratio, order oretically well-calibrated test statistic. roles, recentering rescaling, played  compact operator decreasing eigenvalues), quantity (?, defined   def (?,    oretical results sequel studentized test statistic:           2d2 ) paper, asymptotic behavior Tbn null hyposis, fixed alternative. establish nonparametric test procedure consistent power.  Asymptotic normality null hyposis section, derive distribution test statistics null hyposis homogeneity.   zero, orem assume).  tbn ) proof postponed Section assumptions orem sequence tests rejects null hyposis ?? ??   )-quantile standard normal distribution, asymptotically level note limiting distribution depend kernel regularization parameter.  Power consistency study power test based Tbn alternative hyposes. minimal requirement prove sequence tests consistent power. sequence tests constant level consistent power probability accepting null hyposis homogeneity sample size infinity fixed alternative. proposition shows limit finite, strictly positive independent kernel orwise (see] similar results canonical correlation analysis). result  insights . population counterpart  test statistics based upon. proposition assume).  probability distributions     probability measure absolutely continuous.  densities respect  norm finite -divergence  finite. equal -divergence null, combining previous propositions, refore obtain consistency orem. orem assume). distributions,  experiments PHA (tbn (?) ??    ) section, investigate experimental performances test statistic kfda, compare terms power nonparametric test statistics.  Artificial data focus simple setting, order analyze major issues arising applying approach practice. indeed, periodic smoothing spline kernel (see KFDA MMD.0032.0023.0062. .0031. .0001. table Evolution power KFDA MMD respectively,  ] detailed derivation), explicit formulae eigenvalues covariance operator underlying \\x0cdistribution uniform. alleviate issue estimating spectrum covariance operator, weigh practical impact regularization power test statistic. periodic smoothing spline kernel Consider two-dimensional circle identified interval, (with periodicity conditions). strictly positive sequence ??  norm, , k2h    ) cos  ) sin   ) RKHS norm kernel, B2m   ?) )! b2m Bernoulli polynomial. ) . testing problem uniform density., density respect Lebesgue measure equal densities  covariance operator  eigenvectors  eigenvalues ors. comparison MMD conducted experimental comparison terms power, 104 . quantities involving eigenvalues covariance operator computed counterparts estimated. sampling pn2 performed inverting cumulative distribution function. table displays results, averaged monte-carlo runs.  Speaker verification conducted experiments speaker verification task], subset female speakers data NIST 2004 Speaker Recognition evaluation. refer reader] instance details pre-processing data. figure shows averaged results couples speakers. couple speaker, run 3000 samples speaker launched kfda-test decide wher samples speaker not, computed type error comparing prediction ground truth. averaged results 100 runs couple, couples speaker. level set , empirical level match prescribed level noticed previous subsection. performed experiments Maximum Mean Discrepancy tajvidi-hall test statistic]). summed results plotting roc-curve competing methods. method reaches good empirical power small prescribed level  % %). maximum Mean Discrepancy yields good empirical performance task. conclusion proposed well-calibrated test statistic, built kernel Fisher discriminant analysis, proved asymptotic limit distribution null hyposis standard normal distribution. test statistic readily computed Gram matrices kernel defined, ROC Curve Power KFDA MMD Level Figure Comparison ROC curves speaker verification task perform nonparametric hyposis testing homogeneity highdimensional data. kfda-test statistic yields competitive performance speaker identification. sketch proof asymptotic normality null hyposis outline. proof asymptotic normality test statistics null hyposis steps. step, derive asymptotic approximation test statistics test statistics spanned remaining stochastic term eigenbasis decomposed terms step prove asymptotic negligibility step establishes asymptotic normality martingale central limit orem (mclt). step Tbn). first, prove, perturbation results covariance operators, that  ) )  (?,  tbn) ) 2d2 (?, ease notation, following, omit quantities involving. hence stand  (?, define  )    def )     give formulas moments proof. straightforward calculations give cauchy-schwarz inequality reproducing property give def Denote def cov —?  )   . ), test statistics writes 2d2       esn 2cn ) defined def eyn def )      ) Step). proof consists computing variance term. variables independent var  def Var   cov  Using.   negligible, assumption   rhs step ). central limit orem (mclt) triangular arrays martingale differences (see. , orem]).    denote def  def  ,   }, ,   }). note that, construction martingale increment.  step proof CLT establish s2n  step proof establish negligibility condition. , orem], requires establish max1 (smallness(max1 bounded (tightness defined). establish conditions simultaneously checking max)  s2n diagonal terms off-diagonal terms ) Splitting sum Consider diagonal terms  .   compute mean. note      d22    refore). next, prove ) negligible, checking var). finally defined), prove. ). concludes proof. ).  finally show. ).  —?  bound max   max  Doob inequality implies [max1   Plugging bound), Minkowski inequality  max   proof concluded fact assumption). important problem statistics machine learning consists testing wher distributions random variables identical alternative differ ways. more) precisely   Xn1   Xn2 independent random variables taking values input space), common distributions respectively. problem consists testing null hyposis alternative this problem arises applications, ranging computational anatomy] process monitoring]. input space general, including finitedimensional Euclidean spaces sophisticated structures strings graphs (see]) arising applications bioinformatics]. traditional approaches problem based distribution functions distance empirical distributions obtained samples. popular procedures two-sample kolmogorov-smirnov tests cramer-von Mises tests, standard addressing issues dimension input space small). although tests popular due simplicity, insensitive characteristics distribution, densities highfrequency components local features bumps. low-power traditional density based statistics improved test statistics based kernel density estimators] wavelet \\x0cestimators]. recent work] shown difference means RKHSs order consistently test homogeneity. paper, show taking account covariance structure RKHS obtain simple limiting distributions. paper organized follows: Section Section state main definitions construct test statistics. Section give asymptotic distribution test statistic null hyposis, investigate, consistency power test fixed alternatives. Section provide experimental evidence performance test statistic artificial real datasets. detailed proofs presented sections. Mean covariance reproducing kernel Hilbert spaces highlight main assumptions make paper reproducing kernel, introduce operator-oretic tools working distributions infinite-dimensional spaces.  Reproducing kernel Hilbert spaces Let, separable metric space, denote ?algebra. let Xvalued random variable, probability measure expectation denoted Consider Hilbert space functions Hilbert space RKHS point evaluation operator maps ) bounded linear functional. point corresponds element )  call feature map) )  ),   positive definite kernel. denote, norm. assumed remainder separable Hilbert space. note case separable metric space kernel continuous (see]). throughout paper, make assumptions kernel) kernel bounded—? sup, ) For probability measures, RKHS(?, dense). asymptotic normality test statistics valid assumption), consistency results fixed alternatives). assumption) true translation-invariant kernels], Gaussian kernel]. note require compactness Mean element covariance operator operator-oretic tools define elements covariance operators rkhs. linear operator bounded number  operatornorm defined infimum numbers supkf (see]). recall basic facts second-order moments rkhs-valued random variables. ) element defined functions unique element satisfying, def ) furrmore) covariance operator defined unique linear operator \\x0conto satisfying def, gih   ) Note assumption) satisfied, map injective. operator self-adjoint nonnegative trace-class operator. sequel, dependence omitted risk confusion. empirical estimates element covariance operator defined empirical moments lead:   Given sample           ???  ) operator self-adjoint nonnegative trace-class operators. hence, diagonalized orthonormal basis, spectrum composed strictly decreasing sequence tending potentially null space (?) composed functions ., functions constant support null space reduced null element Gaussian kernel), infinite-dimensional. similarly, infinitely strictly positive eigenvalues (true nonparametric case) finitely (underlying finite dimensional problems). kfda-based test statistic feature space, two-sample homogeneity test procedure formulated follows. given   Xn1   Xn2 distributions independent identically distributed samples covariance operators test null hyposis alternative hyposis paper, tackle problem (regularized) kernelized version Fisher disdef criminant analysis. denote pooled covariance operator, def within-class covariance matrix finite-dimensional setting def (see]. let denote   between-class covariance oper? empirical estimates element ator. for denote   def )? covariance operator, defined previously stated). denote )?  def empirical pooled covariance estimator,  )(?       empirical between-class covariance operator. let sequence strictly positive numbers. maximum Fisher discriminant ratioeserves basis test statistics  max)?    denotes identity operator. note input space euclidean. kernel linear, quantity matches-called hotelling statistic two-sample case]. moreover, practice computed kernel trick, adapted kernel Fisher discriminant analysis outlined, Chapter]. make assumptions ) For eigenvalues satisfy ) For infinitely strictly positive eigenvalues statistical analysis conducted Section demonstrate, rate, recenter rescale standard statistical transformation studentization) maximum Fisher discriminant ratio, order oretically well-calibrated test statistic. roles, recentering rescaling, played  compact operator decreasing eigenvalues), quantity (?, defined   def (?,    oretical results sequel studentized test statistic:           2d2 ) paper, asymptotic behavior Tbn null hyposis, fixed alternative. this establish nonparametric test procedure consistent power.  Asymptotic normality null hyposis section, derive distribution test statistics null hyposis homogeneity.   zero, orem assume).  Tbn ) proof postponed Section under assumptions orem sequence tests rejects null hyposis ?? ??   )-quantile standard normal distribution, asymptotically level note limiting distribution depend kernel regularization parameter.  Power consistency study power test based Tbn alternative hyposes. minimal requirement prove sequence tests consistent power. sequence tests constant level consistent power probability accepting null hyposis homogeneity sample size infinity fixed alternative. proposition shows limit finite, strictly positive independent kernel orwise (see] similar results canonical correlation analysis). result  insights . population counterpart  test statistics based upon. proposition assume).  probability distributions     probability measure absolutely continuous.  densities respect  norm finite -divergence  finite. equal -divergence null, combining previous propositions, refore obtain consistency orem. orem assume). let distributions,  Experiments PHA (tbn (?) ??    ) section, investigate experimental performances test statistic kfda, compare terms power nonparametric test statistics.  Artificial data focus simple setting, order analyze major issues arising applying approach practice. indeed, periodic smoothing spline kernel (see KFDA MMD.0032.0023.0062. .0031. .0001. table Evolution power KFDA MMD respectively,  ] detailed derivation), explicit formulae eigenvalues covariance operator underlying \\x0cdistribution uniform. this alleviate issue estimating spectrum covariance operator, weigh practical impact regularization power test statistic. periodic smoothing spline kernel Consider two-dimensional circle identified interval, (with periodicity conditions). strictly positive sequence ??  norm, , k2h    ) cos  ) sin   ) this RKHS norm kernel, B2m   ?) )! B2m Bernoulli polynomial. ) . testing problem uniform density., density respect Lebesgue measure equal densities  covariance operator  eigenvectors  eigenvalues ors. comparison MMD conducted experimental comparison terms power, 104 . all quantities involving eigenvalues covariance operator computed counterparts estimated. sampling pn2 performed inverting cumulative distribution function. table displays results, averaged monte-carlo runs.  Speaker verification conducted experiments speaker verification task], subset female speakers data NIST 2004 Speaker Recognition evaluation. refer reader] instance details pre-processing data. figure shows averaged results couples speakers. for couple speaker, run 3000 samples speaker launched kfda-test decide wher samples speaker not, computed type error comparing prediction ground truth. averaged results 100 runs couple, couples speaker. level set , empirical level match prescribed level noticed previous subsection. performed experiments Maximum Mean Discrepancy tajvidi-hall test statistic]). summed results plotting roc-curve competing methods. our method reaches good empirical power small prescribed level  % %). maximum Mean Discrepancy yields good empirical performance task. Conclusion proposed well-calibrated test statistic, built kernel Fisher discriminant analysis, proved asymptotic limit distribution null hyposis standard normal distribution. our test statistic readily computed Gram matrices kernel defined, ROC Curve Power KFDA MMD Level Figure Comparison ROC curves speaker verification task perform nonparametric hyposis testing homogeneity highdimensional data. kfda-test statistic yields competitive performance speaker identification. Sketch proof asymptotic normality null hyposis outline. proof asymptotic normality test statistics null hyposis steps. step, derive asymptotic approximation test statistics test statistics spanned remaining stochastic term eigenbasis decomposed terms step prove asymptotic negligibility step establishes asymptotic normality martingale central limit orem (mclt). step Tbn). first, prove, perturbation results covariance operators, that  ) )  (?,  tbn) ) 2d2 (?, for ease notation, following, omit quantities involving. hence stand  (?, define  )    def )     give formulas moments proof. straightforward calculations give cauchy-schwarz inequality reproducing property give def Denote def cov —?  )   using. ), test statistics writes 2d2       esn 2cn ) defined def eyn def )      ) Step). proof consists computing variance term. since variables independent var  def Var   cov  Using.   negligible, assumption   RHS step ). central limit orem (mclt) triangular arrays martingale differences (see. , orem]). for   denote def  def  ,   }, ,   }). note that, construction martingale increment.  step proof CLT establish s2n  step proof establish negligibility condition. , orem], requires establish max1 (smallness(max1 bounded (tightness defined). establish conditions simultaneously checking max)  s2n diagonal terms off-diagonal terms ) Splitting sum Consider diagonal terms  using.   compute mean. Note      d22    refore). next, prove ) negligible, checking var). finally defined), prove. ). this concludes proof. ).  finally show. ). since —?  bound max   max  Doob inequality implies [max1   Plugging bound), Minkowski inequality  max   proof concluded fact Assumption).',\n",
       " 'PP3339': 'here, present algorithm support vector machine (svm) classification indefinite kernels. interest indefinite kernels motivated observations. first, similarity measures advantage applicationspecific structure data display excellent empirical classification performance. unlike popular kernels support vector machine classification, similarity matrices indefinite necessarily correspond reproducing kernel Hilbert space (see] discussion). application classification indefinite kernels image classification Earth mover Distance discussed]. similarity measures protein sequences SmithWaterman BLAST scores indefinite provided hints constructing positive semidefinite kernels decribed] transformed positive semidefinite kernels (see] example). instead, objective directly indefinite similarity measures classification. work closely recent results kernel learning (see]), kernel matrix learned linear combination kernels, resulting kernel explicitly constrained positive semidefinite authors] adapted SMO algorithm \\x0csolve case kernel written positively weighted combination kernels). case however, explicitly optimize kernel matrix part problem solved explicitly, means complexity method substantially lower classical kernel learning methods closer spirit algorithm], formulate multiple kernel learning problem] semi-infinite linear program solve column generation technique similar analytic center cutting plane method here. finally, impossible prove kernels satisfy mercer condition numerical complexity evaluating exact positive semidefinite kernel high proxy (and necessarily positive semidefinite) kernel (see] example). cases, method bypass limitations.  Current results Several methods proposed dealing indefinite kernels svms. direction embeds data pseudo-euclidean) space] example, formulates classification problem indefinite kernel minimizing distance convex hulls formed categories data embedded space. nonseparable case handled manner reduced convex hulls (see] discussion SVM geometric interpretations). anor direction applies direct spectral transformations indefinite kernels: ﬂipping negative eigenvalues shifting kernel eigenvalues reconstructing kernel original eigenvectors order produce positive semidefinite kernel (see]). anor option reformulate eir maximum margin problem dual order indefinite kernel convex optimization problem (see]). equivalent formulation SVM objective kernel appears constraints modified convex problem eliminating kernel objective. directly solving nonconvex problem good results (see]).  Contribution here, directly transforming indefinite kernel, simultaneously learn support vector weights proxy positive semidefinite kernel matrix, penalizing distance proxy kernel original, indefinite one. main result kernel learning part problem solved explicitly, meaning classification problem indefinite kernels simply formulated perturbation positive semidefinite case. formulation interpreted worst-case robust classification problem uncertainty kernel matrix. sense, indefinite similarity matrices noisy observations unknown positive semidefinite kernel. complexity standpoint, original SVM classification problem indefinite kernel nonconvex, robustification detail convex problem, solved eﬃciently guaranteed complexity bounds. paper organized follows. section formulate main classification problem detail interpretation robust svm. section describe algorithm solving problem. finally, Section test numerical performance methods applications. svm indefinite kernels here, introduce robustification \\x0csvm classification problem indefinite kernels.  Robust classification Let kernel matrix vector labels, diag) matrix diagonal set symmetric matrices size set-vectors real numbers. write dual SVM classification problem hinge loss quadratic penalty: maximize    subject?? ) variable  -vector ones. positive semidefinite, problem convex quadratic program. suppose indefinite kernel matrix  formulate robust version problem) restricting positive semidefinite kernel matrix neighborhood original (indefinite) kernel matrix) max min   ??  k2f ??} variables   parameter controls distance original matrix proxy kernel This interpreted worst-case robust classification problem bounded uncertainty kernel matrix problem infeasible values replace hard constraint penalty distance proxy positive semidefinite kernel indefinite matrix. problem solve now: max min    k2f?? } variables   parameter controls magnitude penalty distance minimization problem convex conic program also, pointwise minimum family concave quadratic functions solution problem concave function outer optimization problem convex (see] furr details). thus) concave maximization problem subject linear constraints refore convex problem key result kernel learning optimization problem solved closed form. fixed minimization problem equivalent problem: minimize  subject   )k2f variable  projection?   cone positive semidefinite matrices. optimal solution problem: ?   ) positive part matrix. max ith eigenvalue eigenvector matrix plugging solution), get: max      k2f?? } variable    rank matrix coeﬃcients   rewrite eigenvalue optimization problem eigenvalue representation letting eigenvalue decomposition?    and, ith column write      max   ) ith eigenvalue quantity technique, rewrite term   eigenvalue decomposition. original optimization problem) finally becomes: maximize max  ?  (max  ? ?  vit )max  ?))   subject   variable    Dual problem Because problem) convex compact feasible set, formulate dual problem) simply switching max min. maximization quadratic program quadratic program dual. dual plugging dual quadratic program outer minimization, problem: minimize         k2f subject  ) variables     this dual problem quadratic program variables  correspond primal constraints    dual variable constraint earlier, feasible solution primal problem produces kernel), plugging kernel dual problem) calculate dual feasible point solving quadratic program dual objective value. upper bound optimum). bound compute duality gap track convergence.  Interpretation noted problem viewed worst-case robust classification problem uncertainty kernel matrix. explicit solution optimal worst-case kernel) projection penalized rank-one update indefinite kernel cone positive semidefinite matrices.  infinity, rank-one update effect limit, optimal kernel kernel zeroing negative eigenvalues indefinite kernel. means indefinite kernel small amount noise, positive semidefinite kernel SVM framework positive part indefinite kernel. limit infinity motivates heuristic transformation kernel testing set. negative eigenvalues training kernel thresholded limit, transformation occur test kernel. hence, update entries full kernel training instances rank-one update resulting optimal solution) threshold negative eigenvalues full kernel matrix zero. test kernel values resulting positive semidefinite matrix. algorithms detail algorithms solve Problem). optimization problem maximization nondifferentiable concave function subject convex constraints. optimal point exists \\x0csince feasibility set bounded nonempty. numerical stability, algorithms, quadratically smooth objective calculate gradient instead. describe simple projected gradient method numerically cheap iterations convergence bound. show apply eﬃcient analytic center cutting plane method iterations slightly complex converges linearly. smoothing Our objective terms form max)} function), differentiable (described section below). functions easily smood regularization technique (see] example). replace continuously differentiable -approximation follows:  )) max)   gradient ??? )) ) ) argmax )). gradient Calculating gradient objective requires full eigenvalue decomposition compute gradient eigenvalue. matrix(? derivative ith eigenvalue respect : (?)) (?) vit)  ith eigenvector(?). combine expression smooth approximation gradient. note eigenvalues symmetric matrices differentiable multiplicities greater (see] discussion). practice however, tested kernels full rank distinct eigenvalues ignore issue here. projected subgradient methods, slower, subgradients analytic center cutting plane methods (which affect complexity).  Projected gradient method projected gradient method takes steepest descent, projects point back feasible region (see] example). order methods objective function differentiable method eﬃcient projection step numerically cheap. choose initial point algorithm proceeds follows: Projected gradient method compute  set  gap  stop, orwise back step complexity iteration breaks follows. step requires eigenvalue decomposition costs note line search costly require multiple eigenvalue decompositions recalculate objective multiple times. step projection region   solved explicitly sorting vector entries, cost log). stopping criterion. compute duality gap results? kernel iteration), solving problem) SVM convex kernel produces upper bound), bound suboptimality current solution. complexity. number iterations required method reach target precision typically  Analytic center cutting plane method analytic center cutting plane method (accpm) reduces feasible region iteration cut feasible region computed evaluating subgradient objective function analytic center current set, volume reduced region converges target precision. method require \\x0cdifferentiability. set   write localization set optimal solution. method works (see] complete reference cutting plane methods): Analytic center cutting plane method compute analytic center solving argmin log ati ati represents ith row coeﬃcients left-hand side   compute) center update (polyhedral) localization set  )(?    gap  stop, orwise back step complexity iteration breaks follows. step step computes analytic center polyhedron solved operations interior point methods example. step simply updates polyhedral description. stopping criterion. upper bound computed maximizing order Taylor approximation (?) points ellipsoid covers explicitly. complexity. accpm provably convergent log/? iterations cut elimination complexity localization set bounded. schemes slightly complexities achieved] (cheaper) approximate centers example. experiments section compare generalization performance technique methods applying SVM classification indefinite similarity measure. test SVM classification performance positive semidefinite kernels LIBSVM library. finish experiments showing convergence algorithms. algorithms implemented matlab.  Generalization compare method SVM classification indefinite kernels kernel preprocessing techniques discussed earlier. techniques perform spectral transformations indefinite kernel. first, denoted denoise, thresholds negative eigenvalues zero. transformation, called ﬂip, takes absolute eigenvalues. transformation, shift, adds constant eigenvalue making positive. ] furr details. finally compare SVM original indefinite kernel (svm converges solution stationary point guaranteed optimal). experiment data USPS handwritten digits database (described]) indefinite Simpson score) compare digits data sets UCI repository (see]) indefinite Epanechnikov) kernel. data randomly divided training testing data. apply-fold cross validation accuracy measure (described below) determine optimal parameters train model full training set optimal parameters test independent test set. table Statistics data sets. data Set Train Test ?min ?max usps \\x0c767 773 453 usps 829 857 413 diabetes 614 154 liver 276.38e Table statistics including minimum maximum eigenvalues training kernels. main observation USPS data highly indefinite kernels UCI data kernels positive semidefinite. table displays performance comparing accuracy recall. accuracy defined percentage total instances predicted correctly. recall percentage true positives correctly predicted positive. method referred Indefinite svm. method performs favorably USPS data. measures performance high methods. method perform UCI data sets favorable measures experiment. notice recall good liver data set result overfitting classification categories. liver data set kernel positive semidefinite input true kernel Indefinite SVM finds slightly better. postulate method perform situations similarity measure highly indefinite USPS dataset, measures positive semidefinite small amount noise. table Performance Measures data sets. data Set usps usps diabetes liver Measure Denoise Flip Shift SVM Indefinite SVM Accuracy Recall Accuracy Recall Accuracy Recall Accuracy Recall Algorithm Convergence ran algorithms data sets created randomly perturbing USPS data sets above. average results standard deviation displayed Figure duality gap log scale (note codes stopped target gap improvement smaller expected, ACCPM converges faster fact linearly) higher precision iteration requires solving linear program size gradient projection method converges faster beginning stalls higher precision, iteration requires sorting current point. Duality Gap Duality Gap 100 150 200 Iteration \\x0c200 400 600 800 1000 Iteration Figure Convergence plots ACCPM (left) projected gradient method (right) randomly perturbed USPS data sets (average gap versus iteration number, dashed lines minus standard deviation). conclusion proposed technique incorporating indefinite kernels SVM framework explicit transformations. shown view indefinite kernel noisy instance true kernel, learn explicit solution optimal kernel tractable convex optimization problem. give convergent algorithms solving problem large data sets. initial experiments show method fare comparably methods handling indefinite kernels SVM framework clearer interpretation heuristics. here, present algorithm support vector machine (svm) classification indefinite kernels. our interest indefinite kernels motivated observations. first, similarity measures advantage applicationspecific structure data display excellent empirical classification performance. unlike popular kernels support vector machine classification, similarity matrices indefinite necessarily correspond reproducing kernel Hilbert space (see] discussion). application classification indefinite kernels image classification Earth mover Distance discussed]. similarity measures protein sequences SmithWaterman BLAST scores indefinite provided hints constructing positive semidefinite kernels decribed] transformed positive semidefinite kernels (see] example). here instead, objective directly indefinite similarity measures classification. our work closely recent results kernel learning (see]), kernel matrix learned linear combination kernels, resulting kernel explicitly constrained positive semidefinite authors] adapted SMO algorithm \\x0csolve case kernel written positively weighted combination kernels). case however, explicitly optimize kernel matrix part problem solved explicitly, means complexity method substantially lower classical kernel learning methods closer spirit algorithm], formulate multiple kernel learning problem] semi-infinite linear program solve column generation technique similar analytic center cutting plane method here. finally, impossible prove kernels satisfy mercer condition numerical complexity evaluating exact positive semidefinite kernel high proxy (and necessarily positive semidefinite) kernel (see] example). cases, method bypass limitations.  Current results Several methods proposed dealing indefinite kernels svms. direction embeds data pseudo-euclidean) space] example, formulates classification problem indefinite kernel minimizing distance convex hulls formed categories data embedded space. nonseparable case handled manner reduced convex hulls (see] discussion SVM geometric interpretations). anor direction applies direct spectral transformations indefinite kernels: ﬂipping negative eigenvalues shifting kernel eigenvalues reconstructing kernel original eigenvectors order produce positive semidefinite kernel (see]). yet anor option reformulate eir maximum margin problem dual order indefinite kernel convex optimization problem (see]). equivalent formulation SVM objective kernel appears constraints modified convex problem eliminating kernel objective. directly solving nonconvex problem good results (see]).  Contribution here, directly transforming indefinite kernel, simultaneously learn support vector weights proxy positive semidefinite kernel matrix, penalizing distance proxy kernel original, indefinite one. our main result kernel learning part problem solved explicitly, meaning classification problem indefinite kernels simply formulated perturbation positive semidefinite case. our formulation interpreted worst-case robust classification problem uncertainty kernel matrix. sense, indefinite similarity matrices noisy observations unknown positive semidefinite kernel. from complexity standpoint, original SVM classification problem indefinite kernel nonconvex, robustification detail convex problem, solved eﬃciently guaranteed complexity bounds. paper organized follows. Section formulate main classification problem detail interpretation robust svm. Section describe algorithm solving problem. finally, Section test numerical performance methods applications. SVM indefinite kernels here, introduce robustification \\x0csvm classification problem indefinite kernels.  Robust classification Let kernel matrix vector labels, diag) matrix diagonal set symmetric matrices size set-vectors real numbers. write dual SVM classification problem hinge loss quadratic penalty: maximize    subject?? ) variable  -vector ones. when positive semidefinite, problem convex quadratic program. suppose indefinite kernel matrix  formulate robust version problem) restricting positive semidefinite kernel matrix neighborhood original (indefinite) kernel matrix) max min   ??  k2f ??} variables   parameter controls distance original matrix proxy kernel This interpreted worst-case robust classification problem bounded uncertainty kernel matrix problem infeasible values replace hard constraint penalty distance proxy positive semidefinite kernel indefinite matrix. problem solve now: max min    k2f?? } variables   parameter controls magnitude penalty distance minimization problem convex conic program also, pointwise minimum family concave quadratic functions solution problem concave function outer optimization problem convex (see] furr details). thus) concave maximization problem subject linear constraints refore convex problem our key result kernel learning optimization problem solved closed form. for fixed minimization problem equivalent problem: minimize  subject   )k2f variable  this projection?   cone positive semidefinite matrices. optimal solution problem: ?   ) positive part matrix. max ith eigenvalue eigenvector matrix plugging solution), get: max      k2f?? } variable    rank matrix coeﬃcients   rewrite eigenvalue optimization problem eigenvalue representation letting eigenvalue decomposition?    and, ith column write      max   ) ith eigenvalue quantity using technique, rewrite term   eigenvalue decomposition. our original optimization problem) finally becomes: maximize max  ?  (max  ? ?  vit )max  ?))   subject   variable    Dual problem Because problem) convex compact feasible set, formulate dual problem) simply switching max min. maximization quadratic program quadratic program dual. dual plugging dual quadratic program outer minimization, problem: minimize         k2f subject  ) variables     This dual problem quadratic program variables  correspond primal constraints    dual variable constraint earlier, feasible solution primal problem produces kernel), plugging kernel dual problem) calculate dual feasible point solving quadratic program dual objective value. upper bound optimum). this bound compute duality gap track convergence.  Interpretation noted problem viewed worst-case robust classification problem uncertainty kernel matrix. our explicit solution optimal worst-case kernel) projection penalized rank-one update indefinite kernel cone positive semidefinite matrices.  infinity, rank-one update effect limit, optimal kernel kernel zeroing negative eigenvalues indefinite kernel. this means indefinite kernel small amount noise, positive semidefinite kernel SVM framework positive part indefinite kernel. this limit infinity motivates heuristic transformation kernel testing set. since negative eigenvalues training kernel thresholded limit, transformation occur test kernel. hence, update entries full kernel training instances rank-one update resulting optimal solution) threshold negative eigenvalues full kernel matrix zero. test kernel values resulting positive semidefinite matrix. Algorithms detail algorithms solve Problem). optimization problem maximization nondifferentiable concave function subject convex constraints. optimal point exists \\x0csince feasibility set bounded nonempty. for numerical stability, algorithms, quadratically smooth objective calculate gradient instead. describe simple projected gradient method numerically cheap iterations convergence bound. show apply eﬃcient analytic center cutting plane method iterations slightly complex converges linearly. smoothing Our objective terms form max)} function), differentiable (described section below). functions easily smood regularization technique (see] example). replace continuously differentiable -approximation follows:  )) max)   gradient ??? )) ) ) argmax )). gradient Calculating gradient objective requires full eigenvalue decomposition compute gradient eigenvalue. given matrix(? derivative ith eigenvalue respect : (?)) (?) vit)  ith eigenvector(?). combine expression smooth approximation gradient. note eigenvalues symmetric matrices differentiable multiplicities greater (see] discussion). practice however, tested kernels full rank distinct eigenvalues ignore issue here. one projected subgradient methods, slower, subgradients analytic center cutting plane methods (which affect complexity).  Projected gradient method projected gradient method takes steepest descent, projects point back feasible region (see] example). order methods objective function differentiable method eﬃcient projection step numerically cheap. choose initial point algorithm proceeds follows: Projected gradient method compute  set  gap  stop, orwise back step complexity iteration breaks follows. step this requires eigenvalue decomposition costs note line search costly require multiple eigenvalue decompositions recalculate objective multiple times. step this projection region   solved explicitly sorting vector entries, cost log). stopping criterion. compute duality gap results? kernel iteration), solving problem) SVM convex kernel produces upper bound), bound suboptimality current solution. complexity. number iterations required method reach target precision typically  Analytic center cutting plane method analytic center cutting plane method (accpm) reduces feasible region iteration cut feasible region computed evaluating subgradient objective function analytic center current set, volume reduced region converges target precision. this method require \\x0cdifferentiability. set   write localization set optimal solution. method works (see] complete reference cutting plane methods): Analytic center cutting plane method compute analytic center solving argmin log ati ati represents ith row coeﬃcients left-hand side   compute) center update (polyhedral) localization set  )(?    gap  stop, orwise back step complexity iteration breaks follows. step this step computes analytic center polyhedron solved operations interior point methods example. Step this simply updates polyhedral description. stopping criterion. upper bound computed maximizing order Taylor approximation (?) points ellipsoid covers explicitly. complexity. accpm provably convergent log/? iterations cut elimination complexity localization set bounded. schemes slightly complexities achieved] (cheaper) approximate centers example. Experiments section compare generalization performance technique methods applying SVM classification indefinite similarity measure. test SVM classification performance positive semidefinite kernels LIBSVM library. finish experiments showing convergence algorithms. our algorithms implemented matlab.  Generalization compare method SVM classification indefinite kernels kernel preprocessing techniques discussed earlier. techniques perform spectral transformations indefinite kernel. first, denoted denoise, thresholds negative eigenvalues zero. transformation, called ﬂip, takes absolute eigenvalues. transformation, shift, adds constant eigenvalue making positive. see] furr details. finally compare SVM original indefinite kernel (svm converges solution stationary point guaranteed optimal). experiment data USPS handwritten digits database (described]) indefinite Simpson score) compare digits data sets UCI repository (see]) indefinite Epanechnikov) kernel. data randomly divided training testing data. apply-fold cross validation accuracy measure (described below) determine optimal parameters train model full training set optimal parameters test independent test set. table Statistics data sets. data Set Train Test ?min ?max usps \\x0c767 773 453 usps 829 857 413 diabetes 614 154 liver 276.38e Table statistics including minimum maximum eigenvalues training kernels. main observation USPS data highly indefinite kernels UCI data kernels positive semidefinite. table displays performance comparing accuracy recall. accuracy defined percentage total instances predicted correctly. recall percentage true positives correctly predicted positive. our method referred Indefinite svm. method performs favorably USPS data. both measures performance high methods. our method perform UCI data sets favorable measures experiment. notice recall good liver data set result overfitting classification categories. liver data set kernel positive semidefinite input true kernel Indefinite SVM finds slightly better. postulate method perform situations similarity measure highly indefinite USPS dataset, measures positive semidefinite small amount noise. Table Performance Measures data sets. data Set usps usps diabetes liver Measure Denoise Flip Shift SVM Indefinite SVM Accuracy Recall Accuracy Recall Accuracy Recall Accuracy Recall Algorithm Convergence ran algorithms data sets created randomly perturbing USPS data sets above. average results standard deviation displayed Figure duality gap log scale (note codes stopped target gap improvement smaller expected, ACCPM converges faster fact linearly) higher precision iteration requires solving linear program size gradient projection method converges faster beginning stalls higher precision, iteration requires sorting current point. Duality Gap Duality Gap 100 150 200 Iteration \\x0c200 400 600 800 1000 Iteration Figure Convergence plots ACCPM (left) projected gradient method (right) randomly perturbed USPS data sets (average gap versus iteration number, dashed lines minus standard deviation). Conclusion proposed technique incorporating indefinite kernels SVM framework explicit transformations. shown view indefinite kernel noisy instance true kernel, learn explicit solution optimal kernel tractable convex optimization problem. give convergent algorithms solving problem large data sets. our initial experiments show method fare comparably methods handling indefinite kernels SVM framework clearer interpretation heuristics.',\n",
       " 'PP3375': 'survival analysis well-established field medical statistics concerned analyzing/predicting time occurrence event interest., death, onset disease, failure machine. applied clinical research, epidemiology, reliability engineering, marketing, insurance, etc. time well-defined starting point occurrence event called survival time failure time, measured clock time anor scale., mileage car. survival time data amenable standard statistical methods special features? ) continuous survival time skewed distribution, normal) large portion data censored (see sec. ). paper machine learning perspective cast survival analysis ranking problem?where task rank data points based survival times rar predict actual survival times. popular performance measures assessing learned models survival analysis Concordance Index), similar wilcoxon-mannwhitney statistic-partite ranking problems. performance measure, develop approaches learn models directly optimizing. optimization computationally expensive, focus maximizing lower bounds, log-sigmoid exponential bounds, sec.  interestingly, log-sigmoid bound arises natural Proportional Hazard) model, standard model classical survival analysis, sec. . moreover, models learned optimizing cox partial likelihood classical survival analysis, show sec. maximizing likelihood ends (approximately) maximizing. experiments sec. show optimizing lower bounds cox likelihood yields similar results respect, proposed lower bounds slightly better. Survival analysis Survival analysis extensively studied statistics community decades]. primary focus build statistical models survival time? individual population.  Censored data major problem fact period observation? censored individuals instance, patient move town longer clinical trial. end trial lot patients survive. cases exact survival time longer observation period. data referred right-censored? called censoring time. individuals, survived? ., actual observation min? ?   -dimensional vector covariates (explanatory variables) ith individual. clinical studies, covariates typically include demographic variables, age, gender, race; diagnosis information lab tests; treatment information., dosage. important assumption generally made? ? independent conditional., censoring independent survival time. indicator function equals failure observed?  ? data censored? ? training data summarized patients. objective learn predictive model survival time function covariates.  Failure time distributions failures times typically modeled follow distribution, absorbs random effects unexplained (available) covariates. distribution characterized survival function probability individual alive time related function commonly hazard function. density \\x0cfunction hazard function defined ) lim ). hazard function measures instantaneous rate failure, insight failure mechanisms. function )  called cumulative hazard function, holds?? ].  Proportional hazard model Proportional hazard) models standard studying effect covariates survival time distributions]. specifically, model assumes multiplicative effect covariates hazard function., ) ) hazard function person covariates-called baseline hazard function), typically based exponential Weibull distributions; set unknown regression parameters, relative hazard function. equivalent formulations cumulative hazard function survival function include )   ) cox partial likelihood Cox noticed semi-parametric approach suﬃcient estimating weights models., baseline hazard function remain completely unspecified. parametric assumption effect covariates hazard function required. parameter estimates model obtained maximizing cox partial likelihood weights)  uncensored) Indicator function log?sigmoid lower bound Exponential lower bound) Figure Order graphs representing ranking constraints. ) censored data) censored data. empty circle represents censored point. points arranged increasing survival times \\x0clowest bottom. ) Two concave lower bounds indicator function. term product probability ith individual failed time failure occurred time individuals risk failing. cox ors shown partial log-likelihood treated ordinary log-likelihood derive valid (partial) maximum likelihood estimates]. interesting properties cox partial likelihood include) due parametric form, optimized computationally eﬃcient way) depends ranks observed survival times. inequality . rar actual numerical values. outline connection ranking times ?and concordance index sec.  ordering Survival times Casting survival analysis ranking problem elegant dealing typically skewed distributions survival times, censoring data: Two subjects? survival times ordered) uncensored) uncensored time smaller censored survival time. visualized means order graph. fig.  set vertices represents individuals, filled vertex observed/uncensored survival time, empty circle denotes censored observation. existence edge Eij implies edge originate censored point.  Concordance index For reasons, concordance index-index commonly performance measures survival models]. interpreted fraction pairs subjects predicted survival times correctly ordered subjects ordered. words, probability concordance predicted observed survival. written— Eij indicator function orwise— denotes number edges order graph.  predicted survival time subject model equivalently, concordance index written explicitly — uncensored This index generalization wilcoxon-mann-whitney statistics] area ROC curve (auc) regression problems) applied continuous output variables) account censoring data. auc, perfect prediction accuracy good random predictor.  Maximizing? Ranking Problem Since evaluate predictive accuracy survival model terms concordance index, natural formulate learning problem directly maximize concordance index. note that, concordance index \\x0cbeen widely evaluate learnt model, generally objective function training. concordance index invariant monotone transformation survival times, model learnt maximizing-index ranking/scoring function. goal predict wher survival time individual larger anor individual. doctor wher kind treatment results increase survival time exact absolute survival time important. terms ranking problems studied machine learning -partite ranking problem, data point class itself. formulating ranking problem naturally incorporate censored data. formulated ranking problem ranking algorithms proposed machine learning literature]. paper algorithm proposed]. formally, learn ranking function suitable function class implies survival time patient larger patient data order graph optimal ranking function arg maxf, prevent overfitting training data, regularization added equation, secs.  cases, suﬃcient regularization achieved restricting function class., linear functions. ease exposition family linear ranking functions paper ) lower bounds Maximizing discrete optimization problem, computationally expensive. reason, resort maximizing differentiable concave lower bound indicator function concordance index. eqs.  paper focus log-sigmoid lower bound. sec. exponential lower bound. sec. suitably scaled tight origin asymptotic limit large positive values, fig. ). show bounds relate classical approaches survival analysis: turns out, family linear ranking functions, approaches closely related model commonly survival analysis. sec. . log-sigmoid lower bound subsection discusses lower bound concordance index based log-sigmoid function. subsection shows bound arises naturally proportional hazard models.  Lower bound sigmoid function defined  While approximation indicator function, lower bound. contrast, scaled version log sigmoid function, log? )]/ log lower bound indicator function (fig.  (log )/log). ) log-sigmoid function concave asymptotically linear large negative values, considered differentiable approximation hinge loss, commonly Generalization non-linear functions achieved easily kernels: linear ranking function class replaced reproducing kernel Hilbert space (rkhs). ranking function form, kernel RHKS training support vector machines. lower bound concordance index. . immediately (log   )]/log cls— Eij Eij eﬃciently maximized gradient-based methods. sec). linear ranking function) bound cls cls) (log  )]/log). — Eij avoid overfitting, penalize functions large norm standard way, obtain regularized version clsreg) kwk2 cls).  Connection model concordance index interpreted probability correct ranking defined order graph) function probabilistic version cast likelihood. assumption pair, independent pair, log-likelihood reads log] ) Eij independence assumption hold pairs due transitivity (even individual samples assumed. lower bound concordance index. probability correct pairwise ordering], chosen sigmoid ranking literature], show sigmoid function arises naturally context models. ¿ denote survival time patient covariates relative log-hazard larger hazard corresponds smaller survival time. sec.      density function patient covariate corre0 sponding survival function). . model, continue manipulations:    )]. ) This derivation shows probability correct pairwise ordering sigmoid function. assuming prior, regularization, optimal maximum aposteriori (map) estimator \\x0cform bmap arg max), posterior) takes form penalized log-likelihood: ) kwk2 log   ) Eij This expression equivalent) constants irrelevant optimization problem, justifies choice regularization.  exponential lower bound exponential  serve alternative lower bound step indicator function (see fig. )). concordance index lower-bounded     — Eij Analogous log-sigmoid bound, linear ranking function) lower bound simplifies — Eij and, penalizing functions large norm regularized version reads cereg) kwk2  ) Eij Gradient based learning order maximize regularized concave surrogate gradient-based learning technique. polak-ribi‘ ere variant nonlinear conjugate gradients) algorithm]. method gradient) require evaluation function. avoids computing derivatives. convergence faster steepest descent. fact    )]   ), gradient. (log-sigmoid bound) clsreg)  — log Eij   gradient. (exponential bound) cereg)   eij — cox partial likelihood lower bound experimental results (sec. coxs method proposed methods showed similar performance assessed. proposed method formulated explicitly maximize lower bound concordance index, Coxs method maximized partial likelihood. suspects wher Coxs partial likelihood lower bound concordance index. argument presented give indication method maximizes partial likelihood ends (approximately) maximizing concordance index. -write exponential bound proportional hazard models sec. )  — uncensored uncensored  ]. )  uncensored Note replaced assuming ties data., survival times identical, analogous \\x0ccox partial likelihood approach. sec. ). number uncensored observations denoted cox partial likelihood written terms) uncensored hzi geom hzi igeom denotes geometric uncensored inequality min concordance index bounded?  — min This maximizing min maximizes lower bound concordance index. cox partial likelihood insight. max (because largest uncensored maximizing min expected approximately maximize geometric cox partial likelihood. table Summary data sets used. number patients. number covariates used. dataset MAASTRO support support support MELANOMA 285 477 314 149 191 Missing% Censored% Experiments section compare performance lower bounds? logsigmoid, exponential, cox partial likelihood medical data sets.  Medical datasets Table summarizes data sets experiments. substantial amount data censored missing. maastro dataset concerns survival time non-small cell lung cancer patients, analyzed part collaboration. medical data sets publicly available: SUPPORT dataset random sample Phases SUPPORT](study Understand Prognoses Preferences Outcomes Risks treatment) study. suggested] split dataset datasets, death. melanoma data clinical study skin cancer.  Evaluation procedure For data set% examples training remaining% hold-out set testing. chose optimal regularization parameter . eqs. ) based five-fold cross validation training set. tolerance conjugate gradient procedure set conjugate-gradient optimization procedure initialized vector. covariates normalized unit variance. missing values focus paper, simple imputation technique. missing value, imputed sample drawn Gaussian distribution variance estimated values patients.  Results performance evaluated terms concordance index results tabulated Table compare methods? ) \\x0ccox partial likelihood method) proposed ranking methods log-sigmoid exponential lower bounds. observations made? ) proposed linear ranking method performs slightly cox partial likelihood method, difference significant. agrees insights cox partial likelihood end maximizing. ) exponential bound shows slightly performance log-sigmoid bound, tightness bound positive fig. ) important negative data sets. difference significant. conclusions paper, outlined approaches maximizing concordance index, standard performance measure survival analysis cast ranking problem. showed that, widely-used proportional hazard models, log-sigmoid function arises natural lower bound concordance index. presented approach directly optimizing lower bound computationally eﬃcient way. optimization procedure applied lower bounds, exponential one. that, showed maximizing cox partial likelihood understood (approximately) maximizing lower bound concordance index, explains high-scores proportional hazard models observed practice. optimization lower bounds results-score experiments, approach giving tentatively results. http://biostat.vanderbilt.edu/twiki/bin/view/main/datasets. www.stat.uni muenchen/service/datenarchiv/melanoma/melanoma.html Table Concordance indices methods datasets. standard deviation computed fold cross-validation. results shown fixed holdout set. training set std] test set std] holdout set MAASTRO Cox log-sigmoid exponential support Cox log-sigmoid exponential support Cox log-sigmoid exponential support Cox log-sigmoid exponential MELANOMA Cox log-sigmoid exponential Acknowledgements grateful Bharat Rao encouragement support work, anonymous reviewers valuable comments. Survival analysis well-established field medical statistics concerned analyzing/predicting time occurrence event interest., death, onset disease, failure machine. applied clinical research, epidemiology, reliability engineering, marketing, insurance, etc. time well-defined starting point occurrence event called survival time failure time, measured clock time anor scale., mileage car. survival time data amenable standard statistical methods special features? ) continuous survival time skewed distribution, normal) large portion data censored (see sec. ). paper machine learning perspective cast survival analysis ranking problem?where task rank data points based survival times rar predict actual survival times. one popular performance measures assessing learned models survival analysis Concordance Index), similar wilcoxon-mannwhitney statistic-partite ranking problems. given performance measure, develop approaches learn models directly optimizing. optimization computationally expensive, focus maximizing lower bounds, log-sigmoid exponential bounds, sec.  interestingly, log-sigmoid bound arises natural Proportional Hazard) model, standard model classical survival analysis, sec. . moreover, models learned optimizing cox partial likelihood classical survival analysis, show sec. maximizing likelihood ends (approximately) maximizing. our experiments sec. show optimizing lower bounds cox likelihood yields similar results respect, proposed lower bounds slightly better. Survival analysis Survival analysis extensively studied statistics community decades]. primary focus build statistical models survival time? individual population.  Censored data major problem fact period observation? censored individuals for instance, patient move town longer clinical trial. also end trial lot patients survive. for cases exact survival time longer observation period. such data referred right-censored? called censoring time. for individuals, survived? ., actual observation min? ?  let -dimensional vector covariates (explanatory variables) ith individual. clinical studies, covariates typically include demographic variables, age, gender, race; diagnosis information lab tests; treatment information., dosage. important assumption generally made? ? independent conditional., censoring independent survival time. with indicator function equals failure observed?  ? data censored? ? training data summarized patients. objective learn predictive model survival time function covariates.  Failure time distributions failures times typically modeled follow distribution, absorbs random effects unexplained (available) covariates. this distribution characterized survival function probability individual alive time related function commonly hazard function. density \\x0cfunction hazard function defined ) lim ). hazard function measures instantaneous rate failure, insight failure mechanisms. function )  called cumulative hazard function, holds?? ].  Proportional hazard model Proportional hazard) models standard studying effect covariates survival time distributions]. specifically, model assumes multiplicative effect covariates hazard function., ) ) hazard function person covariates-called baseline hazard function), typically based exponential Weibull distributions; set unknown regression parameters, relative hazard function. equivalent formulations cumulative hazard function survival function include )   ) cox partial likelihood Cox noticed semi-parametric approach suﬃcient estimating weights models., baseline hazard function remain completely unspecified. only parametric assumption effect covariates hazard function required. parameter estimates model obtained maximizing cox partial likelihood weights)  uncensored) Indicator function log?sigmoid lower bound Exponential lower bound) Figure Order graphs representing ranking constraints. ) censored data) censored data. empty circle represents censored point. points arranged increasing survival times \\x0clowest bottom. ) Two concave lower bounds indicator function. each term product probability ith individual failed time failure occurred time individuals risk failing. cox ors shown partial log-likelihood treated ordinary log-likelihood derive valid (partial) maximum likelihood estimates]. interesting properties cox partial likelihood include) due parametric form, optimized computationally eﬃcient way) depends ranks observed survival times. inequality . rar actual numerical values. outline connection ranking times ?and concordance index sec.  Ordering Survival times Casting survival analysis ranking problem elegant dealing typically skewed distributions survival times, censoring data: Two subjects? survival times ordered) uncensored) uncensored time smaller censored survival time. this visualized means order graph. fig.  set vertices represents individuals, filled vertex observed/uncensored survival time, empty circle denotes censored observation. existence edge Eij implies edge originate censored point.  Concordance index For reasons, concordance index-index commonly performance measures survival models]. interpreted fraction pairs subjects predicted survival times correctly ordered subjects ordered. words, probability concordance predicted observed survival. written— Eij indicator function orwise— denotes number edges order graph.  predicted survival time subject model equivalently, concordance index written explicitly — uncensored This index generalization wilcoxon-mann-whitney statistics] area ROC curve (auc) regression problems) applied continuous output variables) account censoring data. like auc, perfect prediction accuracy good random predictor.  Maximizing? Ranking Problem Since evaluate predictive accuracy survival model terms concordance index, natural formulate learning problem directly maximize concordance index. note that, concordance index \\x0cbeen widely evaluate learnt model, generally objective function training. concordance index invariant monotone transformation survival times, model learnt maximizing-index ranking/scoring function. our goal predict wher survival time individual larger anor individual. very doctor wher kind treatment results increase survival time exact absolute survival time important. terms ranking problems studied machine learning -partite ranking problem, data point class itself. formulating ranking problem naturally incorporate censored data. once formulated ranking problem ranking algorithms proposed machine learning literature]. paper algorithm proposed]. more formally, learn ranking function suitable function class implies survival time patient larger patient given data order graph optimal ranking function arg maxf, prevent overfitting training data, regularization added equation, secs.  cases, suﬃcient regularization achieved restricting function class., linear functions. for ease exposition family linear ranking functions paper ) Lower bounds Maximizing discrete optimization problem, computationally expensive. for reason, resort maximizing differentiable concave lower bound indicator function concordance index. eqs.  paper focus log-sigmoid lower bound. sec. exponential lower bound. sec. suitably scaled tight origin asymptotic limit large positive values, fig. ). show bounds relate classical approaches survival analysis: turns out, family linear ranking functions, approaches closely related model commonly survival analysis. sec. . log-sigmoid lower bound subsection discusses lower bound concordance index based log-sigmoid function. subsection shows bound arises naturally proportional hazard models.  Lower bound sigmoid function defined  While approximation indicator function, lower bound. contrast, scaled version log sigmoid function, log? )]/ log lower bound indicator function (fig.  (log )/log). ) log-sigmoid function concave asymptotically linear large negative values, considered differentiable approximation hinge loss, commonly Generalization non-linear functions achieved easily kernels: linear ranking function class replaced reproducing kernel Hilbert space (rkhs). ranking function form, kernel RHKS training support vector machines. lower bound concordance index. . immediately (log   )]/log cls— Eij Eij eﬃciently maximized gradient-based methods. sec). given linear ranking function) bound cls cls) (log  )]/log). — Eij avoid overfitting, penalize functions large norm standard way, obtain regularized version clsreg) kwk2 cls).  Connection model concordance index interpreted probability correct ranking defined order graph) function its probabilistic version cast likelihood. under assumption pair, independent pair, log-likelihood reads log] ) Eij independence assumption hold pairs due transitivity (even individual samples assumed. lower bound concordance index. while probability correct pairwise ordering], chosen sigmoid ranking literature], show sigmoid function arises naturally context models. let¿ denote survival time patient covariates relative log-hazard larger hazard corresponds smaller survival time. sec.  hence    density function patient covariate corre0 sponding survival function). using. model, continue manipulations:    )]. ) This derivation shows probability correct pairwise ordering sigmoid function. assuming prior, regularization, optimal maximum aposteriori (map) estimator \\x0cform bmap arg max), posterior) takes form penalized log-likelihood: ) kwk2 log   ) Eij This expression equivalent) constants irrelevant optimization problem, justifies choice regularization.  Exponential lower bound exponential  serve alternative lower bound step indicator function (see fig. )). concordance index lower-bounded     — Eij Analogous log-sigmoid bound, linear ranking function) lower bound simplifies — Eij and, penalizing functions large norm regularized version reads cereg) kwk2  ) Eij Gradient based learning order maximize regularized concave surrogate gradient-based learning technique. polak-ribi‘ ere variant nonlinear conjugate gradients) algorithm]. method gradient) require evaluation function. avoids computing derivatives. convergence faster steepest descent. using fact    )]   ), gradient. (log-sigmoid bound) clsreg)  — log Eij   gradient. (exponential bound) cereg)   eij — cox partial likelihood lower bound our experimental results (sec. Coxs method proposed methods showed similar performance assessed. while proposed method formulated explicitly maximize lower bound concordance index, Coxs method maximized partial likelihood. one suspects wher Coxs partial likelihood lower bound concordance index. argument presented give indication method maximizes partial likelihood ends (approximately) maximizing concordance index. -write exponential bound proportional hazard models sec. )  — uncensored uncensored  ]. )  uncensored Note replaced assuming ties data., survival times identical, analogous \\x0ccox partial likelihood approach. sec. ). number uncensored observations denoted cox partial likelihood written terms) uncensored hzi geom hzi igeom denotes geometric uncensored using inequality min concordance index bounded?  — min This maximizing min maximizes lower bound concordance index. while cox partial likelihood insight. since max (because largest uncensored maximizing min expected approximately maximize geometric cox partial likelihood. Table Summary data sets used. number patients. number covariates used. dataset MAASTRO support support support MELANOMA 285 477 314 149 191 Missing% Censored% Experiments section compare performance lower bounds? logsigmoid, exponential, cox partial likelihood medical data sets.  Medical datasets Table summarizes data sets experiments. substantial amount data censored missing. MAASTRO dataset concerns survival time non-small cell lung cancer patients, analyzed part collaboration. medical data sets publicly available: SUPPORT dataset random sample Phases SUPPORT](study Understand Prognoses Preferences Outcomes Risks treatment) study. suggested] split dataset datasets, death. MELANOMA data clinical study skin cancer.  Evaluation procedure For data set% examples training remaining% hold-out set testing. chose optimal regularization parameter . eqs. ) based five-fold cross validation training set. tolerance conjugate gradient procedure set conjugate-gradient optimization procedure initialized vector. all covariates normalized unit variance. missing values focus paper, simple imputation technique. for missing value, imputed sample drawn Gaussian distribution variance estimated values patients.  Results performance evaluated terms concordance index results tabulated Table compare methods? ) \\x0ccox partial likelihood method) proposed ranking methods log-sigmoid exponential lower bounds. observations made? ) proposed linear ranking method performs slightly cox partial likelihood method, difference significant. this agrees insights cox partial likelihood end maximizing. ) exponential bound shows slightly performance log-sigmoid bound, tightness bound positive fig. ) important negative data sets. however difference significant. Conclusions paper, outlined approaches maximizing concordance index, standard performance measure survival analysis cast ranking problem. showed that, widely-used proportional hazard models, log-sigmoid function arises natural lower bound concordance index. presented approach directly optimizing lower bound computationally eﬃcient way. this optimization procedure applied lower bounds, exponential one. apart that, showed maximizing cox partial likelihood understood (approximately) maximizing lower bound concordance index, explains high-scores proportional hazard models observed practice. optimization lower bounds results-score experiments, approach giving tentatively results. http://biostat.vanderbilt.edu/twiki/bin/view/main/datasets. www.stat.uni muenchen/service/datenarchiv/melanoma/melanoma.html Table Concordance indices methods datasets. standard deviation computed fold cross-validation. results shown fixed holdout set. training set std] test set std] holdout set MAASTRO Cox log-sigmoid exponential support Cox log-sigmoid exponential support Cox log-sigmoid exponential support Cox log-sigmoid exponential MELANOMA Cox log-sigmoid exponential Acknowledgements grateful Bharat Rao encouragement support work, anonymous reviewers valuable comments.',\n",
       " 'PP3441': 'analysis large scale sequential data important task machine learning data mining, inspired applications biological sequence analysis, text audio mining. classification string data, sequences discrete symbols, attracted interest led number algorithms]. exhibit state--art performance tasks protein superfamily fold prediction, music genre classification document topic elucidation. classification data sequential domains made challenging variability sequence lengths, potential existence important features multiple scales, size alphabets datasets. typical alphabet sizes vary widely, ranging size nucleotides DNA sequences, thousands words language lexicon text documents. strings class, proteins fold documents politics, exhibit wide variability \\x0cprimary sequence content. moreover, important datasets continue increase size, easily reaching millions sequences. consequence, resulting algorithms ability eﬃciently handle large alphabets datasets establish measures similarity complex sequence transformations order accurately classify data. number state--art approaches scoring similarity pairs sequences database rely fixed, spectral representations sequential data notion mismatch kernels. ]. framework induced representation sequence typically spectra (counts) short substrings-mers) contained sequence. similarity score established allowing transformations original-mers based models deletions, insertions mutations. however, computing representations eﬃciently large alphabet sizes ?loose? similarity models computationally challenging. instance, complexity eﬃcient trie-based computation, mismatch kernel strings strongly depends alphabet size number mismatches allowed-mers (contiguous substring length mismatches alphabet size —?—. limits applicability algorithms simpler transformation models (shorter smaller alphabets, reducing practical utility complex real data. alternative, complex transformation models] lead state--art predictive performance expense increased computational effort. work propose algorithms modeling sequences complex transformations (such multiple insertions, deletions, mutations) exhibit state--art performance variety distinct classification tasks. particular, present algorithms inexact. mismatches) string comparison improve time bounds tasks show orders-magnitude running time improvements. algorithms rely eﬃcient implicit computation mismatch neighborhoods-mer statistic sets sequences. leads mismatch kernel algorithm complexity independent alphabet algorithm easily generalized families string kernels, spectrum gapped kernels], semisupervised settings neighborhood kernel]. demonstrate benefits algorithms challenging classification problems, detecting homology (evolutionary similarity) remotely related proteins, recognizing protein fold, performing classification music samples. algorithms display state--art classification performance run substantially faster existing methods. low computational complexity algorithms opens possibility analyzing large datasets fully-supervised semi-supervised setting modest computational resources. related Work Over past decade, methods proposed solve string classification problem, including generative, hmms, discriminative approaches. discriminative approaches, sequence analysis \\x0ctasks, kernel-based] machine learning methods provide accurate results]. sequence matching frequently based common-occurrence exact sub-patterns-mers, features), spectrum kernels] substring kernels]. inexact comparison framework typically achieved families mismatch] profile] kernels. spectrum mismatch) kernel directly extract string features based observed sequence, hand, profile kernel, proposed Kuang. ], builds profile] similar -dimensional representation, derived constructing profile sequence practical application domains, size profile dependent size alphabet set. bio-sequences, music text classification —?— potentially large, order tens thousands symbols. case, simple semi-supervised learning method, sequence neighborhood kernel, employed] alternative lone-mers mismatches. eﬃcient trie-based algorithms, mismatch kernels strong dependency size alphabet set number allowed mismatches, restricted practice control complexity algorithm. trie-based framework, list-mers extracted strings traversed depth-first search branches   leaf node depth corresponds-mer feature (eir exact inexact instance observed exact string features) list matching features string. kernel matrix updated leaf nodes counts. complexity trie-based algorithm mismatch kernel computation strings]. algorithm complexity depends size trie traversal, substitutions drawn explicitly; consequently, control complexity algorithm restrict number allowed mismatches), alphabet size (—?—). limitations hinder wide application powerful computational tool, biological sequence analysis, mutation, insertions deletions frequently-occur, establishing relax parameter hand, restricting size alphabet sets strongly limits applications mismatch model. eﬃcient string algorithms exist] suﬃx-tree based algorithms], readily extend mismatch framework. study, aim extend works presented] close existing gap oretical complexity mismatch fast string kernels. combinatorial Algorithm section develop improved algorithm kernel computations mismatches, serves starting point main algorithm Section  Spectrum Mismatch Kernels Definition Given sequence symbols alphabet spectrum kernel] mismatch) kernel] induce -dimensional \\x0crepresentation sequence:  ) (?, )  (?,     (?) mutational neighborhood, set-mers differ mismatches. note that, definition, spectrum kernels, mismatch kernel defined) ) (?, number times contiguous substring length-mer) occurs mismatches.  intersection-based Algorithm Our algorithm presents performing local inexact string matching key properties: parameter independent: complexity independent —?— mismatch parameter-place: min, extra space auxiliary look table linear complexity: length substring opposed exponential develop algorithm, write mismatch kernel (equation equivalent form nxx nyx nxx nyx, xix, yiy (xix (yiy(xix yiy)  nxx nyx, number induced (neighboring-mers common. , size intersection mismatch neighborhoods). key observation compute, eﬃciently kernel evaluation problem reduces performing pairwise comparison based pairs observed-mers, sequences. complexity procedure cost evaluating-mers fact, fixed quantity depends Hamming distance. number mismatches) evaluated advance, show Section. result, intersection values looked table constant time matching. note summation shows explicit dependency —?— summary, strings algorithm (algorithm compares pairs observed-mers computes mismatch kernel Equation algorithm (hamming-mismatch) Mismatch algorithm based Hamming distance input: strings— parameters lookup table intersection sizes Evaluate kernel Equation Pny, ixx(xix yiy) intersection size distance output: Mismatch kernel, complexity algorithm(knx Hamming distances-mer pairs observed known. section, discuss eﬃciently compute size intersection.  Intersection size: Closed Form Solution number neighboring-mers shared observed-mers directly computed, closed-form, Hamming distance, fixed requiring explicit traversal-mer space case trie-based computations. case. ). intersection size corresponds size)-mismatch neighborhood.  (—?—  higher values Hamming distance key observation fixed distance, constant, mismatch positions. result, intersection values pre-computed once, stored looked necessary. illustrate this, show examples ,      (—?—  )(—?— ,  ,  )(—?— (—?— ,    (—?— ,    , general, intersection size found weighted form (—?—  precomputed constant time. mismatch Algorithm based Suﬃcient Statistics section, furr develop ideas previous section present improved mismatch algorithm require pairwise comparison-mers strings dependes linearly sequence length. crucial observation Equation, non-zero, . result, kernel computed Equation incremented min, distinct values, min, intersection sizes. -write equation form nxx nyx min(xix yiy size intersection-mer mutational neighborhood Hamming distance number observed-mer pairs Hamming distance problem computing kernel furr reduced single summation. shown Section compute advance. crucial task computing suﬃcient statistics eﬃciently. following, show compute mismatch statistics time constant depend alphabet size. formulate task inferring matching statistics auxiliary counting problem: Mismatch Statistic counting: Given set-mers strings Hamming distance ..., min), output number-mer pairs),  , number exact matches. problem distance pair-mers; number pairs distance show problem computing matching statistics solved linear time number-mers) multiple rounds counting sort sub-algorithm. problem computing number-mers distance. case, apply counting sort order-mers lexicographically find number exact matches scanning sorted list. counting requires linear) time. eﬃcient direct computation diﬃcult (requires quadratic time); anor approach compute inexact cumulative mismatch statistics overcount number-mer pairs distance follows. -mers pick positions remove-mers symbols positions obtain)-mers   key observation?   ,  result-mers, compute cumulative mismatch statistics linear time rounds counting sort )-mers. exact mismatch statistics obtained subtracting exact counts compensate overcounting follows   min(min), )  mismatch statistic computed subtracting preceding statistics total number matches:   ).  Our algorithm mismatch kernel computations based suﬃcient statistics summarized Algorithm complexity algorithm(nck constant pmin ), independent size alphabet set, number rounds counting sort evaluating cumulative \\x0cmismatch statistics algorithm (mismatch) Mismatch kernel algorithm based Suﬃcient Statistics input: strings— parameters pre-computed intersection values compute min, cumulative matching statistics, counting sort compute exact matching statistics, Equation pmin) evaluate kernel Equation output: Mismatch kernel, Extensions Our algorithmic approach applied variety existing string kernels, leading eﬃcient simple algorithms benefit applications. spectrum kernels. spectrum kernel] notation suﬃcient statistic. ) computed rounds counting sort. ) time). gapped kernels. gapped kernels] measure similarity strings based-occurrence gapped instances-long substrings) (?, subsequence Similar algorithmic approach extracting cumulative mismatch statistics algorithm, compute gapped) kernel, perform single round counting sort-mers contained-mers. simple eﬃcient) time algorithm gapped kernel computations. wildcard kernels. wildcard) kernel] notation sum cumulative statistics. computed rounds counting sort, giving simple eﬃcient ) algorithm. spatial kernels. spatial) kernel] computed sorting-mers iteratively arrangement-mers spatially constrained distance Neighborhood kernels. sequence neighborhood kernels] proved powerful tool sequence analysis tasks. method unlabeled data form set neighbors train/test sequences measure similarity sequences neighborhoods) sequence neighborhood neighboring sequences unlabeled data set, including itself. note kernel value, computed directly Equation, incur quadratic complexity size neighborhoods. similar single string case, algorithmic approach, compute neighborhood kernel (over string sets), jointly sort observed-mers) apply desired kernel evaluation method (spectrum, mismatch, gapped). setting, neighborhood kernel evaluated time linear neighborhood size. leads eﬃcient algorithms computing sequence neighborhood kernels large datasets, show experimental section. evaluation study performance algorithms, running time predictive accuracy, syntic data standard benchmark datasets protein sequence analysis music genre classification. reduced running time requirements algorithms open possibility ?looser? mismatch measures larger results presented demonstrate mismatch kernels larger, lead state--art predictive performance compared complex models]. standard benchmark datasets compare previously published results: SCOP dataset (7329 sequences 2862 labeled] remote protein homology detection, DingDubchak dataset1 folds, 694 sequences] protein fold recognition, music genre data2 classes, 1000 sequences, —?— 1024] multi-class genre prediction. protein sequence classification semi-supervised setting, Protein Data Bank (pdb, 232 sequences), swiss-prot (101, 602 sequences), non-redundant) databases unlabeled datasets, setup]. experiments performed single.8ghz cpu. datasets experiments suplementary data/code http://seqam.rutgers.edu/new-inexact/new-inexact.html.  Running time analysis compare running time algorithm syntic real data trie-based computations. syntic data, generate strings length 105 alphabets sizes measure running time trie-based suﬃcient statistics based algorithms evaluating mismatch string kernel. figure shows relative running time Ttrie /tss logarithmic scale, mismatch-trie mismatch function alphabet size. plot, algorithm demonstrates orders magnitude improvements, large alphabet sizes. table compares running times algorithm trie-based algorithm real dataset (proteins, dna, text, music) single kernel entry (pair strings) computation. observe speed improvements ranging 100 106 times depending alphabet size. measure running time full 7329-7329 mismatch) kernel matrix computations SCOP dataset supervised setting. running time algorithm 1525 seconds compared 196052 seconds trie-based computations. obtained speed 128 times expected oretical analysis (our algorithm performs counting-sort iterations total-, 1mers, running time ratio approximately 125 compared trie-based complexity). observe similar improvements http://ranger.uta.edu/?chqding/bioinfo.html http://opihi.uvic/sound/genres relative running time, ttrie/tss \\x0ctable Running time seconds) kernel computation strings real data 100 200 300 400 500 600 alphabet size 700 800 900 Figure 1000 Relative running time Ttrie /tss logarithmic scale) mismatch-trie mismatch function alphabet size (mismatch) kernel, 105)-trie time ratio)-trie time ratio long protein 36672.6268.1987.5519.2957 100 protein dna text music 116.0212.0052.2918.0067 570.0260.0054.4800.0064 242 29224 20398.0178 106.0649 6892 1024 526.0331,000.0941 semi-supervised setting neighborhood mismatch kernels; example, computing smaller neighborhood mismatch) kernel matrix labeled sequences (2862-2862 matrix) swiss-prot unlabeled dataset takes 480 seconds algorithm, performing task trie-based algorithm takes days.  Empirical performance analysis section show predictive performance results sequence analysis tasks algorithms. tasks multiclass music genre classification], results Table protein remote homology (superfamily) prediction] Table include preliminary results multi-class fold prediction] Table music classification task, observe significant improvements accuracy larger number mismatches. obtained error rate%) dataset \\x0ccompares state-art results based signal representation]. remote protein homology detection, evident Table benefits larger number allowed mismatches remotely related proteins separated multiple mutations insertions/deletions. example, observe improvement average roc score fully-supervised setting, similar significant improvements semi-supervised settings. particular, result swiss-prot dataset)-mismatch kernel promising compares results state--art, computationally demanding, profile kernels]. neighborhood kernels proposed Weston. shown promising results], slightly worse profile kernel. however, algorithm significantly improves speed neighborhood kernels, show larger number allowed mismatches neighborhood perform stateof--art profile kernel)-mismatch neighborhood achieves average roc score, compared profile kernel swiss-prot dataset. important result addresses main drawback neighborhood kernels, running time]. table Classification pertable Classification performance protein remote homology formance music genre prediction classification (multi-class) dataset mismatch) mismatch) mismatch) Method Error ROC ROC50 ROC ROC50 ROC ROC50 Mismatch SCOP (supervised Mismatch SCOP (unlabeled SCOP (pdb SCOP (swiss-prot For multi-class protein fold recognition (table), similarly observe improvements performance larger numbers allowed mismatches. balanced error)-mismatch neighborhood kernel swiss-prot compares error rate% state--art profile kernel adaptive codes] larger non-redundant) dataset. , balanced error furr reduces)-mismatch. table Classification performance fold prediction (multi-class) Top Top Balanced Top Top Balanced Recall Method Error Precision Error Error Recall Precision Error Mismatch Mismatch Mismatch)?  Mismatch Mismatch)?  Mismatch)?  top5 swiss-prot sequence database; (non-redundant) database Conclusions presented algorithms inexact matching discrete-valued string representations reduce computational complexity current algorithms, demonstrate state--art performance significantly improved running times. improvement makes string kernels approximate looser matching viable alternative practical tasks sequence analysis. algorithms work large databases supervised semi-supervised settings scale alphabet size number allowed mismatches. consequence, proposed algorithms readily applied challenging problems sequence analysis mining. Analysis large scale sequential data important task machine learning data mining, inspired applications biological sequence analysis, text audio mining. classification string data, sequences discrete symbols, attracted interest led number algorithms]. exhibit state--art performance tasks protein superfamily fold prediction, music genre classification document topic elucidation. classification data sequential domains made challenging variability sequence lengths, potential existence important features multiple scales, size alphabets datasets. typical alphabet sizes vary widely, ranging size nucleotides DNA sequences, thousands words language lexicon text documents. strings class, proteins fold documents politics, exhibit wide variability \\x0cprimary sequence content. moreover, important datasets continue increase size, easily reaching millions sequences. consequence, resulting algorithms ability eﬃciently handle large alphabets datasets establish measures similarity complex sequence transformations order accurately classify data. number state--art approaches scoring similarity pairs sequences database rely fixed, spectral representations sequential data notion mismatch kernels. ]. framework induced representation sequence typically spectra (counts) short substrings-mers) contained sequence. similarity score established allowing transformations original-mers based models deletions, insertions mutations. however, computing representations eﬃciently large alphabet sizes ?loose? similarity models computationally challenging. for instance, complexity eﬃcient trie-based computation, mismatch kernel strings strongly depends alphabet size number mismatches allowed-mers (contiguous substring length mismatches alphabet size —?—. this limits applicability algorithms simpler transformation models (shorter smaller alphabets, reducing practical utility complex real data. alternative, complex transformation models] lead state--art predictive performance expense increased computational effort. work propose algorithms modeling sequences complex transformations (such multiple insertions, deletions, mutations) exhibit state--art performance variety distinct classification tasks. particular, present algorithms inexact. mismatches) string comparison improve time bounds tasks show orders-magnitude running time improvements. algorithms rely eﬃcient implicit computation mismatch neighborhoods-mer statistic sets sequences. this leads mismatch kernel algorithm complexity independent alphabet algorithm easily generalized families string kernels, spectrum gapped kernels], semisupervised settings neighborhood kernel]. demonstrate benefits algorithms challenging classification problems, detecting homology (evolutionary similarity) remotely related proteins, recognizing protein fold, performing classification music samples. algorithms display state--art classification performance run substantially faster existing methods. low computational complexity algorithms opens possibility analyzing large datasets fully-supervised semi-supervised setting modest computational resources. Related Work Over past decade, methods proposed solve string classification problem, including generative, hmms, discriminative approaches. among discriminative approaches, sequence analysis \\x0ctasks, kernel-based] machine learning methods provide accurate results]. sequence matching frequently based common-occurrence exact sub-patterns-mers, features), spectrum kernels] substring kernels]. inexact comparison framework typically achieved families mismatch] profile] kernels. both spectrum mismatch) kernel directly extract string features based observed sequence, hand, profile kernel, proposed Kuang. ], builds profile] similar -dimensional representation, derived constructing profile sequence practical application domains, size profile dependent size alphabet set. while bio-sequences, music text classification —?— potentially large, order tens thousands symbols. case, simple semi-supervised learning method, sequence neighborhood kernel, employed] alternative lone-mers mismatches. eﬃcient trie-based algorithms, mismatch kernels strong dependency size alphabet set number allowed mismatches, restricted practice control complexity algorithm. under trie-based framework, list-mers extracted strings traversed depth-first search branches   each leaf node depth corresponds-mer feature (eir exact inexact instance observed exact string features) list matching features string. kernel matrix updated leaf nodes counts. complexity trie-based algorithm mismatch kernel computation strings]. algorithm complexity depends size trie traversal, substitutions drawn explicitly; consequently, control complexity algorithm restrict number allowed mismatches), alphabet size (—?—). such limitations hinder wide application powerful computational tool, biological sequence analysis, mutation, insertions deletions frequently-occur, establishing relax parameter hand, restricting size alphabet sets strongly limits applications mismatch model. while eﬃcient string algorithms exist] suﬃx-tree based algorithms], readily extend mismatch framework. study, aim extend works presented] close existing gap oretical complexity mismatch fast string kernels. Combinatorial Algorithm section develop improved algorithm kernel computations mismatches, serves starting point main algorithm Section  Spectrum Mismatch Kernels Definition Given sequence symbols alphabet spectrum kernel] mismatch) kernel] induce -dimensional \\x0crepresentation sequence:  ) (?, )  (?,     (?) mutational neighborhood, set-mers differ mismatches. note that, definition, spectrum kernels, mismatch kernel defined) ) (?, number times contiguous substring length-mer) occurs mismatches.  intersection-based Algorithm Our algorithm presents performing local inexact string matching key properties: parameter independent: complexity independent —?— mismatch parameter-place: min, extra space auxiliary look table linear complexity: length substring opposed exponential develop algorithm, write mismatch kernel (equation equivalent form nxx nyx nxx nyx, xix, yiy (xix (yiy(xix yiy)  nxx nyx, number induced (neighboring-mers common. , size intersection mismatch neighborhoods). key observation compute, eﬃciently kernel evaluation problem reduces performing pairwise comparison based pairs observed-mers, sequences. complexity procedure cost evaluating-mers fact, fixed quantity depends Hamming distance. number mismatches) evaluated advance, show Section. result, intersection values looked table constant time matching. note summation shows explicit dependency —?— summary, strings algorithm (algorithm compares pairs observed-mers computes mismatch kernel Equation algorithm (hamming-mismatch) Mismatch algorithm based Hamming distance input: strings— parameters lookup table intersection sizes Evaluate kernel Equation Pny, ixx(xix yiy) intersection size distance output: Mismatch kernel, complexity algorithm(knx Hamming distances-mer pairs observed known. section, discuss eﬃciently compute size intersection.  Intersection size: Closed Form Solution number neighboring-mers shared observed-mers directly computed, closed-form, Hamming distance, fixed requiring explicit traversal-mer space case trie-based computations. case. ). intersection size corresponds size)-mismatch neighborhood.  (—?—  for higher values Hamming distance key observation fixed distance, constant, mismatch positions. result, intersection values pre-computed once, stored looked necessary. illustrate this, show examples ,      (—?—  )(—?— ,  ,  )(—?— (—?— ,    (—?— ,    , general, intersection size found weighted form (—?—  precomputed constant time. Mismatch Algorithm based Suﬃcient Statistics section, furr develop ideas previous section present improved mismatch algorithm require pairwise comparison-mers strings dependes linearly sequence length. crucial observation Equation, non-zero, . result, kernel computed Equation incremented min, distinct values, min, intersection sizes. -write equation form nxx nyx min(xix yiy size intersection-mer mutational neighborhood Hamming distance number observed-mer pairs Hamming distance problem computing kernel furr reduced single summation. shown Section compute advance. crucial task computing suﬃcient statistics eﬃciently. following, show compute mismatch statistics time constant depend alphabet size. formulate task inferring matching statistics auxiliary counting problem: Mismatch Statistic counting: Given set-mers strings Hamming distance ..., min), output number-mer pairs),  , number exact matches. problem distance pair-mers; number pairs distance show problem computing matching statistics solved linear time number-mers) multiple rounds counting sort sub-algorithm. problem computing number-mers distance. case, apply counting sort order-mers lexicographically find number exact matches scanning sorted list. counting requires linear) time. eﬃcient direct computation diﬃcult (requires quadratic time); anor approach compute inexact cumulative mismatch statistics overcount number-mer pairs distance follows. consider-mers pick positions remove-mers symbols positions obtain)-mers   key observation?   ,  result-mers, compute cumulative mismatch statistics linear time rounds counting sort )-mers. exact mismatch statistics obtained subtracting exact counts compensate overcounting follows   min(min), )  mismatch statistic computed subtracting preceding statistics total number matches:   ).  Our algorithm mismatch kernel computations based suﬃcient statistics summarized Algorithm complexity algorithm(nck constant pmin ), independent size alphabet set, number rounds counting sort evaluating cumulative \\x0cmismatch statistics algorithm (mismatch) Mismatch kernel algorithm based Suﬃcient Statistics input: strings— parameters pre-computed intersection values compute min, cumulative matching statistics, counting sort compute exact matching statistics, Equation pmin) evaluate kernel Equation output: Mismatch kernel, Extensions Our algorithmic approach applied variety existing string kernels, leading eﬃcient simple algorithms benefit applications. spectrum kernels. spectrum kernel] notation suﬃcient statistic. ) computed rounds counting sort. ) time). gapped kernels. gapped kernels] measure similarity strings based-occurrence gapped instances-long substrings) (?, subsequence Similar algorithmic approach extracting cumulative mismatch statistics algorithm, compute gapped) kernel, perform single round counting sort-mers contained-mers. this simple eﬃcient) time algorithm gapped kernel computations. wildcard kernels. wildcard) kernel] notation sum cumulative statistics. computed rounds counting sort, giving simple eﬃcient ) algorithm. spatial kernels. spatial) kernel] computed sorting-mers iteratively arrangement-mers spatially constrained distance Neighborhood kernels. sequence neighborhood kernels] proved powerful tool sequence analysis tasks. method unlabeled data form set neighbors train/test sequences measure similarity sequences neighborhoods) sequence neighborhood neighboring sequences unlabeled data set, including itself. note kernel value, computed directly Equation, incur quadratic complexity size neighborhoods. similar single string case, algorithmic approach, compute neighborhood kernel (over string sets), jointly sort observed-mers) apply desired kernel evaluation method (spectrum, mismatch, gapped). under setting, neighborhood kernel evaluated time linear neighborhood size. this leads eﬃcient algorithms computing sequence neighborhood kernels large datasets, show experimental section. Evaluation study performance algorithms, running time predictive accuracy, syntic data standard benchmark datasets protein sequence analysis music genre classification. reduced running time requirements algorithms open possibility ?looser? mismatch measures larger results presented demonstrate mismatch kernels larger, lead state--art predictive performance compared complex models]. standard benchmark datasets compare previously published results: SCOP dataset (7329 sequences 2862 labeled] remote protein homology detection, DingDubchak dataset1 folds, 694 sequences] protein fold recognition, music genre data2 classes, 1000 sequences, —?— 1024] multi-class genre prediction. for protein sequence classification semi-supervised setting, Protein Data Bank (pdb, 232 sequences), swiss-prot (101, 602 sequences), non-redundant) databases unlabeled datasets, setup]. all experiments performed single.8ghz cpu. datasets experiments suplementary data/code http://seqam.rutgers.edu/new-inexact/new-inexact.html.  Running time analysis compare running time algorithm syntic real data trie-based computations. for syntic data, generate strings length 105 alphabets sizes measure running time trie-based suﬃcient statistics based algorithms evaluating mismatch string kernel. figure shows relative running time Ttrie /tss logarithmic scale, mismatch-trie mismatch function alphabet size. plot, algorithm demonstrates orders magnitude improvements, large alphabet sizes. table compares running times algorithm trie-based algorithm real dataset (proteins, dna, text, music) single kernel entry (pair strings) computation. observe speed improvements ranging 100 106 times depending alphabet size. measure running time full 7329-7329 mismatch) kernel matrix computations SCOP dataset supervised setting. running time algorithm 1525 seconds compared 196052 seconds trie-based computations. obtained speed 128 times expected oretical analysis (our algorithm performs counting-sort iterations total-, 1mers, running time ratio approximately 125 compared trie-based complexity). observe similar improvements http://ranger.uta.edu/?chqding/bioinfo.html http://opihi.uvic/sound/genres relative running time, ttrie/tss \\x0ctable Running time seconds) kernel computation strings real data 100 200 300 400 500 600 alphabet size 700 800 900 Figure 1000 Relative running time Ttrie /tss logarithmic scale) mismatch-trie mismatch function alphabet size (mismatch) kernel, 105)-trie time ratio)-trie time ratio long protein 36672.6268.1987.5519.2957 100 protein dna text music 116.0212.0052.2918.0067 570.0260.0054.4800.0064 242 29224 20398.0178 106.0649 6892 1024 526.0331,000.0941 semi-supervised setting neighborhood mismatch kernels; example, computing smaller neighborhood mismatch) kernel matrix labeled sequences (2862-2862 matrix) swiss-prot unlabeled dataset takes 480 seconds algorithm, performing task trie-based algorithm takes days.  Empirical performance analysis section show predictive performance results sequence analysis tasks algorithms. tasks multiclass music genre classification], results Table protein remote homology (superfamily) prediction] Table include preliminary results multi-class fold prediction] Table music classification task, observe significant improvements accuracy larger number mismatches. obtained error rate%) dataset \\x0ccompares state-art results based signal representation]. remote protein homology detection, evident Table benefits larger number allowed mismatches remotely related proteins separated multiple mutations insertions/deletions. for example, observe improvement average roc score fully-supervised setting, similar significant improvements semi-supervised settings. particular, result swiss-prot dataset)-mismatch kernel promising compares results state--art, computationally demanding, profile kernels]. neighborhood kernels proposed Weston. shown promising results], slightly worse profile kernel. however, algorithm significantly improves speed neighborhood kernels, show larger number allowed mismatches neighborhood perform stateof--art profile kernel)-mismatch neighborhood achieves average roc score, compared profile kernel swiss-prot dataset. this important result addresses main drawback neighborhood kernels, running time]. Table Classification pertable Classification performance protein remote homology formance music genre prediction classification (multi-class) dataset mismatch) mismatch) mismatch) Method Error ROC ROC50 ROC ROC50 ROC ROC50 Mismatch SCOP (supervised Mismatch SCOP (unlabeled SCOP (pdb SCOP (swiss-prot For multi-class protein fold recognition (table), similarly observe improvements performance larger numbers allowed mismatches. balanced error)-mismatch neighborhood kernel swiss-prot compares error rate% state--art profile kernel adaptive codes] larger non-redundant) dataset. using, balanced error furr reduces)-mismatch. table Classification performance fold prediction (multi-class) Top Top Balanced Top Top Balanced Recall Method Error Precision Error Error Recall Precision Error Mismatch Mismatch Mismatch)?  Mismatch Mismatch)?  Mismatch)?  Top5 swiss-prot sequence database; (non-redundant) database Conclusions presented algorithms inexact matching discrete-valued string representations reduce computational complexity current algorithms, demonstrate state--art performance significantly improved running times. this improvement makes string kernels approximate looser matching viable alternative practical tasks sequence analysis. our algorithms work large databases supervised semi-supervised settings scale alphabet size number allowed mismatches. consequence, proposed algorithms readily applied challenging problems sequence analysis mining.',\n",
       " 'PP3458': 'observer moves environment, retinal image time create multiple complex motion ﬂows, including translational, circular radial motion. human observers process motion patterns infer ego motion global structure world. however, inherent ambiguity local motion signals requires visual system employ eﬃcient integration strategy combine local measurements order perceive global motion. psychophysical experiments identified variety phenomena, motion capture motion cooperativity], consequences integration. number computational Bayesian models proposed explain effects based prior assumptions motion. particular, shown slowand-smooth prior, related models, qualitatively account range experimental results] quantitatively account ors]. however, integration strategy modeled slow-and-smooth prior generalize complex motion types, circular radial \\x0cmotion, critically important estimating ego motion. paper concerned questions. ) What integration priors motion input? ) How local motion measurements combined proper priors estimate motion ﬂow? framework Bayesian inference, answers questions based model selection parameter estimation. field motion perception, work focused question, parameter estimation estimate motion ﬂow. however, Stocker Simoncelli] recently proposed conditioned Bayesian model strong biases precise motion direction estimates arise consequence preceding decision hyposis (left. motion). goal paper provide computational explanation questions Bayesian inference. address question, develop prior models smooth rotation expansion motion. address second, propose human visual system multiple models motion integration motion patterns. visual system decides integration strategy based perceived motion information, choice turn affects estimation motion ﬂow. paper, present computational ory section) includes integration strategies, derived framework. test ory sections) comparing predictions human performance psychophysical experiments, subjects asked discriminate motion direction translational, rotational, expanding stimuli. employ commonly stimuli, random dot patterns moving gratings, show model apply variety inputs. background enormous literature visual motion phenomena room summarize work relevant paper. computational model relates closely work, formulates motion perception Bayesian inference prior probability biasing slowand-smooth motion. psychophysical], physiological, fmri data] suggests humans sensitive variety motion patterns including translation, rotation, expansion. particular, Lee] demonstrated human performance discrimination tasks translation, rotation, expansion motion inconsistent predictions slow-andsmooth ory (our simulations independently verify result). instead, propose human motion perception performed levels inference) model selection) estimating velocity selected model. concept model selection literature], recently applied model motion phenomena]. motion models rotation expansion formulated similarly original slow-andsmooth model] similar mamatical analysis] obtain forms solutions terms Greens functions differential operators priors.  \\x0cmodel Formulation Bayesian Framework formulate motion perception problem Bayesian inference parts. part selects model explains observed motion pattern. part estimates motion properties selected model. velocity field estimated velocity measurements} discrete positions   maximizing prior exp) differs models discussed section. likelihood function exp depends measurement process discussed section. model explains measurement} chosen maximizing model evidence) equivalent maximizing posterior probability model (assuming uniform prior models arg max}) arg max arg max  Priors define priors types motion translation, rotation, expansion. motion type, encourage slowness smoothness. prior translation similar slowand-smooth prior] drop higher-order derivative terms introduce extra parameter ensure models similar degrees freedom). define priors energy functions equation). label models }, denote translation, rotation, expansion respectively.  note prior expansion account contraction). slow-and-smooth-translation ) slow-and-smooth-rotation  ) slow-andsmooth-expansion   models motivated follows.  bias slowness smoothness common models. derivative term differences models. translation model prefers constant translation motion constant, type motion. rotation model prefers rigid rotation expansion, respectively, ideal form       (unknown) centers, angular speed expansion rate. forms motion preferred models since, type motion (rotation (independent ?). similarly, \\x0ctype motion preferred expansion contraction) model (again independent).  translation model similar terms slow-andsmooth energy function] restriction set parameters. formally   . Our computer simulations showed translation model performs   similar slow-and-smooth model.  Likelihood Functions likelihood function differs classes stimuli examined) For moving dot stimuli], information estimate local velocity) For gratings stimuli], information estimate component velocity field. dot stimuli, energy term likelihood function set  For gratings stimuli, likelihood function energy func tion     unit vector direction direction local image gradient.  MAP estimator velocities MAP estimate velocities model obtained solving arg max}, arg min) For slow-and-smooth model], shown regularization analysis] solution expressed terms linear combination Green function differential operator imposes slow-andsmoothness constraint precise form constraint chosen gaussian). obtain similar results types models , introduced paper. main difference models require vector valued Green functions constraint (g1x \\x0cg1y vector-valued Green functions required perform coupling velocity component required rotation expansion, figure). translation model coupling required G1y  top panel, left-right: Figure vector-valued Green function G1x G1x G1x translation, rotation expansion models. bottom panel: left right: translation, rotation, expansion models. observe G2x G2x similar models, vanishes translation model. coupling veloc2x ity components), peaks correspond directions rotation expansion. recall G2x G2y G1x estimated velocity model form   For dot stimuli, {?} obtained solving linear equations  ?ie1 ?ie2    denote (orthogonal) coordinate axes. express {?} -dim vectors vectors define matrices components    , g2x g1y g2y g1x spectively, express linear equations: g1x g2x) g1y g2y Similarly gratings stimuli)      matrix components similarly  Model Selection-express model evidence terms)dadb introduce notation form matrices) g1x g1y g2x g2y similarly  model evidence dot stimuli computed analytically (exploiting properties multidimensional gaussians) obtain exp[?   det) similarly, gratings stimuli obtain: det   exp[?    det(?) )    results random dot motion investigate motion perception moving dots stimuli Freeman Harris], shown figure). stimuli consist 128 moving dots random spatial pattern. dots speed motion patterns, including translation, rotation expansion. simulations select correct model stimulus estimate speed threshold detection type motion. parameter values .001, , .125.0054.  Figure Moving random dot stimuli. left panel: translation; middle panel: rotation; panel: expansion.  Model selection Model selection results shown figure). speed increases range, model evidence decreases models. due slowness term model priors. neverless correct model selected entire range speed, type motion stimuli. 482 482 rotation model expansion model translation model 482 rotation model expansion model translation model 481 rotation model expansion model translation model 481 480 log)) log)) log)) 482 479 480 479 482 478 482 speed 477 478 speed 477 speed Figure Model selection results random dot motion. plots log probability model function speed type stimuli. left: translation stimuli; middle: rotation stimuli; right: expansion stimuli. green curves cross translation model. red curves circles rotation model. blue curves squares expansion model.  Speed threshold Detection reported], humans lower speed threshold detecting rotation/expansion translation motion. experiment formulated model selection task additional ?static? motion prior. ?static? motion prior modeled translation prior  significantly large emphasize slowness. simulation,  ?static? model, .001 models. low speed, ?static? model favored due stronger bias slowness, stimulus speed increases, loses advantage models. speed thresholds detection motion patterns model evidence plots figure), lower rotation/expansion translation. threshold values rotation expansion translation. consistent experimental result]. 488 486 484 482 rotation model expansion model translation model static model 481 481 log)) log)) 481 rotation model expansion model translation model static model 481 480 480 480 480 480 478.102.104.106 Speed 481.108.0502 rotation model expansion model translation model static model log)) 480.0508 Speed threshold 481.0504.0506 Speed 480.0502.0504.0506 Speed.0508 translation rotation expansion Figure Speed threshold detection. upper left panel: model evidence plot translation stimuli. upper panel: model evidence plot rotation stimuli. lower left panel: model eviddence plot expansion stimuli. lower panel: bar graph speed thresholds.  Results randomly oriented gratings Stimuli When randomly oriented grating elements drift apertures, perceived direction motion heavily biased orientation gratings, shape contrast apertures. recently, Nishida colleagues developed global motion stimulus consisting number gratings elements, randomly assigned orientation]. coherent motion perceived drifting velocities elements consistent velocity. examples stimuli psychophysical experiments shown left side figure). stimuli consisted 728 gratings (drifting sine-wave gratings windowed stationary gaussians). orientations gratings randomly assigned, drifting velocities determined global motion ﬂow pattern. motions signal grating elements consistent global motion, motions noise grating elements randomized. task identify global motion direction alternatives: left/right translation, clockwise/counterclockwise rotation, inward/outward expansion. motion sensitivity measured coherence threshold, defined proportion signal elements yielded performance level% correct. similar stimuli 328 gratings generated test computational models. input models velocity component perpendicular assigned orientation grating, illustrated upper panels figure).  Figure randomly-oriented grating stimuli estimated motion ﬂow. upper left panel: rotation stimulus (with% coherence ratio). upper panel: expansion stimulus (with% coherence ratio). lower left panel: motion ﬂow estimated stimulus panel rotation model. lower panel: motion ﬂow estimated stimulus panel expansion model.  Result results psychophysical experiments (middle panel figure showed worse performance perceiving translation rotation/expansion motion]. clearly, shown panel figure, model performs rotation expansion, worst translation. finding agrees human performance psychophysical experiments. conclusion Humans motion sensitivities depend motion patterns (translation/rotation/expansion). propose computational model prior motions compete fit data levels Human Model Coherence Ratio Threshold Coherence Ratio Threshold translation rotation expansion translation rotation expansion Figure Stimulus results. left panel: illustration grating stimulus. blue arrows drifting velocity grating. middle panel: human coherence thresholds motion stimuli. panel: Model prediction coherence thresholds consistent human trends. inference. analysis involves formulating prior models rotation expansion model deriving properties. competitive prior approach good fits empirical data accounts dominant trends reported]. current work aims extend findings range motions. aﬃne motion) increasingly naturalistic appearance/intensity models. important determine motion patterns humans sensitive correspond appearing regularly natural motion sequences. observer moves environment, retinal image time create multiple complex motion ﬂows, including translational, circular radial motion. human observers process motion patterns infer ego motion global structure world. however, inherent ambiguity local motion signals requires visual system employ eﬃcient integration strategy combine local measurements order perceive global motion. psychophysical experiments identified variety phenomena, motion capture motion cooperativity], consequences integration. number computational Bayesian models proposed explain effects based prior assumptions motion. particular, shown slowand-smooth prior, related models, qualitatively account range experimental results] quantitatively account ors]. however, integration strategy modeled slow-and-smooth prior generalize complex motion types, circular radial \\x0cmotion, critically important estimating ego motion. paper concerned questions. ) What integration priors motion input? ) How local motion measurements combined proper priors estimate motion ﬂow? within framework Bayesian inference, answers questions based model selection parameter estimation. field motion perception, work focused question, parameter estimation estimate motion ﬂow. however, Stocker Simoncelli] recently proposed conditioned Bayesian model strong biases precise motion direction estimates arise consequence preceding decision hyposis (left. motion). goal paper provide computational explanation questions Bayesian inference. address question, develop prior models smooth rotation expansion motion. address second, propose human visual system multiple models motion integration motion patterns. visual system decides integration strategy based perceived motion information, choice turn affects estimation motion ﬂow. paper, present computational ory section) includes integration strategies, derived framework. test ory sections) comparing predictions human performance psychophysical experiments, subjects asked discriminate motion direction translational, rotational, expanding stimuli. employ commonly stimuli, random dot patterns moving gratings, show model apply variety inputs. Background enormous literature visual motion phenomena room summarize work relevant paper. our computational model relates closely work, formulates motion perception Bayesian inference prior probability biasing slowand-smooth motion. but psychophysical], physiological, fmri data] suggests humans sensitive variety motion patterns including translation, rotation, expansion. particular, Lee] demonstrated human performance discrimination tasks translation, rotation, expansion motion inconsistent predictions slow-andsmooth ory (our simulations independently verify result). instead, propose human motion perception performed levels inference) model selection) estimating velocity selected model. concept model selection literature], recently applied model motion phenomena]. our motion models rotation expansion formulated similarly original slow-andsmooth model] similar mamatical analysis] obtain forms solutions terms Greens functions differential operators priors.  \\x0cmodel Formulation Bayesian Framework formulate motion perception problem Bayesian inference parts. part selects model explains observed motion pattern. part estimates motion properties selected model. velocity field estimated velocity measurements} discrete positions   maximizing prior exp) differs models discussed section. likelihood function exp depends measurement process discussed section. model explains measurement} chosen maximizing model evidence) equivalent maximizing posterior probability model (assuming uniform prior models arg max}) arg max arg max  Priors define priors types motion translation, rotation, expansion. for motion type, encourage slowness smoothness. prior translation similar slowand-smooth prior] drop higher-order derivative terms introduce extra parameter ensure models similar degrees freedom). define priors energy functions equation). label models }, denote translation, rotation, expansion respectively.  note prior expansion account contraction). slow-and-smooth-translation ) slow-and-smooth-rotation  ) slow-andsmooth-expansion   models motivated follows.  bias slowness smoothness common models. derivative term differences models. translation model prefers constant translation motion constant, type motion. rotation model prefers rigid rotation expansion, respectively, ideal form       (unknown) centers, angular speed expansion rate. forms motion preferred models since, type motion (rotation (independent ?). similarly, \\x0ctype motion preferred expansion contraction) model (again independent).  translation model similar terms slow-andsmooth energy function] restriction set parameters. formally   . Our computer simulations showed translation model performs   similar slow-and-smooth model.  Likelihood Functions likelihood function differs classes stimuli examined) For moving dot stimuli], information estimate local velocity) For gratings stimuli], information estimate component velocity field. for dot stimuli, energy term likelihood function set  For gratings stimuli, likelihood function energy func tion     unit vector direction direction local image gradient.  MAP estimator velocities MAP estimate velocities model obtained solving arg max}, arg min) For slow-and-smooth model], shown regularization analysis] solution expressed terms linear combination Green function differential operator imposes slow-andsmoothness constraint precise form constraint chosen gaussian). obtain similar results types models , introduced paper. main difference models require vector valued Green functions constraint (g1x \\x0cg1y vector-valued Green functions required perform coupling velocity component required rotation expansion, figure). for translation model coupling required G1y  top panel, left-right: Figure vector-valued Green function G1x G1x G1x translation, rotation expansion models. bottom panel: left right: translation, rotation, expansion models. observe G2x G2x similar models, vanishes translation model. coupling veloc2x ity components), peaks correspond directions rotation expansion. recall G2x G2y G1x estimated velocity model form   For dot stimuli, {?} obtained solving linear equations  ?ie1 ?ie2    denote (orthogonal) coordinate axes. express {?} -dim vectors vectors define matrices components    , g2x g1y g2y g1x spectively, express linear equations: g1x g2x) g1y g2y Similarly gratings stimuli)      matrix components similarly  Model Selection-express model evidence terms)dadb introduce notation form matrices) g1x g1y g2x g2y similarly  model evidence dot stimuli computed analytically (exploiting properties multidimensional gaussians) obtain exp[?   det) similarly, gratings stimuli obtain: det   exp[?    det(?) )    Results random dot motion investigate motion perception moving dots stimuli Freeman Harris], shown figure). stimuli consist 128 moving dots random spatial pattern. all dots speed motion patterns, including translation, rotation expansion. our simulations select correct model stimulus estimate speed threshold detection type motion. parameter values .001, , .125.0054.  Figure Moving random dot stimuli. left panel: translation; middle panel: rotation; panel: expansion.  Model selection Model selection results shown figure). speed increases range, model evidence decreases models. this due slowness term model priors. neverless correct model selected entire range speed, type motion stimuli. 482 482 rotation model expansion model translation model 482 rotation model expansion model translation model 481 rotation model expansion model translation model 481 480 log)) log)) log)) 482 479 480 479 482 478 482 speed 477 478 speed 477 speed Figure Model selection results random dot motion. plots log probability model function speed type stimuli. left: translation stimuli; middle: rotation stimuli; right: expansion stimuli. green curves cross translation model. red curves circles rotation model. blue curves squares expansion model.  Speed threshold Detection reported], humans lower speed threshold detecting rotation/expansion translation motion. experiment formulated model selection task additional ?static? motion prior. ?static? motion prior modeled translation prior  significantly large emphasize slowness. simulation,  ?static? model, .001 models. low speed, ?static? model favored due stronger bias slowness, stimulus speed increases, loses advantage models. speed thresholds detection motion patterns model evidence plots figure), lower rotation/expansion translation. threshold values rotation expansion translation. this consistent experimental result]. 488 486 484 482 rotation model expansion model translation model static model 481 481 log)) log)) 481 rotation model expansion model translation model static model 481 480 480 480 480 480 478.102.104.106 Speed 481.108.0502 rotation model expansion model translation model static model log)) 480.0508 Speed threshold 481.0504.0506 Speed 480.0502.0504.0506 Speed.0508 translation rotation expansion Figure Speed threshold detection. upper left panel: model evidence plot translation stimuli. upper panel: model evidence plot rotation stimuli. lower left panel: model eviddence plot expansion stimuli. lower panel: bar graph speed thresholds.  Results randomly oriented gratings Stimuli When randomly oriented grating elements drift apertures, perceived direction motion heavily biased orientation gratings, shape contrast apertures. recently, Nishida colleagues developed global motion stimulus consisting number gratings elements, randomly assigned orientation]. coherent motion perceived drifting velocities elements consistent velocity. examples stimuli psychophysical experiments shown left side figure). stimuli consisted 728 gratings (drifting sine-wave gratings windowed stationary gaussians). orientations gratings randomly assigned, drifting velocities determined global motion ﬂow pattern. motions signal grating elements consistent global motion, motions noise grating elements randomized. task identify global motion direction alternatives: left/right translation, clockwise/counterclockwise rotation, inward/outward expansion. motion sensitivity measured coherence threshold, defined proportion signal elements yielded performance level% correct. similar stimuli 328 gratings generated test computational models. input models velocity component perpendicular assigned orientation grating, illustrated upper panels figure).  Figure randomly-oriented grating stimuli estimated motion ﬂow. upper left panel: rotation stimulus (with% coherence ratio). upper panel: expansion stimulus (with% coherence ratio). lower left panel: motion ﬂow estimated stimulus panel rotation model. lower panel: motion ﬂow estimated stimulus panel expansion model.  Result results psychophysical experiments (middle panel figure showed worse performance perceiving translation rotation/expansion motion]. clearly, shown panel figure, model performs rotation expansion, worst translation. this finding agrees human performance psychophysical experiments. Conclusion Humans motion sensitivities depend motion patterns (translation/rotation/expansion). propose computational model prior motions compete fit data levels Human Model Coherence Ratio Threshold Coherence Ratio Threshold translation rotation expansion translation rotation expansion Figure Stimulus results. left panel: illustration grating stimulus. blue arrows drifting velocity grating. middle panel: human coherence thresholds motion stimuli. right panel: Model prediction coherence thresholds consistent human trends. inference. this analysis involves formulating prior models rotation expansion model deriving properties. this competitive prior approach good fits empirical data accounts dominant trends reported]. our current work aims extend findings range motions. aﬃne motion) increasingly naturalistic appearance/intensity models. important determine motion patterns humans sensitive correspond appearing regularly natural motion sequences.',\n",
       " 'PP3466': ' reproducing kernel Hilbert space (rkhs) kernel  training set    -insensitive SVM proposed Vapnik-workers] regression tasks finds unique minimizer,?  regularized empirical risk k2h denotes -insensitive loss defined, max,   fixed  known. , Proposition], solution form,?  ?   coeﬃcients? solution optimization problem maxi mize  )     ) Here set). note equality constraint needed, Proposition] superﬂuous include offset term primal problem). following, write,? ? set indices belong support vectors,?  furrmore, write counting measure,? denotes number support vectors,?  subject obvious,? crucial inﬂuence time needed compute,? ). due fact, -insensitive loss originally motivated goal achieve sparse decision functions., decision functions,? ,? although empirically well-known insensitive SVM achieves sparsity, far, oretical explanation sense]. goal work provide explanation establishing asymptotically tight lower upper bounds number support vectors. based bounds investigate trade-off sparsity estimation accuracy -insensitive svm. main results Before formulate main results introduce notations. end, probability measure measurable space. measurable define -risk)). moreover, recall split marginal distribution regular conditional probability ). RKHS bounded kernel] showed,? arg inf k2h exists uniquely determined) write ) Dirac measure,  empirical measure  training set     ,? solution). finally, introduce sets?low,  )  ,  )     arbitrary function  moreover, short forms Alow A0low Aup A0up formulate main result. orem Let probability measure separable RKHS bounded measurable kernel satisfying kkk?       satisfying   ,?   ?low,?     ?  ?  ,?   ,?    ?  ?  before present main result, brieﬂy illustrate orem case fix regularization parameter   corollary Let probability measure separable RKHS bounded measurable kernel satisfying kkk?     ,?  aup,?   lim   Alow,?    ?? note corollary describes asymptotic behavior fraction support vectors modulo probability set Aup,? )alow,? ,? )   ,? )  particular, conditional distributions ), discrete components, corollary exact description. course, situation realistic assume stays fixed sample size grows. instead, well-known], regularization parameter vanish order achieve consistency. investigate case, introduce additional notations] related -risk. begin denoting Bayes -risk inf distribution infimum measurable functions addition, distribution, Chapter defined -risks) minimal -risks denoted? inf). obviously( ) dpx, Lemma, Lemma], furr established intuitive formula dpx). moreover, sets conditional minimizers( ) ) ( ? ( ) lemma collects properties sets. lemma Let probability measure   ) nonempty compact interval -almost  function lemma shows -almost exists unique )  ) ) )  )  ) ) words, ) element ) smallest distance). following, write?? ) ,? emphasize dependence ) elements, finally introduce sets mlow,  ? )   mup,  ? )      moreover, short forms Mlow Mlow Mup Mup formulate main result. orem Let probability measure separable RKHS bounded measurable kernel satisfying kkk?   assume) dense  exist    ,  ,?    Mlow,?     mup,?    ???   choose sequence regularization parameters  resulting SVM -risk consistent assumptions orem]. case, obvious corollary orem establishes lower upper bounds number support vectors. corollary Let probability measure separable RKHS bounded measurable kernel satisfying kkk?   assume) dense furrmore , sequence    probability   satisfying lim inf Mlow    lim Mup ?? ?? converges  general, probabilities sets Mlow,? mup,? hard control since., fixed   diﬃcult show,? ) ?ﬂipping? left hand side ) hand side. indeed, general ), ﬂipping give values?? )  )  result significantly sets Mlow,? mup,?  consequence, hard show that, probability measures conditional distributions ), discrete components, lim inf Mlow,? lim Mup,?  )  however, situations equality easily established. example, assume sets ) -almost surely singletons. case?? ) fact independent Mlow,? mup,?  namely, case sets pairs, contained closed open -tube ), respectively. consequently) holds provided conditional distributions ), discrete components, Corollary tight bound number support vectors. moreover, case additionally assume., absolute loss, easily find(mlow,? (mup,? corollary shows SVM tend produce sparse decision functions. finally, recall specific loss function, ) equals median ), ) singleton median ) unique. illustrate Corollary end, assume conditional distributions ) symmetric., -almost exists conditional center) ) ) measurable note, easy) median ). furrmore, assumption) imposed results ensures conditional? ) ) exists -almost surely, easy conclude? ) -almost  moreover, Proposition Lemma] immediately obtain lemma. lemma Let probability measure ) assume conditional distributions ), symmetric -almost exists )  , ? )   )   ) -almost  ? ? ) equals -almost surely unique median ). obviously, condition) means conditional distributions mass median? ) means conditional distributions mass?   moreover] showed assumptions Lemma, -insensitive SVM estimate conditional median. illustrate inﬂuences accuracy estimate sparsity. end, assume sake simplicity conditional distributions ) continuous Lebesgue densities ) , ?). symmetry conditional distributions easy densities symmetric? ). now, continuity densities) satisfied? ) satisfied? ) case conditional distributions equal modulo translations. words, assume exists continuous Lebesgue density , symmetric -almost ? ) note assumption essentially identical classical ?signal noise? assumption. furr assume unimodal., local global maximum easily) satisfied) satisfied() lemma discussion) conclude assumptions Corollary fraction support vectors asymptotically approaches([, )), probability measure defined confirms intuition larger values lead sparser decision functions. particular([, ?)) svm produces super sparse decision functions., decision functions number support vectors grow linearly sample size. however, surprisingly, price paid sparsity. indeed, Lemma] size() direct inﬂuence ability,? estimate conditional median?  describe detail. end, find, Lemma] convexity ) , )  ()    literal repetition proof, orem] find self-calibration inequality ? kl1 () ) holds   . now, situation Corollary  probability ) shows approximates conditional median? respect )-norm. however, guarantee approximation worse smaller() becomes., larger. words, sparsity decision functions paid accurate estimates conditional median. hand, results show moderate values lead reasonable estimates conditional median sparse decision functions. regard furr note, Lemma] establish selfcalibration inequalities measure distance?  case, however, obvious self-calibration inequalities worse larger, informal conclusions remain unchanged. finally, mention that, conditional distributions equal modulo translations, situation involved. particular situation? ? ) inf? ) inf? ) selfcalibration inequalities form) general impossible, weaker self-calibration inequalities require additional assumptions refer] case considered. proofs Setting introducing slack variables, restate optimization problem k2h minimize     subject     denote (unique) solution)   ??? note ,?  well-known. , 117], dual optimization problem) maximize    subject    )(?      )     denotes solution kernel RKHS furr?     ), recover more?      primal solution    ?  ?  ? ??? max,   ) max,   )   moreover, karush-kuhn-tucker conditions?     ?        ? ?  ?  ?  ?   ??? ?    )   finally, note setting   problem) simplified), consequently, solution  ) form        simple lemma lower upper bounds set support vectors. lemma Using notations,?   ?  ,?    proof: Let prove inclusion left hand side. end, begin fixing index,?   ,?  ), find? ) implies? from) conclude   ?     ?  case ,?  shown analogously, obtain inclusion. order show inclusion fix index?  ?     ? ) eir?     case?     kkt condition) toger,?  implies,?   ? ?  ,?    case   shown analogously. furr Hilbert space version hoeffding inequality, Chapter, Chapter] slightly sharper inequality. orem Let (?, probability space separable Hilbert space. moreover,     independent random variables satisfying            ??   finally, orem, Corollary], essentially shown, orem Let probability measure separable RKHS bounded measurable kernel satisfying kkk?   write  canonical feature map., ) ),   exists function  ,    kfd,?  ,?   ked   denotes empirical average respect Proof orem: order show estimate fix        implies   combining orems obtain ??     ked        kfd,?  ,?    ) Let assume training set   kfp,?  ,?   pair, ?low,?  ,? )  ,? ) ,? ) ,? )— ,? )  triangle inequality kkk?  implies     words?low,?  alow,?  consequently, Lemma yields,?  ,?   ,?   ?low,?   Combining estimate) obtain ,?  ?low,?   ?     moreover, hoeffding inequality, see. , orem], shows  ?low,? ?low,?     ?    estimates union bound conclude inequality. order show estimate observe training sets   kfp,?  ,?   aup,?  ,?  lemma shows,?  ,? ) yields ,?    ,?   ?   Using hoeffding inequality analogously proof estimate obtain estimate. proof Corollary: observe?low,?  ?low,?     show?low,? alow,?   obviously, inclusion ??? directly monotonicity. conversely, alow,? ) )   ., shown, ?low,?  ) conclude lim?low,? alow,?   addition,?  ,?    easy check,? aup,?   indeed, ,?  )     conclude)  . , aup,?  conversely, inclusion ??? directly monotonicity sets Aup ) conclude lim,? aup,?   Let fix decreasing sequence ,   combining) estimates orem, obtain assertion. proof Lemma: Since loss function Lipschitz continuous convex easy verify ( ) Lipschitz continuous convex -almost  ) closed interval. order prove remaining assertions suﬃces show limt??? ( ) -almost  end, observe implies? ( ) -almost  fix sequence  ??. shape exists,     furrmore, exists) ,  furr exists     ,  finally find(  )   case  shown analogously. proof orem intermediate results. orem Let probability measure separable RKHS bounded measurable kernel satisfying kkk?   assume) dense   exists  , ,? )    )  proof: Since dense inf, orem], lim? ,?  obtain assertion, orem]. lemma Let probability measure separable RKHS bounded measurable kernel satisfying kkk?   assume) dense   exists  ,  mlow,?  ?low,?  mup,?  ,?    proof: write?? ) real number defined,? ).   mlow,?  mlow,?  ,  ,? ) ?? )—   ,  ,? ) )— )  )  moreover, mlow,?  ,  ,? ) ?? )—  find  ? )  ,? ) ?? ,? )   ,? ) , ?low,?  estimating probability remaining set orem yields assertion. order prove estimate observe,?  ,?  ,  ,? ) ?? )—   ,  ,? ) )— )  ) , ,?  ,  ,? ) ?? )—  furr   ,? )  ,? ) ?? ?? )   ?? )  ,?  again, assertion orem , mup Proof orem: Analogously proofs), find  lim Mlow,? mlow,? lim Mup,? mup,?  Combining equations orem Lemma, obtain assertion.  given reproducing kernel Hilbert space (rkhs) kernel  training set    -insensitive SVM proposed Vapnik-workers] regression tasks finds unique minimizer,?  regularized empirical risk k2h denotes -insensitive loss defined, max,   fixed  known. , Proposition], solution form,?  ?   coeﬃcients? solution optimization problem maxi mize  )     ) Here set). note equality constraint needed, Proposition] superﬂuous include offset term primal problem). following, write,? ? set indices belong support vectors,?  furrmore, write counting measure,? denotes number support vectors,?  subject obvious,? crucial inﬂuence time needed compute,? ). due fact, -insensitive loss originally motivated goal achieve sparse decision functions., decision functions,? ,? Although empirically well-known insensitive SVM achieves sparsity, far, oretical explanation sense]. goal work provide explanation establishing asymptotically tight lower upper bounds number support vectors. based bounds investigate trade-off sparsity estimation accuracy -insensitive svm. Main results Before formulate main results introduce notations. end, probability measure measurable space. given measurable define -risk)). moreover, recall split marginal distribution regular conditional probability ). given RKHS bounded kernel] showed,? arg inf k2h exists uniquely determined) let write ) Dirac measure,  empirical measure  training set     ,? solution). finally, introduce sets?low,  )  ,  )     arbitrary function  moreover, short forms Alow A0low Aup A0up now formulate main result. orem Let probability measure separable RKHS bounded measurable kernel satisfying kkk?       satisfying   ,?   ?low,?     ?  ?  ,?   ,?    ?  ?  Before present main result, brieﬂy illustrate orem case fix regularization parameter   corollary Let probability measure separable RKHS bounded measurable kernel satisfying kkk?     ,?  Aup,?   lim   Alow,?    ?? Note corollary describes asymptotic behavior fraction support vectors modulo probability set Aup,? )alow,? ,? )   ,? )  particular, conditional distributions ), discrete components, corollary exact description. course, situation realistic assume stays fixed sample size grows. instead, well-known], regularization parameter vanish order achieve consistency. investigate case, introduce additional notations] related -risk. let begin denoting Bayes -risk inf distribution infimum measurable functions addition, distribution, Chapter defined -risks) minimal -risks denoted? inf). obviously( ) dpx, Lemma, Lemma], furr established intuitive formula dpx). moreover, sets conditional minimizers( ) ) ( ? ( ) lemma collects properties sets. lemma Let probability measure   ) nonempty compact interval -almost  given function Lemma shows -almost exists unique )  ) ) )  )  ) ) words, ) element ) smallest distance). following, write?? ) ,? emphasize dependence ) with elements, finally introduce sets mlow,  ? )   mup,  ? )      moreover, short forms Mlow Mlow Mup Mup now formulate main result. orem Let probability measure separable RKHS bounded measurable kernel satisfying kkk?   assume) dense  exist    ,  ,?    Mlow,?     Mup,?    ???   choose sequence regularization parameters  resulting SVM -risk consistent assumptions orem]. for case, obvious corollary orem establishes lower upper bounds number support vectors. corollary Let probability measure separable RKHS bounded measurable kernel satisfying kkk?   assume) dense furrmore , sequence   for probability   satisfying lim inf Mlow    lim Mup ?? ?? converges  general, probabilities sets Mlow,? Mup,? hard control since., fixed   diﬃcult show,? ) ?ﬂipping? left hand side ) hand side. indeed, general ), ﬂipping give values?? )  )  result significantly sets Mlow,? Mup,?  consequence, hard show that, probability measures conditional distributions ), discrete components, lim inf Mlow,? lim Mup,?  )  however, situations equality easily established. for example, assume sets ) -almost surely singletons. case?? ) fact independent Mlow,? Mup,?  namely, case sets pairs, contained closed open -tube ), respectively. consequently) holds provided conditional distributions ), discrete components, Corollary tight bound number support vectors. moreover, case additionally assume., absolute loss, easily find(mlow,? (mup,? Corollary shows SVM tend produce sparse decision functions. finally, recall specific loss function, ) equals median ), ) singleton median ) unique. let illustrate Corollary end, assume conditional distributions ) symmetric., -almost exists conditional center) ) ) measurable Note, easy) median ). furrmore, assumption) imposed results ensures conditional? ) ) exists -almost surely, easy conclude? ) -almost  moreover, Proposition Lemma] immediately obtain lemma. lemma Let probability measure ) assume conditional distributions ), symmetric -almost exists )  , ? )   )   ) -almost  ? ? ) equals -almost surely unique median ). obviously, condition) means conditional distributions mass median? ) means conditional distributions mass?   moreover] showed assumptions Lemma, -insensitive SVM estimate conditional median. let illustrate inﬂuences accuracy estimate sparsity. end, assume sake simplicity conditional distributions ) continuous Lebesgue densities ) , ?). symmetry conditional distributions easy densities symmetric? ). now, continuity densities) satisfied? ) satisfied? ) let case conditional distributions equal modulo translations. words, assume exists continuous Lebesgue density , symmetric -almost ? ) Note assumption essentially identical classical ?signal noise? assumption. furr assume unimodal., local global maximum from easily) satisfied) satisfied() Lemma discussion) conclude assumptions Corollary fraction support vectors asymptotically approaches([, )), probability measure defined this confirms intuition larger values lead sparser decision functions. particular([, ?)) SVM produces super sparse decision functions., decision functions number support vectors grow linearly sample size. however, surprisingly, price paid sparsity. indeed, Lemma] size() direct inﬂuence ability,? estimate conditional median?  let describe detail. end, find, Lemma] convexity ) , )  ()    literal repetition proof, orem] find self-calibration inequality ? kl1 () ) holds   . now, situation Corollary  probability ) shows approximates conditional median? respect )-norm. however, guarantee approximation worse smaller() becomes., larger. words, sparsity decision functions paid accurate estimates conditional median. hand, results show moderate values lead reasonable estimates conditional median sparse decision functions. regard furr note, Lemma] establish selfcalibration inequalities measure distance?  case, however, obvious self-calibration inequalities worse larger, informal conclusions remain unchanged. finally, mention that, conditional distributions equal modulo translations, situation involved. particular situation? ? ) inf? ) inf? ) selfcalibration inequalities form) general impossible, weaker self-calibration inequalities require additional assumptions refer] case considered. Proofs Setting introducing slack variables, restate optimization problem k2h minimize     subject     denote (unique) solution)   ??? note ,?  well-known. , 117], dual optimization problem) maximize    subject    )(?      )     denotes solution kernel RKHS furr?     ), recover more?      primal solution    ?  ?  ? ??? max,   ) max,   )   moreover, karush-kuhn-tucker conditions?     ?        ? ?  ?  ?  ?   ??? ?    )   finally, note setting   problem) simplified), consequently, solution  ) form        simple lemma lower upper bounds set support vectors. lemma Using notations,?   ?  ,?    proof: Let prove inclusion left hand side. end, begin fixing index,?   ,?  ), find? ) implies? From) conclude   ?     ?  case ,?  shown analogously, obtain inclusion. order show inclusion fix index?  ?     ? ) eir?     let case?     KKT condition) toger,?  implies,?   ? ?  ,?    case   shown analogously. furr Hilbert space version hoeffding inequality, Chapter, Chapter] slightly sharper inequality. orem Let (?, probability space separable Hilbert space. moreover,     independent random variables satisfying            ??   finally, orem, Corollary], essentially shown, orem Let probability measure separable RKHS bounded measurable kernel satisfying kkk?   write  canonical feature map., ) ),   exists function  ,    kfd,?  ,?   ked   denotes empirical average respect Proof orem: order show estimate fix     let   implies   combining orems obtain ??     ked        kfd,?  ,?    ) Let assume training set   kfp,?  ,?   given pair, ?low,?  ,? )  ,? ) ,? ) ,? )— ,? )  triangle inequality kkk?  implies     words?low,?  alow,?  consequently, Lemma yields,?  ,?   ,?   ?low,?   Combining estimate) obtain ,?  ?low,?   ?     moreover, hoeffding inequality, see. , orem], shows  ?low,? ?low,?     ?    from estimates union bound conclude inequality. order show estimate observe training sets   kfp,?  ,?   Aup,?  ,?  lemma shows,?  ,? ) yields ,?    ,?   ?   Using hoeffding inequality analogously proof estimate obtain estimate. Proof Corollary: observe?low,?  ?low,?     let show?low,? Alow,?   obviously, inclusion ??? directly monotonicity. conversely, alow,? ) )   ., shown, ?low,?  from) conclude lim?low,? Alow,?   addition,?  ,?    easy check,? Aup,?   indeed, ,?  )     conclude)  . , aup,?  conversely, inclusion ??? directly monotonicity sets Aup from) conclude lim,? Aup,?   Let fix decreasing sequence ,   combining) estimates orem, obtain assertion. proof Lemma: Since loss function Lipschitz continuous convex easy verify ( ) Lipschitz continuous convex -almost  ) closed interval. order prove remaining assertions suﬃces show limt??? ( ) -almost  end, observe implies? ( ) -almost  let fix sequence  ??. shape exists,     furrmore, exists) ,  furr exists    for ,  finally find(  )   case  shown analogously. for proof orem intermediate results. orem Let probability measure separable RKHS bounded measurable kernel satisfying kkk?   assume) dense   exists  , ,? )    )  proof: Since dense inf, orem], lim? ,?  now obtain assertion, orem]. lemma Let probability measure separable RKHS bounded measurable kernel satisfying kkk?   assume) dense   exists  ,  Mlow,?  ?low,?  Mup,?  ,?    proof: write?? ) real number defined,? ).   mlow,?  mlow,?  ,  ,? ) ?? )—   ,  ,? ) )— )  )  moreover, mlow,?  ,  ,? ) ?? )—  find  ? )  ,? ) ?? ,? )   ,? ) , ?low,?  estimating probability remaining set orem yields assertion. order prove estimate observe,?  ,?  ,  ,? ) ?? )—   ,  ,? ) )— )  ) for, ,?  ,  ,? ) ?? )—  furr   ,? )  ,? ) ?? ?? )   ?? )  ,?  again, assertion orem , mup Proof orem: Analogously proofs), find  lim Mlow,? Mlow,? lim Mup,? Mup,?  Combining equations orem Lemma, obtain assertion.',\n",
       " 'PP3506': 'present approach low-level vision combines main ideas: convolutional networks image processing architecture unsupervised learning procedure synsizes training samples specific noise models. demonstrate approach challenging problem natural image denoising. test set hundred natural images, find convolutional networks provide comparable cases superior performance state art wavelet Markov random field (mrf) methods. moreover, find convolutional network offers similar performance blind denoising setting compared techniques non-blind \\x0csetting. show convolutional networks mamatically related MRF approaches presenting field ory MRF specially designed image denoising. approaches related, convolutional networks avoid computational diﬃculties MRF approaches arise probabilistic learning inference. makes learn image processing architectures high degree representational power train models,000 parameters), computational expense significantly inference MRF approaches hundreds parameters. background low-level image processing tasks include edge detection, interpolation, deconvolution. tasks mselves, front-end high-level visual tasks object recognition. paper focuses task denoising, defined recovery underlying image observation subjected Gaussian noise. approach image denoising transform image pixel intensities anor representation statistical regularities easily captured. example, Gaussian scale mixture (gsm) model introduced Portilla colleagues based multiscale wavelet decomposition effective description local image statistics]. anor approach capture statistical regularities pixel intensities directly Markov random fields (mrfs) define prior image space. initial work handdesigned settings parameters, recently increasing success learning parameters models databases natural images]. prior models tasks image denoising augmenting prior noise model. alternatively, MRF model probability distribution clean image conditioned noisy image. conditional random field (crf) approach discriminative, contrast generative MRF approach. researchers shown CRF approach outperform generative learning image restoration labeling tasks]. crfs recently applied problem image denoising]. present work closely related CRF approach. indeed, special cases convolutional networks performing maximum likelihood inference CRF]. advantage convolutional network approach avoids general diﬃculty applying MRFbased methods image analysis: computational expense parameter estimation inference probabilistic models. example, naive methods learning MRFbased models involve calculation partition function, normalization factor generally intractable realistic models image dimensions. result, great deal research devoted approximate MRF learning inference techniques meliorate computational diﬃculties, generally cost eir representational power oretical guarantees]. convolutional networks largely avoid diﬃculties posing computational task statistical framework regression rar density estimation. regression tractable computation refore permits models greater representational power methods based density estimation. claim argued empirical results denoising problem, mamatical connections MRF convolutional network approaches. Convolutional Networks Convolutional networks extensively applied visual object recognition architectures accept image input and, alternating layers convolution subsampling, produce output values thresholded yield binary predictions object identity]. contrast, study networks accept image input produce entire image output. previous work architectures produce images binary targets image restoration problems specialized microscopy data]. show similar architectures produce images analog ﬂuctuations found intensity distributions natural images. network Dynamics Architecture convolutional network alternating sequence linear filtering nonlinear transformation operations. input output layers include images, intermediate layers ?hidden” units images called feature maps internal computations algorithm. activity feature map layer    feature maps provide input denotes convolution operation. function sigmoid bias parameter. restrict experiments monochrome images networks single image input layer. straightforward extend approach color images assuming input layer multiple images., RGB color channels). numerical reasons, preferable input target values range-bit integer intensity values dataset (values 255) normalized lie explicitly encode border image padding area surrounding image values. learning Denoise Parameter learning performed modification backpropagation algorithm feedfoward neural networks takes account weight-sharing structure convolutional networks]. however, issues addressed order learn architecture Figure task natural image denoising. firstly, image denoising task formulated learning problem order train convolutional network. assume access database clean, noiseless images, implicitly desired image processing task integrating noise process training procedure. particular, assume noise process) operates image drawn distribution natural images entire convolutional network function Architecture CN1 CN2 input image             output image Figure Architecture convolutional network denoising. network hidden layers feature maps hidden layer. layers feature map connected randomly chosen feature maps previous layer. arrow represents single convolution filter, network,697 free parameters requires 624 convolutions process forward pass.  free parameters parameter estimation problem minimize reconstruction error images subject noise process: min?    secondly, ineﬃcient batch learning context. training sets experiments millions pixels, practical perform forward backward pass entire training set gradient learning requires tens thousands updates converge reasonable solution. stochastic online gradient learning eﬃcient learning procedure adapted problem. typically, procedure selects small number independent examples training set averages toger gradients perform single update. compute gradient update patches randomly sampled images training set. localized image patch violates independence assumption stochastic online learning, combining gradient separate images yields  cube practice suﬃcient approximation gradient effective. larger patches  ) reduce correlations training sample improve accuracy. scheme eﬃcient computation local patch shared. found training time minimized generalization accuracy maximized incrementally learning layer weights. greedy, layer-wise training strategies recently explored context unsupervised initialization multi-layer networks, fine tuned discriminative task cost function]. maintain cost function throughout. procedure starts training network single hidden layer. thirty epochs, weights hidden layer copied network hidden layers; weights connecting hidden layer output layer discarded. hidden layer network optimized anor thirty epochs, procedure repeated layers. finally, learning networks hidden layers important small learning rate final layer.001) larger learning rate) layers. implementation Convolutional network inference learning implemented lines MATLAB code multi-dimensional convolution crosscorrelation routines. makes approach easy optimize parallel computing GPU computing strategies. experiments derive training test sets experiments natural images Berkeley segmentation database, previously study denoising]. restrict experiments case monochrome images; color images Berkeley dataset converted grayscale averaging color channels. test set consists 100 images, dimensions 321 481 dimensions 481 321. quantitative comparisons performed Peak Signal Denoising Performance Comparison FoE bls?gsm bls?gsm CN1 CN2 CNBlind Average PSNR Denoised Images Noise 100 Figure Denoising results measured peak signal noise ratio (psnr) noise levels. case, results average denoised PSNR hundred images test set. cn1 CNBlind learned forty image training set Field Experts model (foe). cn2 learned training set additional sixty images. bls-gsm1 bls-gsm2 parameter settings algorithm]. methods CNBlind assume noise distribution. noise Ratio (psnr): log10 (255 standard deviation error. psnr widely evaluate denoising performance]. denoising noise conditions task assumed images subjected Gaussian noise variance. noise model training process learn five-layer network noise level. Bayes Least squares-gaussian Scale Mixture (blsgsm) Field Experts (foe) method optimize denoising process based noise level. learn sets networks task differ training set. set networks, refer cn1, training set subset Berkeley database learn anor set networks, called cn2, training set FoE model]. augmented additional sixty images Berkeley database. architecture networks shown fig.  quantitative results networks noise levels shown fig. results FoE bls-gsm method (bls-gsm settings] bls-gsm default settings code provided authors). FoE results, number iterations magnitude step size optimized noise level grid search training set. visual comparison results shown fig.  find convolutional network highest average PSNR eir training set, margin statistical insignificance standard error computed distribution PSNR values entire image. however, conservative estimate standard error, smaller measured pixel patch-wise basis. blind denoising task assumed images subjected Gaussian noise unknown variance. denoising context diﬃcult problem non-blind situation. train single six-layer network network refer CNBlind randomly varying amount noise added training process, range , 100] inference, noise level unknown image provided input. training set FoE model cn1. architecture shown fig. hidden layers results noise levels shown fig.  find convolutional network trained blind denoising performs compared methods non-blind conditions. fig. show filters learned network. CLEAN CLEAN CN2 bls-gsm FoE NOISY psnr CN2 psnr bls-gsm psnr FoE psnr Figure Denoising results image test set. noisy image generated adding Gaussian noise clean image. non-blind denoising results bls-gsm, foe, convolutional network methods shown. lower left panel shows results outlined region upper left panel. zoomed region shows areas CN2 output severe artifacts wavelet-based results sharper FoE results. cn1 results (psnr) visually similar cn2. relationship MRF Convolutional Network Approaches introduction, claim convolutional networks similar greater representational power compared mrfs. support claim show special cases convolutional networks correspond field inference mrf. rigorously prove convolutional networks representational power greater equal mrfs, field inference approximation. however, plausible case. previous work pointed Field Experts MRF interpreted convolutional network (see]) MRFs Isinglike prior related convolutional networks (see]). here, analyze MRF specially designed image denoising show closely related convolutional network Figure particular, MRF defines distribution analog ?visible? variables binary ?hidden? variables, exp      ) iab correspond ith pixel location image, partition function, standard deviation Gaussian noise. note symmetry Layer Layer Figure Filters learned hidden layers network cnblind. hidden layer 192 filters feature maps filters map). layer recognizable structure filters, including derivative filters high frequency filters similar learned FoE model]. assume w0aa interaction model case, transfer term linear hai lead additional bias term field approximation). hence, constitutes undirected graphical model conceptualized separate layers visible hidden variables. intralayer interactions visible layer convolutional structure (instead full connectivity) intralayer interactions hidden variables interlayer interactions visible hidden layer. definition conditional distribution, exp Gaussian equal conditional expectation]. model denoising fixing visible variables noisy image, computing hidden variables MAP inference, conditional expectation denoised image. inference calculate maxh), diﬃcult partition function. however, field approximation solved equation dynamics iterating. compare. find equivalent convolutional network hidden layer weights feature map directly receives input image. results suggest convolutional networks interpreted performing approximate inference MRF models designed denoising. practice, convolutional network architectures train related MRF models weights hidden layer constrained same, image input feature map layer. interesting question future research additional architectural constraints affect performance convolutional network approach. finally, special case non-blind Gaussian denoising direct integration noise model MRF equations, empirical results blind denoising suggest convolutional network approach adaptable general complex noise models implicitly learning cost function. discussion Prior versus learned structure Before learning, convolutional network structure specialized natural images. contrast, GSM model multi-scale wavelet representation suitability representing natural image statistics. moreover, inference FoE model procedure similar non-linear diffusion methods, previously natural image processing learning. architecture FoE MRF chosen random settings free parameters provide impressive performance]. random parameter settings convolutional networks produce computation. parameters CN2 randomized layer, denoising performance image fig. drops psnr. random parameters layers yields worse results. consistent idea cn2 representation specialized natural images training, localized receptive field structure convolutions. approach relies gradient learning algorithm tune thousands parameters examples natural images. assume approach require vastly training data methods prior structure. however, obtain good generalization performance training set learn Field Experts model, fewer degrees freedom. disadvantage approach produces architecture performance diﬃcult understand due numerous free parameters. advantage approach lead accurate performance, applied forms imagery statistics natural images previously studied dataset specialized image restoration problem studied]). network architecture image context amount image context convolutional network produce output specific image location determined number layers network size filter layer. example-layer networks explored  image patch. small amount context compared FoE BLSGSM models, permit correlations extend entire image. surprising major difference, convolutional network approach good performance. explanation scale objects chosen image dataset relevant information captured small field view. noneless, interest denoising applications increase amount context network. simple strategy furr increase number layers; however, computationally intensive ineﬃcient exploit multi-scale properties natural images. adding additional machinery network architecture work better. integrating operations sub-sampling super-sampling network process image multiple scales, amenable gradient learning. computational eﬃciency With free parameters, convolutional networks computationally expensive image processing architecture. contrary-layer CN1 CN2 architecture (fig. requires 624 image convolutions process image. comparison, FoE model performs inference means dynamic process require thousand iterations. one-thousand iterations dynamics requires,000 convolutions (for FoE model filters). report wall-clock speed denoising 512 512 pixel image.16ghz Intel Core processor. averaged trials, cn1/cn2 requires  sec,000 iterations FoE requires 1664  sec. (using code authors]), bls-gsm model parameter settings? requires  sec., parameter setting? requires  sec. (using code authors]). implementations matlab. true, however, training convolutional network architecture requires substantial computation. gradient learning require thousands updates converge, training denoising networks required parallel implementation utilized dozen processors week. significant amount computation, performed off-line. learning complex image transformations generalized image attractors models work explored image processing task easily formulated learning problem synsizing training examples abundantly noiseless natural images. approach extended tasks noise model variable complex form? results blind denoising, amount \\x0cnoise vary severe, evidence can. preliminary experiments image inpainting encouraging. said, major virtue image prior approach ability easily reuse single image model situations simply augmenting prior observation model. image prior observation model decoupled. explicit probabilistic modeling computationally diﬃcult makes learning simple models challenging. convolutional networks forgo probabilistic modeling and, developed here, focus specific image image transformations regression problem. interesting combine approaches learn models ?unnormalized priors? sense energy-based image attractors; regression tool unsupervised learning capturing dependencies variables distribution]. acknowledgements: grateful Ted adelson, liu, Srinivas turaga, Yair Weiss helpful discussions. authors] making code available. present approach low-level vision combines main ideas: convolutional networks image processing architecture unsupervised learning procedure synsizes training samples specific noise models. demonstrate approach challenging problem natural image denoising. using test set hundred natural images, find convolutional networks provide comparable cases superior performance state art wavelet Markov random field (mrf) methods. moreover, find convolutional network offers similar performance blind denoising setting compared techniques non-blind \\x0csetting. show convolutional networks mamatically related MRF approaches presenting field ory MRF specially designed image denoising. although approaches related, convolutional networks avoid computational diﬃculties MRF approaches arise probabilistic learning inference. this makes learn image processing architectures high degree representational power train models,000 parameters), computational expense significantly inference MRF approaches hundreds parameters. Background low-level image processing tasks include edge detection, interpolation, deconvolution. tasks mselves, front-end high-level visual tasks object recognition. this paper focuses task denoising, defined recovery underlying image observation subjected Gaussian noise. one approach image denoising transform image pixel intensities anor representation statistical regularities easily captured. for example, Gaussian scale mixture (gsm) model introduced Portilla colleagues based multiscale wavelet decomposition effective description local image statistics]. anor approach capture statistical regularities pixel intensities directly Markov random fields (mrfs) define prior image space. initial work handdesigned settings parameters, recently increasing success learning parameters models databases natural images]. prior models tasks image denoising augmenting prior noise model. alternatively, MRF model probability distribution clean image conditioned noisy image. this conditional random field (crf) approach discriminative, contrast generative MRF approach. several researchers shown CRF approach outperform generative learning image restoration labeling tasks]. crfs recently applied problem image denoising]. present work closely related CRF approach. indeed, special cases convolutional networks performing maximum likelihood inference CRF]. advantage convolutional network approach avoids general diﬃculty applying MRFbased methods image analysis: computational expense parameter estimation inference probabilistic models. for example, naive methods learning MRFbased models involve calculation partition function, normalization factor generally intractable realistic models image dimensions. result, great deal research devoted approximate MRF learning inference techniques meliorate computational diﬃculties, generally cost eir representational power oretical guarantees]. convolutional networks largely avoid diﬃculties posing computational task statistical framework regression rar density estimation. regression tractable computation refore permits models greater representational power methods based density estimation. this claim argued empirical results denoising problem, mamatical connections MRF convolutional network approaches. Convolutional Networks Convolutional networks extensively applied visual object recognition architectures accept image input and, alternating layers convolution subsampling, produce output values thresholded yield binary predictions object identity]. contrast, study networks accept image input produce entire image output. previous work architectures produce images binary targets image restoration problems specialized microscopy data]. here show similar architectures produce images analog ﬂuctuations found intensity distributions natural images. network Dynamics Architecture convolutional network alternating sequence linear filtering nonlinear transformation operations. input output layers include images, intermediate layers ?hidden” units images called feature maps internal computations algorithm. activity feature map layer    feature maps provide input denotes convolution operation. function sigmoid bias parameter. restrict experiments monochrome images networks single image input layer. straightforward extend approach color images assuming input layer multiple images., RGB color channels). for numerical reasons, preferable input target values range-bit integer intensity values dataset (values 255) normalized lie explicitly encode border image padding area surrounding image values. learning Denoise Parameter learning performed modification backpropagation algorithm feedfoward neural networks takes account weight-sharing structure convolutional networks]. however, issues addressed order learn architecture Figure task natural image denoising. firstly, image denoising task formulated learning problem order train convolutional network. since assume access database clean, noiseless images, implicitly desired image processing task integrating noise process training procedure. particular, assume noise process) operates image drawn distribution natural images consider entire convolutional network function Architecture CN1 CN2 input image             output image Figure Architecture convolutional network denoising. network hidden layers feature maps hidden layer. layers feature map connected randomly chosen feature maps previous layer. each arrow represents single convolution filter, network,697 free parameters requires 624 convolutions process forward pass.  free parameters parameter estimation problem minimize reconstruction error images subject noise process: min?    secondly, ineﬃcient batch learning context. training sets experiments millions pixels, practical perform forward backward pass entire training set gradient learning requires tens thousands updates converge reasonable solution. stochastic online gradient learning eﬃcient learning procedure adapted problem. typically, procedure selects small number independent examples training set averages toger gradients perform single update. compute gradient update patches randomly sampled images training set. using localized image patch violates independence assumption stochastic online learning, combining gradient separate images yields  cube practice suﬃcient approximation gradient effective. larger patches  ) reduce correlations training sample improve accuracy. this scheme eﬃcient computation local patch shared. found training time minimized generalization accuracy maximized incrementally learning layer weights. greedy, layer-wise training strategies recently explored context unsupervised initialization multi-layer networks, fine tuned discriminative task cost function]. maintain cost function throughout. this procedure starts training network single hidden layer. after thirty epochs, weights hidden layer copied network hidden layers; weights connecting hidden layer output layer discarded. hidden layer network optimized anor thirty epochs, procedure repeated layers. finally, learning networks hidden layers important small learning rate final layer.001) larger learning rate) layers. implementation Convolutional network inference learning implemented lines MATLAB code multi-dimensional convolution crosscorrelation routines. this makes approach easy optimize parallel computing GPU computing strategies. Experiments derive training test sets experiments natural images Berkeley segmentation database, previously study denoising]. restrict experiments case monochrome images; color images Berkeley dataset converted grayscale averaging color channels. test set consists 100 images, dimensions 321 481 dimensions 481 321. quantitative comparisons performed Peak Signal Denoising Performance Comparison FoE bls?gsm bls?gsm CN1 CN2 CNBlind Average PSNR Denoised Images Noise 100 Figure Denoising results measured peak signal noise ratio (psnr) noise levels. case, results average denoised PSNR hundred images test set. cn1 CNBlind learned forty image training set Field Experts model (foe). cn2 learned training set additional sixty images. bls-gsm1 bls-gsm2 parameter settings algorithm]. all methods CNBlind assume noise distribution. Noise Ratio (psnr): log10 (255 standard deviation error. psnr widely evaluate denoising performance]. denoising noise conditions task assumed images subjected Gaussian noise variance. noise model training process learn five-layer network noise level. both Bayes Least squares-gaussian Scale Mixture (blsgsm) Field Experts (foe) method optimize denoising process based noise level. learn sets networks task differ training set. set networks, refer cn1, training set subset Berkeley database learn anor set networks, called cn2, training set FoE model]. augmented additional sixty images Berkeley database. architecture networks shown fig.  quantitative results networks noise levels shown fig. results FoE bls-gsm method (bls-gsm settings] bls-gsm default settings code provided authors). for FoE results, number iterations magnitude step size optimized noise level grid search training set. visual comparison results shown fig.  find convolutional network highest average PSNR eir training set, margin statistical insignificance standard error computed distribution PSNR values entire image. however, conservative estimate standard error, smaller measured pixel patch-wise basis. blind denoising task assumed images subjected Gaussian noise unknown variance. denoising context diﬃcult problem non-blind situation. train single six-layer network network refer CNBlind randomly varying amount noise added training process, range , 100] during inference, noise level unknown image provided input. training set FoE model cn1. architecture shown fig. hidden layers results noise levels shown fig.  find convolutional network trained blind denoising performs compared methods non-blind conditions. fig. show filters learned network. CLEAN CLEAN CN2 bls-gsm FoE NOISY psnr CN2 psnr bls-gsm psnr FoE psnr Figure Denoising results image test set. noisy image generated adding Gaussian noise clean image. non-blind denoising results bls-gsm, foe, convolutional network methods shown. lower left panel shows results outlined region upper left panel. zoomed region shows areas CN2 output severe artifacts wavelet-based results sharper FoE results. cn1 results (psnr) visually similar cn2. Relationship MRF Convolutional Network Approaches introduction, claim convolutional networks similar greater representational power compared mrfs. support claim show special cases convolutional networks correspond field inference mrf. this rigorously prove convolutional networks representational power greater equal mrfs, field inference approximation. however, plausible case. previous work pointed Field Experts MRF interpreted convolutional network (see]) MRFs Isinglike prior related convolutional networks (see]). here, analyze MRF specially designed image denoising show closely related convolutional network Figure particular, MRF defines distribution analog ?visible? variables binary ?hidden? variables, exp      ) iab correspond ith pixel location image, partition function, standard deviation Gaussian noise. note symmetry Layer Layer Figure Filters learned hidden layers network cnblind. hidden layer 192 filters feature maps filters map). layer recognizable structure filters, including derivative filters high frequency filters similar learned FoE model]. assume w0aa interaction model case, transfer term linear hai lead additional bias term field approximation). hence, constitutes undirected graphical model conceptualized separate layers visible hidden variables. intralayer interactions visible layer convolutional structure (instead full connectivity) intralayer interactions hidden variables interlayer interactions visible hidden layer. from definition conditional distribution, exp Gaussian this equal conditional expectation]. model denoising fixing visible variables noisy image, computing hidden variables MAP inference, conditional expectation denoised image. inference calculate maxh), diﬃcult partition function. however, field approximation solved equation dynamics iterating. compare. find equivalent convolutional network hidden layer weights feature map directly receives input image. results suggest convolutional networks interpreted performing approximate inference MRF models designed denoising. practice, convolutional network architectures train related MRF models weights hidden layer constrained same, image input feature map layer. interesting question future research additional architectural constraints affect performance convolutional network approach. finally, special case non-blind Gaussian denoising direct integration noise model MRF equations, empirical results blind denoising suggest convolutional network approach adaptable general complex noise models implicitly learning cost function. Discussion Prior versus learned structure Before learning, convolutional network structure specialized natural images. contrast, GSM model multi-scale wavelet representation suitability representing natural image statistics. moreover, inference FoE model procedure similar non-linear diffusion methods, previously natural image processing learning. architecture FoE MRF chosen random settings free parameters provide impressive performance]. random parameter settings convolutional networks produce computation. parameters CN2 randomized layer, denoising performance image fig. drops psnr. random parameters layers yields worse results. this consistent idea cn2 representation specialized natural images training, localized receptive field structure convolutions. our approach relies gradient learning algorithm tune thousands parameters examples natural images. one assume approach require vastly training data methods prior structure. however, obtain good generalization performance training set learn Field Experts model, fewer degrees freedom. disadvantage approach produces architecture performance diﬃcult understand due numerous free parameters. advantage approach lead accurate performance, applied forms imagery statistics natural images previously studied dataset specialized image restoration problem studied]). network architecture image context amount image context convolutional network produce output specific image location determined number layers network size filter layer. for example-layer networks explored  image patch. this small amount context compared FoE BLSGSM models, permit correlations extend entire image. surprising major difference, convolutional network approach good performance. one explanation scale objects chosen image dataset relevant information captured small field view. noneless, interest denoising applications increase amount context network. simple strategy furr increase number layers; however, computationally intensive ineﬃcient exploit multi-scale properties natural images. adding additional machinery network architecture work better. integrating operations sub-sampling super-sampling network process image multiple scales, amenable gradient learning. computational eﬃciency With free parameters, convolutional networks computationally expensive image processing architecture. contrary-layer CN1 CN2 architecture (fig. requires 624 image convolutions process image. comparison, FoE model performs inference means dynamic process require thousand iterations. one-thousand iterations dynamics requires,000 convolutions (for FoE model filters). report wall-clock speed denoising 512 512 pixel image.16ghz Intel Core processor. averaged trials, cn1/cn2 requires  sec,000 iterations FoE requires 1664  sec. (using code authors]), bls-gsm model parameter settings? requires  sec., parameter setting? requires  sec. (using code authors]). all implementations matlab. true, however, training convolutional network architecture requires substantial computation. gradient learning require thousands updates converge, training denoising networks required parallel implementation utilized dozen processors week. while significant amount computation, performed off-line. learning complex image transformations generalized image attractors models work explored image processing task easily formulated learning problem synsizing training examples abundantly noiseless natural images. can approach extended tasks noise model variable complex form? our results blind denoising, amount \\x0cnoise vary severe, evidence can. preliminary experiments image inpainting encouraging. that said, major virtue image prior approach ability easily reuse single image model situations simply augmenting prior observation model. this image prior observation model decoupled. yet explicit probabilistic modeling computationally diﬃcult makes learning simple models challenging. convolutional networks forgo probabilistic modeling and, developed here, focus specific image image transformations regression problem. interesting combine approaches learn models ?unnormalized priors? sense energy-based image attractors; regression tool unsupervised learning capturing dependencies variables distribution]. acknowledgements: grateful Ted adelson, liu, Srinivas turaga, Yair Weiss helpful discussions. authors] making code available.',\n",
       " 'PP3508': 'moreover, model introduces concise, normative definition high level cognitive concepts working memory cognitive control terms maximizing discounted future rewards.” Working memory central topic cognitive neuroscience critical solving real-world problems information multiple temporally distant sources combined generate behavior. however, neglected fact learning working memory effectively diﬃcult problem. gating framework] collection psychological models show dopamine train basal ganglia prefrontal cortex form working memory representations types problems. unite Gating machine learning ory general \\x0cproblem memory-based optimal control]. present normative model learns, online temporal difference methods, working memory maximize discounted future reward partially observable settings. model successfully solves benchmark working memory problem, exhibits limitations similar observed humans. purpose introduce concise, normative definition high level cognitive concepts working memory cognitive control terms maximizing discounted future rewards. working memory loosely defined cognitive neuroscience information) internally maintained temporary short term basis) required tasks observations mapped correct actions. widely assumed prefrontal cortex (pfc) plays role maintaining updating working memory. however, PFC develops working memory representations task. furrmore, current work focuses describing structure limitations working memory, why, general class tasks, necessary. borrowing ory optimal control partially observable Markov decision problems (pomdps), frame psychological concept working memory internal state representation, developed employed maximize future reward partially observable environments. combine computational insights POMDPs neurobiologically plausible models cognitive neuroscience suggest simple reinforcement learning) model working memory function implemented dopaminergic training basal ganglia pfc. gating framework series cognitive neuroscience models developed explain dopaminergic signals shape working memory representations]. computationally framework models working memory collection past observations, occasionally replaced current observation, addresses problem learning update memory element versus maintaining. original Gating model] PFC contained unitary working memory representation updated phasic dopamine) burst occurred., due unexpected reward novelty). model connect working memory temporal difference) model firing], suggest working memory serve normative purpose. however, model limited computational ﬂexibility due unitary nature working memory., singleobservation memory controlled scalar signal). recent work] partially repositioned Gating framework actor/critic model mesostriatal], positing memory updating anor cortical action controlled dorsal striatal ”actor.” This architecture increased computational ﬂexibility introducing multiple working memory elements, multiple corticostriatal loops, quasi-independently updated. however, model combined number components (including supervised unsupervised learning, complex neural network dynamics), making diﬃcult \\x0cunderstand relationship simple mechanisms working memory function. moreover, model rescorla-wagner-like PVLV algorithm] rar] model phasic bursts, model behavior working memory representations directly shaped standard normative criteria models., discounted future reward reward unit time). present Gating model, synsizing mesostriatal actor/critic architecture] normative POMDP framework, reducing Gating model fourparameter, pure model process. produces model similar previous machine learning work ”modelfree” approximate POMDP solvers], attempt form good solutions explicit knowledge environment structure dynamics. , model working memory discrete memory system collection recent observations) rar continuous ”belief state inferred probability distribution hidden states). environments permit approximate solution. however, strength system requires prior knowledge, potentially animals, learn effective behavior memorymanagement policies completely environments., absence ?world model?). refore, retain computational ﬂexibility recent Gating models], reestablishing goal defining working memory normative terms]. illustrate strengths limitations model, apply representative working-memory tasks.  task proposed Gating benchmark]. contrary previous claims learning suﬃcient solve task, show eligibility trace(?)  ), model achieve optimal behavior. task highlights important limitations model. model POMDP solver POMDPs are, general, intractable., solution algorithms require infeasible number computations), clear model ultimately fail achieve optimal performance environments increase moderate complexity. however, human working memory exhibits sharp limitations. apply model implicit artificial grammar learning task] show fails ways reminiscent human performance. moreover, simulating task increased working memory capacity reveals diminishing returns capacity increases small number, suggesting ”magic number” limited working memory capacity found humans] fact optimal learning standpoint. working memory tasks, POMDP admit optimal behavior policy based current observation. instead, optimal policy generally depends combination memory current observation. type memory required varies pomdps, cases finite memory system suﬃcient basis optimal policy. peshkin, meuleau, Kaelbling] external finite memory device., shopping list) improve performance model-free POMDP setting. model ”state” variable consisted current observation augmented memory device. augmented action space, consisting memory actions motor actions, allowed model learn effective memorymanagement motor policies simultaneously. integrate approach Gating model, altering semantics external memory device internal working memory (presumed Choose motor action, gating action, current state, softmax motor gating action preferences, respectively. update motor gating action eligibility traces,  respectively. (update shown motor action eligibility trace. gating action trace analogous.) update (hidden) environment state, motor action. reward, observation, update internal state based previous state, gating action, observation Compute state-value prediction error, based critic state-value approximation, ?(?)   softmax(?    softmax(?   (?—?)      (?—?)      ??? (?,      environment(?    environment(?  (?,           ??(?  ?(? ??? (?)    ???     update state-value eligibility traces,   (?) update state-values ?(?) ?(?)      Update motor action preferences Update gating action preferences Next trial?  (?,  (?,   (?,  (?,  (?,   (?,       table Pseudocode trial model, based actor/critic architecture eligibility traces. ], substitute critic state-value prediction error williams   term]. describe single gating actor, straightforward generalize array independent gating actors simulations.  discount rate; eligibility trace decay rate; =learning rate. simulations, , . supported pfc), altering Gating model role working memory explicitly support optimal behavior terms discounted future reward) pomdp. ], key difference model standard methods state variable includes controlled memory elements., working memory), augment current observation. action space similarly augmented include memory gating actions, model learns trial-and-error update working memory resolve hidden states resolution leads greater rewards) motor policy. task model learn working \\x0cmemory policy current internal state., memory current observation) admits optimal behavioral policy. model (table consists critic, motor actor, gating actors. standard actor/critic architecture, critic learns evaluate (internal) states and, based ongoing temporal difference values, generates time step prediction error) signal (thought correspond phasic bursts dips]). train critic state values policies actors. motor actor fulfills usual role, choosing actions send environment based policy current internal state. finally, gating actors correspond one-one memory element. time point, gating actor independently chooses (via policy based internal state) wher) maintain element memory anor time step) replace (update) element memory current observation. remain aligned actor/critic online learning framework mesostriatal [910], learning model based REINFORCE] modified expected discounted future reward], rar monte-carlo policy learning algorithm] (which suitable oﬄine, episodic learning). furrmore, shown eligibility traces applying POMDPs(? taking characteristic eligibilities REINFORCE algorithm] impulse function replacing eligibility trace]. simplicity exposition interpretation, tabular policy state-value representations throughout. figure: Average performance training runs, consisting?107 timesteps. ) reward rate 105 time steps, model learns optimal policy eligibility trace parameter, one. ) time required model reach 300 consecutive correct trials increases rapidly decreases. ) Sample sequence task. benchmark Performance Psychological Data describe model performance task proposed benchmark Gating models]. turn comparison model behavior actual psychological data.  Performance task] illustrate problem learning task correct behavior depends multiple previous observations. task (figure), subjects presented sequence observations drawn set}. gain rewards responding rules: Respond) current observation observation set, observation set) current observation observation set, observation set, respond orwise. implementation, reward correct responses current observation correct responses, incorrect responses. modeled task \\x0cusing memory elements, minimum oretically optimal performance. results (figure) show(?) gating model achieve optimal performance. results demonstrate reliance model eligibility trace parameter, performance high intermediate values  model finds suboptimal policy slightly optimal policy model working memory.  performance worse, expected online policy improvement method non-decaying traces point comparison] return discussion). results consistent previous work showing) performs poorly partially observable (non-markovian) settings(?) (without memory)   performs]. indeed, early training, model learns convert POMDP MDP working memory, internal state dynamics markovian, eligibility trace necessary.  Psychological data interpret Gating framework (and working memory) attempt solve pomdps. brings large body oretical work bear properties Gating models. importantly, implies that, task complexity increases, Gating model humans fail find optimal solutions reasonable time frames Figure) Artificial grammar]. starting node grammar generates continuing sequence observations. nodes transitions (edges) make eir transition. edge labels mark grammatical observations. transition, grammatical observation replaced random, ungrammatical, observation. task predict observation time point. ) model shows gradual increase sensitivity sequences length length replicating human data. sensitivity measured probability choosing grammatical action true state, minus probability choosing grammatical action aliased state; complete aliasing, complete resolution. ) Model performance (reward rate) averaged training runs variable numbers time steps shows diminishing returns number memory elements increases. due generally intractable nature pomdps. inescapable conclusion, interesting compare model failures human failures: pattern failures matching human data provide support model. subsection describe simulation artificial grammar learning], offer account pervasive ”magic number” observations limits working memory capacity]). artificial grammar learning, subjects seemingly random sequence observations, instructed mimic observation quickly predict observation) action. unknown subjects, observation sequence generated stochastic process called ”grammar” (figure). artificial grammar tasks constitute pomdps: (recent) observation history predict observation current observation alone, optimal performance requires subjects remember information distilled history. subjects typically report knowledge underlying structure, training reaction times (rts) reveal implicit structural knowledge. specifically, RTs significantly faster ”grammatical” compared ”ungrammatical” observations (see Figure). cleeremans McClelland] examined limits subjects’ capacity detect grammar structure. grammar shown Figure. found that, subjects grew increasingly sensitive sequences length training measured transient increases ungrammatical observations), remained insensitive,000 time steps training, sequences length four. reﬂected failure subjects’ implicit working memory learning mechanisms, confirmed experiment]. replicated results, shown Figure. simulate task, gave model memory elements (results elements), reward correct prediction. tested model ability resolve states based previous observations contrasting behavior pairs observation sequences differed observation. state resolution based sequences length two, three, represented versus (leading predictions, respectively), SQX versus XQX), XTVX versus PTVX), respectively. task, optimal information sequences length proved impossible model and, apparently, humans. understand intuitively limitation, problem hidden states, optimal actions respectively. states preceded identical observation sequences length however, time steps past, observation precedes state observation precedes state probability held memory required time steps decreases geometrically probability resolving states decreases geometrically. agent resolve state state learn action preferences explores actions, insidious problem agent faces fully observable setting. result, model can reinforce optimal gating policies, eventually learning internal state space dynamics fail reﬂect true environment. problem credit assignment., learning mapping working memory actions) internal state corresponds true hidden state pomdp, leading ?chicken-and-egg? problem. preceding argument, obvious modification lead improved performance increase number memory elements. number memory elements increases, probability model remembers observation required amount time approaches one. however, strategy introduces curse dimensionality due rapidly increasing size internal state space. intuitive analysis suggests normative explanation famous ”magic number” limitation observed human working memory capacity, thought independent elements]). demonstrate idea simulating artificial grammar task, time averaging performance range training times million time steps capture idea humans practice tasks typical, variable, amount time. averaged results show diminishing returns increasing memory elements (figure). simulation tabular (rar neurally plausible) representations highly simplified model, exact number policy parameters state values estimated, time steps, working memory elements arbitrary relation human learning. still, model qualitative behavior (evidenced shape resulting curve order magnitude optimal number working memory elements) surprisingly reminiscent human behavior. based suggest limitation working memory capacity due limitation learning rar storage: impractical learn utilize small number., smaller) independent working memory elements, due curse dimensionality. presented psychological model suggests dopaminergic signals implicitly shape working memory representations pfc. model synsizes recent advances Gating literature] normative ory model-free, finite memory solutions POMDPs]. showed model learns behave optimally benchmark task. related model computational limitations limitations human working memory].  Relation oretical work recent work neural argued brain applies memorybased POMDP solution mechanisms real-world problems faced animals]. work primarily considers model-based mechanisms, temporary memory continuous belief state, assumes function cerebral cortex learn required world model, specifically PFC represent temporary goalor policy-related information optimal POMDP behavior. model present related line thinking, demonstrating model-free, rar model-based, mechanism learning store policy-related information pfc. learning systems form types working memory representations. future work investigate relationship implicit learning Gating model) modelfree POMDP solutions, versus kinds learning model-based POMDP solutions. irrespective POMDP framework, work assumed exists gating policy controls task-relevant working memory updating PFC]). present work furr develops model policy learned. interesting compare model previous work model-free POMDP solutions. mccallum emphasized importance learning utile distinctions], learning resolve hidden states optimal actions. emphasis model shares, spirit. humans extremely ﬂexible behavior. refore inherent tension focus cognitive resources learning task, form basis general task knowledge]. interesting future work explore closely working memory representations learned model align mccallum utile (and generalizable) distinctions opposed generalizable representations underlying hidden structure world, wher model modified incorporate mixture kinds knowledge, depending exploration/exploitation parameter. model closely Gating model], oretical model]. model abstract biologically detailed]. however, intent wher important insights capabilities model captured four-parameter, pure model clear normative basis. accordingly, shown model comparably equipped simulate range psychological phenomena. model makes equally testable (albeit different) predictions neural signal. relative], model places biological psychological concerns forefront, eliminating episodic memory requirements monte-carlo algorithm. interesting, vis vis], model performed poorly produces monte-carlo scheme. difference due model online learning., updated policy time step rar ends episodes), invalidates monte-carlo approach. model uniquely psychological variant previous architecture.  Subjects cognitive control experiments typically face situations correct behavior indeterminate observation. working memory thought repository temporary information augments observation permit correct behavior, called goals, context, task set, decision categories. concepts diﬃcult define. proposed formal oretical definition cognitive control working memory constructs. due importance temporally distant goals information immediately observable, canonical cognitive control environment captured pomdp. working memory temporary information, defined updated memory control policy, animal solve pomdps. modelbased research identify working memory continuous belief states, model-free framework identifies working memory discrete collection recent observations. correspond products learning systems, outcome eir case: cognitive control defined animal memory-based POMDP solver, working memory defined information, derived recent history, solver requires.  Psychological neural validity Although intractability solving POMDP means models present ultimately fail find optimal solution practical amount time all), manifestation computational limitations model aligns qualitatively observed \\x0chumans. working memory, psychological construct Gating model addresses, famously limited (see] review). canonical working memory capacity limitations, work shown subtler limitations arising learning contexts]). results presented promising, remains future work fully explore relation failures exhibited model exhibited humans. conclusion, shown Gating framework connection high level cognitive concepts working memory cognitive control, systems neuroscience, current neural ory. framework trialand-error method solving POMDPs rise limitations reminiscent observed psychological limits. remains future work furr investigate model ability capture range specific psychological neural phenomena. hope link working memory POMDPs fruitful generating insights, suggesting furr experimental oretical work. acknowledgments Peter dayan, Randy’reilly, Michael Frank productive discussions, anonymous reviewers helpful comments. moreover, model introduces concise, normative definition high level cognitive concepts working memory cognitive control terms maximizing discounted future rewards.” Working memory central topic cognitive neuroscience critical solving real-world problems information multiple temporally distant sources combined generate behavior. however, neglected fact learning working memory effectively diﬃcult problem. Gating framework] collection psychological models show dopamine train basal ganglia prefrontal cortex form working memory representations types problems. unite Gating machine learning ory general \\x0cproblem memory-based optimal control]. present normative model learns, online temporal difference methods, working memory maximize discounted future reward partially observable settings. model successfully solves benchmark working memory problem, exhibits limitations similar observed humans. our purpose introduce concise, normative definition high level cognitive concepts working memory cognitive control terms maximizing discounted future rewards. Working memory loosely defined cognitive neuroscience information) internally maintained temporary short term basis) required tasks observations mapped correct actions. widely assumed prefrontal cortex (pfc) plays role maintaining updating working memory. however, PFC develops working memory representations task. furrmore, current work focuses describing structure limitations working memory, why, general class tasks, necessary. borrowing ory optimal control partially observable Markov decision problems (pomdps), frame psychological concept working memory internal state representation, developed employed maximize future reward partially observable environments. combine computational insights POMDPs neurobiologically plausible models cognitive neuroscience suggest simple reinforcement learning) model working memory function implemented dopaminergic training basal ganglia pfc. Gating framework series cognitive neuroscience models developed explain dopaminergic signals shape working memory representations]. computationally framework models working memory collection past observations, occasionally replaced current observation, addresses problem learning update memory element versus maintaining. original Gating model] PFC contained unitary working memory representation updated phasic dopamine) burst occurred., due unexpected reward novelty). that model connect working memory temporal difference) model firing], suggest working memory serve normative purpose. however, model limited computational ﬂexibility due unitary nature working memory., singleobservation memory controlled scalar signal). more recent work] partially repositioned Gating framework actor/critic model mesostriatal], positing memory updating anor cortical action controlled dorsal striatal ”actor.” This architecture increased computational ﬂexibility introducing multiple working memory elements, multiple corticostriatal loops, quasi-independently updated. however, model combined number components (including supervised unsupervised learning, complex neural network dynamics), making diﬃcult \\x0cunderstand relationship simple mechanisms working memory function. moreover, model rescorla-wagner-like PVLV algorithm] rar] model phasic bursts, model behavior working memory representations directly shaped standard normative criteria models., discounted future reward reward unit time). present Gating model, synsizing mesostriatal actor/critic architecture] normative POMDP framework, reducing Gating model fourparameter, pure model process. this produces model similar previous machine learning work ”modelfree” approximate POMDP solvers], attempt form good solutions explicit knowledge environment structure dynamics. that, model working memory discrete memory system collection recent observations) rar continuous ”belief state inferred probability distribution hidden states). environments permit approximate solution. however, strength system requires prior knowledge, potentially animals, learn effective behavior memorymanagement policies completely environments., absence ?world model?). refore, retain computational ﬂexibility recent Gating models], reestablishing goal defining working memory normative terms]. illustrate strengths limitations model, apply representative working-memory tasks.  task proposed Gating benchmark]. contrary previous claims learning suﬃcient solve task, show eligibility trace(?)  ), model achieve optimal behavior. task highlights important limitations model. since model POMDP solver POMDPs are, general, intractable., solution algorithms require infeasible number computations), clear model ultimately fail achieve optimal performance environments increase moderate complexity. however, human working memory exhibits sharp limitations. apply model implicit artificial grammar learning task] show fails ways reminiscent human performance. moreover, simulating task increased working memory capacity reveals diminishing returns capacity increases small number, suggesting ”magic number” limited working memory capacity found humans] fact optimal learning standpoint. working memory tasks, POMDP admit optimal behavior policy based current observation. instead, optimal policy generally depends combination memory current observation. although type memory required varies pomdps, cases finite memory system suﬃcient basis optimal policy. peshkin, meuleau, Kaelbling] external finite memory device., shopping list) improve performance model-free POMDP setting. model ”state” variable consisted current observation augmented memory device. augmented action space, consisting memory actions motor actions, allowed model learn effective memorymanagement motor policies simultaneously. integrate approach Gating model, altering semantics external memory device internal working memory (presumed Choose motor action, gating action, current state, softmax motor gating action preferences, respectively. update motor gating action eligibility traces,  respectively. (update shown motor action eligibility trace. gating action trace analogous.) update (hidden) environment state, motor action. get reward, observation, update internal state based previous state, gating action, observation Compute state-value prediction error, based critic state-value approximation, ?(?)   softmax(?    softmax(?   (?—?)      (?—?)      ??? (?,      environment(?    environment(?  (?,           ??(?  ?(? ??? (?)    ???     update state-value eligibility traces,   (?) Update state-values ?(?) ?(?)      Update motor action preferences Update gating action preferences Next trial?  (?,  (?,   (?,  (?,  (?,   (?,       table Pseudocode trial model, based actor/critic architecture eligibility traces. following], substitute critic state-value prediction error williams   term]. describe single gating actor, straightforward generalize array independent gating actors simulations.  discount rate; eligibility trace decay rate; =learning rate. simulations, , . supported pfc), altering Gating model role working memory explicitly support optimal behavior terms discounted future reward) pomdp. like], key difference model standard methods state variable includes controlled memory elements., working memory), augment current observation. action space similarly augmented include memory gating actions, model learns trial-and-error update working memory resolve hidden states resolution leads greater rewards) motor policy. task model learn working \\x0cmemory policy current internal state., memory current observation) admits optimal behavioral policy. our model (table consists critic, motor actor, gating actors. standard actor/critic architecture, critic learns evaluate (internal) states and, based ongoing temporal difference values, generates time step prediction error) signal (thought correspond phasic bursts dips]). train critic state values policies actors. motor actor fulfills usual role, choosing actions send environment based policy current internal state. finally, gating actors correspond one-one memory element. time point, gating actor independently chooses (via policy based internal state) wher) maintain element memory anor time step) replace (update) element memory current observation. remain aligned actor/critic online learning framework mesostriatal [910], learning model based REINFORCE] modified expected discounted future reward], rar monte-carlo policy learning algorithm] (which suitable oﬄine, episodic learning). furrmore, shown eligibility traces applying POMDPs(? taking characteristic eligibilities REINFORCE algorithm] impulse function replacing eligibility trace]. for simplicity exposition interpretation, tabular policy state-value representations throughout. figure: Average performance training runs, consisting?107 timesteps. ) reward rate 105 time steps, model learns optimal policy eligibility trace parameter, one. ) time required model reach 300 consecutive correct trials increases rapidly decreases. ) Sample sequence task. Benchmark Performance Psychological Data describe model performance task proposed benchmark Gating models]. turn comparison model behavior actual psychological data.  Performance task] illustrate problem learning task correct behavior depends multiple previous observations. task (figure), subjects presented sequence observations drawn set}. gain rewards responding rules: Respond) current observation observation set, observation set) current observation observation set, observation set, respond orwise. implementation, reward correct responses current observation correct responses, incorrect responses. modeled task \\x0cusing memory elements, minimum oretically optimal performance. results (figure) show(?) gating model achieve optimal performance. results demonstrate reliance model eligibility trace parameter, performance high intermediate values when model finds suboptimal policy slightly optimal policy model working memory. with performance worse, expected online policy improvement method non-decaying traces point comparison] return discussion). results consistent previous work showing) performs poorly partially observable (non-markovian) settings(?) (without memory)   performs]. indeed, early training, model learns convert POMDP MDP working memory, internal state dynamics markovian, eligibility trace necessary.  Psychological data interpret Gating framework (and working memory) attempt solve pomdps. this brings large body oretical work bear properties Gating models. importantly, implies that, task complexity increases, Gating model humans fail find optimal solutions reasonable time frames Figure) Artificial grammar]. starting node grammar generates continuing sequence observations. all nodes transitions (edges) make eir transition. edge labels mark grammatical observations. transition, grammatical observation replaced random, ungrammatical, observation. task predict observation time point. ) model shows gradual increase sensitivity sequences length length replicating human data. sensitivity measured probability choosing grammatical action true state, minus probability choosing grammatical action aliased state; complete aliasing, complete resolution. ) Model performance (reward rate) averaged training runs variable numbers time steps shows diminishing returns number memory elements increases. due generally intractable nature pomdps. given inescapable conclusion, interesting compare model failures human failures: pattern failures matching human data provide support model. subsection describe simulation artificial grammar learning], offer account pervasive ”magic number” observations limits working memory capacity]). artificial grammar learning, subjects seemingly random sequence observations, instructed mimic observation quickly predict observation) action. unknown subjects, observation sequence generated stochastic process called ”grammar” (figure). artificial grammar tasks constitute pomdps: (recent) observation history predict observation current observation alone, optimal performance requires subjects remember information distilled history. although subjects typically report knowledge underlying structure, training reaction times (rts) reveal implicit structural knowledge. specifically, RTs significantly faster ”grammatical” compared ”ungrammatical” observations (see Figure). cleeremans McClelland] examined limits subjects’ capacity detect grammar structure. grammar shown Figure. found that, subjects grew increasingly sensitive sequences length training measured transient increases ungrammatical observations), remained insensitive,000 time steps training, sequences length four. this reﬂected failure subjects’ implicit working memory learning mechanisms, confirmed experiment]. replicated results, shown Figure. simulate task, gave model memory elements (results elements), reward correct prediction. tested model ability resolve states based previous observations contrasting behavior pairs observation sequences differed observation. state resolution based sequences length two, three, represented versus (leading predictions, respectively), SQX versus XQX), XTVX versus PTVX), respectively. task, optimal information sequences length proved impossible model and, apparently, humans. understand intuitively limitation, problem hidden states, optimal actions respectively. states preceded identical observation sequences length however, time steps past, observation precedes state observation precedes state probability held memory required time steps decreases geometrically probability resolving states decreases geometrically. because agent resolve state state learn action preferences explores actions, insidious problem agent faces fully observable setting. result, model can reinforce optimal gating policies, eventually learning internal state space dynamics fail reﬂect true environment. problem credit assignment., learning mapping working memory actions) internal state corresponds true hidden state pomdp, leading ?chicken-and-egg? problem. given preceding argument, obvious modification lead improved performance increase number memory elements. number memory elements increases, probability model remembers observation required amount time approaches one. however, strategy introduces curse dimensionality due rapidly increasing size internal state space. this intuitive analysis suggests normative explanation famous ”magic number” limitation observed human working memory capacity, thought independent elements]). demonstrate idea simulating artificial grammar task, time averaging performance range training times million time steps capture idea humans practice tasks typical, variable, amount time. indeed averaged results show diminishing returns increasing memory elements (figure). this simulation tabular (rar neurally plausible) representations highly simplified model, exact number policy parameters state values estimated, time steps, working memory elements arbitrary relation human learning. still, model qualitative behavior (evidenced shape resulting curve order magnitude optimal number working memory elements) surprisingly reminiscent human behavior. based suggest limitation working memory capacity due limitation learning rar storage: impractical learn utilize small number., smaller) independent working memory elements, due curse dimensionality. presented psychological model suggests dopaminergic signals implicitly shape working memory representations pfc. our model synsizes recent advances Gating literature] normative ory model-free, finite memory solutions POMDPs]. showed model learns behave optimally benchmark task. related model computational limitations limitations human working memory].  Relation oretical work recent work neural argued brain applies memorybased POMDP solution mechanisms real-world problems faced animals]. that work primarily considers model-based mechanisms, temporary memory continuous belief state, assumes function cerebral cortex learn required world model, specifically PFC represent temporary goalor policy-related information optimal POMDP behavior. model present related line thinking, demonstrating model-free, rar model-based, mechanism learning store policy-related information pfc. different learning systems form types working memory representations. future work investigate relationship implicit learning Gating model) modelfree POMDP solutions, versus kinds learning model-based POMDP solutions. irrespective POMDP framework, work assumed exists gating policy controls task-relevant working memory updating PFC]). present work furr develops model policy learned. interesting compare model previous work model-free POMDP solutions. mccallum emphasized importance learning utile distinctions], learning resolve hidden states optimal actions. this emphasis model shares, spirit. humans extremely ﬂexible behavior. refore inherent tension focus cognitive resources learning task, form basis general task knowledge]. would interesting future work explore closely working memory representations learned model align mccallum utile (and generalizable) distinctions opposed generalizable representations underlying hidden structure world, wher model modified incorporate mixture kinds knowledge, depending exploration/exploitation parameter. our model closely Gating model], oretical model]. our model abstract biologically detailed]. however, intent wher important insights capabilities model captured four-parameter, pure model clear normative basis. accordingly, shown model comparably equipped simulate range psychological phenomena. our model makes equally testable (albeit different) predictions neural signal. relative], model places biological psychological concerns forefront, eliminating episodic memory requirements monte-carlo algorithm. interesting, vis vis], model performed poorly produces monte-carlo scheme. difference due model online learning., updated policy time step rar ends episodes), invalidates monte-carlo approach. thus model uniquely psychological variant previous architecture.  Subjects cognitive control experiments typically face situations correct behavior indeterminate observation. working memory thought repository temporary information augments observation permit correct behavior, called goals, context, task set, decision categories. concepts diﬃcult define. here proposed formal oretical definition cognitive control working memory constructs. due importance temporally distant goals information immediately observable, canonical cognitive control environment captured pomdp. working memory temporary information, defined updated memory control policy, animal solve pomdps. modelbased research identify working memory continuous belief states, model-free framework identifies working memory discrete collection recent observations. correspond products learning systems, outcome eir case: cognitive control defined animal memory-based POMDP solver, working memory defined information, derived recent history, solver requires.  Psychological neural validity Although intractability solving POMDP means models present ultimately fail find optimal solution practical amount time all), manifestation computational limitations model aligns qualitatively observed \\x0chumans. working memory, psychological construct Gating model addresses, famously limited (see] review). beyond canonical working memory capacity limitations, work shown subtler limitations arising learning contexts]). results presented promising, remains future work fully explore relation failures exhibited model exhibited humans. conclusion, shown Gating framework connection high level cognitive concepts working memory cognitive control, systems neuroscience, current neural ory. framework trialand-error method solving POMDPs rise limitations reminiscent observed psychological limits. remains future work furr investigate model ability capture range specific psychological neural phenomena. our hope link working memory POMDPs fruitful generating insights, suggesting furr experimental oretical work. acknowledgments Peter dayan, Randy’reilly, Michael Frank productive discussions, anonymous reviewers helpful comments.',\n",
       " 'PP3517': 'actor-critic) algorithms] algorithmic approaches reinforcement learning). recent years work focused state, state-action, functions basis learning. methods, possessing desirable convergence attributes context table lookup representation, led convergence problems function approximation involved. recent line research based directly (and parametrically) representing policy, performing stochastic gradient ascent expected reward, estimated actions sampling trajectories]. however, direct policy methods lead slow convergence due large estimation variance. approach suggested recent years remedy problem utilization approaches, function estimated critic, passed actor selects action, based approximated function. convergence result policy gradient algorithm based function \\x0capproximation established], extended recently]. stage based algorithms provide solid foundation provably effective approaches based function approximation. wher methods yield solutions practical problems remains seen. playing increasingly important role neuroscience, experimentalists directly recorded activities neurons animals perform learning tasks], imaging techniques characterize human brain activities] learning. suggested long ago basal ganglia, set ancient sub-cortical brain nuclei, implicated. moreover, nuclei naturally divided components, based separation striatum main input channel basal ganglia) ventral dorsal components. imaging studies] suggested ventral stream estimation called critic, dorsal stream implicated motor output, action selection, learning called actor. furr experimental findings support view work. first, observed] short latency phasic response dopamine neurons midbrain strongly resembles temporal difference) signal introduced ory TDlearning], algorithms actor critic. mid-brain dopaminergic neurons project diffusively ventral dorsal components striatum, results consistent-based learning interpretation basal ganglia. second, recent results suggest synaptic plasticity occurring cortico-striatal synapses strongly modulated dopamine]. based observations suggested basal ganglia part based, (global) phasic dopamine signal serving signal] modulating synaptic plasticity. recent work devoted implementing networks spiking neurons]). approach lead specific experimentally verifiable hyposes interaction synaptic plasticity rules. fact, tantalizing possibility test derived rules context-vivo cultured neural networks]), connected environment input (sensory) output (motor) channels. envision dopamine serving biological substrate implementing signal system. work cited based direct policy gradient algorithms]), leading nonac approaches. moreover, algorithms based directly reward, rar biologically motivated signal, information reward itself, expected lead improved convergence. temporal Difference Based actor-critic Algorithm-based algorithm developed section related presented]. derivation present algorithm differs work (which stressed issue natural gradient) essential oretical feature establishment convergence1 restriction time scales]. result \\x0cimportant biological context, where, aware, evidence time scale separation.  Problem Formulation finite Markov Decision Process (mdp) discrete time finite state set size finite action set mdp models environment agent acts. selected action determines stochastic matrix, transition probability state state control parameterized policy conditional probability function, denoted , maps observation control parameter   state agent receives reward). agent goal adjust parameter order attain maximum average reward time.   markov Chain) induced, , ?). state transitions obtained generating action , generating state thus, transition matrix (?) , , ? , ?). denote set denote, transition probabilities (?)—?  closure stationary probability state choose action state technical assumptions required proofs below.  aperiodic, recurrent, single Assumption. ) Each (?)  equivalence class. ) function , differentiable. moreover, exist positive constants       )— —?? , ?)/?     , ?)/?    result assumption), lemma stationary distribution (orem]). throughout paper convergence refers convergence small ball stationary point; orem precise definition.  unique stationary distribution, Lemma. Assumption, (?)  denoted ?(? satisfying ?(? (?) ?(? transpose vector next, define measure performance agent environment. average reward stage starting initial state defined  —?) lim   [?] denotes expectation probability measure state time agent goal find  maximizes—?). lemma shows Assumption, average reward stage depend initial states (see orem]). lemma. Assumption Lemma, average reward stage—? independent starting state, denoted ?(? satisfies ?(?) ?(? based Lemma, agent goal find parameter vector maximizes average reward stage ?(?). performing maximization directly ?(?) hard. sequel show maximization performed optimizing ?(? ??(?). consequence Assumption definition ?(?) lemma (see Lemma]). lemma.    functions, —? ?(? bounded, differentiable, bounded derivatives. next, define differential function state represents average reward agent receives starting state reaching recurrent state time. mamatically,  —?)   ?(?  min define(?)      —?))      —? ), ?(?) satisfy poisson equation (see orem—?) ) ?(?) —?).  Based differential definition define temporal difference) states   formally) ?(?) —?)  —?). ) measures difference differential estimate receipt reward) move state estimate current differential state state  Algorithmic details single time scale convergence start definition likelihood ratio derivative, —?)  , ?)/? , assume bounded. assumption.     exists positive constant,  —?    order improve agent performance, follow gradient direction. orem shows gradient average reward stage calculated signal. similar variants orem proved-value] state-signal. orem. gradient average reward stage  expressed ??(?) )? —?) ) arbitrary).  orem proved advantage function argument]. provide direct proof section supplementary material. ﬂexibility resulting function) encode signal biologically realistic positive values only, inﬂuencing convergence proof. paper, simplicity) based orem, suggest-based algorithm. algorithm motivated] actor algorithm proposed. ] differential function-estimated afresh regenerative cycle leading large estimation variance. continuity actor policy function difference estimates regenerative cycles small. thus, critic good initial estimate beginning cycle, order reduce variance. related algorithm proposed], time scales assumed order borkar time scales convergence orem]. proposed algorithm, convergence orem, assume time scales actor critic. present batch mode update equations2 Algorithm actor critic. algorithm based recurrent state visit times state denoted   updated occur times (batch \\x0cmode). define cycle algorithm time indices ), satisfy  variables critic estimates—? ?(?) respectively. Algorithm Temporal Difference Based Actor Critic Algorithm Given mdp finite set states recurrent state satisfying).  hitting times  state      step coeﬃcients  parameterized policy ,   satisfies Assumption).  set constants?  operator Assumption.  step parameters satisfying orem. initiate critic variables:  estimate average reward stage) ) estimate differential function) initiate actor: choose) (see)) state xtm visited critic: For ) min}, (min(?)           ) )           ptm  actor   (see Assumption.). project component end order prove convergence Algorithm establish basic results. shows algorithm converges set ordinary differential equations), establishes conditions differential equations converge locally. order prove convergence boundedness conditions imposed, step algorithm. lack space, precise definition set Assumption supplementary material. orem. Assumptions, Algorithm converges set ode   )    (?)??(?) (?) (?(?)    (?) —?)  )      )  —?)  ) (?) (?(?)  ??)       ???  (?) (?(?)  ??) probability  min) (?)  (?)  (?)                   (? ) (?) continuous respect orem proved section supplementary material, based ory stochastic approximation, specifically, orem]. advantage proof technique assume time scales. orem, proved section supplementary material, states conditions  converges ball local optimum. orem. choose  ? /??  bh2?  positive constants lim supt?? ??(?     constants?? ? defined Section supplementary material. neural Algorithm Actor Using mcculloch-pitts Neurons section apply previously developed algorithm case neural networks. start classic binary valued mcculloch-pitts neuron, realistic spiking neuron model. algorithm presented Section derived proved converge batch mode, apply online fashion. derivation online learning algorithm batch version]), proof convergence setting underway. mcculloch-pitts actor network dynamics binary valued neurons, time) }, assumed based stochastic discrete time parallel updates) ) wij      Here  exp)), parameters algorithm {wij wij) synaptic weight time Each neuron stochastic output viewed action. applying actor update Algorithm obtain online learning rule wij wij)  ))) ). )) signal. update) interpreted error-driven hebbian-like learning rule modulated signal. resembles direct policy update rule presented], rule reward signal replaced signal (computed critic). moreover, eligibility trace formalism] differs formulation. describe simulation experiment conducted layered feedforward artificial neural network functions actor, combined biologically motivated critic. purpose experiment examine simple neuronal model, actor critic architectures. actor network consists single layered feed-forward network McCullochPitts neurons, modulated synapses above, signal calculated critic. environment maze barriers consisting states, Figure), reward provided top corner, elsewhere. time agent receives reward, transferred randomly location maze. time step, agent input vector represents state. output layer consists output neurons neuron represents action action set, down, left, right}. input representations actor, consisting eir neurons (note minimum number input neurons represent states maximum number). architecture input neurons represents maze state exclusive neuron, thus, overlap input vectors. architecture input neurons representation state represented neurons, leading overlaps input vectors. tested types critic: table based critic performs iterates Algorithm exact optimal policy. results shown Figure), averaged runs, demonstrate importance good input representations precise estimates.  Average Reward Stage) Number Steps) Figure) illustration mcculloch-pitts network. ) diagram maze agent reach reward upper corner. ) average reward stage cases: actor consisting input neurons table based critic (blue crosses), actor consisting input neurons table based critic (green stars), actor consisting input neurons exact critic (red circles), actor consisting input neurons exact (black crosses). optimal average reward stage denoted dotted line, random agent achieves reward.005. spiking neuron actor Actual neurons function continuous time producing action potentials. extension], developed update rule based Spike Response Model (srm]. neuron define state variable) represents membrane potential. dynamics  tfj)  wij tfj wij) synaptic eﬃcacy spike time neuron prior) refractory response, tfj times presynaptic \\x0cspikes emitted prior time  tfj response induced neuron neuron summation) spike times neuron emitted prior time neuron model assumed noisy threshold, model escape noise model]. model, neuron fires time interval) probability) vth, vth firing threshold (?) monotonically increasing function. neuron reaches threshold assumed fire membrane potential reset network continuous time neurons synapses. based Algorithm small time step, find wij) wij). ) define output neuron (interpreted action) time). note neuron output discrete time neuron fire) quiescent) definition section, yields (similar])  ) ) Htj  ? )      ) Fpost ({tfi Taking limit yields continuous time update rule  dwij))  tfi ) Fpre ({tfj    ) Htj Similarly, interpret update rule) modulated spike time dependent plasticity rule. detailed discussion interpretation update biological context left full paper. applied update rule) actor network consisting spiking neurons based). network goal reach circle center plain agent move, Newtonian dynamics, principle directions. actor composed input layer single layer modifiable weights. input layer consists ?sensory? neurons fire agent location environment. synaptic dynamics actor determined). critic receives inputs actor, linear function approximation architecture rar table lookup Algorithm standard parameter update rule architecture. ]) update critic parameters3 output layer actor consists neuronal groups, representing directions agent move, coded based firing rate model Gaussian tuning curves. signal calculated). reaches centered \\x0ccircle, receives reward, transferred randomly position environment. results simulation presented Figure figure displays agent typical random walk behavior prior learning, figure depicts typical trajectories representing agent actions learning phase. finally, Figure demonstrates increase average reward stage. time. .015 .005) 200 400 time[sec] 600) Figure) Typical agent tracks prior learning. ) Agent trajectories learning. ) Average reward stage plotted time. discussion presented temporal difference based actor critic learning algorithm reinforcement learning. algorithm derived principles based noisy gradient Algorithm relies table lookup critic, function approximation based critic, due large (continuous) state space. average reward, convergence proof presented relying widely time scale separation actor critic. derived algorithm applied neural networks, demonstrating effective operation maze problems. motivation proposed algorithm biological, providing coherent computational explanation recently observed phenomena: actor critic architectures basal ganglia, relation phasic dopaminergic neuromodulators signal, modulation spike time dependent plasticity rules dopamine. great deal furr work oretical biological components framework, hope results provide tentative step (noisy!) direction explaining biological. actor-critic) algorithms] algorithmic approaches reinforcement learning). recent years work focused state, state-action, functions basis learning. methods, possessing desirable convergence attributes context table lookup representation, led convergence problems function approximation involved. recent line research based directly (and parametrically) representing policy, performing stochastic gradient ascent expected reward, estimated actions sampling trajectories]. however, direct policy methods lead slow convergence due large estimation variance. one approach suggested recent years remedy problem utilization approaches, function estimated critic, passed actor selects action, based approximated function. convergence result policy gradient algorithm based function \\x0capproximation established], extended recently]. stage based algorithms provide solid foundation provably effective approaches based function approximation. wher methods yield solutions practical problems remains seen. playing increasingly important role neuroscience, experimentalists directly recorded activities neurons animals perform learning tasks], imaging techniques characterize human brain activities] learning. suggested long ago basal ganglia, set ancient sub-cortical brain nuclei, implicated. moreover, nuclei naturally divided components, based separation striatum main input channel basal ganglia) ventral dorsal components. several imaging studies] suggested ventral stream estimation called critic, dorsal stream implicated motor output, action selection, learning called actor. two furr experimental findings support view work. first, observed] short latency phasic response dopamine neurons midbrain strongly resembles temporal difference) signal introduced ory TDlearning], algorithms actor critic. since mid-brain dopaminergic neurons project diffusively ventral dorsal components striatum, results consistent-based learning interpretation basal ganglia. second, recent results suggest synaptic plasticity occurring cortico-striatal synapses strongly modulated dopamine]. based observations suggested basal ganglia part based, (global) phasic dopamine signal serving signal] modulating synaptic plasticity. some recent work devoted implementing networks spiking neurons]). such approach lead specific experimentally verifiable hyposes interaction synaptic plasticity rules. fact, tantalizing possibility test derived rules context-vivo cultured neural networks]), connected environment input (sensory) output (motor) channels. envision dopamine serving biological substrate implementing signal system. work cited based direct policy gradient algorithms]), leading nonac approaches. moreover, algorithms based directly reward, rar biologically motivated signal, information reward itself, expected lead improved convergence. Temporal Difference Based actor-critic Algorithm-based algorithm developed section related presented]. while derivation present algorithm differs work (which stressed issue natural gradient) essential oretical feature establishment convergence1 restriction time scales]. this result \\x0cimportant biological context, where, aware, evidence time scale separation.  Problem Formulation finite Markov Decision Process (mdp) discrete time finite state set size finite action set MDP models environment agent acts. each selected action determines stochastic matrix, transition probability state state control parameterized policy conditional probability function, denoted , maps observation control parameter   for state agent receives reward). agent goal adjust parameter order attain maximum average reward time. for  Markov Chain) induced, , ?). state transitions obtained generating action , generating state thus, transition matrix (?) , , ? , ?). denote set denote, transition probabilities (?)—?  closure stationary probability state choose action state several technical assumptions required proofs below.  aperiodic, recurrent, single Assumption. ) Each (?)  equivalence class. ) function , differentiable. moreover, exist positive constants       )— —?? , ?)/?     , ?)/?    result assumption), lemma stationary distribution (orem]). Throughout paper convergence refers convergence small ball stationary point; orem precise definition.  unique stationary distribution, Lemma. under Assumption, (?)  denoted ?(? satisfying ?(? (?) ?(? transpose vector next, define measure performance agent environment. average reward stage starting initial state defined  —?) lim   [?] denotes expectation probability measure state time agent goal find  maximizes—?). lemma shows Assumption, average reward stage depend initial states (see orem]). lemma. under Assumption Lemma, average reward stage—? independent starting state, denoted ?(? satisfies ?(?) ?(? Based Lemma, agent goal find parameter vector maximizes average reward stage ?(?). performing maximization directly ?(?) hard. sequel show maximization performed optimizing ?(? ??(?). consequence Assumption definition ?(?) lemma (see Lemma]). lemma. for for  functions, —? ?(? bounded, differentiable, bounded derivatives. next, define differential function state represents average reward agent receives starting state reaching recurrent state time. mamatically,  —?)   ?(?  min define(?)      —?))   for   —? ), ?(?) satisfy poisson equation (see orem—?) ) ?(?) —?).  Based differential definition define temporal difference) states   formally) ?(?) —?)  —?). ) measures difference differential estimate receipt reward) move state estimate current differential state state  Algorithmic details single time scale convergence start definition likelihood ratio derivative, —?)  , ?)/? , assume bounded. assumption. for    exists positive constant,  —?    order improve agent performance, follow gradient direction. orem shows gradient average reward stage calculated signal. similar variants orem proved-value] state-signal. orem. gradient average reward stage  expressed ??(?) )? —?) ) arbitrary).  orem proved advantage function argument]. provide direct proof section supplementary material. ﬂexibility resulting function) encode signal biologically realistic positive values only, inﬂuencing convergence proof. paper, simplicity) based orem, suggest-based algorithm. this algorithm motivated] actor algorithm proposed. ] differential function-estimated afresh regenerative cycle leading large estimation variance. using continuity actor policy function difference estimates regenerative cycles small. thus, critic good initial estimate beginning cycle, order reduce variance. related algorithm proposed], time scales assumed order borkar time scales convergence orem]. proposed algorithm, convergence orem, assume time scales actor critic. present batch mode update equations2 Algorithm actor critic. algorithm based recurrent state visit times state denoted   updated occur times (batch \\x0cmode). define cycle algorithm time indices ), satisfy  variables critic estimates—? ?(?) respectively. Algorithm Temporal Difference Based Actor Critic Algorithm Given MDP finite set states recurrent state satisfying).  hitting times  state      step coeﬃcients  parameterized policy ,   satisfies Assumption).  set constants?  operator Assumption.  step parameters satisfying orem. Initiate critic variables:  estimate average reward stage) ) estimate differential function) Initiate actor: choose) (see)) state xtm visited critic: For ) min}, (min(?)           ) )           ptm  actor   (see Assumption.). Project component end order prove convergence Algorithm establish basic results. shows algorithm converges set ordinary differential equations), establishes conditions differential equations converge locally. order prove convergence boundedness conditions imposed, step algorithm. for lack space, precise definition set Assumption supplementary material. orem. under Assumptions, Algorithm converges set ode   )    (?)??(?) (?) (?(?)    (?) —?)  )      )  —?)  ) (?) (?(?)  ??)       ???  (?) (?(?)  ??) probability  min) (?)  (?)  (?)                   (? ) (?) continuous respect orem proved section supplementary material, based ory stochastic approximation, specifically, orem]. advantage proof technique assume time scales. orem, proved section supplementary material, states conditions  converges ball local optimum. orem. choose  ? /??  bh2?  positive constants lim supt?? ??(?     constants?? ? defined Section supplementary material. Neural Algorithm Actor Using mcculloch-pitts Neurons section apply previously developed algorithm case neural networks. start classic binary valued mcculloch-pitts neuron, realistic spiking neuron model. while algorithm presented Section derived proved converge batch mode, apply online fashion. derivation online learning algorithm batch version]), proof convergence setting underway. mcculloch-pitts actor network dynamics binary valued neurons, time) }, assumed based stochastic discrete time parallel updates) ) wij      Here  exp)), parameters Algorithm {wij wij) synaptic weight time Each neuron stochastic output viewed action. applying actor update Algorithm obtain online learning rule wij wij)  ))) ). )) signal. update) interpreted error-driven hebbian-like learning rule modulated signal. resembles direct policy update rule presented], rule reward signal replaced signal (computed critic). moreover, eligibility trace formalism] differs formulation. describe simulation experiment conducted layered feedforward artificial neural network functions actor, combined biologically motivated critic. purpose experiment examine simple neuronal model, actor critic architectures. actor network consists single layered feed-forward network McCullochPitts neurons, modulated synapses above, signal calculated critic. environment maze barriers consisting states, Figure), reward provided top corner, elsewhere. every time agent receives reward, transferred randomly location maze. time step, agent input vector represents state. output layer consists output neurons neuron represents action action set, down, left, right}. input representations actor, consisting eir neurons (note minimum number input neurons represent states maximum number). architecture input neurons represents maze state exclusive neuron, thus, overlap input vectors. architecture input neurons representation state represented neurons, leading overlaps input vectors. tested types critic: table based critic performs iterates Algorithm exact optimal policy. results shown Figure), averaged runs, demonstrate importance good input representations precise estimates.  Average Reward Stage) Number Steps) Figure) illustration mcculloch-pitts network. ) diagram maze agent reach reward upper corner. ) average reward stage cases: actor consisting input neurons table based critic (blue crosses), actor consisting input neurons table based critic (green stars), actor consisting input neurons exact critic (red circles), actor consisting input neurons exact (black crosses). optimal average reward stage denoted dotted line, random agent achieves reward.005. spiking neuron actor Actual neurons function continuous time producing action potentials. extension], developed update rule based Spike Response Model (srm]. for neuron define state variable) represents membrane potential. dynamics  tfj)  wij tfj wij) synaptic eﬃcacy spike time neuron prior) refractory response, tfj times presynaptic \\x0cspikes emitted prior time  tfj response induced neuron neuron summation) spike times neuron emitted prior time neuron model assumed noisy threshold, model escape noise model]. according model, neuron fires time interval) probability) vth, vth firing threshold (?) monotonically increasing function. when neuron reaches threshold assumed fire membrane potential reset network continuous time neurons synapses. based Algorithm small time step, find wij) wij). ) define output neuron (interpreted action) time). note neuron output discrete time neuron fire) quiescent) using definition Section, yields (similar])  ) ) Htj  ? )      ) Fpost ({tfi Taking limit yields continuous time update rule  dwij))  tfi ) Fpre ({tfj    ) Htj Similarly, interpret update rule) modulated spike time dependent plasticity rule. detailed discussion interpretation update biological context left full paper. applied update rule) actor network consisting spiking neurons based). network goal reach circle center plain agent move, Newtonian dynamics, principle directions. actor composed input layer single layer modifiable weights. input layer consists ?sensory? neurons fire agent location environment. synaptic dynamics actor determined). critic receives inputs actor, linear function approximation architecture rar table lookup Algorithm standard parameter update rule architecture. ]) update critic parameters3 output layer actor consists neuronal groups, representing directions agent move, coded based firing rate model Gaussian tuning curves. signal calculated). whenever reaches centered \\x0ccircle, receives reward, transferred randomly position environment. results simulation presented Figure figure displays agent typical random walk behavior prior learning, figure depicts typical trajectories representing agent actions learning phase. finally, Figure demonstrates increase average reward stage. time. .015 .005) 200 400 time[sec] 600) Figure) Typical agent tracks prior learning. ) Agent trajectories learning. ) Average reward stage plotted time. Discussion presented temporal difference based actor critic learning algorithm reinforcement learning. algorithm derived principles based noisy gradient Algorithm relies table lookup critic, function approximation based critic, due large (continuous) state space. average reward, convergence proof presented relying widely time scale separation actor critic. derived algorithm applied neural networks, demonstrating effective operation maze problems. motivation proposed algorithm biological, providing coherent computational explanation recently observed phenomena: actor critic architectures basal ganglia, relation phasic dopaminergic neuromodulators signal, modulation spike time dependent plasticity rules dopamine. while great deal furr work oretical biological components framework, hope results provide tentative step (noisy!) direction explaining biological.',\n",
       " 'PP3588': 'most facts world learned firsthand experience, result information passed person anor. raises natural question: processes information transmission affected capacities agents involved? decades memory research charted ways memories distort reality, changing details experiences introducing events occurred (see] overview). expect memory biases affect transmission information, process relies person remembering fact accurately. question memory biases affect information transmission investigated detail Sir Frederic bartlett ?serial reproduction? experiments]. bartlett interpreted studies \\x0cshowing people biased culture reconstruct information memory, bias exaggerated serial reproduction. serial reproduction standard methods simulate process cultural transmission, subsequent studies paradigm]). however, phenomenon systematically formally analyzed, studies complex stimuli semantically rich hard control. paper, formally analyze empirically evaluate information changed serial reproduction process relates memory biases. particular, provide rational analysis serial reproduction spirit]), information change passed chain rational agents. biased reconstructions found tasks. example, people biased knowledge structure categories reconstruct simple stimuli memory. common effect kind people judge stimuli cross boundaries categories furr category, distances stimuli situations]. however, biases reﬂect suboptimal performance. assume memory solving problem extracting storing information noisy signal presented senses, analyze process reconstruction memory Bayesian inference. view, reconstructions combine prior knowledge world information provided noisy stimuli. prior knowledge result biases, biases ultimately make memory accurate]. account reconstruction memory true, expect inference process occur step serial reproduction. effects memory biases accumulated. assuming participants share prior knowledge world, serial reproduction ultimately reveal nature knowledge. drawing recent work exploring processes information transmission], show rational analysis serial reproduction makes prediction. test predictions account, explore special case task reconstruct one-dimensional stimulus information drawn fixed Gaussian distribution. case precisely characterize behavior step serial reproduction. specifically, show defines simple first-order autoregressive), process, allowing draw variety results characterizing processes. predictions test Bayesian models serial reproduction laboratory experiments show predictions hold serial reproduction betweenand within-subjects. plan paper follows. section lays Bayesian account serial reproduction. section show Bayesian account corresponds) process. sections present experiments testing model prediction serial reproduction reveals memory biases. section concludes paper. Bayesian view serial reproduction outline Bayesian approach serial reproduction con \\x0csidering problem reconstruction memory, solution problem repeated times, serial reproduction.  Reconstruction memory Our goal give rational account reconstruction memory, underlying computational problem finding optimal solution problem. formulate problem reconstruction memory problem inferring storing accurate information world noisy sensory data. noisy stimulus seek recover true state world generated stimulus, storing estimate  memory. optimal solution problem provided Bayesian statistics. previous experience ?prior? distribution states world(?). observing updated ?posterior? distribution) applying bayes? rule(?) (?)  —?)  ?likelihood?  probability observing true state world. computed), number schemes select estimate?  store. simplest scheme sampling posterior,   ). analysis general schema modeling reconstruction memory, applicable form simple special case vary single continuous dimension. experiment presented paper dimension width fish, showing people fish reconstruct width memory, dimension interest subjective quantity perceived length, loudness, duration, brightness stimulus. assume previous experience establishes gaussian distribution,   noise process means Gaussian distribution centered—?   case, standard results Bayesian statistics] show outcome Equation Gaussian distribution     analysis presented previous paragraph makes clear prediction: reconstruction  compromise observed prior terms compromise set ratio noise data uncertainty prior model predicts systematic bias reconstruction consequence error memory, optimal solution problem extracting information noisy stimulus. huttenlocher colleagues] conducted experiments testing account memory biases, showing people reconstructions interpolate observed stimuli trained distribution predicted. similar notion recosntruction memory, Hemmer Steyvers] conducted experiments show people formed Bayesian reconstructions realistic stimuli images fruit, capable drawing prior knowledge multiple levels abstraction.  Serial reproduction With model people approach problem reconstruction \\x0cfrom memory hand, position analyze serial reproduction, stimuli people receive trial results previous reconstruction. nth trial, participant sees stimulus participant computes outlined previous section, stores sample  distribution memory. asked produce reconstruction, participant generates distribution depends  likelihood—? reﬂects perceptual noise, reasonable assume sampled distribution, substituting    stimulus trial. viewed perspective, serial reproduction defines stochastic process: sequence random variables evolving time. particular, Markov chain, reconstruction produced current trial depends produced preceding trial. ]). transition probabilities Markov chain  ) probability produced reconstruction stimulus markov chain ergodic (see] details) converge stationary distribution  tending   , reproductions, expect probability stimulus produced reproduction stabilize fixed distribution. identifying distribution understand consequences serial reproduction. transition probabilities Equation special form, result sampling posterior distribution sampling likelihood —?). case, identify stationary distribution Markov chain]. stationary distribution Markov chain prior predictive distribution (?)  ) probability observing stimulus sampled prior. Markov chain Gibbs sampler joint distribution defined multiplying—?) (?) ]. clear characterization consequences serial reproduction: reproductions, stimuli produced sampled prior distribution assumed participants. convergence prior predictive distribution formal justification traditional claims serial reproduction reveals cultural biases, biases reﬂected prior. special case reconstruction stimuli vary single dimension, analytically compute probability density functions transition probabilities stationary distribution. applying Equation results summarized previous section     likewise, Equation stationary distribution )). rate Markov chain converges stationary distribution depends  close convergence slow close  closer inﬂuenced convergence faster.   convergence rate depends ratio participant perceptual noise variance prior distribution perceptual noise results \\x0cfaster convergence, specific trusted less; uncertainty prior results slower convergence, greater weight. serial reproduction one-dimensional stimuli) process special case serial reproduction one-dimensional stimuli give furr insight consequences modifying assumptions storage reconstruction memory, exploiting furr property underlying stochastic process: first-order autoregressive process, abbreviated). general form) process,   equation familiar form regression equation, predicting variable linear function anor, Gaussian noise. defines stochastic process variable predicted precedes sequence. ) models widely model timeseries data, simplest models capturing temporal dependency. showing stochastic process Markov chain information dynamics asymptotic behavior, showing reduces) process access number results characterizing properties processes.  process stationary distribution Gaussian  variance   autocovariance lag  decays geometrically ) process converges stationary distribution rate determined straightforward show stochastic process defined serial reproduction sample posterior distribution stored memory sampled likelihood) process. results previous section iteration  )    ) process       find stationary distribution substituting values expressions above. identifying serial reproduction single-dimensional stimuli) process relax assumptions people storing reconstructing information. ) model accommodate assumptions memory storage reconstruction All ways characterizing serial reproduction lead basic prediction: repeatedly reconstructing stimuli memory result convergence distribution corresponds prior. remainder paper test prediction. sections, present serial reproduction experiments conducted stimuli vary dimension (width fish). experiment previous research betweensubjects design, reconstructions participant serving stimuli \\x0cfor next. experiment within-subjects design person reconstructs stimuli mselves produced previous trial, testing potential design reveal memory biases individuals. experiment between-subjects serial reproduction This experiment directly tested basic prediction outcome serial reproduction reﬂect people priors. groups participants trained distributions onedimensional quantity width schematic fish serve prior reconstructing memorization phase, participant memory  sample posterior distribution assumed above,  argmax?  expected Gaussian posterior reproduction phase, participant reproduction noisy reconstruction, sample likelihood  assumed above, perfect reconstruction memory  defines models serial reproduction, correspond) processes differ variance (although maximizing storing perfect reconstruction degenerate, ). cases serial reproduction converges Gaussian stationary distribution variances. similar stimuli memory. distributions differed means, allowing examine wher distribution produced serial reproduction affected prior.  Method experiment basic procedure bartlett classic experiments]. participants members university community. stimuli]: fish elliptical bodies fan-shaped tails. fish stimuli varied dimension, width fish, ranging.63cm.76cm. stimuli presented Apple imac computer Matlab script PsychToolBox extensions]. participants trained discriminate fish-farm ocean fish. width fish-farm fish distributed ocean fish uniformly distributed.75cm. groups participants trained distributions fish-farm fish (prior distributions), means standard deviations. condition.66cm.3cm; condition.72cm.3cm. training phase, participants received block trials. trial, stimulus presented center computer monitor participants predict type fish pressing keys keyboard received feedback correctness prediction. participants tested trials knowledge types fish. procedure training block feedback. trainingtesting loop repeated participants reached% correct optimal decision strategy. participant pass test iterations, experiment halted. reproduction phase, partici \\x0cpants told record fish sizes fish farm. trial, fish stimulus ﬂashed center screen 500ms disappeared. anor fish random size appeared positions center screen participants arrow keys adjust width fish thought matched fish saw. fish widths participant condition 120 values randomly sampled uniform distribution.75cm. participant memorize random samples gave reconstructions. subsequent participant condition presented data generated previous participant reconstruct fish widths. thus, participant data constitute slice time 120 serial reproduction chains. end experiment, participants final-trial test check prior distributions drifted. ten participants? data excluded chains based criteria: final testing score% optimal performance; difference reproduced stimulus shown greater difference largest smallest stimuli training distribution trial; adjustments starting fish width half trials.  Results Discussion participants condition, resulting generations serial reproduction. figure shows initial final distributions reconstructions, toger autoregression plots conditions. reconstructed fish widths produced participants conditions.21cm respectively, statistically significantly(238). final participants chain, reconstructed fish widths.68cm respectively, statistically significant difference(238.001). difference means matches direction difference training provided conditions size difference reduced means stationary distributions lower distributions training. autoregression plots provide furr quantitative test predictions Bayesian model. basic prediction model reconstruction regression, Figure correlation stimulus reconstruction correlation) model predictions data, correlation high conditions.001) conditions respectively. finally, examined wher Markov assumption underlying analysis valid, computing Initial distribution Autoregression Final distribution Stimuli Condition Condition Fish width Fish width Figure Initial final distributions conditions Experiment ) distribution stimuli Gaussian fits reconstructions participants conditions. ) Gaussian fits reconstructions generated 18th participants condition. ) Autoregression plot function conditions. correlation resulting partial correlation low conditions conditions (both). experiment within-subjects serial reproduction between-subjects design reproduce process information transmission, analysis suggests serial reproduction promise method investigating memory biases individuals. explore potential method, tested model within-subjects design, participant reproduction current trial stimulus participant trial. participant responses entire experiment produced chain reproductions. participant produced chains, starting widely separated initial values. control trials careful instructions participants realize stimuli reproductions.  Method forty-six undergraduates university research participation pool participated experiment. basic procedure Experiment reproduction phase. participant responses phase formed chains trials. chains started original stimuli width values.63cm.19cm.76cm, trials, stimuli participants reproductions previous trials chain. prevent participants realizing fact, chain order randomized Markov chain trials intermixed control trials widths drawn prior distribution.  Results Discussion participants? data excluded based criteria Experiment lower testing score% optimal performance additional criterion relevant withinsubjects case: participants excluded chains converge, criterion convergence lower upper chains cross middle chain. screening procedures, participants? data accepted, condition condition participants trials chains converge, half chains (trials) \\x0canalyzed furr. locations stationary distributions measured computing means reproduced fish widths participant. conditions.66cm.72cm), average means.01cm.021). panel Figure Figure stimuli, training distributions stationary distributions Experiment data point panel shows iterations single participant. boxes show% confidence interval condition. serial Reproduction Training Gaussian Fit Auto Regression condition Fish Width Fish Width condition Iteration Figure Chains stationary distributions individual participants conditions. ) Markov chains generated participant, starting values. ) Training distributions condition. ) Gaussian fits iterations participant data. ) Autoregression iterations participant data. shows values conditions. basic prediction model borne out: participants converged distributions differed significantly means exposed data suggesting prior. however, means general lower prior. effect prominent control trials, produced means.53cm respectively Figure shows chains, training distributions, Gaussian fits autoregression half Markov chains participants conditions. correlation analysis showed) \\x0cmodel predictions highly correlated data generated participant, correlations conditions respectively. since experiments produced stationary distributions means lower training distributions, conducted separate experiment examining reconstructions people produced training. fish width produced participants.43cm, significantly initial values chain.19cm). result suggested people priori expectation fish widths smaller category means, suggesting people experiments prior compromise expectation training data. correlations significant participants. partial correlation low conditions respectively, suggesting Markov assumption satisfied. partial correlations significant) participant condition conclusion presented Bayesian account serial reproduction, tested basic predictions account strictly controlled laboratory experiments. results experiments consistent predictions account, serial reproduction converging distribution inﬂuenced prior distribution established training. analysis connects biases revealed serial reproduction general Bayesian strategy combining prior knowledge noisy data achieve higher accuracy]. shows serial reproduction analyzed Markov chains first-order autoregressive models, providing opportunity draw rich body work dynamics asymptotic behavior processes. connections provide formal justification idea serial reproduction information transmitted reﬂects biases people transmitting, establishing result holds characterizations processes involved storage reconstruction memory. acknowledgments This work supported grant number 0704034 National Science foundation. Most facts world learned firsthand experience, result information passed person anor. this raises natural question: processes information transmission affected capacities agents involved? decades memory research charted ways memories distort reality, changing details experiences introducing events occurred (see] overview). expect memory biases affect transmission information, process relies person remembering fact accurately. question memory biases affect information transmission investigated detail Sir Frederic bartlett ?serial reproduction? experiments]. bartlett interpreted studies \\x0cshowing people biased culture reconstruct information memory, bias exaggerated serial reproduction. serial reproduction standard methods simulate process cultural transmission, subsequent studies paradigm]). however, phenomenon systematically formally analyzed, studies complex stimuli semantically rich hard control. paper, formally analyze empirically evaluate information changed serial reproduction process relates memory biases. particular, provide rational analysis serial reproduction spirit]), information change passed chain rational agents. biased reconstructions found tasks. for example, people biased knowledge structure categories reconstruct simple stimuli memory. one common effect kind people judge stimuli cross boundaries categories furr category, distances stimuli situations]. however, biases reﬂect suboptimal performance. assume memory solving problem extracting storing information noisy signal presented senses, analyze process reconstruction memory Bayesian inference. under view, reconstructions combine prior knowledge world information provided noisy stimuli. use prior knowledge result biases, biases ultimately make memory accurate]. account reconstruction memory true, expect inference process occur step serial reproduction. effects memory biases accumulated. assuming participants share prior knowledge world, serial reproduction ultimately reveal nature knowledge. drawing recent work exploring processes information transmission], show rational analysis serial reproduction makes prediction. test predictions account, explore special case task reconstruct one-dimensional stimulus information drawn fixed Gaussian distribution. case precisely characterize behavior step serial reproduction. specifically, show defines simple first-order autoregressive), process, allowing draw variety results characterizing processes. predictions test Bayesian models serial reproduction laboratory experiments show predictions hold serial reproduction betweenand within-subjects. plan paper follows. section lays Bayesian account serial reproduction. Section show Bayesian account corresponds) process. sections present experiments testing model prediction serial reproduction reveals memory biases. section concludes paper. Bayesian view serial reproduction outline Bayesian approach serial reproduction con \\x0csidering problem reconstruction memory, solution problem repeated times, serial reproduction.  Reconstruction memory Our goal give rational account reconstruction memory, underlying computational problem finding optimal solution problem. formulate problem reconstruction memory problem inferring storing accurate information world noisy sensory data. given noisy stimulus seek recover true state world generated stimulus, storing estimate  memory. optimal solution problem provided Bayesian statistics. previous experience ?prior? distribution states world(?). observing updated ?posterior? distribution) applying bayes? rule(?) (?)  —?)  ?likelihood?  probability observing true state world. having computed), number schemes select estimate?  store. perhaps simplest scheme sampling posterior,   ). this analysis general schema modeling reconstruction memory, applicable form simple special case vary single continuous dimension. experiment presented paper dimension width fish, showing people fish reconstruct width memory, dimension interest subjective quantity perceived length, loudness, duration, brightness stimulus. assume previous experience establishes Gaussian distribution,   noise process means Gaussian distribution centered—?   case, standard results Bayesian statistics] show outcome Equation Gaussian distribution     analysis presented previous paragraph makes clear prediction: reconstruction  compromise observed prior terms compromise set ratio noise data uncertainty prior this model predicts systematic bias reconstruction consequence error memory, optimal solution problem extracting information noisy stimulus. huttenlocher colleagues] conducted experiments testing account memory biases, showing people reconstructions interpolate observed stimuli trained distribution predicted. using similar notion recosntruction memory, Hemmer Steyvers] conducted experiments show people formed Bayesian reconstructions realistic stimuli images fruit, capable drawing prior knowledge multiple levels abstraction.  Serial reproduction With model people approach problem reconstruction \\x0cfrom memory hand, position analyze serial reproduction, stimuli people receive trial results previous reconstruction. nth trial, participant sees stimulus participant computes outlined previous section, stores sample  distribution memory. when asked produce reconstruction, participant generates distribution depends  likelihood—? reﬂects perceptual noise, reasonable assume sampled distribution, substituting   this stimulus trial. viewed perspective, serial reproduction defines stochastic process: sequence random variables evolving time. particular, Markov chain, reconstruction produced current trial depends produced preceding trial. ]). transition probabilities Markov chain  ) probability produced reconstruction stimulus Markov chain ergodic (see] details) converge stationary distribution  tending   that, reproductions, expect probability stimulus produced reproduction stabilize fixed distribution. identifying distribution understand consequences serial reproduction. transition probabilities Equation special form, result sampling posterior distribution sampling likelihood —?). case, identify stationary distribution Markov chain]. stationary distribution Markov chain prior predictive distribution (?)  ) probability observing stimulus sampled prior. this Markov chain Gibbs sampler joint distribution defined multiplying—?) (?) ]. this clear characterization consequences serial reproduction: reproductions, stimuli produced sampled prior distribution assumed participants. convergence prior predictive distribution formal justification traditional claims serial reproduction reveals cultural biases, biases reﬂected prior. special case reconstruction stimuli vary single dimension, analytically compute probability density functions transition probabilities stationary distribution. applying Equation results summarized previous section     likewise, Equation stationary distribution )). rate Markov chain converges stationary distribution depends when close convergence slow close  closer inﬂuenced convergence faster. since  convergence rate depends ratio participant perceptual noise variance prior distribution more perceptual noise results \\x0cfaster convergence, specific trusted less; uncertainty prior results slower convergence, greater weight. Serial reproduction one-dimensional stimuli) process special case serial reproduction one-dimensional stimuli give furr insight consequences modifying assumptions storage reconstruction memory, exploiting furr property underlying stochastic process: first-order autoregressive process, abbreviated). general form) process,   equation familiar form regression equation, predicting variable linear function anor, Gaussian noise. defines stochastic process variable predicted precedes sequence. ) models widely model timeseries data, simplest models capturing temporal dependency. just showing stochastic process Markov chain information dynamics asymptotic behavior, showing reduces) process access number results characterizing properties processes.  process stationary distribution Gaussian  variance   autocovariance lag  decays geometrically ) process converges stationary distribution rate determined straightforward show stochastic process defined serial reproduction sample posterior distribution stored memory sampled likelihood) process. using results previous section iteration  )    this) process     since  find stationary distribution substituting values expressions above. identifying serial reproduction single-dimensional stimuli) process relax assumptions people storing reconstructing information. ) model accommodate assumptions memory storage reconstruction All ways characterizing serial reproduction lead basic prediction: repeatedly reconstructing stimuli memory result convergence distribution corresponds prior. remainder paper test prediction. sections, present serial reproduction experiments conducted stimuli vary dimension (width fish). experiment previous research betweensubjects design, reconstructions participant serving stimuli \\x0cfor next. experiment within-subjects design person reconstructs stimuli mselves produced previous trial, testing potential design reveal memory biases individuals. Experiment between-subjects serial reproduction This experiment directly tested basic prediction outcome serial reproduction reﬂect people priors. two groups participants trained distributions onedimensional quantity width schematic fish serve prior reconstructing memorization phase, participant memory  sample posterior distribution assumed above,  argmax?  expected Gaussian posterior reproduction phase, participant reproduction noisy reconstruction, sample likelihood  assumed above, perfect reconstruction memory  this defines models serial reproduction, correspond) processes differ variance (although maximizing storing perfect reconstruction degenerate, ). cases serial reproduction converges Gaussian stationary distribution variances. similar stimuli memory. distributions differed means, allowing examine wher distribution produced serial reproduction affected prior.  Method experiment basic procedure bartlett classic experiments]. participants members university community. stimuli]: fish elliptical bodies fan-shaped tails. all fish stimuli varied dimension, width fish, ranging.63cm.76cm. stimuli presented Apple imac computer Matlab script PsychToolBox extensions]. participants trained discriminate fish-farm ocean fish. width fish-farm fish distributed ocean fish uniformly distributed.75cm. two groups participants trained distributions fish-farm fish (prior distributions), means standard deviations. condition.66cm.3cm; condition.72cm.3cm. training phase, participants received block trials. trial, stimulus presented center computer monitor participants predict type fish pressing keys keyboard received feedback correctness prediction. participants tested trials knowledge types fish. procedure training block feedback. trainingtesting loop repeated participants reached% correct optimal decision strategy. participant pass test iterations, experiment halted. reproduction phase, partici \\x0cpants told record fish sizes fish farm. trial, fish stimulus ﬂashed center screen 500ms disappeared. anor fish random size appeared positions center screen participants arrow keys adjust width fish thought matched fish saw. fish widths participant condition 120 values randomly sampled uniform distribution.75cm. participant memorize random samples gave reconstructions. each subsequent participant condition presented data generated previous participant reconstruct fish widths. thus, participant data constitute slice time 120 serial reproduction chains. end experiment, participants final-trial test check prior distributions drifted. ten participants? data excluded chains based criteria: final testing score% optimal performance; difference reproduced stimulus shown greater difference largest smallest stimuli training distribution trial; adjustments starting fish width half trials.  Results Discussion participants condition, resulting generations serial reproduction. figure shows initial final distributions reconstructions, toger autoregression plots conditions. reconstructed fish widths produced participants conditions.21cm respectively, statistically significantly(238). for final participants chain, reconstructed fish widths.68cm respectively, statistically significant difference(238.001). difference means matches direction difference training provided conditions size difference reduced means stationary distributions lower distributions training. autoregression plots provide furr quantitative test predictions Bayesian model. basic prediction model reconstruction regression, Figure correlation stimulus reconstruction correlation) model predictions data, correlation high conditions.001) conditions respectively. finally, examined wher Markov assumption underlying analysis valid, computing Initial distribution Autoregression Final distribution Stimuli Condition Condition Fish width Fish width Figure Initial final distributions conditions Experiment ) distribution stimuli Gaussian fits reconstructions participants conditions. ) Gaussian fits reconstructions generated 18th participants condition. ) Autoregression plot function conditions. correlation resulting partial correlation low conditions conditions (both). Experiment within-subjects serial reproduction between-subjects design reproduce process information transmission, analysis suggests serial reproduction promise method investigating memory biases individuals. explore potential method, tested model within-subjects design, participant reproduction current trial stimulus participant trial. each participant responses entire experiment produced chain reproductions. each participant produced chains, starting widely separated initial values. control trials careful instructions participants realize stimuli reproductions.  Method forty-six undergraduates university research participation pool participated experiment. basic procedure Experiment reproduction phase. each participant responses phase formed chains trials. chains started original stimuli width values.63cm.19cm.76cm, trials, stimuli participants reproductions previous trials chain. prevent participants realizing fact, chain order randomized Markov chain trials intermixed control trials widths drawn prior distribution.  Results Discussion participants? data excluded based criteria Experiment lower testing score% optimal performance additional criterion relevant withinsubjects case: participants excluded chains converge, criterion convergence lower upper chains cross middle chain. after screening procedures, participants? data accepted, condition condition participants trials chains converge, half chains (trials) \\x0canalyzed furr. locations stationary distributions measured computing means reproduced fish widths participant. for conditions.66cm.72cm), average means.01cm.021). panel Figure Figure stimuli, training distributions stationary distributions Experiment each data point panel shows iterations single participant. boxes show% confidence interval condition. serial Reproduction Training Gaussian Fit Auto Regression condition Fish Width Fish Width condition Iteration Figure Chains stationary distributions individual participants conditions. ) Markov chains generated participant, starting values. ) Training distributions condition. ) Gaussian fits iterations participant data. ) Autoregression iterations participant data. shows values conditions. basic prediction model borne out: participants converged distributions differed significantly means exposed data suggesting prior. however, means general lower prior. this effect prominent control trials, produced means.53cm respectively Figure shows chains, training distributions, Gaussian fits autoregression half Markov chains participants conditions. correlation analysis showed) \\x0cmodel predictions highly correlated data generated participant, correlations conditions respectively. Since experiments produced stationary distributions means lower training distributions, conducted separate experiment examining reconstructions people produced training. fish width produced participants.43cm, significantly initial values chain.19cm). this result suggested people priori expectation fish widths smaller category means, suggesting people experiments prior compromise expectation training data. correlations significant participants. partial correlation low conditions respectively, suggesting Markov assumption satisfied. partial correlations significant) participant condition Conclusion presented Bayesian account serial reproduction, tested basic predictions account strictly controlled laboratory experiments. results experiments consistent predictions account, serial reproduction converging distribution inﬂuenced prior distribution established training. our analysis connects biases revealed serial reproduction general Bayesian strategy combining prior knowledge noisy data achieve higher accuracy]. shows serial reproduction analyzed Markov chains first-order autoregressive models, providing opportunity draw rich body work dynamics asymptotic behavior processes. connections provide formal justification idea serial reproduction information transmitted reﬂects biases people transmitting, establishing result holds characterizations processes involved storage reconstruction memory. acknowledgments This work supported grant number 0704034 National Science foundation.',\n",
       " 'PP3718': 'segmenting semantic objects, broadly image parsing, fundamentally challenging problem. task painfully under-constrained single image, extremely diﬃcult partition semantically meaningful elements, blobs similar color texture. example, algorithm figure doors windows building, different, belong segment? grey pavement grey house segments? clearly, information image required solve problem. paper, argue extra information extracted images visually similar one. increasing availability Internetscale image collections millions images! idea data-driven scene matching recently shown promise variety tasks. simply finding matching images low-dimentinal descriptor transfering labels input image, impressive results demonstrated \\x0cobject scene recognition], object detection], image geo-location], object event annotation], ors. image collection labels, shown tasks image completion exploration], image colorization], surface layout estimation]. however, noted authors illustrated Figure major stumbling block scene-matching approaches that, large quantities data, types images quality matches good. part reason low-level image descriptors matching powerful capture semantic similarity. approaches proposed address shortcoming, including syntically increasing dataset transformed copies images], cleaning matching results clustering], automatically prefiltering dataset], simply picking good matches hand]. appraoches improve performance don alleviate issue entirely. fundamental problem variability visual world vast, exponential number object combinations scene,  willow project-team, Laboratoire?informatique?ecole Normale sup?erieure ens/inria/cnrs UMR 8548 Figure Illustration scene matching problem. left: Input image (along output segmentation system overlaid) matched dataset 100k street images. notice output segment boundaries align depicted objects scene. top right: top retrieved images, based matching gist descriptor] entire image. matches good. bottom right: Searching matches estimated segment (using gist representation segment) compositing results yields matches input image. futile expect find single good match all! instead, argue input image explained spatial composite regions database images. aim break image chunks small good matches database, large matches retain informative power.  Overview work, propose apply scene matching problem segmenting semantically meaningful objects. seek segment objects enclosed principal occlusion contact boundaries objects part attached objects). idea turn advantage fact scene matches perfect. typically scene matching part image matched well, parts matched approximately, coarse level. example, street scene, matching image building match well, shape road wrong, anor matching image road right, tree building. differences matching provide powerful signal identify objects segmentation boundaries. computing matching image composite, explain input image. match region input image semantically similar regions images) single match. starting point algorithm input image ?image stack?  set coarsely matching images (5000 case) retrieved large dataset standard image matching technique (gist] case). essence, image stack dataset, tailor-made match scene structure input image. intuitively, goal image stack segment (and ?explain?) input image semantically meaningful way. idea that, stack more-less aligned, regions semantic objects present images consistently spatial location. input image explained patch-work consistent regions, simultaneously producing segmentation, composite matches, individual matches stack. prior work producing resulting image stack aligned images depicting scene, PhotoMontage work], optimally selects regions globally aligned images based quality score composite visually pleasing output image. recently, work based PhotoMontage framework automatically align images depicting scene objects perform segmentation], region-filling], outlier detection]. contrast, work, attempting work stack visually similar, physically different, scenes. spirit contemporary work], work supervised data, completely unsupervised. related contemporary work]. approach combines boundary-based region-based segmentation processes toger single MRF framework. boundary process (section stack determine semantic boundaries objects. region process (section aims group pixels belonging object stack. cues combined toger MRF framework solved GraphCut optimization (section). present results Section boundary process: data driven boundary detection Information single image cases suﬃcient recovering boundaries objects. strong image edges correspond internal object structures, window wheel car. additionally, boundaries objects produce weak image evidence, boundary building road similar color partially occluding. here, propose analyze statistics large number related images stack) recover boundaries objects. exploit fact objects tend rest location relative scene. example, street scene, car adjacent regions belonging number objects, building, person, road, etc. hand, relative positions internal object structures consistent images. example, wheels windows car consistently roughly similar positions images. recover object boundaries, measure ability consistently match locally set images stack. intuitively, regions inside object tend match set images, similar appearance, regions opposite sides boundary match sets images. formally, oriented line passing image point orientation analyze statistics sets images similar appearance side line. side oriented line, independently query stack images forming local image descriptor modulated weighted mask. half-gaussian weighting mask oriented line centered image point This local mask modulates Gabor filter responses orientations scales) RGB color channels, descriptor formed averaging Gabor energy color pixel spatial bins. gaussian modulated descriptor, captures appearance information side boundary point orientation appearance descriptors extracted manner image stack compared query image descriptor distance. images stack assumed coarsely aligned, matches considered query location orientation stack. matching translation invariant. type spatially dependent matching suitable scene images consistent spatial layout considered work. quality matches furr improved fine aligning stack images query]. image point orientation output local matching sides oriented line ranked lists image stack indices, ordering list distance local descriptors, query image image stack. compute spearman rank correlation coeﬃcient rank-ordered lists d2i) ,   number images stack difference ranks stack image ranked lists, high rank correlation point lies inside object extent, low correlation point object boundary orientation note however, low rank correlations caused poor quality local matches. figure illustrates boundary detection process. eﬃciency reasons, compute rank correlation score points orientations marked boundaries probability boundary edge detector], boundary orientations  , quantized steps. final boundary score proposed data Figure Data driven boundary detection. left: Input image query edges shown. right: top matches large collection images side query edges. rank correlation occlusion boundary.0998; rank correlation road region.6067. notice point lying inside object road), ranked sets retrieved images sides oriented line similar, resulting high rank correlation score. point lying occlusion boundary building sky, sets retrieved images different, resulting low rank correlation score. driven boundary detector gating maximum response orientations, rank correlation coeﬃcient  ,   , max, ?)]. ) note type data driven boundary detection image based edge detection) strong image edges receive low score provided matched image structures side boundary-occur places image collection) weak image edges receive high score, provided neighboring image structures side weak image boundary-occur database. contrast detector, trained manually labelled object boundaries, data driven boundary scores determined based-occurrence statistics similar scenes require additional manual supervision. figure shows examples data driven boundary detection results. quantitative evaluation section pdb, , region process: data driven image grouping goal group pixels query image belong object major scene element (such building, tree, road). relying local appearance similarity, color texture, turn dataset scenes image stack suggest groupings. hyposis regions semantically meaningful objects coherent large part stack. refore, goal find clusters stack) self-consistent) explain query image. note now, make hard decisions, refore, multiple clusters explain overlapping parts query image. example, tree cluster building cluster (drawn parts stack) explain patch image, hyposes retained. way, final segmentation step section free chose set clusters based information global framework. refore approach find clusters image patches match images stack. words, patches query image belong group sets matching images database similar. boundary process section query image compared database image query patch location. matching translation invariant. note patches appearance grouped toger long match database images. example) Figure Data driven boundary detection. ) Input image. ) Ground truth boundaries. ]. ) Proposed data driven boundary detection. notice enhanced object boundaries suppressed false positives boundaries inside objects. door window building grouped toger shape appearance long-occur toger (and matched) images. type matching selfsimilarity matching] image patches image grouped toger similar. formally, database scene images, rectangular patch query image dimensional binary vector element] set image database 1000 nearest neighbors patch. elements set nearest neighbors patch obtained matching local gist color descriptors image location section center weighted full Gaussian mask pixels. find cluster centers , }. methods exist finding clusters space. example, desired object clusters ?topics image stack? apply standard topic discovery methods probabilistic latent semantic analysis (plsa] Latent Dirichlet Allocation (lda]. however, found simple-means algorithm applied indicator vectors produced good results. clearly, number clusters, important parameter. discover semantic objects stack, explain query image, found small number clusters. suﬃcient. figure shows heat maps similarity (measured binary vector recovered cluster centers. notice regions belonging major scene components highlighted. hard-means clustering applied cluster patches stage, soft similarity score patch cluster segmentation cost function incorporating region boundary cues, next.   image segmentation combining boundary region cues preceding sections developed models estimating data-driven scene boundaries coherent regions image stack. note boundary region processes data, fact producing different, complementary, types information. region process aims find large groups coherent pixels-occur toger often, concerned precise localization. boundary process, hand, focuses rar myopically local image behavior boundaries excellent localiza5 Figure Data driven image grouping. left: input image. right: heat maps indicating groupings pixels belonging scene component, found clustering image patches match set images stack (warmer colors correspond higher similarity cluster center). notice regions belonging major scene components highlighted. also, local regions appearances. doors windows interior building) map cluster match set images. finally, highlighted regions tend overlap, reby providing multiple hyposes local region. tion. pieces information needed successful scene segmentation explanation. section, propose single mrf-based optimization framework task, negotiate global \\x0cregion process well-localized boundary process. set multi-state MRF pixels segmentation, states correspond image stack groups section mrf formulated follows: min) ,   state pixel image stack groups (section), unary costs defined similarity patch pixel indicator vector (section), image stack groups,  binary costs boundarydependent Potts model (section). additional outlier state regions match clusters well. pairwise term, assume 4neighbourhood structure. extent adjacent horizontal vertical neighbors. unary term Equation encourages pixels explained group images stack receive label. binary term encourages neighboring pixels label, case strong boundary evidence. details, unary term ,    ) scalar parameter ctk similarity indicator vector describing local image appearance pixel (section cluster center pairwise term defined )) , function dependent output data-driven boundary detector (equation),  scalar parameters. line process output strength orientation defined pixels rar pixels, standard contrast dependent pairwise term], care place pairwise costs consistently side continuous boundary. this, max? pdb, argmax? pdb, ?). vertical neighbors, top, max,  horizontal neighbors, left, max,     ]}. notice PDB non-negative everywhere, incorporate cost model difference adjacent elements positive. minimize Equation) graph cuts alpha-beta swaps]. optimized parameters validation set manual tuning boundary detection task (section). set , , . note number recovered segments necessarily equal number image stack groups data?driven detector (pdb Segmentation \\x0cfigure Evaluation boundary detection task principal occlusion contact boundaries extracted LabelMe database]. show precision-recall curves] (blue triangle line) data-driven boundary detector (red circle line). notice achieve improved performance recalls. show precision recall output segmentations (green star), achieves precision recall. recall level, data-driven boundary detector achieves precision, respectively.  Precision Recall Experimental evaluation section, evaluate data-driven boundary detector proposed image segmentation model challenging dataset complex street scenes LabelMe database]. unlabelled scene database, dataset 100k street scene images gared Flickr]. boundary detection image grouping applied candidate set images. figure shows final segmentations. notice recovered segments correspond large objects depicted images, segment boundaries aligning objects? boundaries. segment-query image stack segment weighted mask retrieve images match appearance segment. top matches segment stitched toger form composite, shown Figure comparison, show top matches global descriptor. notice composites align contents depicted input image. quantitatively evaulate system measuring detect ground truth object boundaries provided human labelers. evaluate object boundary detection, 100 images depicting street scenes benchmark set LabelMe database]. benchmark set consists fully labeled images world. number types edges implicitly labeled LabelMe database, arising occlusion, attachment, contact ground. work, filter attached objects. window attached building generate object boundaries) techniques outlined]. note benchmark task BSDS] dataset explicitly occlusion boundaries interior contours. measure performance, evaluation procedure outlined], aligns output boundaries threshold ground truth boundaries compute precision recall. curve generated evaluating thresholds. boundary considered correct, assume lie pixels ground truth boundary. figure shows precision-recall curve data-driven boundary detector. compare color]. notice achieve higher precision recall levels. plot precision recall output segmentation produced system. notice segmentation produced highest precision recall. improvement performance low recall largely due ability suppress interior contours due attached objects. figure). however, tend miss small, moveable objects, accounts lower performance high recall. conclusion shown unsupervised analysis large image collection segmenting complex scenes semantically coherent parts. exploit object variations related images mrf-based segmentation optimizes matches preserving scene boundaries obtained data driven boundary detection process. demonstrated improved performance detecting principal occlusion contact boundaries previous methods challenging dataset complex street scenes labelme. work suggests applications Figure left: Output segmentation produced system. notice segment boundaries align depicted objects scene. Top right: Top matches recovered segment, stitched toger form composite. bottom right: Top whole-image matches gist descriptor. recovering segmentation, recover improved semantic matches. scene matching, object recognition computer graphics, benefit segment-based explanations query scene. acknowledgments: This work partially supported ONR MURI n00014-0734, ONR MURI n00014-0182, NGA negi-1582-0004, NSF grant iis-0546547, gifts Microsoft Research google, Guggenheim Sloan fellowships. Segmenting semantic objects, broadly image parsing, fundamentally challenging problem. task painfully under-constrained single image, extremely diﬃcult partition semantically meaningful elements, blobs similar color texture. for example, algorithm figure doors windows building, different, belong segment? grey pavement grey house segments? clearly, information image required solve problem. paper, argue extra information extracted images visually similar one. with increasing availability Internetscale image collections millions images! idea data-driven scene matching recently shown promise variety tasks. simply finding matching images low-dimentinal descriptor transfering labels input image, impressive results demonstrated \\x0cobject scene recognition], object detection], image geo-location], object event annotation], ors. even image collection labels, shown tasks image completion exploration], image colorization], surface layout estimation]. however, noted authors illustrated Figure major stumbling block scene-matching approaches that, large quantities data, types images quality matches good. part reason low-level image descriptors matching powerful capture semantic similarity. several approaches proposed address shortcoming, including syntically increasing dataset transformed copies images], cleaning matching results clustering], automatically prefiltering dataset], simply picking good matches hand]. all appraoches improve performance don alleviate issue entirely. fundamental problem variability visual world vast, exponential number object combinations scene,  willow project-team, Laboratoire?informatique?ecole Normale sup?erieure ens/inria/cnrs UMR 8548 Figure Illustration scene matching problem. left: Input image (along output segmentation system overlaid) matched dataset 100k street images. notice output segment boundaries align depicted objects scene. top right: top retrieved images, based matching gist descriptor] entire image. matches good. bottom right: Searching matches estimated segment (using gist representation segment) compositing results yields matches input image. futile expect find single good match all! instead, argue input image explained spatial composite regions database images. aim break image chunks small good matches database, large matches retain informative power.  Overview work, propose apply scene matching problem segmenting semantically meaningful objects. seek segment objects enclosed principal occlusion contact boundaries objects part attached objects). idea turn advantage fact scene matches perfect. what typically scene matching part image matched well, parts matched approximately, coarse level. for example, street scene, matching image building match well, shape road wrong, anor matching image road right, tree building. differences matching provide powerful signal identify objects segmentation boundaries. computing matching image composite, explain input image. match region input image semantically similar regions images) single match. starting point algorithm input image ?image stack?  set coarsely matching images (5000 case) retrieved large dataset standard image matching technique (gist] case). essence, image stack dataset, tailor-made match scene structure input image. intuitively, goal image stack segment (and ?explain?) input image semantically meaningful way. idea that, stack more-less aligned, regions semantic objects present images consistently spatial location. input image explained patch-work consistent regions, simultaneously producing segmentation, composite matches, individual matches stack. prior work producing resulting image stack aligned images depicting scene, PhotoMontage work], optimally selects regions globally aligned images based quality score composite visually pleasing output image. recently, work based PhotoMontage framework automatically align images depicting scene objects perform segmentation], region-filling], outlier detection]. contrast, work, attempting work stack visually similar, physically different, scenes. this spirit contemporary work], work supervised data, completely unsupervised. also related contemporary work]. our approach combines boundary-based region-based segmentation processes toger single MRF framework. boundary process (section stack determine semantic boundaries objects. region process (section aims group pixels belonging object stack. cues combined toger MRF framework solved GraphCut optimization (section). present results Section Boundary process: data driven boundary detection Information single image cases suﬃcient recovering boundaries objects. strong image edges correspond internal object structures, window wheel car. additionally, boundaries objects produce weak image evidence, boundary building road similar color partially occluding. here, propose analyze statistics large number related images stack) recover boundaries objects. exploit fact objects tend rest location relative scene. for example, street scene, car adjacent regions belonging number objects, building, person, road, etc. hand, relative positions internal object structures consistent images. for example, wheels windows car consistently roughly similar positions images. recover object boundaries, measure ability consistently match locally set images stack. intuitively, regions inside object tend match set images, similar appearance, regions opposite sides boundary match sets images. more formally, oriented line passing image point orientation analyze statistics sets images similar appearance side line. for side oriented line, independently query stack images forming local image descriptor modulated weighted mask. half-gaussian weighting mask oriented line centered image point This local mask modulates Gabor filter responses orientations scales) RGB color channels, descriptor formed averaging Gabor energy color pixel spatial bins. Gaussian modulated descriptor, captures appearance information side boundary point orientation appearance descriptors extracted manner image stack compared query image descriptor distance. images stack assumed coarsely aligned, matches considered query location orientation stack. matching translation invariant. type spatially dependent matching suitable scene images consistent spatial layout considered work. quality matches furr improved fine aligning stack images query]. for image point orientation output local matching sides oriented line ranked lists image stack indices, ordering list distance local descriptors, query image image stack. compute spearman rank correlation coeﬃcient rank-ordered lists d2i) ,   number images stack difference ranks stack image ranked lists, high rank correlation point lies inside object extent, low correlation point object boundary orientation note however, low rank correlations caused poor quality local matches. figure illustrates boundary detection process. for eﬃciency reasons, compute rank correlation score points orientations marked boundaries probability boundary edge detector], boundary orientations  , quantized steps. final boundary score proposed data Figure Data driven boundary detection. left: Input image query edges shown. right: top matches large collection images side query edges. rank correlation occlusion boundary.0998; rank correlation road region.6067. notice point lying inside object road), ranked sets retrieved images sides oriented line similar, resulting high rank correlation score.  point lying occlusion boundary building sky, sets retrieved images different, resulting low rank correlation score. driven boundary detector gating maximum response orientations, rank correlation coeﬃcient  ,   , max, ?)]. ) note type data driven boundary detection image based edge detection) strong image edges receive low score provided matched image structures side boundary-occur places image collection) weak image edges receive high score, provided neighboring image structures side weak image boundary-occur database. contrast detector, trained manually labelled object boundaries, data driven boundary scores determined based-occurrence statistics similar scenes require additional manual supervision. figure shows examples data driven boundary detection results. quantitative evaluation section pdb, , Region process: data driven image grouping goal group pixels query image belong object major scene element (such building, tree, road). instead relying local appearance similarity, color texture, turn dataset scenes image stack suggest groupings. our hyposis regions semantically meaningful objects coherent large part stack. refore, goal find clusters stack) self-consistent) explain query image. note now, make hard decisions, refore, multiple clusters explain overlapping parts query image. for example, tree cluster building cluster (drawn parts stack) explain patch image, hyposes retained. this way, final segmentation step section free chose set clusters based information global framework. refore approach find clusters image patches match images stack. words, patches query image belong group sets matching images database similar. boundary process section query image compared database image query patch location. matching translation invariant. note patches appearance grouped toger long match database images. for example) Figure Data driven boundary detection. ) Input image. ) Ground truth boundaries. ]. ) Proposed data driven boundary detection. notice enhanced object boundaries suppressed false positives boundaries inside objects. door window building grouped toger shape appearance long-occur toger (and matched) images. this type matching selfsimilarity matching] image patches image grouped toger similar. formally, database scene images, rectangular patch query image dimensional binary vector element] set image database 1000 nearest neighbors patch. elements set nearest neighbors patch obtained matching local gist color descriptors image location section center weighted full Gaussian mask pixels. find cluster centers , }. many methods exist finding clusters space. for example, desired object clusters ?topics image stack? apply standard topic discovery methods probabilistic latent semantic analysis (plsa] Latent Dirichlet Allocation (lda]. however, found simple-means algorithm applied indicator vectors produced good results. clearly, number clusters, important parameter. because discover semantic objects stack, explain query image, found small number clusters. suﬃcient. figure shows heat maps similarity (measured binary vector recovered cluster centers. notice regions belonging major scene components highlighted. although hard-means clustering applied cluster patches stage, soft similarity score patch cluster segmentation cost function incorporating region boundary cues, next.   Image segmentation combining boundary region cues preceding sections developed models estimating data-driven scene boundaries coherent regions image stack. note boundary region processes data, fact producing different, complementary, types information. region process aims find large groups coherent pixels-occur toger often, concerned precise localization. boundary process, hand, focuses rar myopically local image behavior boundaries excellent localiza5 Figure Data driven image grouping. left: input image. right: heat maps indicating groupings pixels belonging scene component, found clustering image patches match set images stack (warmer colors correspond higher similarity cluster center). notice regions belonging major scene components highlighted. also, local regions appearances. doors windows interior building) map cluster match set images. finally, highlighted regions tend overlap, reby providing multiple hyposes local region. tion. both pieces information needed successful scene segmentation explanation. section, propose single mrf-based optimization framework task, negotiate global \\x0cregion process well-localized boundary process. set multi-state MRF pixels segmentation, states correspond image stack groups section MRF formulated follows: min) ,   state pixel image stack groups (section), unary costs defined similarity patch pixel indicator vector (section), image stack groups,  binary costs boundarydependent Potts model (section). additional outlier state regions match clusters well. for pairwise term, assume 4neighbourhood structure. extent adjacent horizontal vertical neighbors. unary term Equation encourages pixels explained group images stack receive label. binary term encourages neighboring pixels label, case strong boundary evidence. details, unary term ,    ) scalar parameter ctk similarity indicator vector describing local image appearance pixel (section cluster center pairwise term defined )) , function dependent output data-driven boundary detector (equation),  scalar parameters. since line process output strength orientation defined pixels rar pixels, standard contrast dependent pairwise term], care place pairwise costs consistently side continuous boundary. for this, max? pdb, argmax? pdb, ?). vertical neighbors, top, max,  horizontal neighbors, left, max,     ]}. notice PDB non-negative everywhere, incorporate cost model difference adjacent elements positive. minimize Equation) graph cuts alpha-beta swaps]. optimized parameters validation set manual tuning boundary detection task (section). set , , . note number recovered segments necessarily equal number image stack groups data?driven detector (pdb Segmentation \\x0cfigure Evaluation boundary detection task principal occlusion contact boundaries extracted LabelMe database]. show precision-recall curves] (blue triangle line) data-driven boundary detector (red circle line). notice achieve improved performance recalls. show precision recall output segmentations (green star), achieves precision recall. recall level, data-driven boundary detector achieves precision, respectively.  Precision Recall Experimental evaluation section, evaluate data-driven boundary detector proposed image segmentation model challenging dataset complex street scenes LabelMe database]. for unlabelled scene database, dataset 100k street scene images gared Flickr]. boundary detection image grouping applied candidate set images. figure shows final segmentations. notice recovered segments correspond large objects depicted images, segment boundaries aligning objects? boundaries. for segment-query image stack segment weighted mask retrieve images match appearance segment. top matches segment stitched toger form composite, shown Figure comparison, show top matches global descriptor. notice composites align contents depicted input image. quantitatively evaulate system measuring detect ground truth object boundaries provided human labelers. evaluate object boundary detection, 100 images depicting street scenes benchmark set LabelMe database]. benchmark set consists fully labeled images world. number types edges implicitly labeled LabelMe database, arising occlusion, attachment, contact ground. for work, filter attached objects. window attached building generate object boundaries) techniques outlined]. note benchmark task BSDS] dataset explicitly occlusion boundaries interior contours. measure performance, evaluation procedure outlined], aligns output boundaries threshold ground truth boundaries compute precision recall. curve generated evaluating thresholds. for boundary considered correct, assume lie pixels ground truth boundary. figure shows precision-recall curve data-driven boundary detector. compare color]. notice achieve higher precision recall levels. plot precision recall output segmentation produced system. notice segmentation produced highest precision recall. improvement performance low recall largely due ability suppress interior contours due attached objects. figure). however, tend miss small, moveable objects, accounts lower performance high recall. Conclusion shown unsupervised analysis large image collection segmenting complex scenes semantically coherent parts. exploit object variations related images mrf-based segmentation optimizes matches preserving scene boundaries obtained data driven boundary detection process. demonstrated improved performance detecting principal occlusion contact boundaries previous methods challenging dataset complex street scenes labelme. our work suggests applications Figure left: Output segmentation produced system. notice segment boundaries align depicted objects scene. Top right: Top matches recovered segment, stitched toger form composite. bottom right: Top whole-image matches gist descriptor. recovering segmentation, recover improved semantic matches. scene matching, object recognition computer graphics, benefit segment-based explanations query scene. acknowledgments: This work partially supported ONR MURI n00014-0734, ONR MURI n00014-0182, NGA negi-1582-0004, NSF grant iis-0546547, gifts Microsoft Research google, Guggenheim Sloan fellowships.',\n",
       " 'PP3733': 'neural implementations reinforcement learning solve basic credit assignment problems) temporal credit assignment problem., question actions past crucial receiving reward) spatial credit assignment problem., question, neurons population important reward not. here, argue additional credit assignment problem arises implementations reinforcement learning spiking neurons. presume spike pattern specific neuron specific time interval crucial reward (that, solved credit assignment problems). question remains: Which feature spike pattern important reward? spike train number spikes yield reward precisely timed spikes? credit assignment problem essence question neural code output neuron) using. important, change neuronal parameters synaptic weights order maximize likelihood reward future. spike count relevant, effective spend lot time energy diﬃcult task learning precisely timed spikes. modest versatile solving problem make assumption neural code assume features spike train important.  -mail: henning.sprekeler@epﬂ case, neuronal parameters changed likelihood repeating spike train synaptic input maximized. approach leads learning rule derived number recent publications]. here, show class learning rules emerges prior knowledge neural code hand available. policygradient framework, derive learning rules neural parameters synaptic weights threshold parameters maximize expected reward. aims) develop systematic framework derive learning rules arbitrary neural parameters neural codes) provide intuitive understanding resulting learning rules work) derive test learning rules specific codes) provide oretical basis code-specific learning rules superior general-purpose rules. finally, argue learning rules types prediction problems, related reward prediction, response prediction.  General framework Coding features policy-gradient approach basic setup following: set input spike trains single postsynaptic neuron, response generates stochastic output spike trains  language partially observable Markov decision processes, input spike trains observations provide information state animal output spike trains controls inﬂuence action choice. depending spike trains, system receives reward. goal adjust set parameters postsynaptic neuron maximizes expectation reward. central assumption reward depend full output spike train, set coding features output spike train). coding features reward depends fact choice neural code, features spike train behaviorally relevant. note conceptual difference notion neural code sensory processing, coding features convey information input signals, output signal rewards. expectation reward hri, ) \\x0cdenotes probability presynaptic spike trains, conditional probability generating coding feature input spike train neuronal parameters note component explicitly depends neural parameters conditional probability, ?). reward conditionally independent neural parameters coding feature refore, optimize expected reward employing gradient ascent method, learning rule form )? ,  , )? ,   choose small learning rate average presynaptic patterns coding features replaced time average. online learning rule refore results dropping average)? ,  ) This general form learning rule policy-gradient approaches reinforcement learning].  Learning rules exponentially distributed coding features joint distribution coding features factorized set conditional distributions make assumption conditional distributions belong natural exponential family (nef  exp  )), parameters depend input spike train coding features neural parameters  function function characteristic distribution depends parameters note NEF rich class distributions, includes canonical distributions poisson, Bernoulli Gaussian distribution fixed variance). assumptions, learning rule) takes characteristic shape: ,  variance conditional distribution refore depend input coding features parameters note correlations coding features implicitly accounted dependence features. summation coding features arises factorization distribution, specific shape summands relies assumption normal exponential distributions [for proof. ]. simple intuition learning rule) performs gradient ascent reward. term ﬂuctuates trial-trial basis. ﬂuctuations positively correlated trial ﬂuctuations reward higher values lead higher reward, coding feature increased. increase \\x0cimplemented term neural parameter increases. examples Coding Features section, illustrate framework deriving policy-gradient rules neural codes show solve simple computational tasks. neuron type simple poisson-type neuron model postsynaptic firing rate nonlinear function ) membrane potential membrane potential turn, sum EPSPs evoked presynaptic spikes, weighted respective synaptic weights) tfi PSPi tfi denote time spike presynaptic neuron.  tfi denotes shape postsynaptic potential evoked single presynaptic spike time tfi future use, introduced PSPi postsynaptic potential evoked presynaptic spike train alone, synaptic weight unity. parameters optimize neuron model) synaptic weights) parameters dependence firing rate membrane potential. case standard case synaptic plasticity, corresponds reward-driven version intrinsic plasticity. ].  Spike Count codes: Synaptic plasticity Let assume coding feature number spikes time window, reward delivered end period. probability distribution spike count Poisson distribution exp(??   integral firing rate interval,  dt0 ) dependence distribution presynaptic spike trains synaptic weights hidden spike count naturally depends factors postsynaptic firing rate because Poisson distribution belongs nef, derive synaptic learning rule equation) calculating form term     )pspi dt0 ) This learning rule structural similarities bienenstock-coopermunro (bcm) rule]: integral term structure eligibility trace driven simple Hebbian learning rule. addition, learning modulated factor compares current spike count (?rate?) expected spike count (?sliding threshold? bcm ory). interestingly, functional role factor original BCM rule: meant introduce selectivity], rar exploit trial ﬂuctuations spike count explore structure \\x0creward landscape. test learning rule-armed bandit task (figure). agent choice actions. depending states agent, action action rewarded), action punished). state information encoded rate pattern 100 presynaptic neurons. state, input pattern generated drawing firing rate input neuron independently exponential distribution 10hz. trial, input spike trains generated anew Poisson processes neuronand statespecific rates. agent chooses action stochastically probabilities proportional spike counts output neurons spike counts depend state presynaptic firing rates, agent choose actions states. figure show learning rule learns task suppressing activity neuron encodes punished action. simulations paper, postsynaptic neurons exponential rate function) exp  )), threshold sharpness parameter set eir (for-armed bandit task) (for spike latency task). moreover, postsynaptic neurons membrane potential reset spike., relative refractoriness), assumption Poisson distribution spike counts necessarily fulfilled. worth noting impeding effect learning performance.  Spike Count codes: Intrinsic plasticity Let assume rate neuron function )  depends threshold parameters typical choices function exponential simulations), sigmoid threshold linear function exp)). intrinsic plasticity parameters learned addition synaptic weights. learning rules parameters essentially synaptic weights, derivative spike count respect respectively:       )          ) )    ) here) denotes derivative rate function respect argument.  First spike-latency code: Synaptic plasticity coding scheme, assume reward depends latency spike stimulus onset. precisely, assume trial starts onset presynaptic spike trains reward delivered time spike. reward depends latency spike, latencies favored. figure Simulations code-specific learning rules. -armed bandit task: agent choose actions depending state action rewarded (thick arrows). input states modelled firing rate patterns input neurons. probability choosing actions proportional spike counts \\x0coutput neurons learning curves-armed bandit. blue: Spike count learning rule), red: Full spike train rule). evolution spike count response input states learning. rewards (panel spike counts (panel low-pass filtered time constant 4000 trials. learning spike latencies latency rule). output neurons learn fire spike target latencies present fixed input spike train patterns (?stimuli?) neurons randomly interleaved trials. input spike train input neuron drawn separately stimulus sampling Poisson process rate 10hz. reward negative squared difference target latency (stimulus 10ms, 30ms, stimulus 30ms, 10ms) actual latency trial, summed neurons. colored curves show spike latencies neurons (green, red) neuron (purple, blue) converge target latencies. black curve (scale axis) shows evolution reward learning. probability distribution spike latency product firing probability time probability neuron fire earlier) exp ) Using. ) distribution, synaptic learning rule)pspi )pspi ) Figure, show learning rule learn adjust weights neurons spike latencies approximate set target latencies.  Full Spike Train code: Synaptic plasticity finally, general coding feature, namely, full spike train. start time-discretized version spike train discretization suﬃciently narrow spike time bin. time bin], number spikes Bernoulli distribution spiking probability depends input recent history neuron. Bernoulli distribution belongs nef, policy-gradient rule derived equation): ) firing probability depends instantaneous firing rate?exp(? ), yielding:  [??   pspi) This rule discretized simulations. limit approximated  , leads continuous time version rule: lim pspi )  )) PSPi) ) ) here)  sum ?-functions. note learning rule) proposed Xie Seung] Florian] and, slightly modified supervised learning, Pfister. ]. line, policy gradient rules derived intrinsic parameters neuron., threshold parameters (see]). why code-specific rules general rules available? obviously, learning rule) general sense considers spike train coding feature. features refore captured learning rule. natural question advantage rules specialized specific code? say, learning rule coding features correlated reward. learning rule neuronal parameter structure:    )          )                   ) terms lines), term non-vanishing taking trial average. terms simply noise refore hindrance maximize reward. full learning rule features, learning rate decreased agreeable signal-noise ratio drift introduced term diffusion caused terms reached. refore, desirable faster learning reduce effects noise terms. ways: terms. ) reduced reducing achieved subtracting suitable reward baseline current reward. ideally, stimulus-specific (because depends stimulus), leads notion reward prediction error pure reward signal. approach line standard reinforcement learning ory] proposal neuromodulatory signals dopamine represent reward prediction error reward alone.  term. ) removed skipping terms original learning related coding feature corresponds learning rule features fact correlated reward \\x0csuppressing correlated reward. central argument code-specific learning rules refore signalto-noise ratio. extreme cases, general rule specific task, large number coding dimensions give rise noise learning dynamics, relevant systematic changes. considerations suggest spike count rule) outperform full spike train rule) tasks reward based purely spike count. unfortunately, substantiate claim simulations. figure, performance rules similar-armed bandit task. due noise bottleneck effect: sources noise learning process, strongest limits performance. ?code-specific noise? dominant, code-specific learning rules performance general purpose rules. inherent Prediction Problems shown section policy-gradient rule reduced amount noise gradient estimate takes relevant coding features account subtracts trial reward:     ...))  This learning rule conceptually interesting structure: Learning takes place conditions fulfilled: animal unexpected receives unexpected reward  ...)). moreover, raises interesting prediction problems) prediction trial average coding feature conditioned stimulus) reward expected coding feature takes value.  Prediction coding feature cases derive learning rule analytically, trial average coding feature calculated intrinsic properties neuron membrane potential. unfortunately, clear priori information calculating available. problematic extend framework coding features populations, population know., membrane properties members. interesting alternative trial calculated prediction system., topdown signals prior information internal world model predict expected coding feature. learning case modulated mismatch top-down prediction coding feature represented) real calculated ?bottom? approach. interpretation bears interesting parallels approaches sensory coding, interpretation sensory information based comparison sensory input internally generated prediction generative model. ]. experimental evidence neural stimulus prediction comparably low-level systems retina. ]. anor prediction system expected response population coding scheme, population neurons receiving input produce output. neuron population receive average popu \\x0clation activity prediction response. interesting study relation approach recently proposed reinforcement learning populations spiking neurons].  Reward prediction quantity predicted learning rule reward coding feature mean. distribution coding feature suﬃciently narrow range takes stimulus, reward approximated linear function, reward(?) simply expectation reward stimulus(?)  ) relevant quantity learning refore reward prediction error  classical reinforcement learning, term calculated actor-critic architecture, external module critic learns expected future reward eir states stateaction pairs. values calculate expected reward current state state-action pair. difference reward received predicted reward reward prediction error drives learning. evidence dopamine signals brain encode prediction error rar reward]. discussion presented general framework deriving policy-gradient rules spiking neurons shown learning rules emerge depending features spike trains assumed inﬂuence reward signals. oretical arguments suggest code-specific learning rules superior general rules, noise estimate gradient smaller. simulations check case applications code-specific learning rules advantageous. exponentially distributed coding features, learning rule characteristic structure, simple intuitive interpretation. moreover, structure raises prediction problems, provide links concepts) notion reward prediction error reduce variance estimate gradient creates link actor-critic architectures) notion coding feature prediction reminiscent combined topdown?bottom approaches, sensory learning driven mismatch internal predictions sensory signal]. fact class code-specific policy-gradient learning rules opens interesting possibility neuronal learning rules controlled metalearning processes shape learning rule neural code effect. biological perspective, interesting compare spike-based synaptic plasticity brain regions thought neural codes systematic differences. Neural implementations reinforcement learning solve basic credit assignment problems) temporal credit assignment problem., question actions past crucial receiving reward) spatial credit assignment problem., question, neurons population important reward not. here, argue additional credit assignment problem arises implementations reinforcement learning spiking neurons. presume spike pattern specific neuron specific time interval crucial reward (that, solved credit assignment problems). one question remains: Which feature spike pattern important reward? would spike train number spikes yield reward precisely timed spikes? this credit assignment problem essence question neural code output neuron) using. important, change neuronal parameters synaptic weights order maximize likelihood reward future. spike count relevant, effective spend lot time energy diﬃcult task learning precisely timed spikes. modest versatile solving problem make assumption neural code assume features spike train important.  -mail: henning.sprekeler@epﬂ case, neuronal parameters changed likelihood repeating spike train synaptic input maximized. this approach leads learning rule derived number recent publications]. here, show class learning rules emerges prior knowledge neural code hand available. using policygradient framework, derive learning rules neural parameters synaptic weights threshold parameters maximize expected reward. our aims) develop systematic framework derive learning rules arbitrary neural parameters neural codes) provide intuitive understanding resulting learning rules work) derive test learning rules specific codes) provide oretical basis code-specific learning rules superior general-purpose rules. finally, argue learning rules types prediction problems, related reward prediction, response prediction.  General framework Coding features policy-gradient approach basic setup following: set input spike trains single postsynaptic neuron, response generates stochastic output spike trains  language partially observable Markov decision processes, input spike trains observations provide information state animal output spike trains controls inﬂuence action choice. depending spike trains, system receives reward. goal adjust set parameters postsynaptic neuron maximizes expectation reward. our central assumption reward depend full output spike train, set coding features output spike train). which coding features reward depends fact choice neural code, features spike train behaviorally relevant. note conceptual difference notion neural code sensory processing, coding features convey information input signals, output signal rewards. expectation reward hri, ) \\x0cdenotes probability presynaptic spike trains, conditional probability generating coding feature input spike train neuronal parameters note component explicitly depends neural parameters conditional probability, ?). reward conditionally independent neural parameters coding feature refore, optimize expected reward employing gradient ascent method, learning rule form )? ,  , )? ,   choose small learning rate average presynaptic patterns coding features replaced time average. online learning rule refore results dropping average)? ,  ) This general form learning rule policy-gradient approaches reinforcement learning].  Learning rules exponentially distributed coding features joint distribution coding features factorized set conditional distributions make assumption conditional distributions belong natural exponential family (nef  exp  )), parameters depend input spike train coding features neural parameters  function function characteristic distribution depends parameters note NEF rich class distributions, includes canonical distributions poisson, Bernoulli Gaussian distribution fixed variance). under assumptions, learning rule) takes characteristic shape: ,  variance conditional distribution refore depend input coding features parameters note correlations coding features implicitly accounted dependence features. summation coding features arises factorization distribution, specific shape summands relies assumption normal exponential distributions [for proof. ]. simple intuition learning rule) performs gradient ascent reward. term ﬂuctuates trial-trial basis. ﬂuctuations positively correlated trial ﬂuctuations reward higher values lead higher reward, coding feature increased. this increase \\x0cimplemented term neural parameter increases. Examples Coding Features section, illustrate framework deriving policy-gradient rules neural codes show solve simple computational tasks. neuron type simple poisson-type neuron model postsynaptic firing rate nonlinear function ) membrane potential membrane potential turn, sum EPSPs evoked presynaptic spikes, weighted respective synaptic weights) tfi PSPi tfi denote time spike presynaptic neuron.  tfi denotes shape postsynaptic potential evoked single presynaptic spike time tfi for future use, introduced PSPi postsynaptic potential evoked presynaptic spike train alone, synaptic weight unity. parameters optimize neuron model) synaptic weights) parameters dependence firing rate membrane potential. case standard case synaptic plasticity, corresponds reward-driven version intrinsic plasticity. ].  Spike Count codes: Synaptic plasticity Let assume coding feature number spikes time window, reward delivered end period. probability distribution spike count Poisson distribution exp(??   integral firing rate interval,  dt0 ) dependence distribution presynaptic spike trains synaptic weights hidden spike count naturally depends factors postsynaptic firing rate Because Poisson distribution belongs nef, derive synaptic learning rule equation) calculating form term     )pspi dt0 ) This learning rule structural similarities bienenstock-coopermunro (bcm) rule]: integral term structure eligibility trace driven simple Hebbian learning rule. addition, learning modulated factor compares current spike count (?rate?) expected spike count (?sliding threshold? BCM ory). interestingly, functional role factor original BCM rule: meant introduce selectivity], rar exploit trial ﬂuctuations spike count explore structure \\x0creward landscape. test learning rule-armed bandit task (figure). agent choice actions. depending states agent, action action rewarded), action punished). state information encoded rate pattern 100 presynaptic neurons. for state, input pattern generated drawing firing rate input neuron independently exponential distribution 10hz. trial, input spike trains generated anew Poisson processes neuronand statespecific rates. agent chooses action stochastically probabilities proportional spike counts output neurons because spike counts depend state presynaptic firing rates, agent choose actions states. figure show learning rule learns task suppressing activity neuron encodes punished action. simulations paper, postsynaptic neurons exponential rate function) exp  )), threshold sharpness parameter set eir (for-armed bandit task) (for spike latency task). moreover, postsynaptic neurons membrane potential reset spike., relative refractoriness), assumption Poisson distribution spike counts necessarily fulfilled. worth noting impeding effect learning performance.  Spike Count codes: Intrinsic plasticity Let assume rate neuron function )  depends threshold parameters typical choices function exponential simulations), sigmoid threshold linear function exp)). intrinsic plasticity parameters learned addition synaptic weights. learning rules parameters essentially synaptic weights, derivative spike count respect respectively:       )          ) )    ) here) denotes derivative rate function respect argument.  First spike-latency code: Synaptic plasticity coding scheme, assume reward depends latency spike stimulus onset. more precisely, assume trial starts onset presynaptic spike trains reward delivered time spike. reward depends latency spike, latencies favored. Figure Simulations code-specific learning rules. -armed bandit task: agent choose actions depending state action rewarded (thick arrows). input states modelled firing rate patterns input neurons. probability choosing actions proportional spike counts \\x0coutput neurons Learning curves-armed bandit. blue: Spike count learning rule), red: Full spike train rule). Evolution spike count response input states learning. both rewards (panel spike counts (panel low-pass filtered time constant 4000 trials. Learning spike latencies latency rule). two output neurons learn fire spike target latencies present fixed input spike train patterns (?stimuli?) neurons randomly interleaved trials. input spike train input neuron drawn separately stimulus sampling Poisson process rate 10hz. reward negative squared difference target latency (stimulus 10ms, 30ms, stimulus 30ms, 10ms) actual latency trial, summed neurons. colored curves show spike latencies neurons (green, red) neuron (purple, blue) converge target latencies. black curve (scale axis) shows evolution reward learning. probability distribution spike latency product firing probability time probability neuron fire earlier) exp ) Using. ) distribution, synaptic learning rule)pspi )pspi ) Figure, show learning rule learn adjust weights neurons spike latencies approximate set target latencies.  Full Spike Train code: Synaptic plasticity finally, general coding feature, namely, full spike train. let start time-discretized version spike train discretization suﬃciently narrow spike time bin. time bin], number spikes Bernoulli distribution spiking probability depends input recent history neuron. because Bernoulli distribution belongs nef, policy-gradient rule derived equation): ) firing probability depends instantaneous firing rate?exp(? ), yielding:  [??   PSPi) This rule discretized simulations. limit approximated  , leads continuous time version rule: lim PSPi )  )) PSPi) ) ) here)  sum ?-functions. note learning rule) proposed Xie Seung] Florian] and, slightly modified supervised learning, Pfister. ]. following line, policy gradient rules derived intrinsic parameters neuron., threshold parameters (see]). Why code-specific rules general rules available? obviously, learning rule) general sense considers spike train coding feature. all features refore captured learning rule. natural question advantage rules specialized specific code? say, learning rule coding features correlated reward. learning rule neuronal parameter structure:    )          )                   ) terms lines), term non-vanishing taking trial average. terms simply noise refore hindrance maximize reward. when full learning rule features, learning rate decreased agreeable signal-noise ratio drift introduced term diffusion caused terms reached. refore, desirable faster learning reduce effects noise terms. this ways: terms. ) reduced reducing this achieved subtracting suitable reward baseline current reward. ideally, stimulus-specific (because depends stimulus), leads notion reward prediction error pure reward signal. this approach line standard reinforcement learning ory] proposal neuromodulatory signals dopamine represent reward prediction error reward alone.  term. ) removed skipping terms original learning related coding feature this corresponds learning rule features fact correlated reward \\x0csuppressing correlated reward. central argument code-specific learning rules refore signalto-noise ratio. extreme cases, general rule specific task, large number coding dimensions give rise noise learning dynamics, relevant systematic changes. considerations suggest spike count rule) outperform full spike train rule) tasks reward based purely spike count. unfortunately, substantiate claim simulations. Figure, performance rules similar-armed bandit task. this due noise bottleneck effect: sources noise learning process, strongest limits performance. unless ?code-specific noise? dominant, code-specific learning rules performance general purpose rules. Inherent Prediction Problems shown section policy-gradient rule reduced amount noise gradient estimate takes relevant coding features account subtracts trial reward:     ...))  This learning rule conceptually interesting structure: Learning takes place conditions fulfilled: animal unexpected receives unexpected reward  ...)). moreover, raises interesting prediction problems) prediction trial average coding feature conditioned stimulus) reward expected coding feature takes value.  Prediction coding feature cases derive learning rule analytically, trial average coding feature calculated intrinsic properties neuron membrane potential. unfortunately, clear priori information calculating available. this problematic extend framework coding features populations, population know., membrane properties members. interesting alternative trial calculated prediction system., topdown signals prior information internal world model predict expected coding feature. learning case modulated mismatch top-down prediction coding feature represented) real calculated ?bottom? approach. this interpretation bears interesting parallels approaches sensory coding, interpretation sensory information based comparison sensory input internally generated prediction generative model. ]. experimental evidence neural stimulus prediction comparably low-level systems retina. ]. anor prediction system expected response population coding scheme, population neurons receiving input produce output. any neuron population receive average popu \\x0clation activity prediction response. interesting study relation approach recently proposed reinforcement learning populations spiking neurons].  Reward prediction quantity predicted learning rule reward coding feature mean. distribution coding feature suﬃciently narrow range takes stimulus, reward approximated linear function, reward(?) simply expectation reward stimulus(?)  ) relevant quantity learning refore reward prediction error  classical reinforcement learning, term calculated actor-critic architecture, external module critic learns expected future reward eir states stateaction pairs. values calculate expected reward current state state-action pair. difference reward received predicted reward reward prediction error drives learning. evidence dopamine signals brain encode prediction error rar reward]. Discussion presented general framework deriving policy-gradient rules spiking neurons shown learning rules emerge depending features spike trains assumed inﬂuence reward signals. oretical arguments suggest code-specific learning rules superior general rules, noise estimate gradient smaller. more simulations check case applications code-specific learning rules advantageous. for exponentially distributed coding features, learning rule characteristic structure, simple intuitive interpretation. moreover, structure raises prediction problems, provide links concepts) notion reward prediction error reduce variance estimate gradient creates link actor-critic architectures) notion coding feature prediction reminiscent combined topdown?bottom approaches, sensory learning driven mismatch internal predictions sensory signal]. fact class code-specific policy-gradient learning rules opens interesting possibility neuronal learning rules controlled metalearning processes shape learning rule neural code effect. from biological perspective, interesting compare spike-based synaptic plasticity brain regions thought neural codes systematic differences.',\n",
       " 'PP3783': 'source separation problems arise number signals mixed toger, objective estimate underlying sources based observed mixture. supervised, modelbased approach source separation, examples isolated sources train source models, combined order separate mixture. oppositely, unsupervised, blind source separation, general information sources available. estimating models sources, blind source separation based weak criteria minimizing correlations, maximizing statistical independence, fitting data subject constraints. assumptions linear mixing additive noise, blind source separation expressed matrix factorization problem equivalently, xij aik bkj nij \\x0cwhere subscripts matrices denote dimensions. columns represent unknown sources, elements mixing coeﬃcients. columns observation mixture sources additive noise represented columns objective estimate sources, mixing coeﬃcients, data matrix, observed. bayesian formulation, aim compute single infer posterior distribution set model assumptions. assumptions likelihood function), expresses probability data factorizing matrices, prior), describes knowledge observing data. depending specific choice likelihood priors, matrix factorizations characteristics devised. non-negative matrix factorization (nmf), distinguished matrix factorization techniques non-negativity constraints, shown decompose data meaningful, interpretable parts]; however, parts-based decomposition necessarily useful, Linear subspace Aﬃne subspace Simplicial cone Simplicial cone non-neg. orthant) bkj ) aik bkj ) Polytope non-neg. orthant Polytope unit simplex Polytope unit hypercube aik aik bkj bkj)  bkj bkj) bkj constraints) Polytope bkj ) bkj bkj aik  bkj) Figure Examples model spaces attained matrix factorization linear constraints red hatched area feasible region source vectors (columns). dots, examples specific positions source vectors, black hatched area, feasible region data vectors. special cases include) factor analysis) non-negative matrix factorization. finds ?correct? parts. main contribution paper relevant constraints non-negativity substantially \\x0cqualities results obtained matrix factorization. intuition incorporation constraints affects matrix factorization gained geometric implications. figure shows linear constraints constrain model space. example, mixing coeﬃcients constrained non-negative, data modelled convex hull simplicial cone, mixing coeﬃcients furr constrained sum unity, data modelled hull convex polytope. paper, develop general ﬂexible framework Bayesian matrix factorization, unknown sources mixing coeﬃcients treated hidden variables. furrmore, number linear equality inequality constraints specified. unsupervised image separation problem, demonstrate, relevant constraints specified, method finds interpretable features remarkably features computed matrix factorization techniques. proposed method related recently proposed Bayesian matrix factorization techniques: Bayesian matrix factorization based Gibbs sampling demonstrated, scale large datasets avoid problem overfitting non-bayesian techniques. bayesian methods non-negative matrix factorization proposed, eir based variational inference] Gibbs sampling]. special cases algorithm proposed here. paper structured follows: section linearly constrained Bayesian matrix factorization model described. section presents inference procedure based Gibbs sampling. section method applied unsupervised source separation problem compared existing matrix factorization methods. discuss results conclude Section linearly constrained Bayesian matrix factorization model following, describe linearly constrained Bayesian matrix factorization model. make specific choices likelihood priors formulation general allowing eﬃcient inference based Gibbs sampling.  Noise model choose iid. gaussian noise model?vij(nij (nij, vij exp 2vijij) where, general formulation, matrix element variance, vij however, variance parameters easily joined., single noise variance row variance, corresponds isotropic noise model. noise model rise likelihood., probability observations parameters model. likelihood  aik bkj—?) xij aik bkj vij exp ) 2vij?vij {vij denotes parameters model. noise variance parameters choose conjugate inverse-gamma priors,    ) vij exp (vij(vij —?, ?(?) vij Priors sources mixing coeﬃcients define prior distribution \\x0cfactorizing matrices, simplify notation, matrices vectors vec? [a11 a12   aik vec) [b11 b21   bkj  choose Gaussian prior subject inequality constraints, equality constraints,        , ,  ,       orwise. slight abuse denotation, refer  covariance matrix, actual covariance depends constraints. general formulation, constraints, rik?rkj rnq rik?rkj rnr biaﬃne maps, define inequality equality constraints jointly specifically, inequality constraint form ) , )  rearranging terms combining constraints matrix notation, write   )      +qnq        ) ?qnq clear constraints linear likewise, constraints rearranged linear form equality constraints, defined analogously. general formulation priors elements prior dependencies covariance matrix joint constraints; however,  xij bkj aik vij Figure Graphical model linearly constrained Bayesian matrix factorization, independent prior. white grey nodes represent latent observed variables respectively, arrows stochastic dependensies. colored plates denote repeated variables indices. applications relevant practical dependencies advance. ) restrict model independent priori setting) zero) restricting vice versa. furrmore, decouple elements groups elements rows columns choosing block structure. similarly decouple elements  Posterior distribution Having model prior densities, write posterior, distribution parameters conditioned observed data hyperparameters. posterior,  —?) (vij {?, denotes hyper-parameters model. graphical representation model Figure inference Bayesian framework, interested computing posterior distribution parameters, ?). posterior. ), multiplicative constant, direct computation normalizing constant involves integrating unnormalized posterior, analytically tractable. instead, approximate posterior distribution Markov chain Monte Carlo (mcmc).  Gibbs sampling propose inference procedure based Gibbs sampling. gibbs sampling applicable joint density parameters known, parameters partitioned groups, posterior conditional densities known. iteratively sweep groups parameters generate random sample each, conditioned current ors. procedure forms homogenous Markov chain stationary distribution joint posterior. following, derive posterior conditional densities required Gibbs sampler. first, noise variances, vij due choice conjugate prior, posterior density inverse-gamma, (vij —?vij(vij       xij  aik bkj) samples generated standard acceptance-rejection methods. next, factorizing matrices, represented vectors discuss generating samples sampling procedure identical due symmetry model. conditioned prior density constrained gaussian,   —?   ) ) orwise,         . ) standard result conditional Gaussian density. special  furr, case independent prior, simply   conditioning data leads final expression posterior conditional density   —?   ) ) orwise,             diagonal block matrix) diag, diag(v11 v12 vij repetitions gibbs sampler proceeds iteratively: first, noise variance generated inversegamma density. ); second, generated constrained Gaussian density. ); third, generated constrained Gaussian analogous. ).  Sampling constrained Gaussian  essential component proposed matrix factorization method algorithm generating random samples multivariate Gaussian density subject linear equality inequality constraints. slight change notation, generating density   ) ) orwise. similar problem previously treated Geweke], proposes Gibbs sampling procedure, handle equality constraints inequality constraints. rodriguez-yam. ] extends method] arbitrary number inequality constraints, provide algorithm handling equality constraints. here, present general Gibbs sampling procedure handles number equality inequality constraints. equality constraints restrict distribution aﬃne subspace dimensionality number linearly independent constraints. conditional distribution subspace Gaussian subject inequality constraints. handle equality constraints, map distribution subspace. singular decomposition (svd), robustly compute orthonormal basis, constraints, orthogonal complement,   ) diag   holds non-zero singular values. define transformed variable, related   ) vector satisfies equality constraints., computed pseudoinverse??  transformation ensures, satisfies equality constraints. compute distribution conditioned equality constraints, Gaussian subject inequality constraints  ? ) orwise,   ??    )      introduce transformation purpose reducing correlations variables. potentially improve sampling procedure, Gibbs sampling suffer slow mixing distribution highly correlated. correlations elements due Gaussian covariance structure inequality constraints; however, simplicity decorrelate respect covariance underlying unconstrained gaussian. end, define transformed variable??  ) Cholesky factorization covariance matrix?  distribution standard Gaussian subject inequality constraints),    ) orwise, LQy    sample Gibbs sampling procedure sweeping elements generating samples conditional distributions, univariate truncated standard gaussian exp ),   —zzi ) orwise.  erf erf Samples density generated standard methods inverse transform sampling (transforming uniform random variable inverse cumulative density function); eﬃcient mixed rejection sampling algorithm proposed Geweke]; slice sampling]. upper lower points truncation computed max ndkk :}   ]??   min ndkk : denotes ith row: denotes rows ith denotes vector elements ith. finally, sample generated number Gibbs sweeps, transformed sample original variable?? ?  ) sampling procedure illustrated Figure experiments demonstrate proposed linearly constrained Bayesian matrix factorization method blind image separation problem, compare matrix factorization techniques: independent component analysis (ica) non-negative matrix factorization (nmf). data subset MNIST dataset consists pixel grayscale images handwritten digits (see Figure). selected 800 images digit, gave 000 unique images. images created 000 image mixtures adding grayscale intensities images two, digits combined equal proportion. rescaled mixed images pixel intensities interval, arranged vectorized images columns matrix  784 000. examples image mixtures shown Figure.     ) Figure Gibbs sampling multivariate Gaussian density subject linear constraints. twodimensional Gaussian subject inequality constraints. conditional distribution truncated gaussian. gibbs sampling proceeds iteratively sweeping dimensions sampling conditional distribution dimension conditioned current dimensions. task objective factorize data matrix order find number source images explain data. ideally, sources correspond original digits. hope find sources corresponds digit, large variations digit written. reason, hidden sources experiments, allowed exemplars average digit. method For comparison factorized mixed image data standard matrix factorization techniques: ica, FastICA algorithm, nmf, Lee seung multiplicative update algorithm]. sources determined methods shown Figure. linearly constrained Bayesian matrix factorization, isotropic noise model. chose decoupled prior mean, unit diagonal covariance matrix,  hidden sources constrained range pixel intensities image mixtures, aik  constraint sources interpreted images pixel intensities interval meaningful. mixing coeﬃcients constrained non-negative, bkj sum unity bkj thus, observed data modeled convex combination sources. constraints ensure additive combinations sources allowed, introduces negative correlation mixing coeﬃcients. negative correlation implies source contributes mixture sources correspondingly contribute less. idea constraint lead sources compete opposed collaborate explain data. geometric interpretation constraints illustrated Figure: data vectors modeled convex polytope non-negative unit hypercube, hidden sources vertices polytope. computed, 000 Gibbs samples, appeared suﬃcient sampler converge. result matrix factorization shown Figure, displays single sample iteration. results ICA (see Figure) sources constrained non-negative, refore direct interpretation images. computed sources complex patterns, dominated single digit. ICA find structure data, estimated sources lack clear interpretation. sources computed NMF (see Figure) property Lee Seung] refer parts-based representation. spatially, sources \\x0clocal opposed global. decomposition intuitive interpretation: Each source short line segment dot, digits constructed combining parts. linearly constrained Bayesian matrix factorization (see Figure) computes sources clear intuitive interpretation: Almost computed sources visually resemble handwritten digits, aligned sources generate mixtures. compared original data, computed sources bit bolder slightly smeared) Original dataset: MNIST digits) Training data: Mixture digits) Independent component analysis) non-negative matrix factorization) Linearly constrained Bayesian matrix factorization Figure Data results analyses image separation problem. mnist digits data examples shown) generate mixture data. mixture data consists 4000 images mixed digits examples shown). sources computed independent component analysis (color sign). sources computed non-negative matrix factorization. sources computed linearly constrained Bayesian matrix factorization (details explained text). edges. sources stand out: One black blob approximately size digits, anor white feature, adjusting brightness. conclusions presented linearly constrained Bayesian matrix factorization method inference procedure model. unsupervised image separation problem, demonstrated method finds sources clear interpretable meaning. opposed ICA nmf, method finds sources visually resemble handwritten digits. formulated model general terms, specific prior information incorporated factorization. gaussian priors sources knowledge covariance structure sources., sources smooth. constraints experiments separate framework bilinearly dependent constraints specified, constraints data domain., product. general framework constrained Bayesian matrix factorization, proposed method applications areas blind source separation. interesting applications include blind deconvolution, music transcription, spectral unmixing, collaborative filtering. method supervised source separation setting, distributions sources mixing coeﬃcients learned training set isolated sources. interesting challenge develop methods learning relevant constraints data. Source separation problems arise number signals mixed toger, objective estimate underlying sources based observed mixture. supervised, modelbased approach source separation, examples isolated sources train source models, combined order separate mixture. oppositely, unsupervised, blind source separation, general information sources available. instead estimating models sources, blind source separation based weak criteria minimizing correlations, maximizing statistical independence, fitting data subject constraints. under assumptions linear mixing additive noise, blind source separation expressed matrix factorization problem equivalently, xij aik bkj nij \\x0cwhere subscripts matrices denote dimensions. columns represent unknown sources, elements mixing coeﬃcients. each columns observation mixture sources additive noise represented columns objective estimate sources, mixing coeﬃcients, data matrix, observed. Bayesian formulation, aim compute single infer posterior distribution set model assumptions. assumptions likelihood function), expresses probability data factorizing matrices, prior), describes knowledge observing data. depending specific choice likelihood priors, matrix factorizations characteristics devised. non-negative matrix factorization (nmf), distinguished matrix factorization techniques non-negativity constraints, shown decompose data meaningful, interpretable parts]; however, parts-based decomposition necessarily useful, Linear subspace Aﬃne subspace Simplicial cone Simplicial cone non-neg. orthant) bkj ) aik bkj ) Polytope non-neg. orthant Polytope unit simplex Polytope unit hypercube aik aik bkj bkj)  bkj bkj) bkj constraints) Polytope bkj ) bkj bkj aik  bkj) Figure Examples model spaces attained matrix factorization linear constraints red hatched area feasible region source vectors (columns). dots, examples specific positions source vectors, black hatched area, feasible region data vectors. special cases include) factor analysis) non-negative matrix factorization. finds ?correct? parts. main contribution paper relevant constraints non-negativity substantially \\x0cqualities results obtained matrix factorization. some intuition incorporation constraints affects matrix factorization gained geometric implications. figure shows linear constraints constrain model space. for example, mixing coeﬃcients constrained non-negative, data modelled convex hull simplicial cone, mixing coeﬃcients furr constrained sum unity, data modelled hull convex polytope. paper, develop general ﬂexible framework Bayesian matrix factorization, unknown sources mixing coeﬃcients treated hidden variables. furrmore, number linear equality inequality constraints specified. unsupervised image separation problem, demonstrate, relevant constraints specified, method finds interpretable features remarkably features computed matrix factorization techniques. proposed method related recently proposed Bayesian matrix factorization techniques: Bayesian matrix factorization based Gibbs sampling demonstrated, scale large datasets avoid problem overfitting non-bayesian techniques. bayesian methods non-negative matrix factorization proposed, eir based variational inference] Gibbs sampling]. special cases algorithm proposed here. paper structured follows: section linearly constrained Bayesian matrix factorization model described. section presents inference procedure based Gibbs sampling. Section method applied unsupervised source separation problem compared existing matrix factorization methods. discuss results conclude Section linearly constrained Bayesian matrix factorization model following, describe linearly constrained Bayesian matrix factorization model. make specific choices likelihood priors formulation general allowing eﬃcient inference based Gibbs sampling.  Noise model choose iid. Gaussian noise model?vij(nij (nij, vij exp 2vijij) where, general formulation, matrix element variance, vij however, variance parameters easily joined., single noise variance row variance, corresponds isotropic noise model. noise model rise likelihood., probability observations parameters model. likelihood  aik bkj—?) xij aik bkj vij exp ) 2vij?vij {vij denotes parameters model. for noise variance parameters choose conjugate inverse-gamma priors,    ) vij exp (vij(vij —?, ?(?) vij Priors sources mixing coeﬃcients define prior distribution \\x0cfactorizing matrices, simplify notation, matrices vectors vec? [a11 a12   aik vec) [b11 b21   bkj  choose Gaussian prior subject inequality constraints, equality constraints,        , ,  ,       orwise. slight abuse denotation, refer  covariance matrix, actual covariance depends constraints. general formulation, constraints, rik?rkj rnq rik?rkj rnr biaﬃne maps, define inequality equality constraints jointly specifically, inequality constraint form ) , )  rearranging terms combining constraints matrix notation, write   )      +qnq        ) ?qnq clear constraints linear likewise, constraints rearranged linear form equality constraints, defined analogously. this general formulation priors elements prior dependencies covariance matrix joint constraints; however,  xij bkj aik vij Figure Graphical model linearly constrained Bayesian matrix factorization, independent prior. white grey nodes represent latent observed variables respectively, arrows stochastic dependensies. colored plates denote repeated variables indices. applications relevant practical dependencies advance. ) restrict model independent priori setting) zero) restricting vice versa. furrmore, decouple elements groups elements rows columns choosing block structure. similarly decouple elements  Posterior distribution Having model prior densities, write posterior, distribution parameters conditioned observed data hyperparameters. posterior,  —?) (vij {?, denotes hyper-parameters model. graphical representation model Figure Inference Bayesian framework, interested computing posterior distribution parameters, ?). posterior. ), multiplicative constant, direct computation normalizing constant involves integrating unnormalized posterior, analytically tractable. instead, approximate posterior distribution Markov chain Monte Carlo (mcmc).  Gibbs sampling propose inference procedure based Gibbs sampling. gibbs sampling applicable joint density parameters known, parameters partitioned groups, posterior conditional densities known. iteratively sweep groups parameters generate random sample each, conditioned current ors. this procedure forms homogenous Markov chain stationary distribution joint posterior. following, derive posterior conditional densities required Gibbs sampler. first, noise variances, vij due choice conjugate prior, posterior density inverse-gamma, (vij —?vij(vij       xij  aik bkj) samples generated standard acceptance-rejection methods. next, factorizing matrices, represented vectors discuss generating samples sampling procedure identical due symmetry model. conditioned prior density constrained gaussian,   —?   ) ) orwise,         . ) standard result conditional Gaussian density. special  furr, case independent prior, simply   conditioning data leads final expression posterior conditional density   —?   ) ) orwise,             diagonal block matrix) diag, diag(v11 v12 vij repetitions Gibbs sampler proceeds iteratively: first, noise variance generated inversegamma density. ); second, generated constrained Gaussian density. ); third, generated constrained Gaussian analogous. ).  Sampling constrained Gaussian  essential component proposed matrix factorization method algorithm generating random samples multivariate Gaussian density subject linear equality inequality constraints. with slight change notation, generating density   ) ) orwise. similar problem previously treated Geweke], proposes Gibbs sampling procedure, handle equality constraints inequality constraints. rodriguez-yam. ] extends method] arbitrary number inequality constraints, provide algorithm handling equality constraints. here, present general Gibbs sampling procedure handles number equality inequality constraints. equality constraints restrict distribution aﬃne subspace dimensionality number linearly independent constraints. conditional distribution subspace Gaussian subject inequality constraints. handle equality constraints, map distribution subspace. using singular decomposition (svd), robustly compute orthonormal basis, constraints, orthogonal complement,   ) diag   holds non-zero singular values. define transformed variable, related   ) vector satisfies equality constraints., computed pseudoinverse??  this transformation ensures, satisfies equality constraints. compute distribution conditioned equality constraints, Gaussian subject inequality constraints  ? ) orwise,   ??    )      introduce transformation purpose reducing correlations variables. this potentially improve sampling procedure, Gibbs sampling suffer slow mixing distribution highly correlated. correlations elements due Gaussian covariance structure inequality constraints; however, simplicity decorrelate respect covariance underlying unconstrained gaussian. end, define transformed variable??  ) Cholesky factorization covariance matrix?  distribution standard Gaussian subject inequality constraints),    ) orwise, LQy    sample Gibbs sampling procedure sweeping elements generating samples conditional distributions, univariate truncated standard gaussian exp ),   —zzi ) orwise.  erf erf Samples density generated standard methods inverse transform sampling (transforming uniform random variable inverse cumulative density function); eﬃcient mixed rejection sampling algorithm proposed Geweke]; slice sampling]. upper lower points truncation computed max ndkk :}   ]??   min ndkk : denotes ith row: denotes rows ith denotes vector elements ith. finally, sample generated number Gibbs sweeps, transformed sample original variable?? ?  ) sampling procedure illustrated Figure Experiments demonstrate proposed linearly constrained Bayesian matrix factorization method blind image separation problem, compare matrix factorization techniques: independent component analysis (ica) non-negative matrix factorization (nmf). data subset MNIST dataset consists pixel grayscale images handwritten digits (see Figure). selected 800 images digit, gave 000 unique images. from images created 000 image mixtures adding grayscale intensities images two, digits combined equal proportion. rescaled mixed images pixel intensities interval, arranged vectorized images columns matrix  784 000. examples image mixtures shown Figure.     ) Figure Gibbs sampling multivariate Gaussian density subject linear constraints. Twodimensional Gaussian subject inequality constraints. conditional distribution truncated gaussian. Gibbs sampling proceeds iteratively sweeping dimensions sampling conditional distribution dimension conditioned current dimensions. task objective factorize data matrix order find number source images explain data. ideally, sources correspond original digits. hope find sources corresponds digit, large variations digit written. for reason, hidden sources experiments, allowed exemplars average digit. method For comparison factorized mixed image data standard matrix factorization techniques: ica, FastICA algorithm, nmf, Lee seung multiplicative update algorithm]. sources determined methods shown Figure. for linearly constrained Bayesian matrix factorization, isotropic noise model. chose decoupled prior mean, unit diagonal covariance matrix,  hidden sources constrained range pixel intensities image mixtures, aik  this constraint sources interpreted images pixel intensities interval meaningful. mixing coeﬃcients constrained non-negative, bkj sum unity bkj thus, observed data modeled convex combination sources. constraints ensure additive combinations sources allowed, introduces negative correlation mixing coeﬃcients. this negative correlation implies source contributes mixture sources correspondingly contribute less. idea constraint lead sources compete opposed collaborate explain data. geometric interpretation constraints illustrated Figure: data vectors modeled convex polytope non-negative unit hypercube, hidden sources vertices polytope. computed, 000 Gibbs samples, appeared suﬃcient sampler converge. result matrix factorization shown Figure, displays single sample iteration. results ICA (see Figure) sources constrained non-negative, refore direct interpretation images. most computed sources complex patterns, dominated single digit. while ICA find structure data, estimated sources lack clear interpretation. sources computed NMF (see Figure) property Lee Seung] refer parts-based representation. spatially, sources \\x0clocal opposed global. decomposition intuitive interpretation: Each source short line segment dot, digits constructed combining parts. linearly constrained Bayesian matrix factorization (see Figure) computes sources clear intuitive interpretation: Almost computed sources visually resemble handwritten digits, aligned sources generate mixtures. compared original data, computed sources bit bolder slightly smeared) Original dataset: MNIST digits) Training data: Mixture digits) Independent component analysis) non-negative matrix factorization) Linearly constrained Bayesian matrix factorization Figure Data results analyses image separation problem. MNIST digits data examples shown) generate mixture data. mixture data consists 4000 images mixed digits examples shown). Sources computed independent component analysis (color sign). Sources computed non-negative matrix factorization. Sources computed linearly constrained Bayesian matrix factorization (details explained text). edges. two sources stand out: One black blob approximately size digits, anor white feature, adjusting brightness. Conclusions presented linearly constrained Bayesian matrix factorization method inference procedure model. unsupervised image separation problem, demonstrated method finds sources clear interpretable meaning. opposed ICA nmf, method finds sources visually resemble handwritten digits. formulated model general terms, specific prior information incorporated factorization. Gaussian priors sources knowledge covariance structure sources., sources smooth. constraints experiments separate framework bilinearly dependent constraints specified, constraints data domain., product. general framework constrained Bayesian matrix factorization, proposed method applications areas blind source separation. interesting applications include blind deconvolution, music transcription, spectral unmixing, collaborative filtering. method supervised source separation setting, distributions sources mixing coeﬃcients learned training set isolated sources. interesting challenge develop methods learning relevant constraints data.',\n",
       " 'PP3814': 'fundamental challenge facing reinforcement learning) algorithms maintain proper balance exploration exploitation. policy designed based previous experiences construction constrained, optimal result inexperience. refore, desirable actions goal enhancing experience. actions necessarily yield optimal near-term reward ultimate goal, could, long horizon, yield improved long-term reward. fundamental challenge achieve optimal balance exploration exploitation; performed goal enhancing experience preventing premature convergence suboptimal behavior, performed goal employing experience define perceived optimal actions. Markov decision process (mdp), problem balancing exploration exploitation addressed successfully-max] algorithms. important applications, however, environments states \\x0care completely observed, leading partially observable MDPs (pomdps). reinforcement learning POMDPs challenging, context balancing exploration exploitation. recent work targeted solving exploration. exploitation problem based augmented pomdp, product state space environment states unknown POMDP parameters]. this, however, entails solving complicated planning problem, state space grows exponentially number unknown parameters, making problem quickly intractable practice. mitigate complexity, active learning methods proposed pomdps, borrow similar ideas supervised learning, apply selectively query oracle (domain expert) optimal action]. active learning found success collaborative human-machine tasks expert advice available. paper propose dual-policy approach balance exploration exploitation pomdps, simultaneously learning policies partially shared internal structure. policy, termed primary policy, defines actions based previous experience; policy, termed auxiliary policy, meta-level policy maintaining proper balance exploration exploitation. employ regionalized policy representation (rpr] parameterize policies, perform Bayesian learning update policy posteriors. approach applies eir cases) agent explores randomly taking actions insuﬃciently (traditional exploration) agent explores querying oracle optimal action (active learning). case, agent assessed query cost oracle, addition reward received environment. eir) employed exploration vehicle, depending application. dual-policy approach possesses interesting convergence properties, similar] Rmax]. however, approach assumes environment POMDP Rmax assume MDP environment. anor distinction approach learns agent policy directly episodes, estimating POMDP model. contrast Rmax (both learn MDP models) activelearning method] (which learns POMDP models). regionalized Policy Representation provide review regionalized policy representation, parameterize primary policy auxiliary policy discussed above. material section], proofs omitted here. definition regionalized policy representation tuple, ?). finite set actions observations. finite set belief regions. belief-region transition function,  denoting probability transiting taking action results observing   initial distribution belief regions ) denoting probability initially  region-dependent stochastic policies , denoting probability taking action— cardinality denote,   similarly, , —}.      abbreviate    similarly       subscripts indexes discrete time steps. history defined sequence actions performed observations received Let {?, denote RPR parameters. RPR yields joint probability distribution    ?    ?  ) marginalizing), obtain ?). furrmore, history-dependent distribution action choices obtained follows? ?  :? :? :? :?   stochastic policy choosing action  action choice depends solely historical actions observations, unobservable belief regions marginalized out.  Learning Criterion Bayesian learning RPR based experiences collected agent-environment interaction. assuming interaction episodic., breaks subsequences called episodes], represent experiences set episodes. definition episode sequence agent-environment interactions terminated absorbing state transits reward. episode denoted (ak0 r0k ok1 ak1 r1k   oktk aktk rtkk subscripts discrete times, indexes episodes, observations, actions, rewards. definition RPR Optimality criterion) Let) {(ak0 r0k ok1 ak1 r1k   oktk aktk rtkk set episodes obtained agent interacting environment policy select actions, arbitrary stochastic policy action-selecting distributions  action history rpr optimality criterion defined def. ptk ? ? ,?) )      hkt ak0 ok1 ak1   okt history actions observations time episode, discount, denotes RPR parameters. paper, call) empirical function proven] limk?? ) expected sum discounted rewards RPR policy parameterized infinite number steps. refore, RPR resulting maximization) approaches optimal large (assuming— appropriate). bayesian setting discussed below, noninformative prior leading posterior peaked optimal rpr, refore agent guaranteed sample optimal near-optimal policy overwhelming probability.  Bayesian Learning Let (?) represent prior distribution RPR parameters. define posterior def. )  ) ? marginal empirical value. note) empirical function) nonstandard Bayes rule. however) distribution shape incorporates prior empirical information. term) product multinomial distributions, natural choose prior product Dirichlet distributions, (?) (?—?  —?) (?—?) dir ),   —)? (?—?)  Dir ),     —?)  Dir),   —  hyper-parameters. prior chosen, posterior) large mixture Dirichlet products, refore posterior analysis Gibbs sampling ineﬃcient. overcome this, employ variational Bayesian technique] obtain variational posterior maximizing lower bound)  ({qtk(?)) ); ?  ({qtk(?  ?—ak0 ok1 )}) {qtk(?) variational distributions satisfying qtk (?)  ? ptk— (qkp) denotes) ,???    kullback-leibler) distance probability measure factorized form(?)} represents approximation weighted joint posterior  lower bound reaches maximum(?) called variational approximate posterior lower bound maximization accomplished solving(?) alternately, keeping fixed solving. solutions summarized orem; proof]. orem Given initialization iterative application updates produces sequence monotonically increasing lower bounds({qtk(? )), converges maxima. update {qtk —ak0 ok1 qzk set under-normalized probability mass functions,   ,  )??   )??  )  )??  digamma function. (?) form prior), hyper-parameter updated PTk ) PTk ,? ? ptk  ,? ? ? ,? —ak0 ok1 ,? —ak0 ok1   rtk(ak0 —ok1 —?)  ? ? ) dual-rpr: Joint Policy Agent Behavior trade-off Between Exploration Exploitation Assume agent RPR Section govern behavior unknown POMDP environment primary policy). bayesian learning employs empirical function) ) place likelihood function, obtain posterior RPR parameters episodes) obtained environment arbitrary stochastic policy  )  although guarantees optimality resulting rpr, choice affects convergence speed. good choice avoids episodes bring information improve rpr, agent episodes RPR optimal. batch learning, episodes collected learning begins, prechosen change learning]. online learning, however, episodes collected learning, RPR updated completion episode. refore chance exploit RPR avoid repeated learning part environment. agent recognize belief regions familiar with, exploit existing RPR policy; belief regions inferred new, agent explore. balance exploration exploitation performed goal accumulating large long-run reward. online learning RPR primary policy) choose mixture policies: current RPR (exploitation) exploration policy action-choosing probability ), exploitation (exploration). problem choosing good reduces proper balance exploitation exploration: agent exploit highly rewarding, enhance experience improve auxiliary RPR employed represent policy balancing exploration exploitation., history-dependent distribution). auxiliary \\x0crpr shares parameters {?, primary rpr,  ,  replaced  , }, , probability choosing exploitation exploration belief region  prior Beta ),  order encourage exploration agent experience, choose that, beginning learning, auxiliary RPR suggests exploration. agent accumulates episodes experience, part environment episodes collected. knowledge reﬂected auxiliary rpr, which, primary rpr, updated completion episode. environment pomdp, agent knowledge represented space belief states. however, agent directly access belief states, computation belief states requires knowing true POMDP model, available. fortunately, RPR formulation compact representation}, space histories, history corresponds belief state pomdp. RPR formulation, represented internally set distributions belief regions agent access based subset samples Let Hknown part agent., primary RPR optimal Hknown agent begin exploit entering Hknown clear below, Hknown identified Hknown,  }, posterior updated PTk) ,? ), PTk max   ,? ) small positive number) rtk replaced mkt meta-reward received episode mkt rmeta goal reached time episode mkt orwise, rmeta constant. provided oracle (active learning), query cost account), subtracting thus, probability exploration reduced time agent makes query oracle., ytk). number queries small positive number  due max operator), point agent stops querying belief region ), exploitation receives ?credit?, exploration receives credit (exploration discredited oracle). update makes chance exploitation monotonically increases episodes accumulate. exploration receives credit pre-assigned credit prior, chance exploration monotonically decrease accumulation episodes. parameter represents agent prior amount needed exploration. discredited cost agent larger (than obtain amount exploration. fact amount exploration monotonically increases implies that, find large ensure primary RPR optimal Hknown,  }. however, unnecessarily large makes agent over-explore leads slow convergence. umin denote minimum ensures optimality Hknown assume umin exists analysis below. range umin examined experiments. optimality Convergence Analysis Let true POMDP model. introduce equivalent expression empirical function  ) summation elements      complete set episodes length pomdp, repeated elements. condition abbreviation      ) agent RPR (?) here. note empirical) function defined) )  identical) difference acquiring episodes: simple enumeration distinct episodes) identical episodes. multiplicity episode) results sampling process policy interact environment). note empirical) function defined interesting oretical analysis, evaluation requires knowing true POMDP model, practice. define optimistic function) Vbf ?,?, +(rmax?   ,??? ?  agent receives time steps    orwise, receives Rmax upper bound rewards environ) ment. similarly define) equivalent expression Vbf lemma proven appendix. ) Lemma Let vbf Rmax defined above. ) Pexlpore probability executing exploration policy) episode auxiliary RPR (?, exploration policy ) Pexlpore     vbf )—. rmax episode terminates finite time steps practice agent stays absorbing state reward remaining infinite steps episode terminated]. infinite horizon ensure oretically episodes horizon length. ) Proposition Let optimal RPR  optimal RPR complete POMDP environment. auxiliary RPR hyper-parameters \\x0c(?) updated), umin exploration policy   eir?    ?   ) probability auxiliary RPR suggests executing episode) unseen ??) rmax proof: suﬃcient show) hold) hold. assume?  ?     optimal ?   ?  ) implies?  ?        show) vbf?  ?  which, toger Lemma, implies  ) Pexlpore?  ?  ?  rmax     ) ?   ?    Rmax Rmax) show Vbf?  ?   construction, Vbf? optimistic function, agent receives Rmax time     however,     implies?      hknown premise, updated) umin refore optimal Hknown (see discussions)), implies optimal?     }. thus, inequality holds. . proposition shows primary RPR achieves accumulative reward optimal RPR auxiliary RPR suggests exploration probability exceeding   )rmax conversely, auxiliary RPR suggests exploration probability smaller   )rmax primary RPR achieves ?-near optimality. ensures agent eir receiving suﬃcient rewards performing suﬃcient exploration. experimental Results Our experiments based shuttle, benchmark POMDP problem], setup. primary policy RPR— prior), hyper-parameters initially set (which makes initial prior non-informative). auxiliary policy RPR sharing {?, primary RPR prior ). prior initially biased exploration values examine effects. agent performs online learning: termination episode, primary auxiliary RPR posteriors updated previous posteriors current priors. primary RPR update orem auxiliary RPR update)  shares update primary RPR  perform 100 independent Monte Carlo runs. run, agent starts learning random position environment stops learning Ktotal episodes completed. compare methods agent balance exploration exploitation) auxiliary rpr, values adaptively switch exploration exploitation) randomly switching exploration exploitation fixed exploration rate Pexplore (various values Pexplore examined). performing exploitation, agent current primary RPR (using maximizes posterior); performing exploration, exploration policy types) taking random actions) policy obtained solving true POMDP PBVI] 2000 belief samples. eir case, rmeta .001. case), PBVI policy oracle incurs query cost report) sum discounted rewards accrued episode learning; rewards result exploitation exploration. ) quality primary RPR termination learning episode, represented sum discounted rewards averaged 251 episodes primary RPR (using standard testing procedure shuttle: episode terminated eir goal reached maximum 251 steps taken); rewards result exploitation alone. (iii) exploration rate Pexplore learning episode, number time steps exploration performed divided total time steps episode. order examine optimality, rewards) optimal rewards subtracted, optimal rewards obtained PBVI policy; difference reported, difference indicating optimality minus difference indicating sub-optimality. results averaged 100 Monte Carlo runs. results summarized Figure takes random actions Figure oracle PBVI policy). dual?rpr, dual?rpr dual?rpr dual?rpr=200 rpr, Pexplore rpr, Pexplore rpr, Pexplore rpr, Pexplore dual?rpr, dual?rpr dual?rpr, =200 rpr, Pexplore rpr, Pexplore dual?rpr dual?rpr, =200 rpr, Pexplore 500 1000 1500 2000 2500 3000 Number episodes learning phase Exploration rate Accrued testing reward minus optimal reward Accrued learning reward minus optimal reward rpr, Pexplore 500 1000 1500 2000 2500 3000 Number episodes learning phase 500 1000 1500 2000 2500 3000 Number episodes learning phase Figure Results Shuttle random exploration policy, Ktotal 3000. left: accumulative discounted reward accrued learning episode, optimal reward subtracted. middle: accumulative discounted rewards averaged 251 episodes primary RPR obtained learning episode, optimal reward subtracted. right: rate exploration learning episode. results averaged 100 independent Monte Carlo runs. dual?rpr dual?rpr, dual?rpr dual?rpr rpr, Pexplore.158 rpr, Pex plore.448 rpr, Pexplore.657 dual?rpr dual?rpr dual?rpr.448 explore rpr, Pexplore dual?rpr 100 Number episodes learning phase 100 Number episodes learning phase dual?rpr rpr, Pexplore.657 100 Number episodes learning phase.158 rpr, explore rpr, Pexplore rpr, Exploration rate Accrued testing reward minus optimal reward Accrued learning reward minus optimal reward dual?rpr dual?rpr, dual?rpr dual?rpr rpr, Pexplore.081 rpr, Pex plore.295 rpr, Pexplore.431 rpr, Pexplore 100 Number episodes learning phase dual?rpr dual?rpr dual?rpr rpr, Pexplore.081 rpr, Pexplore.295 rpr, Pexplore.431 rpr, Pexplore 100 Number episodes learning phase Exploration rate Accrued testing reward minus optimal reward Accrued learning reward minus optimal reward dual?rpr dual?rpr 100 Number episodes learning phase Figure Results Shuttle oracle exploration policy incurring cost (top row) (bottom row), Ktotal 100. figure row counterpart figure Figure random replaced oracle captions details. figure that, random exploration primary policy converges optimality and, accordingly, Pexplore drops zero, 1500 learning episodes. increases, convergence slower: occur (and Pexplore abound 2500 learning episodes. increased 200, convergence happen Pexplore 3000 learning episodes. results verify analysis Section) primary policy improves Pexplore decreases) agent explores acting optimally acting optimally stops exploring; (iii) exists finite primary policy optimal Pexplore larger umin small ensure convergence 1500 episodes. observe Figure that) agent explores eﬃciently adaptively switched exploration exploitation auxiliary policy, switch random) primary policy converge optimality agent explores; (iii) primary policy converge optimality agent takes random actions, infinite learning episodes converge. results Figure oracle, provide similar conclusions Figure random. however, special observations Figure) Pexplore affected query cost larger agent performs exploration. ) convergence rate primary policy significantly affected query cost. reason) oracle optimal actions, over-exploration harm optimality; long agent takes optimal actions, primary policy continually improves optimal, remains optimal optimal. conclusions presented dual-policy approach jointly learning agent behavior optimal balance exploitation exploration, assuming unknown environment pomdp. identifying part environment terms histories (parameterized rpr), approach adaptively switches exploration exploitation depending wher agent part. provided oretical guarantees agent eir explore eﬃciently exploit eﬃciently. experimental results show good agreement oretical analysis approach finds optimal policy eﬃciently. empirically demonstrated existence small ensure eﬃcient convergence optimality, furr oretical analysis needed find umin tight lower bound ensures convergence optimality amount exploration (without over-exploration). finding exact umin diﬃcult partial observability. however, hopeful find good approximation umin worst case, agent choose optimistic, rmax. optimistic agent large leads over-exploration ensures convergence optimality. acknowledgements authors anonymous reviewers valuable comments suggestions. work supported afosr. appendix Proof Lemma: expand) Vbf —?,  rmax —?,  abbreviation       abbreviation    ) satisfying  sum) episodes difference )      rmax —?,    rmax—?, ?))  —?,  —?, rmax   —?,   rmax —?,  rmax —?, ?)) rmax ) —?, ?))  rmax ) —?, ?)) ??  sum sequences    satisfying }. . fundamental challenge facing reinforcement learning) algorithms maintain proper balance exploration exploitation. policy designed based previous experiences construction constrained, optimal result inexperience. refore, desirable actions goal enhancing experience. although actions necessarily yield optimal near-term reward ultimate goal, could, long horizon, yield improved long-term reward. fundamental challenge achieve optimal balance exploration exploitation; performed goal enhancing experience preventing premature convergence suboptimal behavior, performed goal employing experience define perceived optimal actions. for Markov decision process (mdp), problem balancing exploration exploitation addressed successfully-max] algorithms. many important applications, however, environments states \\x0care completely observed, leading partially observable MDPs (pomdps). reinforcement learning POMDPs challenging, context balancing exploration exploitation. recent work targeted solving exploration. exploitation problem based augmented pomdp, product state space environment states unknown POMDP parameters]. this, however, entails solving complicated planning problem, state space grows exponentially number unknown parameters, making problem quickly intractable practice. mitigate complexity, active learning methods proposed pomdps, borrow similar ideas supervised learning, apply selectively query oracle (domain expert) optimal action]. active learning found success collaborative human-machine tasks expert advice available. paper propose dual-policy approach balance exploration exploitation pomdps, simultaneously learning policies partially shared internal structure. policy, termed primary policy, defines actions based previous experience; policy, termed auxiliary policy, meta-level policy maintaining proper balance exploration exploitation. employ regionalized policy representation (rpr] parameterize policies, perform Bayesian learning update policy posteriors. approach applies eir cases) agent explores randomly taking actions insuﬃciently (traditional exploration) agent explores querying oracle optimal action (active learning). case, agent assessed query cost oracle, addition reward received environment. eir) employed exploration vehicle, depending application. dual-policy approach possesses interesting convergence properties, similar] Rmax]. however, approach assumes environment POMDP Rmax assume MDP environment. anor distinction approach learns agent policy directly episodes, estimating POMDP model. this contrast Rmax (both learn MDP models) activelearning method] (which learns POMDP models). Regionalized Policy Representation provide review regionalized policy representation, parameterize primary policy auxiliary policy discussed above. material section], proofs omitted here. definition regionalized policy representation tuple, ?). finite set actions observations. finite set belief regions. belief-region transition function,  denoting probability transiting taking action results observing   initial distribution belief regions ) denoting probability initially  region-dependent stochastic policies , denoting probability taking action— cardinality denote,   similarly, , —}.      abbreviate    similarly       subscripts indexes discrete time steps. history defined sequence actions performed observations received Let {?, denote RPR parameters. given RPR yields joint probability distribution    ?    ?  ) marginalizing), obtain ?). furrmore, history-dependent distribution action choices obtained follows? ?  :? :? :? :?   stochastic policy choosing action  action choice depends solely historical actions observations, unobservable belief regions marginalized out.  Learning Criterion Bayesian learning RPR based experiences collected agent-environment interaction. assuming interaction episodic., breaks subsequences called episodes], represent experiences set episodes. definition episode sequence agent-environment interactions terminated absorbing state transits reward. episode denoted (ak0 r0k ok1 ak1 r1k   oktk aktk rtkk subscripts discrete times, indexes episodes, observations, actions, rewards. definition RPR Optimality criterion) Let) {(ak0 r0k ok1 ak1 r1k   oktk aktk rtkk set episodes obtained agent interacting environment policy select actions, arbitrary stochastic policy action-selecting distributions  action history RPR optimality criterion defined def. PTk ? ? ,?) )      hkt ak0 ok1 ak1   okt history actions observations time episode, discount, denotes RPR parameters. throughout paper, call) empirical function proven] limk?? ) expected sum discounted rewards RPR policy parameterized infinite number steps. refore, RPR resulting maximization) approaches optimal large (assuming— appropriate). Bayesian setting discussed below, noninformative prior leading posterior peaked optimal rpr, refore agent guaranteed sample optimal near-optimal policy overwhelming probability.  Bayesian Learning Let (?) represent prior distribution RPR parameters. define posterior def. )  ) ? marginal empirical value. note) empirical function) nonstandard Bayes rule. however) distribution shape incorporates prior empirical information. since term) product multinomial distributions, natural choose prior product Dirichlet distributions, (?) (?—?  —?) (?—?) Dir ),   —)? (?—?)  Dir ),     —?)  Dir),   —  hyper-parameters. with prior chosen, posterior) large mixture Dirichlet products, refore posterior analysis Gibbs sampling ineﬃcient. overcome this, employ variational Bayesian technique] obtain variational posterior maximizing lower bound)  ({qtk(?)) ); ?  ({qtk(?  ?—ak0 ok1 )}) {qtk(?) variational distributions satisfying qtk (?)  ? PTk— (qkp) denotes) ,???    kullback-leibler) distance probability measure factorized form(?)} represents approximation weighted joint posterior  lower bound reaches maximum(?) called variational approximate posterior lower bound maximization accomplished solving(?) alternately, keeping fixed solving. solutions summarized orem; proof]. orem Given initialization iterative application updates produces sequence monotonically increasing lower bounds({qtk(? )), converges maxima. update {qtk —ak0 ok1 qzk set under-normalized probability mass functions,   ,  )??   )??  )  )??  digamma function. (?) form prior), hyper-parameter updated PTk ) PTk ,? ? PTk  ,? ? ? ,? —ak0 ok1 ,? —ak0 ok1   rtk(ak0 —ok1 —?)  ? ? ) dual-rpr: Joint Policy Agent Behavior trade-off Between Exploration Exploitation Assume agent RPR Section govern behavior unknown POMDP environment primary policy). bayesian learning employs empirical function) ) place likelihood function, obtain posterior RPR parameters episodes) obtained environment arbitrary stochastic policy  )  Although guarantees optimality resulting rpr, choice affects convergence speed. good choice avoids episodes bring information improve rpr, agent episodes RPR optimal. batch learning, episodes collected learning begins, prechosen change learning]. online learning, however, episodes collected learning, RPR updated completion episode. refore chance exploit RPR avoid repeated learning part environment. agent recognize belief regions familiar with, exploit existing RPR policy; belief regions inferred new, agent explore. this balance exploration exploitation performed goal accumulating large long-run reward. online learning RPR primary policy) choose mixture policies: current RPR (exploitation) exploration policy this action-choosing probability ), exploitation (exploration). problem choosing good reduces proper balance exploitation exploration: agent exploit highly rewarding, enhance experience improve auxiliary RPR employed represent policy balancing exploration exploitation., history-dependent distribution). auxiliary \\x0crpr shares parameters {?, primary rpr,  ,  replaced  , }, , probability choosing exploitation exploration belief region let prior Beta ),  order encourage exploration agent experience, choose that, beginning learning, auxiliary RPR suggests exploration. agent accumulates episodes experience, part environment episodes collected. this knowledge reﬂected auxiliary rpr, which, primary rpr, updated completion episode. since environment pomdp, agent knowledge represented space belief states. however, agent directly access belief states, computation belief states requires knowing true POMDP model, available. fortunately, RPR formulation compact representation}, space histories, history corresponds belief state pomdp. within RPR formulation, represented internally set distributions belief regions agent access based subset samples Let Hknown part agent., primary RPR optimal Hknown agent begin exploit entering Hknown clear below, Hknown identified Hknown,  }, posterior updated PTk) ,? ), PTk max   ,? ) small positive number) rtk replaced mkt meta-reward received episode mkt rmeta goal reached time episode mkt orwise, rmeta constant. when provided oracle (active learning), query cost account), subtracting thus, probability exploration reduced time agent makes query oracle., ytk). after number queries small positive number  due max operator), point agent stops querying belief region ), exploitation receives ?credit?, exploration receives credit (exploration discredited oracle). this update makes chance exploitation monotonically increases episodes accumulate. exploration receives credit pre-assigned credit prior, chance exploration monotonically decrease accumulation episodes. parameter represents agent prior amount needed exploration. when discredited cost agent larger (than obtain amount exploration. fact amount exploration monotonically increases implies that, find large ensure primary RPR optimal Hknown,  }. however, unnecessarily large makes agent over-explore leads slow convergence. let umin denote minimum ensures optimality Hknown assume umin exists analysis below. range umin examined experiments. Optimality Convergence Analysis Let true POMDP model. introduce equivalent expression empirical function  ) summation elements      complete set episodes length pomdp, repeated elements. condition abbreviation      ) agent RPR (?) here. note empirical) function defined) ) when identical) difference acquiring episodes: simple enumeration distinct episodes) identical episodes. multiplicity episode) results sampling process policy interact environment). note empirical) function defined interesting oretical analysis, evaluation requires knowing true POMDP model, practice. define optimistic function) Vbf ?,?, +(rmax?   ,??? ?  agent receives time steps    orwise, receives Rmax upper bound rewards environ) ment. similarly define) equivalent expression Vbf lemma proven appendix. ) Lemma Let Vbf Rmax defined above. let) Pexlpore probability executing exploration policy) episode auxiliary RPR (?, exploration policy ) Pexlpore     vbf )—. rmax episode terminates finite time steps practice agent stays absorbing state reward remaining infinite steps episode terminated]. infinite horizon ensure oretically episodes horizon length. ) Proposition Let optimal RPR  optimal RPR complete POMDP environment. let auxiliary RPR hyper-parameters \\x0c(?) updated), umin let exploration policy   eir?    ?   ) probability auxiliary RPR suggests executing episode) unseen ??) rmax proof: suﬃcient show) hold) hold. let assume?  ?    because optimal ?   ?  ) implies?  ?        show) vbf?  ?  which, toger Lemma, implies  ) Pexlpore?  ?  ?  rmax     ) ?   ?    Rmax Rmax) show Vbf?  ?   construction, Vbf? optimistic function, agent receives Rmax time     however,     implies?      hknown premise, updated) umin refore optimal Hknown (see discussions)), implies optimal?     }. thus, inequality holds. . proposition shows primary RPR achieves accumulative reward optimal RPR auxiliary RPR suggests exploration probability exceeding   )rmax conversely, auxiliary RPR suggests exploration probability smaller   )rmax primary RPR achieves ?-near optimality. this ensures agent eir receiving suﬃcient rewards performing suﬃcient exploration. Experimental Results Our experiments based shuttle, benchmark POMDP problem], setup. primary policy RPR— prior), hyper-parameters initially set (which makes initial prior non-informative). auxiliary policy RPR sharing {?, primary RPR prior ). prior initially biased exploration values examine effects. agent performs online learning: termination episode, primary auxiliary RPR posteriors updated previous posteriors current priors. primary RPR update orem auxiliary RPR update)  shares update primary RPR  perform 100 independent Monte Carlo runs. run, agent starts learning random position environment stops learning Ktotal episodes completed. compare methods agent balance exploration exploitation) auxiliary rpr, values adaptively switch exploration exploitation) randomly switching exploration exploitation fixed exploration rate Pexplore (various values Pexplore examined). when performing exploitation, agent current primary RPR (using maximizes posterior); performing exploration, exploration policy types) taking random actions) policy obtained solving true POMDP PBVI] 2000 belief samples. eir case, rmeta .001. case), PBVI policy oracle incurs query cost report) sum discounted rewards accrued episode learning; rewards result exploitation exploration. ) quality primary RPR termination learning episode, represented sum discounted rewards averaged 251 episodes primary RPR (using standard testing procedure shuttle: episode terminated eir goal reached maximum 251 steps taken); rewards result exploitation alone. (iii) exploration rate Pexplore learning episode, number time steps exploration performed divided total time steps episode. order examine optimality, rewards) optimal rewards subtracted, optimal rewards obtained PBVI policy; difference reported, difference indicating optimality minus difference indicating sub-optimality. all results averaged 100 Monte Carlo runs. results summarized Figure takes random actions Figure oracle PBVI policy). dual?rpr, dual?rpr dual?rpr dual?rpr=200 rpr, Pexplore rpr, Pexplore rpr, Pexplore rpr, Pexplore dual?rpr, dual?rpr dual?rpr, =200 rpr, Pexplore rpr, Pexplore dual?rpr dual?rpr, =200 rpr, Pexplore 500 1000 1500 2000 2500 3000 Number episodes learning phase Exploration rate Accrued testing reward minus optimal reward Accrued learning reward minus optimal reward rpr, Pexplore 500 1000 1500 2000 2500 3000 Number episodes learning phase 500 1000 1500 2000 2500 3000 Number episodes learning phase Figure Results Shuttle random exploration policy, Ktotal 3000. left: accumulative discounted reward accrued learning episode, optimal reward subtracted. middle: accumulative discounted rewards averaged 251 episodes primary RPR obtained learning episode, optimal reward subtracted. right: rate exploration learning episode. all results averaged 100 independent Monte Carlo runs. dual?rpr dual?rpr, dual?rpr dual?rpr rpr, Pexplore.158 rpr, Pex plore.448 rpr, Pexplore.657 dual?rpr dual?rpr dual?rpr.448 explore rpr, Pexplore dual?rpr 100 Number episodes learning phase 100 Number episodes learning phase dual?rpr rpr, Pexplore.657 100 Number episodes learning phase.158 rpr, explore rpr, Pexplore rpr, Exploration rate Accrued testing reward minus optimal reward Accrued learning reward minus optimal reward dual?rpr dual?rpr, dual?rpr dual?rpr rpr, Pexplore.081 rpr, Pex plore.295 rpr, Pexplore.431 rpr, Pexplore 100 Number episodes learning phase dual?rpr dual?rpr dual?rpr rpr, Pexplore.081 rpr, Pexplore.295 rpr, Pexplore.431 rpr, Pexplore 100 Number episodes learning phase Exploration rate Accrued testing reward minus optimal reward Accrued learning reward minus optimal reward dual?rpr dual?rpr 100 Number episodes learning phase Figure Results Shuttle oracle exploration policy incurring cost (top row) (bottom row), Ktotal 100. each figure row counterpart figure Figure random replaced oracle see captions details. Figure that, random exploration primary policy converges optimality and, accordingly, Pexplore drops zero, 1500 learning episodes. when increases, convergence slower: occur (and Pexplore abound 2500 learning episodes. with increased 200, convergence happen Pexplore 3000 learning episodes. results verify analysis Section) primary policy improves Pexplore decreases) agent explores acting optimally acting optimally stops exploring; (iii) exists finite primary policy optimal Pexplore although larger umin small ensure convergence 1500 episodes. observe Figure that) agent explores eﬃciently adaptively switched exploration exploitation auxiliary policy, switch random) primary policy converge optimality agent explores; (iii) primary policy converge optimality agent takes random actions, infinite learning episodes converge. results Figure oracle, provide similar conclusions Figure random. however, special observations Figure) Pexplore affected query cost larger agent performs exploration. ) convergence rate primary policy significantly affected query cost. reason) oracle optimal actions, over-exploration harm optimality; long agent takes optimal actions, primary policy continually improves optimal, remains optimal optimal. Conclusions presented dual-policy approach jointly learning agent behavior optimal balance exploitation exploration, assuming unknown environment pomdp. identifying part environment terms histories (parameterized rpr), approach adaptively switches exploration exploitation depending wher agent part. provided oretical guarantees agent eir explore eﬃciently exploit eﬃciently. experimental results show good agreement oretical analysis approach finds optimal policy eﬃciently. although empirically demonstrated existence small ensure eﬃcient convergence optimality, furr oretical analysis needed find umin tight lower bound ensures convergence optimality amount exploration (without over-exploration). finding exact umin diﬃcult partial observability. however, hopeful find good approximation umin worst case, agent choose optimistic, rmax. optimistic agent large leads over-exploration ensures convergence optimality. Acknowledgements authors anonymous reviewers valuable comments suggestions. this work supported afosr. appendix Proof Lemma: expand) Vbf —?,  rmax —?,  abbreviation       abbreviation    ) satisfying  sum) episodes difference )      Rmax —?,    rmax—?, ?))  —?,  —?, Rmax   —?,   Rmax —?,  rmax —?, ?)) Rmax ) —?, ?))  Rmax ) —?, ?)) ??  sum sequences    satisfying }. .',\n",
       " 'PP3891': 'given undirected, weighted graph, commute distance vertices defined expected time takes random walk starting vertex travel vertex back opposed shortest path distance, takes account paths shortest one. rule thumb, paths connect smaller commute distance becomes. consequence, supposedly satisfies following, highly desirable property: Property): Vertices cluster graph small commute distance, vertices clusters graph ?large? commute distance. property commute distance popular choice widely used, clustering (yen., 2005), semi-supervised learning (zhou sch?olkopf, 2004), social network analysis (liben-nowell kleinberg, 2003), proximity search (sarkar., 2008), image processing (qiu hancock, 2005), dimensionality reduction (ham., 2004), graph embedding (guattery, 1998, Saerens., 2004, Qiu hancock, 2006, Wittmann., 2009) deriving learning oretic bounds \\x0cfor graph labeling (herbster pontil, 2006, cesa-bianchi., 2009). main contributions paper establish property) hold relevant situations. paper study commute distance constant factor equivalent resistance distance, exact definitions) behaves size graph increases. focus case random geometric graphs relevant machine learning, similar results hold general classes graphs mild assumptions. denoting Hij expected hitting time, Cij commute distance vertices degree vertex prove hitting times commute distances approximated constant vol) denotes volume graph) Hij vol) cij vol) intuitive reason behavior graph large, random walk ?gets lost? sheer size graph. takes long travel substantial part graph time random walk close goal ?forgotten? started from. reason, hitting time Hij depend starting vertex more. depends inverse degree target vertex intuitively represents likelihood random walk hits neighborhood. respect shows behavior return time time takes random walk starts return staring point) well-known vol)  well. findings strong implications: raw commute distance distance function large graphs. negative side, approximation result shows contrary popular belief, commute distance account global properties data, graph ?large enough?. considers local density degree vertex) vertices, else. resulting large sample commute distance dist completely meaningless distance graph. example, data points nearest neighbor (namely, vertex largest degree), second-nearest neighbor vertex second-largest degree. particular, main motivation commute distance, Property), longer holds graph ?large enough?. disappointingly, computer simulations show large) breaks down. often, order 1000 make commute distance close approximation expression (see Section details). effect stronger dimensionality underlying data space large. consequently, moderate-sized graphs, raw commute distance basis machine learning algorithms discouraged. correcting commute distance. reported literature hitting times commute times observed small vertices consideration high degree, spread commute distance values large (liben-nowell kleinberg, 2003, brand, 2005, Yen., \\x0c2009). subsequently, authors suggested methods correct unpleasant behavior. light oretical results immediately undesired behavior commute distance occurs. moreover, analyze suggested corrections prove meaningful (see Section). based ory suggest correction, amplified commute distance. distance function derived commute distance, avoids artifacts. distance function euclidean, making well-suited machine learning purposes kernel methods. eﬃcient computation approximate commute distances. applications commute distance distance function, reasons, graph sparsification (spielman srivastava, 2008) computing bounds mixing cover times (aleliunas., 1979, Chandra., 1989, Avin ercal, 2007, Cooper frieze, 2009) graph labeling (herbster pontil, 2006, cesa-bianchi., 2009). obtain commute distance points graph compute pseudo-inverse graph Laplacian matrix, operation time complexity prohibitive large graphs. circumvent matrix inversion, approximations commute distance suggested literature (spielman srivastava, 2008, Sarkar moore, 2007, brand, 2005). results lead simpler well-justified approximating commute distance large random geometric graphs. general setup, definitions notation undirected, weighted graphs, vertices. assume connected bipartite non-negative weight matrix (adjacency matrix) denoted Pby (wij  wij denote degree vertex vol volume graph. denotes diagonal matrix diagonal entries   called degree matrix. our main focus paper class random geometric graphs relevant machine learning. sequence points   drawn. underlying density points form vertices   graph. edges graph defined ?neighboring points? connected: ?-graph connect points Euclidean distance equal undirected, symmetric-nearest neighbor graph connect nearest neighbors vice versa. mutual-nearest neighbor graph connect nearest neighbors vice versa. space constraints discuss case unweighted graphs paper. results carried weighted graphs, weighted knn-graphs Gaussian similarity graphs. natural random walk random walk transition matrix hitting time Hij defined expected time takes random walk starting vertex travel vertex (with Hii definition). commute distance (also called commute time) defined Cij Hij Hji readers commute \\x0cdistance resistance distance. interprets graph electrical network edges represent resistors. conductance resistor edge weight. resistance distance Rij defined effective resistance vertices network. resistance distance coincides commute distance constant: Cij vol)?rij background reading Doyle Snell (1984), Klein Randic (1993), Xiao Gutman (2003), Fouss. (2006), bollob (1998), Lyons Peres (2010). rest paper probability distribution density study behavior commute distance fixed points study density small region for convenience, make definition. definition (valid region) Let density points) call connected subset valid region respect properties satisfied: interior points  density bounded ) pmin constant pmin assume pmax maxx)  ?bottleneck? larger set dist} connected (here denotes topological boundary  boundary regular sense. assume exist positive constants  points vol? )   vol? )) (where vol denotes Lebesgue volume). essentially condition excludes situation boundary arbitrarily thin spikes. readability reasons, state main results constants constants independent graph connectivity parameter respectively) depend dimension, geometry values constants determined explicitly proofs. coincide propositions. notational convenience, formulate results terms resistance distance. obtain results commute distance multiply factor vol). convergence resistance distance random geometric graphs section present oretical main results random geometric graphs. show type graph, resistance distance Rij converges trivial limit space constraints formulate results unweighted knn ?-graphs. similar results hold weighted variants graphs Gaussian similarity graphs. orem (resistance distance knn-graphs) Fix points valid region respect bottleneck density bounds pmin pmax assume distance boundary /2pmax exist constants   probability exp resistance distance symmetric mutual knn-graph satisfies krij  log) probability converges  log)  rhs deviation bound converges   log)  case  case conditions, density continuous additionally krij probability. orem (resistance distance ?-graphs) Fix points valid region respect bottleneck density bounds pmin pmax assume distance boundary  exist constants   probability exp exp resistance distance ?-graph satisfies log/?  rij     probability converges   log)  rhs deviation bound converges  log/?)   case  case conditions, density continuous additionally   Rij probability.  Let discuss orems bloc. start couple technical remarks. note achieve convergence resistance distance rescale appropriately (for example, ?-graph scale factor rescaling chosen limit expressions finite, positive values. scaling factor terms eir leads divergence convergence zero. convergence conditions  respectively) expected random geometric graphs. satisfied degrees order log) (for smaller degrees, graphs connected anyway. penrose, 1999). hence, results hold sparse dense connected random geometric graphs. valid region introduced technical reasons. operate region order control behavior graph. average degrees. assumptions standard assumptions random geometric graph literature. setting, freedom choosing want. order obtain tightest bounds aim valid wide bottleneck high minimal density. generally, results convergence commute distance proved kinds graphs graphs expected degrees power law graphs, assumption minimal degree graph slowly increases Details scope paper. proof outline orems (full proofs presented supplementary material). fixed vertices connected graph graph electrical network edge resistance electrical laws, resistances series add, resistances series resistance resistances parallel lines satisfy consult situation Figure vertex edges neighbors. resistance ?spanned? parallel edges satisfies Pds similarly Between \\x0cneighbors paths. turns contribution paths resistance negligible (essentially, wires neighborhoods electricity ﬂow freely). effective resistance dominated edges adjacent contributions providing clean mamatical proof argument technical. proof based Corollary Section bollob (1998) states resistance distance beween outgoing edges, resistance outgoing edges, resistance paths Figure Intuition proof orems text details. fixed vertices expressed Rst inf unit ﬂow  apply orem construct ﬂow spreads widely possible? graph. counting edges adding resistances leads desired results. details fiddly found supplementary material. correcting resistance distance obviously, large sample resistance distance Rij  completely meaningless distance graph. question discuss section wher correct commute distance unpleasant large sample effect occur. start references literature. observed empirical studies commute distances small vertices consideration high degree, spread commute distance values large. oretical results immediately explain behavior: degrees large small. compared ?spread? spread enormous. heuristics suggested solve problem. liben-nowell Kleinberg (2003) suggest correct hitting times simply multiplying degrees. commute distance, leads suggested correction CLN, Hij Hji prove explicitly paper, convergence results commute time hold individual hitting times. namely, hitting time Hij approximated vol oretical results immediately show correction CLN useful, absolute values. large graphs, simply effect normalizing hitting times leading CLN  however, ranking introduced distance function information data. reason order terms dominate absolute converge two, order terms introduce ?variation two?, variation encode cluster structure. yen. (2009) exploit well-known fact commute distance Euclidean kernel matrix coincides moore-penrose inverse graph Laplacian matrix. authors apply sigmoid transformation KYen exp(?lij /?)) contant idea \\x0csigmoid transformation reduces spread distance similarity) values. however-hoc approach disadvantage resulting ?kernel? kyen positive definite. correction suggested Brand (2005). yen. (2009) considers kernel matrix corresponds commute distance. applying sigmoid transformation centers normalizes kernel matrix feature space. leads corrected kernel    ?rij (rik Rkj kbrand Rkl one glance surprising centered normalized kernel commute distance make difference. however, takes Euclidean distance function form dist, sij  computes centered kernel matrix, obtains Kij Kij  kernel matrix induced Thus off-diagonal terms inﬂuenced decaying factor compared diagonal. longer case normalization (because normalization diagonal terms important, terms depend key brand kernel useful. suitable correction based oretical results? proof main orems shows edges adjacent completely dominate behavior resistance distance: ?bottleneck? ﬂow, contribution dominates terms. interesting information global topology graph contained remainder terms Sij Rij   summarize ﬂow contributions edges graph. key obtaining good distance function remove inﬂuence terms ?amplify? inﬂuence general graph term Sij achieved eir off-diagonal terms pseudo-inverse graph Laplacian ignoring diagonal, building distance function based remainder terms Sij directly. choose option propose distance function. define amplified commute distance Camp, Sij uij Sij Rij   uij 2wij wii /d2i wjj /d2j set Camp, proposition (amplified commute distance euclidean) matrix entries dij Camp Euclidean distance matrix. proof outline. preliminary work show remainder terms written Sij   uij denotes unit vector positive definite matrix (see proof Proposition von Luxburg., 2010). implies desired statement. additionally Euclidean distance, amplified commute distance nice limit behavior.   terms uij dominated terms Sij left ?interesting terms? sij practical purposes, kernel induced amplified commute distance center normalize. formulas, amplified commute kernel     110 )camp 110 Kamp) (where identity matrix, vector ones, Camp \\x0camplified commute distance matrix). section shows kernel Kamp works nicely practice. note correction Brand amplified commute kernel similar, identical. off-diagonal terms kernels close, Equation), interested ranking based similarity values, kernels behave similarly. however, important difference diagonal terms Brand kernel bigger amplified kernel (using convergence techniques show Brand kernel converges identity matrix, diagonal completely dominates off-diagonal terms). lead effect Brand kernel behaves worse kernel algorithms SVM ignore diagonal kernel. experiments Our set experiments considers question fast convergence commute distance takes place practice. small data sets, good approximation takes place. means problems raw commute distance occur small sample size. plots Figure report maximal relative error defined maxij —rij —/rij relative error log10 -scale. show results ?-graphs, unweighted knn graphs Gaussian similarity graphs (fully connected weighted graphs edge weights exp(kxi  )). order plot results figure, match parameters graphs. knn-graph set values ?-graph gaussian graph equal maximal-nearest neighbor distance data set. sample size. set points drawn uniform distribution unit cube R10 figure (first plot), maximal relative error decreases fast increasing sample size. note small sample sizes maximal deviations small. dimension. result surprising glance maximal deviation decreases data uniform, dim data uniform=2000=100 Gaussian epsilon knn log (rel deviation 1000 2000 mixture gaussians=2000, dim=100 Gaussian silon knn 200 500 dim USPS data set log (rel deviation) \\x0cgaussian epsilon knn log10(rel deviation) Gaussian epsilon knn log10(rel deviation) separation log10) Figure Relative deviations true approximate commute distances. solid lines show maximal relative deviations, dashed lines relative deviations. text details. increase dimension, Figure (second plot). intuitive explanation higher dimensions, geometric graphs mix faster exist ?shortcuts? sides point cloud. thus, random walk ?forgets faster? started from. clusteredness. deviation worse data pronounced cluster structure. mixture Gaussians R10 unit variances weight components. call distance centers components separation. figure (third plot) show maximum relative errors (solid lines) relative errors (dashed lines). increasing separation, deviation increases. sparsity. plot Figure shows relative errors increasingly dense graphs, increasing parameter Here well-known USPS data set handwritten digits (9298 points 256 dimensions). plot maximum relative errors (solid lines) relative errors (dashed lines). errors decrease denser graph gets. due fact random walk mixes faster denser graphs. note deviations extremely small real-world data set. set experiments compare corrections raw commute distance. end, built knn graph USPS data set (all 9298 points), computed commute distance matrix corrections. resulting matrices shown Figure (left part) heat plots. cases, plot off-diagonal terms. predicted ory, raw commute distance identify cluster structure. however, cluster structure visible kernel commute distance, pseudoinverse graph Laplacian  reason diagonal matrix \\x0ccan approximated off-diagonal terms encode graph structure, smaller scale diagonal. heat plots, corrections graph Laplacian show cluster structure extent correction LNK small extent, corrections brand, Yen bigger extent). experiment evaluates performance distances semi-supervised learning task. usps data set, chose random points labeled. classified unlabeled points-nearest neighbor classifier based distances labeled data points. classifier, chosen-fold cross-validation }. experiment repeated times. results Figure (right figure). baseline report results based standard Euclidean distance data points. predicted ory, raw commute distance performs extremely poor. euclidean distance behaves reasonably, outperformed corrections commute distance. shows graph structure basic Euclidean distance. naive correction LNK stays close Euclidean distance, corrections brand, Yen virtually lie top outperform semi?supervised learning task Raw commute dist Euclidean dist LNK dist Amplified kernel Brand kernel Yen kernel classification error 100 200 Number labeled points 400 Figure Figures left: Distances kernels based knn graph 9298 USPS points (heat plots, off-diagonal terms only): exact resistance distance, pseudo-inverse graph Laplacian kernels corrections lnk, yen, brand, amplified Kamp figure right: semi-supervised learning results based distances kernels. lines amplified, Brand Yen kernel lie top. methods large margin. conclude tentative statements. correction LNK bit naive?, corrections brand, Yen ?tend work? ranking based setting. based simple experiments impossible judge candidates one?. fond yen correction lead proper kernel. brand kernel converge (different) limit functions. oretical properties limit functions present oretical reason prefer. however, diagonal dominance Brand kernel problematic. discussion paper proved commute distance random geometric graphs approximated simple limit expression. contrary intuition, limit expression longer takes account cluster structure graph, global property (such distances underlying Euclidean space). oretical bounds simulations story: approximation data high-dimensional extremely clustered, standard situations machine learning. shows raw commute distance machine learning purposes problematic. however, structure graph recovered corrections commute distance. suggest eir correction Brand (2005) amplified commute kernel Section corrections well-defined, non-trivial limit perform experiments. intuitive explanation result sample size increases, random walk sample graph ?gets lost? sheer size graph. takes long travel substantial part graph time random walk close goal ?forgotten? started from. stated differently: random walk graph mixed hits desired target vertex. higher level, expect problem ?getting lost? affects commute distance, methods random walks naive explore global properties graph. example, results Nadler. (2009), artifacts semi-supervised learning context unlabeled points studied, strongly related results. general, careful random walk based methods extracting global properties graphs order avoid lost converging meaningless results. Given undirected, weighted graph, commute distance vertices defined expected time takes random walk starting vertex travel vertex back opposed shortest path distance, takes account paths shortest one. rule thumb, paths connect smaller commute distance becomes. consequence, supposedly satisfies following, highly desirable property: Property): Vertices cluster graph small commute distance, vertices clusters graph ?large? commute distance. property commute distance popular choice widely used, clustering (yen., 2005), semi-supervised learning (zhou sch?olkopf, 2004), social network analysis (liben-nowell kleinberg, 2003), proximity search (sarkar., 2008), image processing (qiu hancock, 2005), dimensionality reduction (ham., 2004), graph embedding (guattery, 1998, Saerens., 2004, Qiu hancock, 2006, Wittmann., 2009) deriving learning oretic bounds \\x0cfor graph labeling (herbster pontil, 2006, cesa-bianchi., 2009). one main contributions paper establish property) hold relevant situations. paper study commute distance constant factor equivalent resistance distance, exact definitions) behaves size graph increases. focus case random geometric graphs relevant machine learning, similar results hold general classes graphs mild assumptions. denoting Hij expected hitting time, Cij commute distance vertices degree vertex prove hitting times commute distances approximated constant vol) denotes volume graph) Hij vol) cij vol) intuitive reason behavior graph large, random walk ?gets lost? sheer size graph. takes long travel substantial part graph time random walk close goal ?forgotten? started from. for reason, hitting time Hij depend starting vertex more. depends inverse degree target vertex intuitively represents likelihood random walk hits neighborhood. respect shows behavior return time time takes random walk starts return staring point) well-known vol)  well. our findings strong implications: raw commute distance distance function large graphs. negative side, approximation result shows contrary popular belief, commute distance account global properties data, graph ?large enough?. considers local density degree vertex) vertices, else. resulting large sample commute distance dist completely meaningless distance graph. for example, data points nearest neighbor (namely, vertex largest degree), second-nearest neighbor vertex second-largest degree. particular, main motivation commute distance, Property), longer holds graph ?large enough?. even disappointingly, computer simulations show large) breaks down. often, order 1000 make commute distance close approximation expression (see Section details). this effect stronger dimensionality underlying data space large. consequently, moderate-sized graphs, raw commute distance basis machine learning algorithms discouraged. correcting commute distance. reported literature hitting times commute times observed small vertices consideration high degree, spread commute distance values large (liben-nowell kleinberg, 2003, brand, 2005, Yen., \\x0c2009). subsequently, authors suggested methods correct unpleasant behavior. light oretical results immediately undesired behavior commute distance occurs. moreover, analyze suggested corrections prove meaningful (see Section). based ory suggest correction, amplified commute distance. this distance function derived commute distance, avoids artifacts. this distance function euclidean, making well-suited machine learning purposes kernel methods. eﬃcient computation approximate commute distances. applications commute distance distance function, reasons, graph sparsification (spielman srivastava, 2008) computing bounds mixing cover times (aleliunas., 1979, Chandra., 1989, Avin ercal, 2007, Cooper frieze, 2009) graph labeling (herbster pontil, 2006, cesa-bianchi., 2009). obtain commute distance points graph compute pseudo-inverse graph Laplacian matrix, operation time complexity this prohibitive large graphs. circumvent matrix inversion, approximations commute distance suggested literature (spielman srivastava, 2008, Sarkar moore, 2007, brand, 2005). our results lead simpler well-justified approximating commute distance large random geometric graphs. General setup, definitions notation undirected, weighted graphs, vertices. assume connected bipartite non-negative weight matrix (adjacency matrix) denoted Pby (wij  wij denote degree vertex vol volume graph. denotes diagonal matrix diagonal entries   called degree matrix. Our main focus paper class random geometric graphs relevant machine learning. here sequence points   drawn. underlying density points form vertices   graph. edges graph defined ?neighboring points? connected: ?-graph connect points Euclidean distance equal undirected, symmetric-nearest neighbor graph connect nearest neighbors vice versa. mutual-nearest neighbor graph connect nearest neighbors vice versa. for space constraints discuss case unweighted graphs paper. our results carried weighted graphs, weighted knn-graphs Gaussian similarity graphs. consider natural random walk random walk transition matrix hitting time Hij defined expected time takes random walk starting vertex travel vertex (with Hii definition). commute distance (also called commute time) defined Cij Hij Hji some readers commute \\x0cdistance resistance distance. here interprets graph electrical network edges represent resistors. conductance resistor edge weight. resistance distance Rij defined effective resistance vertices network. resistance distance coincides commute distance constant: Cij vol)?rij for background reading Doyle Snell (1984), Klein Randic (1993), Xiao Gutman (2003), Fouss. (2006), bollob (1998), Lyons Peres (2010). for rest paper probability distribution density study behavior commute distance fixed points study density small region For convenience, make definition. definition (valid region) Let density points) call connected subset valid region respect properties satisfied: interior points  density bounded ) pmin constant pmin assume pmax maxx)  ?bottleneck? larger set dist} connected (here denotes topological boundary  boundary regular sense. assume exist positive constants  points vol? )   vol? )) (where vol denotes Lebesgue volume). essentially condition excludes situation boundary arbitrarily thin spikes. for readability reasons, state main results constants constants independent graph connectivity parameter respectively) depend dimension, geometry values constants determined explicitly proofs. coincide propositions. for notational convenience, formulate results terms resistance distance. obtain results commute distance multiply factor vol). Convergence resistance distance random geometric graphs section present oretical main results random geometric graphs. show type graph, resistance distance Rij converges trivial limit for space constraints formulate results unweighted knn ?-graphs. similar results hold weighted variants graphs Gaussian similarity graphs. orem (resistance distance knn-graphs) Fix points consider valid region respect bottleneck density bounds pmin pmax assume distance boundary /2pmax exist constants   probability exp resistance distance symmetric mutual knn-graph satisfies krij  log) probability converges  log)  rhs deviation bound converges   log)  case  case under conditions, density continuous additionally krij probability. orem (resistance distance ?-graphs) Fix points consider valid region respect bottleneck density bounds pmin pmax assume distance boundary  exist constants   probability exp exp resistance distance ?-graph satisfies log/?  rij     probability converges   log)  rhs deviation bound converges  log/?)   case  case under conditions, density continuous additionally   Rij probability.  Let discuss orems bloc. start couple technical remarks. note achieve convergence resistance distance rescale appropriately (for example, ?-graph scale factor our rescaling chosen limit expressions finite, positive values. scaling factor terms eir leads divergence convergence zero. convergence conditions  respectively) expected random geometric graphs. satisfied degrees order log) (for smaller degrees, graphs connected anyway. penrose, 1999). hence, results hold sparse dense connected random geometric graphs. valid region introduced technical reasons. operate region order control behavior graph. average degrees. assumptions standard assumptions random geometric graph literature. setting, freedom choosing want. order obtain tightest bounds aim valid wide bottleneck high minimal density. more generally, results convergence commute distance proved kinds graphs graphs expected degrees power law graphs, assumption minimal degree graph slowly increases Details scope paper. proof outline orems (full proofs presented supplementary material). consider fixed vertices connected graph graph electrical network edge resistance electrical laws, resistances series add, resistances series resistance resistances parallel lines satisfy now consult situation Figure consider vertex edges neighbors. resistance ?spanned? parallel edges satisfies Pds similarly Between \\x0cneighbors paths. turns contribution paths resistance negligible (essentially, wires neighborhoods electricity ﬂow freely). effective resistance dominated edges adjacent contributions providing clean mamatical proof argument technical. our proof based Corollary Section bollob (1998) states resistance distance beween outgoing edges, resistance outgoing edges, resistance paths Figure Intuition proof orems see text details. fixed vertices expressed Rst inf unit ﬂow  apply orem construct ﬂow spreads widely possible? graph. counting edges adding resistances leads desired results. details fiddly found supplementary material. Correcting resistance distance obviously, large sample resistance distance Rij  completely meaningless distance graph. question discuss section wher correct commute distance unpleasant large sample effect occur. let start references literature. observed empirical studies commute distances small vertices consideration high degree, spread commute distance values large. our oretical results immediately explain behavior: degrees large small. and compared ?spread? spread enormous. several heuristics suggested solve problem. liben-nowell Kleinberg (2003) suggest correct hitting times simply multiplying degrees. for commute distance, leads suggested correction CLN, Hij Hji even prove explicitly paper, convergence results commute time hold individual hitting times. namely, hitting time Hij approximated vol oretical results immediately show correction CLN useful, absolute values. for large graphs, simply effect normalizing hitting times leading CLN  however, ranking introduced distance function information data. reason order terms dominate absolute converge two, order terms introduce ?variation two?, variation encode cluster structure. yen. (2009) exploit well-known fact commute distance Euclidean kernel matrix coincides moore-penrose inverse graph Laplacian matrix. authors apply sigmoid transformation KYen exp(?lij /?)) contant idea \\x0csigmoid transformation reduces spread distance similarity) values. however-hoc approach disadvantage resulting ?kernel? kyen positive definite. correction suggested Brand (2005). Yen. (2009) considers kernel matrix corresponds commute distance. but applying sigmoid transformation centers normalizes kernel matrix feature space. this leads corrected kernel    ?rij (rik Rkj KBrand Rkl One glance surprising centered normalized kernel commute distance make difference. however, takes Euclidean distance function form dist, sij  computes centered kernel matrix, obtains Kij Kij  kernel matrix induced Thus off-diagonal terms inﬂuenced decaying factor compared diagonal. even longer case normalization (because normalization diagonal terms important, terms depend key brand kernel useful. what suitable correction based oretical results? proof main orems shows edges adjacent completely dominate behavior resistance distance: ?bottleneck? ﬂow, contribution dominates terms. interesting information global topology graph contained remainder terms Sij Rij   summarize ﬂow contributions edges graph. key obtaining good distance function remove inﬂuence terms ?amplify? inﬂuence general graph term Sij this achieved eir off-diagonal terms pseudo-inverse graph Laplacian ignoring diagonal, building distance function based remainder terms Sij directly. choose option propose distance function. define amplified commute distance Camp, Sij uij Sij Rij   uij 2wij wii /d2i wjj /d2j set Camp, proposition (amplified commute distance euclidean) matrix entries dij Camp Euclidean distance matrix. proof outline. preliminary work show remainder terms written Sij   uij denotes unit vector positive definite matrix (see proof Proposition von Luxburg., 2010). this implies desired statement. Additionally Euclidean distance, amplified commute distance nice limit behavior. when  terms uij dominated terms Sij left ?interesting terms? sij for practical purposes, kernel induced amplified commute distance center normalize. formulas, amplified commute kernel     110 )camp 110 Kamp) (where identity matrix, vector ones, Camp \\x0camplified commute distance matrix). section shows kernel Kamp works nicely practice. note correction Brand amplified commute kernel similar, identical. off-diagonal terms kernels close, Equation), interested ranking based similarity values, kernels behave similarly. however, important difference diagonal terms Brand kernel bigger amplified kernel (using convergence techniques show Brand kernel converges identity matrix, diagonal completely dominates off-diagonal terms). this lead effect Brand kernel behaves worse kernel algorithms SVM ignore diagonal kernel. Experiments Our set experiments considers question fast convergence commute distance takes place practice. small data sets, good approximation takes place. this means problems raw commute distance occur small sample size. consider plots Figure report maximal relative error defined maxij —rij —/rij relative error log10 -scale. show results ?-graphs, unweighted knn graphs Gaussian similarity graphs (fully connected weighted graphs edge weights exp(kxi  )). order plot results figure, match parameters graphs. given knn-graph set values ?-graph Gaussian graph equal maximal-nearest neighbor distance data set. sample size. consider set points drawn uniform distribution unit cube R10 Figure (first plot), maximal relative error decreases fast increasing sample size. note small sample sizes maximal deviations small. dimension. result surprising glance maximal deviation decreases data uniform, dim data uniform=2000=100 Gaussian epsilon knn log (rel deviation 1000 2000 mixture gaussians=2000, dim=100 Gaussian silon knn 200 500 dim USPS data set log (rel deviation) \\x0cgaussian epsilon knn log10(rel deviation) Gaussian epsilon knn log10(rel deviation) separation log10) Figure Relative deviations true approximate commute distances. solid lines show maximal relative deviations, dashed lines relative deviations. see text details. increase dimension, Figure (second plot). intuitive explanation higher dimensions, geometric graphs mix faster exist ?shortcuts? sides point cloud. thus, random walk ?forgets faster? started from. clusteredness. deviation worse data pronounced cluster structure. consider mixture Gaussians R10 unit variances weight components. call distance centers components separation. Figure (third plot) show maximum relative errors (solid lines) relative errors (dashed lines). increasing separation, deviation increases. sparsity. plot Figure shows relative errors increasingly dense graphs, increasing parameter Here well-known USPS data set handwritten digits (9298 points 256 dimensions). plot maximum relative errors (solid lines) relative errors (dashed lines). errors decrease denser graph gets. again due fact random walk mixes faster denser graphs. note deviations extremely small real-world data set. set experiments compare corrections raw commute distance. end, built knn graph USPS data set (all 9298 points), computed commute distance matrix corrections. resulting matrices shown Figure (left part) heat plots. cases, plot off-diagonal terms. predicted ory, raw commute distance identify cluster structure. however, cluster structure visible kernel commute distance, pseudoinverse graph Laplacian  reason diagonal matrix \\x0ccan approximated off-diagonal terms encode graph structure, smaller scale diagonal. heat plots, corrections graph Laplacian show cluster structure extent correction LNK small extent, corrections brand, Yen bigger extent). experiment evaluates performance distances semi-supervised learning task. USPS data set, chose random points labeled. classified unlabeled points-nearest neighbor classifier based distances labeled data points. for classifier, chosen-fold cross-validation }. experiment repeated times. results Figure (right figure). baseline report results based standard Euclidean distance data points. predicted ory, raw commute distance performs extremely poor. Euclidean distance behaves reasonably, outperformed corrections commute distance. this shows graph structure basic Euclidean distance. while naive correction LNK stays close Euclidean distance, corrections brand, Yen virtually lie top outperform semi?supervised learning task Raw commute dist Euclidean dist LNK dist Amplified kernel Brand kernel Yen kernel classification error 100 200 Number labeled points 400 Figure Figures left: Distances kernels based knn graph 9298 USPS points (heat plots, off-diagonal terms only): exact resistance distance, pseudo-inverse graph Laplacian kernels corrections lnk, yen, brand, amplified Kamp figure right: semi-supervised learning results based distances kernels. lines amplified, Brand Yen kernel lie top. methods large margin. conclude tentative statements. correction LNK bit naive?, corrections brand, Yen ?tend work? ranking based setting. based simple experiments impossible judge candidates one?. fond yen correction lead proper kernel. both brand kernel converge (different) limit functions. oretical properties limit functions present oretical reason prefer. however, diagonal dominance Brand kernel problematic. Discussion paper proved commute distance random geometric graphs approximated simple limit expression. contrary intuition, limit expression longer takes account cluster structure graph, global property (such distances underlying Euclidean space). both oretical bounds simulations story: approximation data high-dimensional extremely clustered, standard situations machine learning. this shows raw commute distance machine learning purposes problematic. however, structure graph recovered corrections commute distance. suggest eir correction Brand (2005) amplified commute kernel Section both corrections well-defined, non-trivial limit perform experiments. intuitive explanation result sample size increases, random walk sample graph ?gets lost? sheer size graph. takes long travel substantial part graph time random walk close goal ?forgotten? started from. stated differently: random walk graph mixed hits desired target vertex. higher level, expect problem ?getting lost? affects commute distance, methods random walks naive explore global properties graph. for example, results Nadler. (2009), artifacts semi-supervised learning context unlabeled points studied, strongly related results. general, careful random walk based methods extracting global properties graphs order avoid lost converging meaningless results.',\n",
       " 'PP3894': 'consider empirical risk minimization hyposis class . non-negative loss function ). , learn predictor small risk) ), minimizing empirical risk . sample    statistical guarantees excess risk understood parametric. finite dimensional) hyposis classes. formally, hyposis classes finite-subgraph dimension] (aka pseudo-dimension). classes learning guarantees obtained bounded loss function. . —?—  relevant measure complexity-subgraph dimension. alternatively, non-parametric hyposis classes. infinite-subgraph dimension. class low-norm linear predictors —kwk guarantees obtained terms scale-sensitive measures complexity fatshattering dimensions], covering numbers] Rademacher complexity]. classical statistical learning ory approach obtaining learning guarantees scalesensitive classes rely Lipschitz constant . . bound derivative. ). excess risk bounded expectation sample):    2drn) arg min) empirical risk minimizer (erm), inf) approxima \\x0cfor -bounded linear) Rademacher tion error. . complexity, typically scales) predictors, kxk2 paper address deficiencies guarantee). first, bound applies loss functions bounded derivative, hinge loss logistic loss popular classification, absolute-value loss regression. directly applicable squared loss ,  derivative bounded, first. simply bound derivative squared loss terms bound magnitude),pbut. norm-bounded linear predictors results disappointing excess risk bound form (max kxk). aim paper provide clean bounds excess risk smooth loss functions squared loss bounded second, rar first, derivative.   deficiency) dependence dependence unavoidable general. finite dimensional (parametric) classes, improved rate distribution separable. exists )  particular, class bounded functions-subgraph-dimension. -dimensional linear predictors), expectation sample]:  log log ddl   term disappears separable case, graceful degradation rate rate separable case.  separable rate, graceful degradation, non-parametric case? show, deficiencies related. for non-parametric classes, non-smooth Lipschitz loss, hinge-loss, excess risk scale, separable case. however-smooth non-negative loss functions, derivative . bounded separable rate possible. section obtain bound excess risk logarithmic factors):     hr2n? )  hrl?    ?  ) particular,pfor -norm-bounded linear predictors kxk2 excess risk bounded  ). anor interesting distinction parametric non-parametric classes, squared-loss, bound) tight non-separable rate unavoidable. contrast parametric (fine dimensional) case, rate squared loss, approximation error ]. differences parametric scale-sensitive \\x0cclasses, non-smooth, smooth strongly convex loss functions discussed Section summarized Table guarantees discussed general learning guarantees stochastic setting rely Rademacher complexity hyposis class, phrased terms minimizing scalar loss function. section online setting, addition stochastic setting, present similar guarantees online stochastic convex optimization]. guarantees Section match equation) special case convex loss function norm-bounded linear predictors, Section capture general setting optimizing arbitrary non-negative convex objective, require smooth separate discussion ?predictor? scalar loss function Section). results Section expressed terms properties norm, rar measure concentration Radamacher complexity) Section however, online stochastic convex optimization setting Section restrictive, require objective convex Section make assumption convexity hyposis class loss function ?). specifically, non-negative-smooth convex objective, domain bounded prove average ). comparing online regret (and excess risk stochastic optimization) bounded bound) loss DLipschitz rar-smooth], relationship discussed erm. unlike bound) erm, convex optimization bound avoids polylogarithmic factors. results Section generalize smoothness boundedness respect non-euclidean norms. studying online stochastic convex optimization setting (section), addition ERM (section), advantages. first, obtain learning guarantee eﬃcient single-pass learning methods, stochastic gradient descent mirror descent), non-stochastic regret. second, bound obtain convex optimization setting (section bound ERM (section avoids polylogarithmic large constant factors. third, bound applicable non-negative online stochastic optimization problems classification, including problems ERM applicable (see]). detailed proofs statements claimed paper found supplementary material paper. empirical Risk Minimization Smooth Loss Recall Rademacher complexity  ) ?unif) Throughout ?worst case? rademacher complexity. starting point learning bound) applies-lipschitz loss \\x0cfunctions. )—  derivatives. argument). type bound obtain bound derivative)? avoid talking derivative explicitly, function-smooth iff derivative-lipschitz. differentiable means central observation, obtain guarantees smooth loss functions, smooth loss, derivative bounded terms function value: Lemma. -smooth non-negative function have)— 4hf) This Lemma argue close optimum value, loss small, derivative. dependence) derivative bound guided heuristic intuition: Since concerned behavior erm, bound  bound ,  applying Lemma), )]— 4hl).  , ERM )— separately, absolute inside expectation?this bound, non-negativity loss plays important role. ignoring important issue moment    ). solving) yields desired bound plugging) yields). rough intuition captured orem: orem -smooth non-negative loss .  )—  probability  random sample size    log/?) log/?)   ) log) ) log:      log) log/?)  log R2n) log/?)  105 numeric constant derived]. note ?confidence? terms depended —?—, typically dominant term obtain bound holds expectation sample (rar high probability) avoids direct dependence —?—. prove orem notion Local Rademacher Complexity], focus behavior close erm. end, empirically restricted loss class  ,  ), ) Lemma, presented below,? solidifies heuristic intuition discussed above, showing \\x0crademacher complexity ) scales. Lemma higher-order version Lipschitz Composition Lemma], states Rademacher complexity unrestricted loss class bounded DRn). here, second, rar first, derivative, obtain bound depends empirical restriction: Lemma. non-negative Hsmooth loss bounded function class bounded   12hb ? )) 12hr) log3 log3) Applying Lemma, orem standard Local Rademacher argument].  Related Results rates faster previously explored conditions, including small. finite Dimensional Case Lee] showed faster rates squared loss, exploiting strong convexity loss function, finite-subgraph-dimension. panchenko] fast rate results general Lipschitz bounded loss functions, finite-subgraphdimension case. bousquet] provided similar guarantees linear predictors Hilbert spaces spectrum kernel matrix (covariance exponentially decaying, making situation finite dimensional. methods rely finiteness effective dimension provide fast rates. case, smoothness necessary. method, hand, establishes fast rates, function classes finite-subgraph-dimension. show non-parametric case, smoothness plays important role (see Table). aggregation Tsybakov] studied learning rates aggregation, predictor chosen convex hull finite set base predictors. equivalent constraint base predictor viewed ?feature?. -based analysis, bounds depend logarithmically number base predictors. dimensionality), rely scale change loss function, ?scale sensitive? nature. aggregate classifier, Tsybakov obtained rate small) risk achieve base classifiers. tsybakov result, risk achieved aggregate. bounded ell1 classifier order obtain faster rate. tsybakov core result sense similar finite dimensional results, rate error achieved finite cardinality (and finite dimension) class. tsybakov approximation error small class base predictors. large hyposis class. covering) obtain learning rates large hyposis class aggregation small class. results imply?fast learning rates hyposis classes low complexity. specifically learning rates results, covering number hyposis class scale behave typical classes, including class linear predictors bounded norm, covering numbers scale methods imply fast rates function classes. fact, rates techniques, requires covering numbers increase all, finite-subgraph-dimension. chesneau] \\x0cextend tsybakov work general losses, deriving similar results Lipschitz loss function.  caveats hold: rates faster require covering numbers grow slower rates essentially require finite-subgraph-dimension. work, hand, applicable Rademacher complexity (equivalently covering numbers) controlled. similar techniques, rar work Tsybakov Chesneau, points importance smoothness obtaining fast rates non-parametric case: Chesneau relied Lipschitz constant, show, Section obtaining fast rates non-parametric case,  local Rademacher Complexities Bartlett] developed general machinery proving fast rates based local Rademacher complexities. however, important note localized complexity term typically dominates rate controlled. example, Steinwart] Local Rademacher Complexity provide fast rate loss Support Vector Machines (svms -regularized hinge-loss minimization) based called ?geometric margin condition? tsybakov margin condition. steinwart analysis specific svms. local Rademacher Complexities order obtain fast rates, general hyposis classes, based standard Rademacher complexity) hyposis classes, smoothness loss function magnitude furr assumptions hyposis classes itself. non-lipschitz Loss Beyond strong connections smoothness fast rates highlight, aware prior work providing explicit easy-use result controlling generic non-lipschitz loss (such squared loss) solely terms Rademacher complexity. online Stochastic Optimization Smooth Convex Objectives turn online stochastic convex optimization. settings learner chooses closed convex set normed vector space, attempting minimize objective, instances   objective function convex This captures learning linear predictors. convex loss function ), )) ), extends supervised learning. case objective-smooth. norm kwk reader choose subset Euclidean Hilbert space, kwk -norm):  , ?     dual norm. key generalize Lemma smoothness. vector rar scalar smoothness: Lemma. -smooth non-negative  ?  4hf) order general norms, rely non-negative regularizer -strongly convex (see Definition. . norm kwk for Euclidean norm squared Euclidean norm regularizer) kwk  Online Optimization Setting online convex optimization setting round game played learner adversary (nature) round player chooses adversary picks  player choice depend Pnon adversary choices previous rounds. goal player low average objective compared single choice hind sight]. classic algorithm setting Mirror Descent], starts arbitrary updates stepsize  discussed later) follows arg min  For Euclidean norm) kwk update) projected online gradient descent  ) arg minw0 projection mirror Descent algorithm orem   stepsize ?     average regret.   . ?  pnfor ?instance sequence  bounded: 4hb  ?   Note stepsize depends bound loss hindsight. orem proved Lemma orem].  Stochastic Optimization online algorithm serve eﬃcient one-pass learning algorithm stochastic setting. here. sample   unknown distribution Section), find low risk)]. , , agrees supervised learning risk discussed Introduction analyzed Section focusing erm, run  standard arguments] convert online Mirror Descent sample, regret bound orem bound excess risk: corollary   run Mirror Descent sample    ?  ?   expectation sample: 4hb   ?   instructive contrast guarantee similar guarantees derived recently stochastic convex optimization literature]. , model stochastic first-order optimization. learner unbiased estimate, gradient). variance estimate assumed bounded  expected accuracy gradient evaluations terms: ?accelerated? term slow(?/ term. result applicable generally (since doesn require non-negativity ‘), immediately clear guarantees derived. main diﬃculty depends norm gradient estimates. thus, bounded advance? small. said, intuitively clear end optimization process, gradient norms typically small? small bounding property (lemma). interesting note stability arguments, guarantee similar Corollary avoiding polylogarithmic factors orem dependence bound loss, obtained ?batch? learning rule similar erm, incorporating regularization. regularization parameter define  ) regularized empirical loss) Regularized Empirical Risk Minimizer  )  arg min orem bound excess risk similar Corollary 128h   orem  set 128n2h 128hl     expectation sample size 2048hb 256hb   ?   prove orem stability arguments similar shalev-shwartz], turn based Bousquet Elisseeff]. however, shalev-shwartz] notion uniform stability, stability expectation faster rates. tightness Section return learning rates ERM parametric scale-sensitive hyposis classes. terms dimensionality terms scale sensitive complexity measures), discussed Introduction analyzed Section compare guarantees learning rates situations, identify differences parametric scalesensitive cases smooth non-smooth cases, argue differences real showing guarantees tight. discuss tightness learning guarantees ERM stochastic setting, similar arguments made online learning. table summarizes bounds excess risk ERM implied orem previous bounds Lipschitz loss finite-dimensional] scalesensitive] classes, bound squared-loss finite-dimensional classes, orem] generalized smooth strongly convex loss. show Loss function-lipschitz-smooth Parametric dim) — ddl? dhl? scale-sensitive)  -smooth ?-strongly Convex hrl? hrl? table Bounds excess risk, polylogarithmic factors.  dependencies Table unavoidable. , class , kwk -bounded linear predictors (all norms inthis Section euclidean), loss functions, spe cific distributions, kxk ]. non-parametric lower-bounds, dimensionality grow sample size Infinite dimensional, Lipschitz (non-smooth), separable Consider absolute difference loss ) —, distribution: uniformly distributed standard basis vectors    } arbitrary sequence signs unknown learner. taking ?  ?  sample    reveals signs information remaining signs. means learning algorithm, exists choice remaining  points learner/she suffer loss yielding risk. infinite dimensional, smooth, non-separable, strongly convex Consider squared loss ) -smooth-strongly convex.   /? distribution: uniform before, time random  pre-determined, unknown learner, random signs. minimizer expected risk ?  ?   furrmore, ) ?   ]        )  ?   ). othif norm constraint tight. kwk erwise, coordinate separate estimation problem, samples, number appearances           sample. ] Finite dimensional, smooth, strongly convex, non-separable: Take probability probability  conditioned deterministically conditioned probability probability consider-smooth loss ) )  )  )   first, irrespective choice) suffer loss. probability  observe, optimal predictor  . however,   . hence)  ) probability sow .    .32l? . ,  probability \\x0cimplications Improved Margin Bounds ?margin bounds? provide bound expected zero-one loss classifiers based margin error training sample. koltchinskii Panchenko] margin bounds generic class based Rademacher complexity class. non-smooth Lipschitz ?ramp? loss upper bounds zero-one loss upper-bounded margin zero-one loss. however, analysis unavoidably leads rate separable case. following? idea smooth ?ramp?  ) +cos/?)  ¡? ??  loss function -smooth lower bounded zero-one loss upper bounded margin loss. orem provide improved margin bounds zero-one loss classifier based empirical margin error. denote err} zeroone risk sample   } define ?-margin empirical loss err  )¡?}  orem hyposis class—  probability simultaneously margins     log(log( log(log( log3 )/?)  )/?) err ) log ) err) err ) numeric constant orem particular, numeric constant err)  err ) log(log( log3 )/?) )  improved margin bounds form previously shown specifically linear prediction Hilbert space based PAC Bayes orem]. pac-bayes based results specific linear function class. orem contrast, generic concentration-based result applied function class.  Interaction Norm Dimension Consider problem learning low-norm linear predictor respect squared loss ,  finite large expected norm low. specifically, Gaussian kxk? , ? learning linear predictor regularization. determines sample complexity? error decrease sample size increases? scale-sensitive statistical learning perspective, expect sample complexity, decrease error, depend norm however, fixed asymptotically number samples increase, excess risk norm-constrained normregularized regression     depends order) dimensionality]. behaves) asymptotic dependence dimensionality understood Table non-separable situation, parametric complexity controls lead rate, ultimately dominating rate resulting scale-sensitive, non-parametric complexity control combining orem asymptotic behavior, noting worst case predict vector, yields picture expected excess risk ridge regression optimally chosen       min?/  Roughly speaking, term describes behavior regime sample size. regime excess risk order occurs  (?low-noise?) regime excess ?  third?(?slow?) risk dominated norm behaves,  ) regime, excess risk controlled norm approximation error behaves?/   ). fourth (?asymptotic?) regime excess risk behaves  . sheds furr light recent work Liang Srebro] based exact asymptotics.  Sparse Prediction norm popular learning sparse predictors high dimensions, lasso.   obtained squared loss ,  minimizing) LASSO estimator] subject kwk1  assume (unknown) sparse reference predictor low expected loss sparsity (number non-zeros) kw0 kxk?    order choose apply orem setting, bound kw0 ., assuming features] support mutually uncorrelated. assumption, have: kw0 k21   . thus, orem Rademacher complexity bounds, log log   ) relax-correlation assumption bound correlations, mutual incoherence, weaker conditions]. case, unlike typical analysis compressed sensing, goal recovering itself, concerned correlations inside support furrmore, require optimal predictor sparse model specified: exists low risk predictor small number fairly uncorrelated features. bounds similar) derived specialized arguments]?here demonstrate bounds forms obtained simple conditions, generic framework suggest. interesting note methods results Section applied setting. entropy regularizer] log non-negative-strongly convex respect kwk1 ] kwk1 ) log weights order  ] include non-negative feature negation). recalling entropy regularizer),    log log  regularized orem empirical minimizer) entropy regularizer) orem advantage orem orem avoids extra logarithmic factors. Consider empirical risk minimization hyposis class . non-negative loss function ). that, learn predictor small risk) ), minimizing empirical risk . sample    statistical guarantees excess risk understood parametric. finite dimensional) hyposis classes. more formally, hyposis classes finite-subgraph dimension] (aka pseudo-dimension). for classes learning guarantees obtained bounded loss function. . —?—  relevant measure complexity-subgraph dimension. alternatively, non-parametric hypothesis classes. infinite-subgraph dimension. class low-norm linear predictors —kwk guarantees obtained terms scale-sensitive measures complexity fatshattering dimensions], covering numbers] Rademacher complexity]. classical statistical learning ory approach obtaining learning guarantees scalesensitive classes rely Lipschitz constant . . bound derivative. ). excess risk bounded expectation sample):    2drn) arg min) empirical risk minimizer (erm), inf) approxima \\x0cfor -bounded linear) Rademacher tion error. . complexity, typically scales) predictors, kxk2 paper address deficiencies guarantee). first, bound applies loss functions bounded derivative, hinge loss logistic loss popular classification, absolute-value loss regression. directly applicable squared loss ,  derivative bounded, first. simply bound derivative squared loss terms bound magnitude),pbut. norm-bounded linear predictors results disappointing excess risk bound form (max kxk). one aim paper provide clean bounds excess risk smooth loss functions squared loss bounded second, rar first, derivative.   deficiency) dependence dependence unavoidable general. but finite dimensional (parametric) classes, improved rate distribution separable. exists )  particular, class bounded functions-subgraph-dimension. -dimensional linear predictors), expectation sample]:  log log ddl   term disappears separable case, graceful degradation rate rate separable case. could separable rate, graceful degradation, non-parametric case? show, deficiencies related. For non-parametric classes, non-smooth Lipschitz loss, hinge-loss, excess risk scale, separable case. however-smooth non-negative loss functions, derivative . bounded separable rate possible. Section obtain bound excess risk logarithmic factors):     hr2n? )  hrl?    ?  ) particular,pfor -norm-bounded linear predictors kxk2 excess risk bounded  ). anor interesting distinction parametric non-parametric classes, squared-loss, bound) tight non-separable rate unavoidable. this contrast parametric (fine dimensional) case, rate squared loss, approximation error ]. differences parametric scale-sensitive \\x0cclasses, non-smooth, smooth strongly convex loss functions discussed Section summarized Table guarantees discussed general learning guarantees stochastic setting rely Rademacher complexity hyposis class, phrased terms minimizing scalar loss function. Section online setting, addition stochastic setting, present similar guarantees online stochastic convex optimization]. guarantees Section match equation) special case convex loss function norm-bounded linear predictors, Section capture general setting optimizing arbitrary non-negative convex objective, require smooth separate discussion ?predictor? scalar loss function Section). results Section expressed terms properties norm, rar measure concentration Radamacher complexity) Section however, online stochastic convex optimization setting Section restrictive, require objective convex Section make assumption convexity hyposis class loss function ?). specifically, non-negative-smooth convex objective, domain bounded prove average ). comparing online regret (and excess risk stochastic optimization) bounded bound) loss DLipschitz rar-smooth], relationship discussed erm. unlike bound) erm, convex optimization bound avoids polylogarithmic factors. results Section generalize smoothness boundedness respect non-euclidean norms. studying online stochastic convex optimization setting (section), addition ERM (section), advantages. first, obtain learning guarantee eﬃcient single-pass learning methods, stochastic gradient descent mirror descent), non-stochastic regret. second, bound obtain convex optimization setting (section bound ERM (section avoids polylogarithmic large constant factors. third, bound applicable non-negative online stochastic optimization problems classification, including problems ERM applicable (see]). detailed proofs statements claimed paper found supplementary material paper. Empirical Risk Minimization Smooth Loss Recall Rademacher complexity  ) ?unif) Throughout ?worst case? rademacher complexity. our starting point learning bound) applies-lipschitz loss \\x0cfunctions. )—  derivatives. argument). what type bound obtain bound derivative)? avoid talking derivative explicitly, function-smooth iff derivative-lipschitz. for differentiable means central observation, obtain guarantees smooth loss functions, smooth loss, derivative bounded terms function value: Lemma. for-smooth non-negative function have)— 4hf) This Lemma argue close optimum value, loss small, derivative. looking dependence) derivative bound guided heuristic intuition: Since concerned behavior erm, bound  bound , what applying Lemma), )]— 4hl).  , ERM )— separately, absolute inside expectation?this bound, non-negativity loss plays important role. ignoring important issue moment    ). solving) yields desired bound plugging) yields). this rough intuition captured orem: orem for-smooth non-negative loss .  )—  probability  random sample size    log/?) log/?)   ) log) ) log:      log) log/?)  log R2n) log/?)  105 numeric constant derived]. note ?confidence? terms depended —?—, typically dominant term obtain bound holds expectation sample (rar high probability) avoids direct dependence —?—. prove orem notion Local Rademacher Complexity], focus behavior close erm. end, empirically restricted loss class  ,  ), ) Lemma, presented below,? solidifies heuristic intuition discussed above, showing \\x0crademacher complexity ) scales. Lemma higher-order version Lipschitz Composition Lemma], states Rademacher complexity unrestricted loss class bounded DRn). here, second, rar first, derivative, obtain bound depends empirical restriction: Lemma. for non-negative Hsmooth loss bounded function class bounded   12hb ? )) 12hr) log3 log3) Applying Lemma, orem standard Local Rademacher argument].  Related Results rates faster previously explored conditions, including small. Finite Dimensional Case Lee] showed faster rates squared loss, exploiting strong convexity loss function, finite-subgraph-dimension. panchenko] fast rate results general Lipschitz bounded loss functions, finite-subgraphdimension case. bousquet] provided similar guarantees linear predictors Hilbert spaces spectrum kernel matrix (covariance exponentially decaying, making situation finite dimensional. all methods rely finiteness effective dimension provide fast rates. case, smoothness necessary. our method, hand, establishes fast rates, function classes finite-subgraph-dimension. show non-parametric case, smoothness plays important role (see Table). aggregation Tsybakov] studied learning rates aggregation, predictor chosen convex hull finite set base predictors. this equivalent constraint base predictor viewed ?feature?. -based analysis, bounds depend logarithmically number base predictors. dimensionality), rely scale change loss function, ?scale sensitive? nature. for aggregate classifier, Tsybakov obtained rate small) risk achieve base classifiers. using tsybakov result, risk achieved aggregate. bounded ell1 classifier order obtain faster rate. tsybakov core result sense similar finite dimensional results, rate error achieved finite cardinality (and finite dimension) class. tsybakov approximation error small class base predictors. large hyposis class. covering) obtain learning rates large hyposis class aggregation small class. however results imply?fast learning rates hyposis classes low complexity. specifically learning rates results, covering number hyposis class scale behave but typical classes, including class linear predictors bounded norm, covering numbers scale methods imply fast rates function classes. fact, rates techniques, requires covering numbers increase all, finite-subgraph-dimension. chesneau] \\x0cextend tsybakov work general losses, deriving similar results Lipschitz loss function.  caveats hold: rates faster require covering numbers grow slower rates essentially require finite-subgraph-dimension. our work, hand, applicable Rademacher complexity (equivalently covering numbers) controlled. although similar techniques, rar work Tsybakov Chesneau, points importance smoothness obtaining fast rates non-parametric case: Chesneau relied Lipschitz constant, show, Section obtaining fast rates non-parametric case,  local Rademacher Complexities Bartlett] developed general machinery proving fast rates based local Rademacher complexities. however, important note localized complexity term typically dominates rate controlled. for example, Steinwart] Local Rademacher Complexity provide fast rate loss Support Vector Machines (svms -regularized hinge-loss minimization) based called ?geometric margin condition? tsybakov margin condition. steinwart analysis specific svms. Local Rademacher Complexities order obtain fast rates, general hyposis classes, based standard Rademacher complexity) hyposis classes, smoothness loss function magnitude furr assumptions hyposis classes itself. non-lipschitz Loss Beyond strong connections smoothness fast rates highlight, aware prior work providing explicit easy-use result controlling generic non-lipschitz loss (such squared loss) solely terms Rademacher complexity. Online Stochastic Optimization Smooth Convex Objectives turn online stochastic convex optimization. settings learner chooses closed convex set normed vector space, attempting minimize objective, instances   objective function convex This captures learning linear predictors. convex loss function ), )) ), extends supervised learning. case objective-smooth. norm kwk reader choose subset Euclidean Hilbert space, kwk -norm):  , ?     dual norm. key generalize Lemma smoothness. vector rar scalar smoothness: Lemma. for-smooth non-negative  ?  4hf) order general norms, rely non-negative regularizer -strongly convex (see Definition. . norm kwk For Euclidean norm squared Euclidean norm regularizer) kwk  Online Optimization Setting online convex optimization setting round game played learner adversary (nature) round player chooses adversary picks  player choice depend Pnon adversary choices previous rounds. goal player low average objective compared single choice hind sight]. classic algorithm setting Mirror Descent], starts arbitrary updates stepsize  discussed later) follows arg min  For Euclidean norm) kwk update) projected online gradient descent  ) arg minw0 projection Mirror Descent algorithm orem for  stepsize ?     average regret.   . ?  Pnfor ?instance sequence  bounded: 4hb  ?   Note stepsize depends bound loss hindsight. orem proved Lemma orem].  Stochastic Optimization online algorithm serve eﬃcient one-pass learning algorithm stochastic setting. here. sample   unknown distribution Section), find low risk)]. when, , agrees supervised learning risk discussed Introduction analyzed Section but focusing erm, run  standard arguments] convert online Mirror Descent sample, regret bound orem bound excess risk: Corollary for  run Mirror Descent sample    ?  ?   expectation sample: 4hb   ?   instructive contrast guarantee similar guarantees derived recently stochastic convex optimization literature]. , model stochastic first-order optimization. learner unbiased estimate, gradient). variance estimate assumed bounded  expected accuracy gradient evaluations terms: ?accelerated? term slow(?/ term. while result applicable generally (since doesn require non-negativity ‘), immediately clear guarantees derived. main diﬃculty depends norm gradient estimates. thus, bounded advance? small. that \\x0csaid, intuitively clear end optimization process, gradient norms typically small? small bounding property (lemma). interesting note stability arguments, guarantee similar Corollary avoiding polylogarithmic factors orem dependence bound loss, obtained ?batch? learning rule similar erm, incorporating regularization. for regularization parameter define  ) regularized empirical loss) Regularized Empirical Risk Minimizer  )  arg min orem bound excess risk similar Corollary 128h   orem for set 128n2h 128hl     expectation sample size 2048hb 256hb   ?   prove orem stability arguments similar shalev-shwartz], turn based Bousquet Elisseeff]. however, shalev-shwartz] notion uniform stability, stability expectation faster rates. Tightness Section return learning rates ERM parametric scale-sensitive hyposis classes. terms dimensionality terms scale sensitive complexity measures), discussed Introduction analyzed Section compare guarantees learning rates situations, identify differences parametric scalesensitive cases smooth non-smooth cases, argue differences real showing guarantees tight. although discuss tightness learning guarantees ERM stochastic setting, similar arguments made online learning. table summarizes bounds excess risk ERM implied orem previous bounds Lipschitz loss finite-dimensional] scalesensitive] classes, bound squared-loss finite-dimensional classes, orem] generalized smooth strongly convex loss. show Loss function-lipschitz-smooth Parametric dim) — ddl? dhl? scale-sensitive)  -smooth ?-strongly Convex hrl? hrl? Table Bounds excess risk, polylogarithmic factors.  dependencies Table unavoidable. , class , kwk -bounded linear predictors (all norms inthis Section euclidean), loss functions, spe cific distributions, kxk ]. for non-parametric lower-bounds, dimensionality grow sample size Infinite dimensional, Lipschitz (non-smooth), separable Consider absolute difference loss ) —, distribution: uniformly distributed standard basis vectors    } arbitrary sequence signs unknown learner. taking ?  ?  however sample    reveals signs information remaining signs. this means learning algorithm, exists choice remaining  points learner/she suffer loss yielding risk. Infinite dimensional, smooth, non-separable, strongly convex Consider squared loss ) -smooth-strongly convex. for  /? distribution: uniform before, time random  pre-determined, unknown learner, random signs. minimizer expected risk ?  ?   furrmore, ) ?   ]        )  ?   ). othif norm constraint tight. kwk erwise, coordinate separate estimation problem, samples, number appearances           sample. ] Finite dimensional, smooth, strongly convex, non-separable: Take probability probability  conditioned deterministically conditioned probability probability Consider-smooth loss ) )  )  )   first, irrespective choice) suffer loss. this probability  next observe, optimal predictor  . however,   . hence)  ) probability sow .    .32l? . however,  probability \\x0cimplications Improved Margin Bounds ?margin bounds? provide bound expected zero-one loss classifiers based margin error training sample. koltchinskii Panchenko] margin bounds generic class based Rademacher complexity class. this non-smooth Lipschitz ?ramp? loss upper bounds zero-one loss upper-bounded margin zero-one loss. however, analysis unavoidably leads rate separable case. following? idea smooth ?ramp?  ) +cos/?)  ¡? ??  this loss function -smooth lower bounded zero-one loss upper bounded margin loss. using orem provide improved margin bounds zero-one loss classifier based empirical margin error. denote err} zeroone risk sample   } define ?-margin empirical loss err  )¡?}  orem for hyposis class—  probability simultaneously margins     log(log( log(log( log3 )/?)  )/?) err ) log ) err) err ) numeric constant orem particular, numeric constant err)  err ) log(log( log3 )/?) )  improved margin bounds form previously shown specifically linear prediction Hilbert space based PAC Bayes orem]. however pac-bayes based results specific linear function class. orem contrast, generic concentration-based result applied function class.  Interaction Norm Dimension Consider problem learning low-norm linear predictor respect squared loss ,  finite large expected norm low. specifically, Gaussian kxk? , ? learning linear predictor regularization. what determines sample complexity? how error decrease sample size increases? from scale-sensitive statistical learning perspective, expect sample complexity, decrease error, depend norm however, fixed asymptotically number samples increase, excess risk norm-constrained normregularized regression     depends order) dimensionality]. behaves) asymptotic dependence dimensionality understood Table non-separable situation, parametric complexity controls lead rate, ultimately dominating rate resulting scale-sensitive, non-parametric complexity control combining orem asymptotic behavior, noting worst case predict vector, yields picture expected excess risk ridge regression optimally chosen       min?/  Roughly speaking, term describes behavior regime sample size. regime excess risk order occurs  (?low-noise?) regime excess ?  third?(?slow?) risk dominated norm behaves,  ) regime, excess risk controlled norm approximation error behaves?/   ). fourth (?asymptotic?) regime excess risk behaves  . this sheds furr light recent work Liang Srebro] based exact asymptotics.  Sparse Prediction norm popular learning sparse predictors high dimensions, lasso.   obtained squared loss ,  minimizing) LASSO estimator] subject kwk1  let assume (unknown) sparse reference predictor low expected loss sparsity (number non-zeros) kw0 kxk?    order choose apply orem setting, bound kw0 this., assuming features] support mutually uncorrelated. under assumption, have: kw0 k21   . thus, orem Rademacher complexity bounds, log log   ) relax-correlation assumption bound correlations, mutual incoherence, weaker conditions]. but case, unlike typical analysis compressed sensing, goal recovering itself, concerned correlations inside support furrmore, require optimal predictor sparse model specified: exists low risk predictor small number fairly uncorrelated features. bounds similar) derived specialized arguments]?here demonstrate bounds forms obtained simple conditions, generic framework suggest. interesting note methods results Section applied setting. entropy regularizer] log non-negative-strongly convex respect kwk1 ] kwk1 ) log weights order  ] include non-negative feature negation). recalling entropy regularizer),    log log  regularized orem empirical minimizer) entropy regularizer) orem advantage orem orem avoids extra logarithmic factors.',\n",
       " 'PP3910': 'encounter complex dynamic scenes everyday life. illustrated motion sequence depicted Figure humans readily perceive baseball player body movements fastermoving baseball simultaneously. however, computational perspective, trivial problem solve. diﬃculty due large speed difference objects, displacement player body smaller displacement baseball frames. separate motion systems proposed explain human perception scenarios example. particular, Braddick] proposed short-range motion system responsible perceiving movements small displacements., player movement), long-range motion system perceives motion large displacements., ﬂying baseball), called \\x0capparent motion. sperling] furr argued existence motion systems human vision. secondorder systems conduct motion analysis luminance texture information respectively, third-order system feature-tracking strategy. baseball example, first-order motion system perceive player movements, third-order system required perceiving faster motion baseball. short-range motion first-order motion apply class phenomena, modeled computational ories based motion energy related techniques. however, long-range motion third-order Figure Left panel: short-range long-range motion: frames baseball sequence ball moves faster speed objects. panel: graphical illustration hierarchical model dimension. node represents motion location scales. child node multiple parents, prior constraints motion expressed parent-child interactions. motion employ qualitatively computational strategies involving tracking features time, require attention-driven processes. contrast previous multi-system ories], develop unified single-system framework account phenomena human motion perception. model motion estimation inference problem ﬂexible prior assumptions motion ﬂows statistical models quantifying uncertainty motion measurement. model differs traditional approaches aspects. first, prior model defined hierarchical graph, Figure nodes graph represent motion scales. hierarchical structure motivated human visual system organized hierarchically]. representation makes define motion priors contextual effects range scales, differs models motion perception based motion priors]. model connects lower level nodes multiple coarser-level nodes, resulting loopy graph structure, imposes ﬂexible prior tree-structured models. ]). define probability distribution graph potentials defined graph cliques capture spatial smoothness constraints] scales slowness constraints]. second, data likelihood terms large space motions, include short-range long-range motion. locally, motion highly ambiguous., likelihood term motions) resolved model imposing hierarchical motion prior. note coarsen image rely coarse-fine processing]. bottom compositional/hierarchical approach local hyposes motion combined form hyposes larger regions image. enables deal simultaneously long-range short-range motion. tested model types stimuli commonly human vision research. stimulus type random dot kinematograms (rdks), dots signal) move coherently large displacements, dots noise) move randomly. rdks important \\x0cstimuli physiological psychophysical studies motion perception. example, electrophysiological studies RDKs analyze neuronal basis motion perception, identifying functional link activity motion-selective neurons behavioral judgments motion perception]. psychophysical studies RDKs measure sensitivity human visual system perceiving coherent motion, infer motion information integrated perceive global motion viewing conditions]. two-frame RDKs long-range motion stimulus. stimulus type moving gratings plaids. stimuli study perceptual phenomena. example, randomly orientated lines grating elements drift apertures, perceived direction motion heavily biased orientation lines/gratings, shape contrast apertures]. multiple-aperture stimuli recently study coherent motion perception short-range motion stimulus]. types stimuli compared model predictions human performance experimental conditions. hierarchical Model Motion Estimation Our hierarchical model represents motion field graph), hierarchical levels.,  ...   ...   level set nodes  ),      forming lattice indexed). specifically, start pixel lattice construct hierarchy follows. nodes )} 0th level correspond pixel position)} image lattice. recursively add higher levels nodes ). level lattice decreases factor coordinate direction level  edges graph connect nodes level hierarchy nodes neighboring levels. specifically, edges connect node , level set child nodes Chl,  level satisfying      here parameter controlling neighboring nodes level share child nodes. figure illustrates graph structure hierarchical model case note graph closed loops due sharing child nodes. apply model motion estimation, define state variable, node represent motion, connect 0th level nodes consecutive image frames)). problem motion estimation estimate motion field) time pixel site input For simplicity, uli denote motion, sections.  Model formulation define probability distribution motion field {uli graph conditioned input image pair ) exp  data term motion based local image cues Eul hierarchical priors motion impose slow smoothness constraints levels. energy terms {eul defined norms encourage robustness]. robust norm helps deal \\x0cmeasurement noise occur motion boundary prevent oversmoothing higher levels. details energy function terms follows: Data Term data energy term defined bottom level hierarchy. terms norm local image intensity values adjacent frames. precisely  u0i ?——u0i, term defines difference measure measurements centered centered u0i respectively. choose pixel values here. term imposes slowness prior motion weighted coeﬃcient note term matching term computes similarity displacement similarity scores confidence local motion hyposes: higher similarity means motion lower means likely. hierarchical Prior {eul define hierarchical prior slowness spatial smoothness motion fields. term prior expressed energy terms nodes levels hierarchy enforces smoothness preference states motion child node similar motion parent. robust norm energy terms violation consistency constraint penalized moderately. imposes weak smoothness motion field abrupt change motion boundaries. term norm motion velocities encourages slowness. figure illustration inference procedure. left top panel: original hierarchical graph loops. left bottom panel: bottom process proceeds tree graph multiple copies nodes (connected solid lines) relaxes problem. top-down process enforces consistency constraints copies node (denoted dash line connection). panel: inference procedure street scene frames. show estimates (bottom (top-down). motions color-coded minimizing displayed arrows. specific, energy function defined:   eul  ulj ?chl) ) weight parameter energy terms lth level controls relative weight slowness prior. note hierarchical smoothness prior differs conventional smoothness constraints], impose smoothness ?sideways? neighboring pixels resolution level, requires motion similar neighboring sites pixel level only. imposing longer range interactions sideways problematic leads Markov Random Field (mrf) models large number edges. structure makes diﬃcult inference standard techniques belief propagation max-ﬂow/min-cut. contrast, impose smoothness requiring child nodes similar motions parent nodes. ?hierarchical? formulation enables impose smoothness interactions hierarchy levels inference eﬃciently exploiting hierarchy.  Motion Estimation arg maxu), estimate motion field computing probable motion) defined Gibbs distribution equation). performing inference model challenging energy defined hierarchical graph structure closed loops, state variables continuous-valued, energy function non-convex. strategy convert discrete optimization problem quantizing motion state space. example, estimate motion integer-valued resolution accuracy suﬃcient experimental settings. discrete state space, algorithm involves bottomup top-down processing sketched Figure algorithm designed parallelizable require computations neighboring nodes. desirable biological plausibility practical advantage implement algorithm GPU type architectures enables fast convergence. describe inference algorithm detail follows. bottom pass. approximate hierarchial graph tree-structured model making multiple copies child nodes child node single parent (see]). enables perform exact inference relaxed model dynamic programming.  recursively exploiting tree specifically, compute approximate energy function structure:  min[eul?chl) ulj bottom level data energy). top level compute  states minimize) top-down pass. top-level motion compute optimal motion configuration levels top-down procedure. top-down pass enforces consistency constraints, relaxed earlier recursively-computed energy function copies node optimal state. minimize energy function recursively node:  arg min[ Eul  ulj) set parents level node top-down pass, spatial smoothness imposed motion estimates higher levels provide context information disambiguate motion estimated lower levels. intuition two-pass inference algorithm motion estimates lower level nodes typically ambiguous motion estimates higher level nodes higher levels integrate information larger number nodes lower levels (although information lost due coarse representation motion field). estimates higher-level nodes noisy give ?context? resolve ambiguities lower level nodes. anor perspective, thought messagepassing type algorithm specific scheduling scheme]. experiments random dot kinematograms stimuli simulation procedures Random dot kinematogram (rdk) stimuli consist image frames dots frame]. shown figure), dots frame located random positions. proportion dots signal dots) moved coherently frame translational motion. remaining  dots noise dots) moved random positions frame. displacement signal dots large frames. result, two-frame RDK stimuli typically considered long-range motion. diﬃculty perceiving coherent motion RDK stimuli due large correspondence uncertainty introduced noise dots shown rightmost panel figure). figure left panels show coherent stimuli respectively. closed open circles denote dots frame respectively. arrows show motion dots moving coherently. correspondence noise illustrated rightmost panel showing dot frame candidate matches frame. barlow Tripathy] RDK stimuli investigate dot density affect human performance global motion discrimination task. found human performance (measured coherence threshold) vary dot density. tested model task judge Figure Estimated motion fields random dot kinematograms. row: dots RDK stimulus; Second row: 100 dots RDK stimulus; column-wise, coherence ratio, respectively. arrows motion estimated dot. global motion direction RDK motion stimulus input image. applied model estimate motion fields average velocity global motion direction left right). ran 500 trials coherence ratio condition. dot number varies, 100, 200, 400, 800 respectively, wide range dot densities. model performance computed coherence ratio fit psychometric functions find coherence threshold model performance reach% accuracy.  Results Figure) shows examples estimated motion field values dot number coherence ratio model outputs provide visually coherent motion estimates coherence ratio greater, consistent human perception. increase coherence ratio, estimated motion ﬂow appears coherent. furr compare human performance], examined wher model performance affected dot density RDK display. plot figure) shows model performance function coherence ratio. coherence threshold, criterion% accuracy, showed model performance varied increase dot density, consistent \\x0cwith human performance reported psychophysical experiments]. experiments multi-aperture stimuli types stimulus multiple-aperture stimulus consisted dense set spatially isolated elements. types elements simulations) drifting sine-wave gratings random orientation) plaids includes gratings orthogonal orientations. element displayed stationary Gaussian window. figure) shows examples types stimuli. grating elements form    denotes center element, (.) represents grating, sin sin cos )), fixed spatial frequency orientation grating. grating stimulus), number elements (which constant). signal gratings, motion set fixed   noise gratings, set direction sampled uniform distribution. grating orientation angles sampled uniform distribution also. coherence Ratio Threshold.035.025.015.005 100 200 400 800 Figure Left panel: Figure] showing coherence ratio threshold varies dot density. panel: Simulations model show similar trend. , 100, 200, 400 800. figure multi-aperture gratings plaids. left column: sample stimuli. column: stimuli local drifting velocity element arrows. stimulus details shown magnified windows upper corner image. plaid elements combine gratings orthogonal orientations (each grating speed motion direction). leads plaid element      , sin sin cos, sin cos sin plaid stimulus). signal plaids, motions set fixed   noise plaids, directions randomly assigned, magnitude fixed.  Accuracy gratings plaids Coherence Ratio \\x0cfigure Left panels: Estimated motion fields grating plaids stimuli. rightmost panel: Psychometric functions gratings plaids stimuli.  Simulation procedures results left panels Figure) show estimated motion fields types stimulus studied coherence ratios. plaids stimuli produce coherent estimated motion field grating stimuli, understandable. ambiguous local motion cues. tested model-direction discrimination task estimating global motion direction]. model raw images frames input. ran 300 trials stimulus type, direction average motion predict global motion direction. prediction accuracy . number times model predicted correct motion direction alternatives calculated coherence ratio levels. difference gratings plaids shown rightmost panel Figure), psychometric function plaids stimuli grating stimuli, indicating performance. simulation results model consistent psychophysics experiments]. discussion paper, proposed unified single-system framework capable dealing short-range long-range motion. differs traditional motion energy models spatiotemporal filtering. note shown] motion energy models suited long-range motion stimuli studied paper. local ambiguities motion resolved hierarchical prior combines slowness smoothness range scales. model accounts human perception short-range long-range motion standard stimulus types (rdks gratings). hierarchical structure model partly motivated properties cortical organization. computational motivation represent prior knowledge motion scales eﬃcient computation. acknowledgments This research supported NSF grants iis-0917141, 613563 bcs-0843880. alan Lee George Papandreou helpful discussions. encounter complex dynamic scenes everyday life. illustrated motion sequence depicted Figure humans readily perceive baseball player body movements fastermoving baseball simultaneously. however, computational perspective, trivial problem solve. diﬃculty due large speed difference objects, displacement player body smaller displacement baseball frames. separate motion systems proposed explain human perception scenarios example. particular, Braddick] proposed short-range motion system responsible perceiving movements small displacements., player movement), long-range motion system perceives motion large displacements., ﬂying baseball), called \\x0capparent motion. Sperling] furr argued existence motion systems human vision. secondorder systems conduct motion analysis luminance texture information respectively, third-order system feature-tracking strategy. baseball example, first-order motion system perceive player movements, third-order system required perceiving faster motion baseball. short-range motion first-order motion apply class phenomena, modeled computational ories based motion energy related techniques. however, long-range motion third-order Figure Left panel: short-range long-range motion: frames baseball sequence ball moves faster speed objects. right panel: graphical illustration hierarchical model dimension. each node represents motion location scales. child node multiple parents, prior constraints motion expressed parent-child interactions. motion employ qualitatively computational strategies involving tracking features time, require attention-driven processes. contrast previous multi-system ories], develop unified single-system framework account phenomena human motion perception. model motion estimation inference problem ﬂexible prior assumptions motion ﬂows statistical models quantifying uncertainty motion measurement. our model differs traditional approaches aspects. first, prior model defined hierarchical graph, Figure nodes graph represent motion scales. this hierarchical structure motivated human visual system organized hierarchically]. such representation makes define motion priors contextual effects range scales, differs models motion perception based motion priors]. this model connects lower level nodes multiple coarser-level nodes, resulting loopy graph structure, imposes ﬂexible prior tree-structured models. ]). define probability distribution graph potentials defined graph cliques capture spatial smoothness constraints] scales slowness constraints]. second, data likelihood terms large space motions, include short-range long-range motion. locally, motion highly ambiguous., likelihood term motions) resolved model imposing hierarchical motion prior. note coarsen image rely coarse-fine processing]. instead bottom compositional/hierarchical approach local hyposes motion combined form hyposes larger regions image. this enables deal simultaneously long-range short-range motion. tested model types stimuli commonly human vision research. stimulus type random dot kinematograms (rdks), dots signal) move coherently large displacements, dots noise) move randomly. rdks important \\x0cstimuli physiological psychophysical studies motion perception. for example, electrophysiological studies RDKs analyze neuronal basis motion perception, identifying functional link activity motion-selective neurons behavioral judgments motion perception]. psychophysical studies RDKs measure sensitivity human visual system perceiving coherent motion, infer motion information integrated perceive global motion viewing conditions]. two-frame RDKs long-range motion stimulus. stimulus type moving gratings plaids. stimuli study perceptual phenomena. for example, randomly orientated lines grating elements drift apertures, perceived direction motion heavily biased orientation lines/gratings, shape contrast apertures]. multiple-aperture stimuli recently study coherent motion perception short-range motion stimulus]. for types stimuli compared model predictions human performance experimental conditions. Hierarchical Model Motion Estimation Our hierarchical model represents motion field graph), hierarchical levels.,  ...   ...   level set nodes  ),      forming lattice indexed). more specifically, start pixel lattice construct hierarchy follows. nodes )} 0th level correspond pixel position)} image lattice. recursively add higher levels nodes ). level lattice decreases factor coordinate direction level  edges graph connect nodes level hierarchy nodes neighboring levels. specifically, edges connect node , level set child nodes Chl,  level satisfying      Here parameter controlling neighboring nodes level share child nodes. figure illustrates graph structure hierarchical model case note graph closed loops due sharing child nodes. apply model motion estimation, define state variable, node represent motion, connect 0th level nodes consecutive image frames)). problem motion estimation estimate motion field) time pixel site input For simplicity, uli denote motion, sections.  Model formulation define probability distribution motion field {uli graph conditioned input image pair ) exp  data term motion based local image cues Eul hierarchical priors motion impose slow smoothness constraints levels. energy terms {eul defined norms encourage robustness]. this robust norm helps deal \\x0cmeasurement noise occur motion boundary prevent oversmoothing higher levels. details energy function terms follows: Data Term data energy term defined bottom level hierarchy. terms norm local image intensity values adjacent frames. more precisely  u0i ?——u0i, term defines difference measure measurements centered centered u0i respectively. choose pixel values here. term imposes slowness prior motion weighted coeﬃcient note term matching term computes similarity displacement similarity scores confidence local motion hyposes: higher similarity means motion lower means likely. Hierarchical Prior {eul define hierarchical prior slowness spatial smoothness motion fields. term prior expressed energy terms nodes levels hierarchy enforces smoothness preference states motion child node similar motion parent. robust norm energy terms violation consistency constraint penalized moderately. this imposes weak smoothness motion field abrupt change motion boundaries. term norm motion velocities encourages slowness. Figure illustration inference procedure. left top panel: original hierarchical graph loops. left bottom panel: bottom process proceeds tree graph multiple copies nodes (connected solid lines) relaxes problem. top-down process enforces consistency constraints copies node (denoted dash line connection). right panel: inference procedure street scene frames. show estimates (bottom (top-down). motions color-coded minimizing displayed arrows. specific, energy function defined:   Eul  ulj ?chl) ) weight parameter energy terms lth level controls relative weight slowness prior. note hierarchical smoothness prior differs conventional smoothness constraints], impose smoothness ?sideways? neighboring pixels resolution level, requires motion similar neighboring sites pixel level only. imposing longer range interactions sideways problematic leads Markov Random Field (mrf) models large number edges. this structure makes diﬃcult inference standard techniques belief propagation max-ﬂow/min-cut. contrast, impose smoothness requiring child nodes similar motions parent nodes. this ?hierarchical? formulation enables impose smoothness interactions hierarchy levels inference eﬃciently exploiting hierarchy.  Motion Estimation arg maxu), estimate motion field computing probable motion) defined Gibbs distribution equation). performing inference model challenging energy defined hierarchical graph structure closed loops, state variables continuous-valued, energy function non-convex. our strategy convert discrete optimization problem quantizing motion state space. for example, estimate motion integer-valued resolution accuracy suﬃcient experimental settings. given discrete state space, algorithm involves bottomup top-down processing sketched Figure algorithm designed parallelizable require computations neighboring nodes. this desirable biological plausibility practical advantage implement algorithm GPU type architectures enables fast convergence. describe inference algorithm detail follows. bottom pass. approximate hierarchial graph tree-structured model making multiple copies child nodes child node single parent (see]). this enables perform exact inference relaxed model dynamic programming. more recursively exploiting tree specifically, compute approximate energy function structure:  min[eul?chl) ulj bottom level data energy). top level compute  states minimize) top-down pass. given top-level motion compute optimal motion configuration levels top-down procedure. top-down pass enforces consistency constraints, relaxed earlier recursively-computed energy function copies node optimal state. minimize energy function recursively node:  arg min[ Eul  ulj) set parents level node top-down pass, spatial smoothness imposed motion estimates higher levels provide context information disambiguate motion estimated lower levels. intuition two-pass inference algorithm motion estimates lower level nodes typically ambiguous motion estimates higher level nodes higher levels integrate information larger number nodes lower levels (although information lost due coarse representation motion field). hence \\x0cestimates higher-level nodes noisy give ?context? resolve ambiguities lower level nodes. from anor perspective, thought messagepassing type algorithm specific scheduling scheme]. Experiments random dot kinematograms stimuli simulation procedures Random dot kinematogram (rdk) stimuli consist image frames dots frame]. shown figure), dots frame located random positions. proportion dots signal dots) moved coherently frame translational motion. remaining  dots noise dots) moved random positions frame. displacement signal dots large frames. result, two-frame RDK stimuli typically considered long-range motion. diﬃculty perceiving coherent motion RDK stimuli due large correspondence uncertainty introduced noise dots shown rightmost panel figure). figure left panels show coherent stimuli respectively. closed open circles denote dots frame respectively. arrows show motion dots moving coherently. correspondence noise illustrated rightmost panel showing dot frame candidate matches frame. barlow Tripathy] RDK stimuli investigate dot density affect human performance global motion discrimination task. found human performance (measured coherence threshold) vary dot density. tested model task judge Figure Estimated motion fields random dot kinematograms. first row: dots RDK stimulus; Second row: 100 dots RDK stimulus; column-wise, coherence ratio, respectively. arrows motion estimated dot. global motion direction RDK motion stimulus input image. applied model estimate motion fields average velocity global motion direction left right). ran 500 trials coherence ratio condition. dot number varies, 100, 200, 400, 800 respectively, wide range dot densities. model performance computed coherence ratio fit psychometric functions find coherence threshold model performance reach% accuracy.  Results Figure) shows examples estimated motion field values dot number coherence ratio model outputs provide visually coherent motion estimates coherence ratio greater, consistent human perception. with increase coherence ratio, estimated motion ﬂow appears coherent. furr compare human performance], examined wher model performance affected dot density RDK display. plot figure) shows model performance function coherence ratio. coherence threshold, criterion% accuracy, showed model performance varied increase dot density, consistent \\x0cwith human performance reported psychophysical experiments]. Experiments multi-aperture stimuli types stimulus multiple-aperture stimulus consisted dense set spatially isolated elements. two types elements simulations) drifting sine-wave gratings random orientation) plaids includes gratings orthogonal orientations. each element displayed stationary Gaussian window. figure) shows examples types stimuli. grating elements form    denotes center element, (.) represents grating, sin sin cos )), fixed spatial frequency orientation grating. grating stimulus), number elements (which constant). for signal gratings, motion set fixed for  noise gratings, set direction sampled uniform distribution. grating orientation angles sampled uniform distribution also. Coherence Ratio Threshold.035.025.015.005 100 200 400 800 Figure Left panel: Figure] showing coherence ratio threshold varies dot density. right panel: Simulations model show similar trend. , 100, 200, 400 800. figure multi-aperture gratings plaids. left column: sample stimuli. right column: stimuli local drifting velocity element arrows. stimulus details shown magnified windows upper corner image. plaid elements combine gratings orthogonal orientations (each grating speed motion direction). this leads plaid element      , sin sin cos, sin cos sin plaid stimulus). for signal plaids, motions set fixed for  noise plaids, directions randomly assigned, magnitude fixed.  Accuracy gratings plaids Coherence Ratio \\x0cfigure Left panels: Estimated motion fields grating plaids stimuli. rightmost panel: Psychometric functions gratings plaids stimuli.  Simulation procedures results left panels Figure) show estimated motion fields types stimulus studied coherence ratios. plaids stimuli produce coherent estimated motion field grating stimuli, understandable. ambiguous local motion cues. tested model-direction discrimination task estimating global motion direction]. model raw images frames input. ran 300 trials stimulus type, direction average motion predict global motion direction. prediction accuracy . number times model predicted correct motion direction alternatives calculated coherence ratio levels. this difference gratings plaids shown rightmost panel Figure), psychometric function plaids stimuli grating stimuli, indicating performance. simulation results model consistent psychophysics experiments]. Discussion paper, proposed unified single-system framework capable dealing short-range long-range motion. differs traditional motion energy models spatiotemporal filtering. note shown] motion energy models suited long-range motion stimuli studied paper. local ambiguities motion resolved hierarchical prior combines slowness smoothness range scales. our model accounts human perception short-range long-range motion standard stimulus types (rdks gratings). hierarchical structure model partly motivated properties cortical organization. computational motivation represent prior knowledge motion scales eﬃcient computation. acknowledgments This research supported NSF grants iis-0917141, 613563 bcs-0843880. Alan Lee George Papandreou helpful discussions.',\n",
       " 'PP3943': 'utility elicitation key component decision support applications recommender systems, decisions recommendations depend critically preferences user behalf decisions made. full elicitation user utility prohibitively expensive cases. time, cognitive effort, etc. rely partial utility information. interactive preference elicitation, selectively decide queries informative relative goal making good optimal recommendations. variety principled approaches proposed problem. number focus directly (myopically heuristically) reducing uncertainty utility parameters quickly possible, including max-margin], volumetric], polyhedral] entropy-based] methods. class approaches attempt reduce utility uncertainty sake, rar focuses discovering utility information \\x0cthat improves quality recommendation. include regret-based] Bayesian] models. focus Bayesian models work, assuming prior distribution user utility parameters conditioning distribution information acquired user., query responses behavioral observations). natural criterion choosing queries expected information (evoi), optimized myopically] sequentially]. however, optimization EVOI online query selection feasible simple cases. hence, practice, heuristics offer oretical guarantees respect query quality. paper problem myopic EVOI optimization choice queries. queries commonly conjoint analysis product design], requiring user choice/product preferred set options. show that, general assumptions, optimization choice queries reduces simpler problem choosing optimal recommendation set., set products that, user forced choose one, /2010/2010 University regina/2011 onwards Aalborg university. maximizes utility choice expectation). optimal recommendation set problem easier computationally, submodular, admitting greedy algorithm approximation guarantees. thus, determine approximately optimal choice queries. develop connection (noisy) user response models. finally, describe query iteration, local search technique that, formal guarantees, finds near-optimal recommendation sets queries faster eir exact greedy optimization. background: Bayesian Recommendation Elicitation assume system charged task recommending option user multiattribute space, instance, space product configurations domain., computers, cars, rental apartments, etc.). products characterized finite set attributes finite domain dom  dom denote set feasible configurations. instance, attributes correspond features cars, color, engine size, fuel economy, etc., defined eir constraints attribute combinations., constraints computer components put toger) explicit database feasible configurations., rental database). user utility function dom precise form critical, experiments assume; linear parameters weights., generalized additive independent (gai) models].) refer user ?utility function? simplicity, assuming fixed form simple additive model car domain(car (mpg (enginesize) (color optimal product user utility parameters maximizes). generally, user utility function certainty. recent models Bayesian elicitation, \\x0csystem uncertainty reﬂected distribution, beliefs; space utility functions].  denotes parameterization model, refer belief state.   define expected utility option; . required make recommendation belief optimal option (?) greatest expected utility (?) maxx;  (?) arg maxx; ?). settings, make set-based recommendations: rar recommending single option, small set options presented, user selects preferred option]. discuss problem constructing optimal recommendation set furr below. recommendation set  denote greatest utility items (for utility function). feasible utility space define    ; ), utility functions satisfying  ignoring ?ties? full-dimensional subsets (which easily dealt with, complicate presentation), regions   partition utility space. recommender system refine belief state learning user utility function reduction uncertainty lead recommendations expectation). sources information assess user preferences?including preferences related users, collaborative filtering], observed user choice behavior focus explicit utility elicitation, user asked questions preferences. variety query types refine one knowledge user utility function refer, furr discussion). comparison queries natural, user prefers option anor comparisons localized specific (subsets) attributes additive GAI models, structured models responses. specific options ?generalize,? providing constraints utility related options. work extension comparions choice sets options] common conjoint analysis]. set interpreted query: user states elements prefers. refer interchangeably query choice set. user response choice set tells preferences; depends user response model. noiseless model, user correctly identifies preferred item slate: choice refines set feasible utility functions imposing linear constraints form ), belief state obtained restricting non-zero density renormalizing. generally, noisy response model user select option maximize utility. choice set denote event user selecting response model dictates, choice set probability selection utility function When beliefs user utility uncertain, define . discuss response models below. treating query set opposed recommendation set interested expected utility, rar expected information (evoi), (expected) degree response increase quality system recommendation. define: Definition Given belief state expected posterior utility (epu query set EPU;    EVOI; epu;    expected improvement decision quality optimal query fixed size maximal, equivalently, maximal EPU settings, present set options user dual goals offering good set recommendations eliciting valuable information user utility. instance, product navigation interfaces-commerce sites display set options user select, give user chance critique proposed options]. motivation exploring connection optimal recommendation sets optimal query sets. moreover, settings queries recommendation separated, query optimization made eﬃcient exploiting relationship. optimal Recommendation Sets problem computing optimal recommendation sets system uncertainty user true utility function Given belief state single recommendation made, recommend option (?) maximizes expected utility, ?). however, suggesting ?shortlist? multiple options allowing user select preferred option. intuitively, set offer options diverse sense: recommended options highly preferred relative wide range ?likely? user utility functions (relative ]. stands contrast recommender systems define diversity relative product attributes], direct reference beliefs user utility. hard ?top systems, present options highest expected utility, generally result good recommendation sets]. broad terms, assume utility recommendation set utility preferred item. however, unrealistic assume users select preferred item complete accuracy]. choice queries, assume response model dictating probability choice Definition expected utility selection (eus) recommendation set : EUS;   expand definition rewrite EUS; : EUS; ;  User behavior largely dictated response model ideal setting, users select option highest utility. true utility function This noiseless model assumed] example. however, unrealistic general. noisy response models admit user ?mistakes? choice optimal sets reﬂect possibility (just belief update \\x0cdoes, defn. ). constraints response models include) preference bias: preferred outcome slate selected probability greater preferred outcome) luce choice axiom], form independence irrelevant alternatives requires relative probability selecting items affected addition deletion items set. response models: noiseless response model, RNL PNL; )] (with indicator function). eus [max. eus;  This identical expected max criterion]. RNL iff  constant noise model assumes multinomial distribution choices responses option preferred option relative selected (small) constant probability independent assume preferred option selected probability   )?  generalizes model, sets size. ) optimal element) utility, EUS;  EUS; ? )} logistic response model commonly choice modeling, variously luce-sheppard], bradley-terry], mixed multinomial logit model. selection probabilities exp)) exp)) temperature parameter.  For comparison queries), logistic function difference utility options. properties expected utility selection EUS models. models satisfy preference bias, RNL satisfy luce choice axiom. EUS monotone noiseless response model RNL addition options recommendation set decrease expected utility EUS; ?). moreover, option dominates relative belief state nonzero density. adding set-wise dominated option., dominated element change expected utility RNL EUS }; eus; ?). stands constrast noisy response models, adding dominated options decrease expected utility. importantly, EUS submodular noiseless constant response models orem For  EUS submodular function set That, recommendation sets option   },  }, have: EUS   eus;  eus?   eus; ) proof omitted, simply shows EUS required property diminishing returns. submodularity serves basis greedy optimization algorithm (see Section worst-case results query optimization below). EUS commonly logistic response model submodular, \\x0cbut related EUS noiseless model discuss next?allowing exploit submodularity noiseless model optimizing.  connection EUS EPU develop connection optimal recommendation sets (using eus) optimal choice queries (using epu/evoi). discussed above interested sets serve good recommendations good queries; epu/evoi computationally diﬃcult, good methods eus-optimization serve generate good queries tight relationship two. following, make transformation modifies set EUS increases (and case RNL decrease). transformation ways) prove optimality (near-optimality case eus-optimal recommendation sets query sets) directly computationally viable heuristic strategy generating query sets. definition Let   set options. define?  . ),    )} optimal option expectation) conditioned intuitively drop subscript clear context) refines recommendation set size producing updated beliefs user choice, replacing option optimal option update. note generally produces sets response models. indeed, construct set response model, measure EUS EPU resulting set response model. oretical results type ?crossevaluation.? show optimal recommendation sets RNL optimal., epu/evoi-maximizing) query sets. lemma EUS);  epu;  proof: For RNL argument relies partitioning. options EPU;        )    )  EUS);  )    )  Compare expression componentwise: components expression same. nonzero density  )    region )  EUS);  epu; component, result follows. proof argument, observation that: EUS;  ?)(?      ])). Lemma fact EUS;  , eus);  eus; ?). state main orem assume size fixed): orem Assume response model  optimal recommendation set.  optimal query set: EPU   epu;  proof: Suppose optimal query set. epu; epu ?). applying query set), results shows: EUS);  epu; epu   eus ?). contradicts eus-optimality  anor consequence Lemma posing query involving infeasible option pointless: set elements epu/evoi great. proved observing lemma holds redefined sets infeasible options. hard admitting noisy responses logistic response model decrease recommendation set., EUS;  eus; ?). however, loss EUS fact bounded. logistic response model that, probability incorrect selection option high, utility option close item, relative loss utility small. conversely, loss incorrect selection great, utility significantly option, rendering event extremely unlikely. bound difference EUS EUS ?max depends set cardinality temperature parameter  derive expression ?max below): orem EUS;  eus;  ?max transformation not, general, improve EUS) recommendation set However set) EUS assuming selection noiseless model, greater expected posterior utility EPU) Lemma EUS);  epu; fact prove optimal recommendation set near-optimal query consequences: first, thm. eus);  epu;  ?max second, EPU optimal query noiseless model great optimal query logistic model: EPU (?)  epu  derive main result logistic responses: EUS optimal recommendation set (and epu) ?max EPU optimal query set. orem EUS (?)  epu (?)  ?max proof: Consider optimal query? set ? obtained applying Lemma, EUS   epu?       epu (?). thm. eus  eus  ?max thm. eus (?) epu (?). EUS (?)  eus   eus   ?max epu (?)  ?max loss ; eus;  eus; eus set due logistic noise characterized function utility difference  options integrating values (weighted ?). specific eus-loss utility difference times probability choosing preferred option (? ) logistic function.   ;  — . derive problem-independent upper bound ;  maximizing)   maximal loss ?max (zmax set hypotical items attained utility difference    numerically zmax  imposing obtain yields zmax .279 ?max .2785  bound expressed scale independent temperature parameter intuitively, ?max corresponds utility difference slight user identifies item probability temperature words, maximum loss small user unable identify preferred item% time asked compare items This derivation generalized sets size yielding ?kmax  (?) lambert function. set Optimization Strategies discuss strategies optimization query/recommendation sets section, summarize oretical computational properties. follows, number options—, size query/recommendation set, ?cost? bayesian inference., number particles Monte Carlo sampling procedure). exact Methods naive maximization EPU computationally intensive eusoptimization, generally impractical. set elements, computing EPU, requires Bayesian update response, expected utility optimization posterior. query optimization requires computed query sets. EPU maximization). exact EUS optimization, demanding) require-maximization updated distributions. thm. compute optimal query sets eus-maximization RNL reducing complexity factor Under thm. eus-optimization approximate optimal query, quality guarantee EPU  ?max greedy Optimization simple greedy algorithm construct recommendation set size iteratively adding option offering greatest improvement value: arg maxx EUS }; ?). RNL EUS submodular (thm. ), greedy algorithm determines set EUS   optimal EPU; necessarily EPU; sets noisy response ?more informative? noiseless \\x0cone. however, case optimal query sets. lambert product-log, defined principal inverse  loss-maximizing set Smax infeasible outcomes; practice loss lower.    thm. greedy maximization EUS determine query set similar gaurantees. EUS longer submodular. however, Lemma thm. eus submodular, proxy. set determined greedy optimization EUS submodularity,  eus eus EUS EUS eus applying thm. gives: EUS eus  thus, derive EUS  eus    eus     ???   EUS EUS EUS EUS) similarly, derive worst-case bound EPU. greedy eus-optimization (using fact EUS lower bound epu, thm. thm. ): EUS  eus    eus   epu  ???     EPU EPU EPU EUS EUS) Greedy maximization. EUS extremely fast), linear number options requires) evaluations EUS cost Query Iteration transformation (defn. rise natural heuristic method computing, good query/recommendation sets. query iteration) starts initial set locally optimizes repeatedly applying operator) EUS); )=eus; ?). sensitive initial set lead fixed points. initialization strategies: random (randomly choose options), sampling (include  sample points; add optimal item forcing distinctness) greedy (initialize greedy set bound performance relative optimal query/recommendation sets assuming RNL initialized performance worse greedy optimization. initialized arbitrary set, note that, submodularity,  eus  keu  condition) implies EUS) EPU). note that, set EPU)   thus, EUS) eus  means comparison queries), achieves% optimal recommendation set value. bound tight corresponds singleton degenerate set?  .., (?)} ? (?)}. solution problematic EVOI zero. however, RNL sampling initialization avoids fixed point provably construction, leading query set positive evoi. complexity iteration., linear number options, greedy. however, practice faster Greedy typically While oretical results limit iterations required converge, practice, fixed point reached quickly (see below). evaluation compare strategies empirically choice problems random user utility functions \\x0cnoiseless noisy response models Bayesian inference realized Monte Carlo method importance sampling (particle weights determined applying response model observed responses). overcome problem particle degeneration (most particles eventually low weight), slice-sampling] regenerate particles. response-updated belief state effective number samples drops significantly (50000 particles simulations). figure) shows average loss strategies apartment rental dataset, 187 outcomes, characterized attributes (eir numeric categorical domain sizes), pairwise comparison queries noiseless responses. note greedy performs exact optimization, optimal item found roughly queries. query iteration performs initialized sampling, poorly random seeds. this% comparison queries worst ?). practical speedup achieved maintaining priority queue outcomes sorted potential eus-contribution (monotonically decreasing due submodularity). choosing item add set, evaluate outcomes top queue (lazy evaluation). utility priors mixtures Gaussians ]  component. exacteus greedy(eus(sampling(rand) random(greedy) greedy(eus) greedy(eus(sampling(rand) random normalized average loss normalized average loss number queries number queries) Average Loss (187 outcomes, runs, RNL) Average Loss (506 outcomes, runs, experiment, Boston Housing dataset 506 items binary continous attributes) logistic noise model responses  compare greedy strategies (exact methods impractical problems size) Figure); hybrid greedy(eus) strategy optimizes ?assuming? noiseless responses, evaluated true response model (sampling) efficient TNL version plotted. experiments show (greedy exact) maximization EUS find optimal near-optimal responses noisy?query sets. finally, compare query optimization times datasets table=187=187=187=506=506=506 exactepu exacteus greedy(epu(greedy(eus)) greedy(eus) greedy(eus(sampling(rand 1815s 405s 10000s.19s 142s.76s.07s.89s.09s.65s.97s.71s.99s.12s.02s.86s.55s.93s.12s.53s.11s.15s.16s.51s.05s.08s.09s.11s.17s.19s.64s.06s.10s.13s Among strategies, eﬃcient computationally, suited large outcome spaces. interestingly, faster sampling initialization random initialization fewer iteration average convergence. ). conclusions provided analysis set-based recommendations Bayesian recommender systems, shown offers tractable means generating myopically optimal nearoptimal choice queries preference elicitation. examined user response models, showing optimal recommendation sets evoi-optimal queries noiseless \\x0cconstant noise models; near-optimal logistic/lucesheppard model (both oretically practically). stress results general depend specific implementation Bayesian update, specific form utility function. greedy strategies? exploiting submodularity EUS computation?perform practice oretical approximation guarantees. finally experimental results demonstrate query iteration, simple local search strategy, well-situated large decision spaces. number important directions future research remain. furr oretical practical investigation local search strategies query iteration important. anor direction development strategies Bayesian recommendation elicitation large-scale configuration problems., outcomes csp, sequential decision problems (such MDPs uncertain rewards). finally, interested elicitation strategies combine probabilistic regret-based models. acknowledgements authors Iain Murray Cristina Manfredotti helpful discussion Monte Carlo methods, sampling techniques particle filters. research supported nserc. Utility elicitation key component decision support applications recommender systems, decisions recommendations depend critically preferences user behalf decisions made. since full elicitation user utility prohibitively expensive cases. time, cognitive effort, etc. rely partial utility information. thus interactive preference elicitation, selectively decide queries informative relative goal making good optimal recommendations. variety principled approaches proposed problem. number focus directly (myopically heuristically) reducing uncertainty utility parameters quickly possible, including max-margin], volumetric], polyhedral] entropy-based] methods. class approaches attempt reduce utility uncertainty sake, rar focuses discovering utility information \\x0cthat improves quality recommendation. include regret-based] Bayesian] models. focus Bayesian models work, assuming prior distribution user utility parameters conditioning distribution information acquired user., query responses behavioral observations). natural criterion choosing queries expected information (evoi), optimized myopically] sequentially]. however, optimization EVOI online query selection feasible simple cases. hence, practice, heuristics offer oretical guarantees respect query quality. paper problem myopic EVOI optimization choice queries. such queries commonly conjoint analysis product design], requiring user choice/product preferred set options. show that, general assumptions, optimization choice queries reduces simpler problem choosing optimal recommendation set., set products that, user forced choose one, from/2010/2010 University regina/2011 onwards Aalborg university. maximizes utility choice expectation). not optimal recommendation set problem easier computationally, submodular, admitting greedy algorithm approximation guarantees. thus, determine approximately optimal choice queries. develop connection (noisy) user response models. finally, describe query iteration, local search technique that, formal guarantees, finds near-optimal recommendation sets queries faster eir exact greedy optimization. background: Bayesian Recommendation Elicitation assume system charged task recommending option user multiattribute space, instance, space product configurations domain., computers, cars, rental apartments, etc.). products characterized finite set attributes finite domain dom let dom denote set feasible configurations. for instance, attributes correspond features cars, color, engine size, fuel economy, etc., defined eir constraints attribute combinations., constraints computer components put toger) explicit database feasible configurations., rental database). user utility function dom precise form critical, experiments assume; linear parameters weights., generalized additive independent (gai) models].) refer user ?utility function? simplicity, assuming fixed form simple additive model car domain(car (mpg (enginesize) (color optimal product user utility parameters maximizes). generally, user utility function certainty. following recent models Bayesian elicitation, \\x0csystem uncertainty reﬂected distribution, beliefs; space utility functions]. here denotes parameterization model, refer belief state. given  define expected utility option; . required make recommendation belief optimal option (?) greatest expected utility (?) maxx;  (?) arg maxx; ?). settings, make set-based recommendations: rar recommending single option, small set options presented, user selects preferred option]. discuss problem constructing optimal recommendation set furr below. given recommendation set  denote greatest utility items (for utility function). given feasible utility space define    ; ), utility functions satisfying  ignoring ?ties? full-dimensional subsets (which easily dealt with, complicate presentation), regions   partition utility space. recommender system refine belief state learning user utility function reduction uncertainty lead recommendations expectation). while sources information assess user preferences?including preferences related users, collaborative filtering], observed user choice behavior focus explicit utility elicitation, user asked questions preferences. variety query types refine one knowledge user utility function refer, furr discussion). comparison queries natural, user prefers option anor comparisons localized specific (subsets) attributes additive GAI models, structured models responses. specific options ?generalize,? providing constraints utility related options. work extension comparions choice sets options] common conjoint analysis]. any set interpreted query: user states elements prefers. refer interchangeably query choice set. user response choice set tells preferences; depends user response model. noiseless model, user correctly identifies preferred item slate: choice refines set feasible utility functions imposing linear constraints form ), belief state obtained restricting non-zero density renormalizing. more generally, noisy response model user select option maximize utility. for choice set denote event user selecting response model dictates, choice set probability selection utility function When beliefs user utility uncertain, define . discuss response models below. when treating query set opposed recommendation set interested expected utility, rar expected information (evoi), (expected) degree response increase quality system recommendation. define: Definition Given belief state expected posterior utility (epu query set EPU;    EVOI; EPU;    expected improvement decision quality optimal query fixed size maximal, equivalently, maximal EPU settings, present set options user dual goals offering good set recommendations eliciting valuable information user utility. for instance, product navigation interfaces-commerce sites display set options user select, give user chance critique proposed options]. this motivation exploring connection optimal recommendation sets optimal query sets. moreover, settings queries recommendation separated, query optimization made eﬃcient exploiting relationship. Optimal Recommendation Sets problem computing optimal recommendation sets system uncertainty user true utility function Given belief state single recommendation made, recommend option (?) maximizes expected utility, ?). however, suggesting ?shortlist? multiple options allowing user select preferred option. intuitively, set offer options diverse sense: recommended options highly preferred relative wide range ?likely? user utility functions (relative ]. this stands contrast recommender systems define diversity relative product attributes], direct reference beliefs user utility. hard ?top systems, present options highest expected utility, generally result good recommendation sets]. broad terms, assume utility recommendation set utility preferred item. however, unrealistic assume users select preferred item complete accuracy]. choice queries, assume response model dictating probability choice Definition expected utility selection (eus) recommendation set : EUS;   expand definition rewrite EUS; : EUS; ;  User behavior largely dictated response model ideal setting, users select option highest utility. true utility function This noiseless model assumed] example. however, unrealistic general. noisy response models admit user ?mistakes? choice optimal sets reﬂect possibility (just belief update \\x0cdoes, defn. ). possible constraints response models include) preference bias: preferred outcome slate selected probability greater preferred outcome) luce choice axiom], form independence irrelevant alternatives requires relative probability selecting items affected addition deletion items set. response models: noiseless response model, RNL PNL; )] (with indicator function). EUS [max. eus;  This identical expected max criterion]. under RNL iff  constant noise model assumes multinomial distribution choices responses option preferred option relative selected (small) constant probability independent assume preferred option selected probability   )?  this generalizes model, sets size. ) optimal element) utility, EUS;  EUS; ? )} logistic response model commonly choice modeling, variously luce-sheppard], bradley-terry], mixed multinomial logit model. selection probabilities exp)) exp)) temperature parameter.  For comparison queries), logistic function difference utility options. properties expected utility selection EUS models. all models satisfy preference bias, RNL satisfy luce choice axiom. EUS monotone noiseless response model RNL addition options recommendation set decrease expected utility EUS; ?). moreover, option dominates relative belief state nonzero density. adding set-wise dominated option., dominated element change expected utility RNL EUS }; EUS; ?). this stands constrast noisy response models, adding dominated options decrease expected utility. importantly, EUS submodular noiseless constant response models orem For  EUS submodular function set That, recommendation sets option   },  }, have: EUS   eus;  eus?   eus; ) proof omitted, simply shows EUS required property diminishing returns. submodularity serves basis greedy optimization algorithm (see Section worst-case results query optimization below). EUS commonly logistic response model submodular, \\x0cbut related EUS noiseless model discuss next?allowing exploit submodularity noiseless model optimizing.  Connection EUS EPU develop connection optimal recommendation sets (using eus) optimal choice queries (using epu/evoi). discussed above interested sets serve good recommendations good queries; epu/evoi computationally diﬃcult, good methods eus-optimization serve generate good queries tight relationship two. following, make transformation modifies set EUS increases (and case RNL decrease). this transformation ways) prove optimality (near-optimality case eus-optimal recommendation sets query sets) directly computationally viable heuristic strategy generating query sets. definition Let   set options. define?  . ),    )} optimal option expectation) conditioned intuitively drop subscript clear context) refines recommendation set size producing updated beliefs user choice, replacing option optimal option update. note generally produces sets response models. indeed, construct set response model, measure EUS EPU resulting set response model. some oretical results type ?crossevaluation.? show optimal recommendation sets RNL optimal., epu/evoi-maximizing) query sets. lemma EUS);  epu;  proof: For RNL argument relies partitioning. options EPU;        )    )  EUS);  )    )  Compare expression componentwise: components expression same. nonzero density  )    region )  since EUS);  EPU; component, result follows. for proof argument, observation that: EUS;  ?)(?      ])). from Lemma fact EUS;  , EUS);  eus; ?). state main orem assume size fixed): orem Assume response model  optimal recommendation set.  optimal query set: EPU   epu;  proof: Suppose optimal query set. epu; EPU ?). applying query set), results shows: EUS);  epu; EPU   eus ?). this contradicts eus-optimality  anor consequence Lemma posing query involving infeasible option pointless: set elements epu/evoi great. this proved observing lemma holds redefined sets infeasible options. hard admitting noisy responses logistic response model decrease recommendation set., EUS;  eus; ?). however, loss EUS fact bounded. logistic response model that, probability incorrect selection option high, utility option close item, relative loss utility small. conversely, loss incorrect selection great, utility significantly option, rendering event extremely unlikely. this bound difference EUS EUS ?max depends set cardinality temperature parameter  derive expression ?max below): orem EUS;  eus;  ?max under transformation not, general, improve EUS) recommendation set However set) EUS assuming selection noiseless model, greater expected posterior utility EPU) Lemma EUS);  epu; fact prove optimal recommendation set near-optimal query consequences: first, thm. EUS);  epu;  ?max second, EPU optimal query noiseless model great optimal query logistic model: EPU (?)  epu  derive main result logistic responses: EUS optimal recommendation set (and epu) ?max EPU optimal query set. orem EUS (?)  epu (?)  ?max proof: Consider optimal query? set ? obtained applying from Lemma, EUS   epu?       epu (?). from thm. EUS  eus  ?max thm. EUS (?) EPU (?). thus EUS (?)  eus   eus   ?max epu (?)  ?max loss ; EUS;  eus; EUS set due logistic noise characterized function utility difference  options integrating values (weighted ?). for specific eus-loss utility difference times probability choosing preferred option (? ) logistic function.   ;  — . derive problem-independent upper bound ;  maximizing)   maximal loss ?max (zmax set hypotical items attained utility difference    numerically zmax  imposing obtain yields zmax .279 ?max .2785  this bound expressed scale independent temperature parameter intuitively, ?max corresponds utility difference slight user identifies item probability temperature words, maximum loss small user unable identify preferred item% time asked compare items This derivation generalized sets size yielding ?kmax  (?) Lambert function. Set Optimization Strategies discuss strategies optimization query/recommendation sets section, summarize oretical computational properties. follows, number options—, size query/recommendation set, ?cost? Bayesian inference., number particles Monte Carlo sampling procedure). exact Methods naive maximization EPU computationally intensive eusoptimization, generally impractical. given set elements, computing EPU, requires Bayesian update response, expected utility optimization posterior. query optimization requires computed query sets. thus EPU maximization). exact EUS optimization, demanding) require-maximization updated distributions. thm. compute optimal query sets eus-maximization RNL reducing complexity factor Under thm. eus-optimization approximate optimal query, quality guarantee EPU  ?max greedy Optimization simple greedy algorithm construct recommendation set size iteratively adding option offering greatest improvement value: arg maxx EUS }; ?). under RNL EUS submodular (thm. ), greedy algorithm determines set EUS   optimal EPU; necessarily EPU; sets noisy response ?more informative? noiseless \\x0cone. however, case optimal query sets. Lambert product-log, defined principal inverse  loss-maximizing set Smax infeasible outcomes; practice loss lower.    thm. greedy maximization EUS determine query set similar gaurantees. under EUS longer submodular. however, Lemma thm. EUS submodular, proxy. let set determined greedy optimization EUS submodularity,  eus eus EUS EUS eus applying thm. gives: EUS eus  thus, derive EUS  eus    eus     ???   EUS EUS EUS EUS) similarly, derive worst-case bound EPU. greedy eus-optimization (using fact EUS lower bound epu, thm. thm. ): EUS  eus    eus   epu  ???     EPU EPU EPU EUS EUS) Greedy maximization. EUS extremely fast), linear number options requires) evaluations EUS cost Query Iteration transformation (defn. rise natural heuristic method computing, good query/recommendation sets. query iteration) starts initial set locally optimizes repeatedly applying operator) EUS); )=eus; ?). sensitive initial set lead fixed points. initialization strategies: random (randomly choose options), sampling (include  sample points; add optimal item forcing distinctness) greedy (initialize greedy set bound performance relative optimal query/recommendation sets assuming RNL initialized performance worse greedy optimization. initialized arbitrary set, note that, submodularity,  eus  keu  condition) implies EUS) EPU). also note that, set EPU)   thus, EUS) EUS  this means comparison queries), achieves% optimal recommendation set value. this bound tight corresponds singleton degenerate set?  .., (?)} ? (?)}. this solution problematic EVOI zero. however, RNL sampling initialization avoids fixed point provably construction, leading query set positive evoi. complexity iteration., linear number options, greedy. however, practice faster Greedy typically While oretical results limit iterations required converge, practice, fixed point reached quickly (see below). evaluation compare strategies empirically choice problems random user utility functions \\x0cnoiseless noisy response models Bayesian inference realized Monte Carlo method importance sampling (particle weights determined applying response model observed responses). overcome problem particle degeneration (most particles eventually low weight), slice-sampling] regenerate particles. response-updated belief state effective number samples drops significantly (50000 particles simulations). figure) shows average loss strategies apartment rental dataset, 187 outcomes, characterized attributes (eir numeric categorical domain sizes), pairwise comparison queries noiseless responses. note greedy performs exact optimization, optimal item found roughly queries. query iteration performs initialized sampling, poorly random seeds. This% comparison queries worst ?). practical speedup achieved maintaining priority queue outcomes sorted potential eus-contribution (monotonically decreasing due submodularity). when choosing item add set, evaluate outcomes top queue (lazy evaluation). Utility priors mixtures Gaussians ]  component. exacteus greedy(eus(sampling(rand) random(greedy) greedy(eus) greedy(eus(sampling(rand) random normalized average loss normalized average loss number queries number queries) Average Loss (187 outcomes, runs, RNL) Average Loss (506 outcomes, runs, experiment, Boston Housing dataset 506 items binary continous attributes) logistic noise model responses  compare greedy strategies (exact methods impractical problems size) Figure); hybrid greedy(eus) strategy optimizes ?assuming? noiseless responses, evaluated true response model (sampling) efficient TNL version plotted. overall experiments show (greedy exact) maximization EUS find optimal near-optimal responses noisy?query sets. finally, compare query optimization times datasets table=187=187=187=506=506=506 exactepu exacteus greedy(epu(greedy(eus)) greedy(eus) greedy(eus(sampling(rand 1815s 405s 10000s.19s 142s.76s.07s.89s.09s.65s.97s.71s.99s.12s.02s.86s.55s.93s.12s.53s.11s.15s.16s.51s.05s.08s.09s.11s.17s.19s.64s.06s.10s.13s Among strategies, eﬃcient computationally, suited large outcome spaces. interestingly, faster sampling initialization random initialization fewer iteration average convergence. ). Conclusions provided analysis set-based recommendations Bayesian recommender systems, shown offers tractable means generating myopically optimal nearoptimal choice queries preference elicitation. examined user response models, showing optimal recommendation sets evoi-optimal queries noiseless \\x0cconstant noise models; near-optimal logistic/lucesheppard model (both oretically practically). stress results general depend specific implementation Bayesian update, specific form utility function. our greedy strategies? exploiting submodularity EUS computation?perform practice oretical approximation guarantees. finally experimental results demonstrate query iteration, simple local search strategy, well-situated large decision spaces. number important directions future research remain. furr oretical practical investigation local search strategies query iteration important. anor direction development strategies Bayesian recommendation elicitation large-scale configuration problems., outcomes csp, sequential decision problems (such MDPs uncertain rewards). finally, interested elicitation strategies combine probabilistic regret-based models. acknowledgements authors Iain Murray Cristina Manfredotti helpful discussion Monte Carlo methods, sampling techniques particle filters. this research supported nserc.',\n",
       " 'PP3960': 'classical supervised machine learning paradigm learner labeled training set examples goal find decision function small generalization error unknown test examples. learning problem easy. learner space decision functions generalization error) training size increases, decision function found learner converges quickly optimal one. learning problem hard learner space decision functions large convergence learning) rate slow. hard learning problem XOR space decision functions-dimensional hyperplanes. obvious question ?can accelerate learning rate learner additional information learning problem??. years paradigms learning additional information proposed that, conditions, provably accelerate learning rate. example, semi-supervised learning additional information unlabeled training examples. paper recently proposed Learning Using Privileged Information (lupi) paradigm], additional information kind. decision space. lupi \\x0cparadigm, addition standard training data,  teacher supplies learner privileged information correcting space  privileged information training examples test examples. lupi paradigm requires, training set find decision function small generalization error unknown test examples  question accelerating learning rate, reformulated terms LUPI paradigm, ?what kind additional information teacher provide learner order accelerate learning rate??. paraphrased, question essentially ?who good teacher??. paper outline conditions additional information provided teacher fast learning rate hard problems. lupi paradigm emerges number applications, time series prediction, protein classification human computation. experiments] domains demonstrated clear advantage LUPI paradigm supervised learning. lupi paradigm implemented svm+ algorithm], turn based wellknown SVM algorithm]. present version svm+ classification, version regression found]. ) sign decision function    correcting function. optimization problem svm+ min?  ?   kwk22? k22.       ?         objective function svm+ hyperparameters,  term? k22) intended restrict capacity-dimension) function space ),  hinge loss decision function, ? ?   ]+ loss correcting function ?   optimization problem) rewritten min? ) kwk22? k22  .        )). optimization problem simplified generalized version): min,??? .         ) \\x0cwhere arbitrary bounded loss functions, space decision functions space correcting functions. constant (that defined later]+ max, , ))  ? ),  ? ) loss composite hyposis, , ). paper study relaxation): min, ,???  refer learning algorithm defined optimization problem) empirical risk minimization privileged information, abbreviated Privileged erm. basic assumption Privileged ERM achieve small loss ? correcting space achieve small loss), decision space. assumption reﬂects human learning process, teacher tells learner important examples small loss correcting space) learner account order find good decision rule. regular empirical risk minimization (erm) finds hyposis minimizes training error regular ERM directly minimizes training error privileged ERM minimizes training error indirectly, minimization training error correcting function relaxation constraint).  decision function terms generalization error) hyposis space Suppose training oracle loss?  fixed losses   find satisfies system inequalities:    ?  ) denote learning algorithm defined) oracleerm. straightforward generalization proof Proposition] shows generalization error hyposis found oracleerm converges rate. rate faster worst-case convergence rate regular ERM]. paper realistic setting, oracle available. subsequent derivations rely heavily definition: Definition decision function uniformly correcting function , non-zero probability,    space decision functions space correcting functions define     uniformly ?}. note    correcting functions good Our results based assumptions: Assumption  assumption restrictive, means optimization problem) Privileged ERM feasible solution training size infinity. assumption exists correcting function  , non-zero probability?    put anor way, assume existence correcting function mimics losses  learning rate Privileged ERM ran joint  space space decision correcting functions  develop upper bound risk decision function found Privileged erm. assumptions bound converges rate This implies correcting space good, Privileged ERM joint  space fast learning rate), Privileged ERM fast learning rate. ) decision space. true ?decision space hard regular ERM decision space slow learning rate. ). illustrate result artificial learning problem, regular ERM decision space learn rate faster correcting space good Privileged ERM learns decision space rate. paper structure. section give additional definitions. section review existing risk bounds derive results. section proof risk bound Privileged erm. section show Privileged ERM provably regular erm. conclude give directions future research Section due space constraints, proofs supplementary material. previous work attempt oretical analysis LUPI Vapnik Vashist]. addition analysis learning oracle (mentioned above), considered algorithm, close, Privileged erm. developed risk bound (proposition]) decision function found algorithm. bound applies Privileged erm. bound] tailored classification setting-loss functions decision correcting space. contrast, bound holds bounded loss functions loss functions different. bound] depends generalization error correcting function found Privileged erm. vapnik Vashist] concluded bound convergence rate bound imply bound convergence rate decision function found algorithm. definitions triple, sampled distribution unknown learner. denote marginal distribution, marginal distribution? ). distribution nature distribution constructed teacher. spaces decision correcting functions chosen learner. let(?) ?   ? )} generalization errors decision function correcting function respectively. assume loss functions range]. assumption satisfied bounded loss function simply dividing maximal value. denote arg minh) arg min??? (?) decision correction function minimal generalization error. loss functions  also, denote loss, R01)} generalization error.  loss arg minh?hpr01) decision function minimal generalization error. let \\x0crn0, , , ? , , ) empirical generalization errors hyposis, . loss function arg min?? , empirical risk minimizer denote  arg min?? , minimizer generalization error. loss function note general   , ,    uniformly ?}. assumption,  additional technical assumption: Assumption exists constant  inf? ),  ? ,  , (?) (?)   assumption satisfied, example, classification setting  loss functions probability density function, underlying distribution bounded points nonzero probability. case inf, , , }. lemma (proved Appendix full version paper) shows suﬃciently large optimization problems) asymptotically (when  equivalent: Lemma Suppose Assumptions hold true. exists finite   , ?). moreover,  subsequent derivations assume finite) equivalent. show choose optimizes forthcoming risk bound. risk bounds presented paper based-dimension function classes. definition-dimension binary functions well-known learning community, real-valued functions review here. set realvalued functions ,    .  )— }. set ) shattered exists function     -dimension defined-dimension set), maximal size set ) shattered Review existing excess risk bounds fast convergence rates derive risk bounds generic excess risk bounds developed Massart Nedelec] generalized Gine Koltchinskii] Koltchinkii]. paper version bounds]. space hyposes  } real-valued loss function ),  ).  ) Hyposis space small) Hyposis space large Figure Visualization hyposis spaces. horisontal axis measures distance terms variance) hyposis \\x0chyposis vertical axis minimal error hyposes fixed distance  note error function displayed graphs non-continuous. large hyposis space graph) caused hyposis significantly nearly-optimal error. arg minf)}, fbn arg minf constant var),  )}  ),  )}. ) This condition generalization tsybakov low-noise condition] arbitrary loss functions arbitrary hyposis spaces. constant) characterizes error surface hyposis space Suppose),  )} small, optimal.  variance left hand side), small. differs significantly variance left hand side), large. thus, variance left hand side) measure distance hyposis spaces large small visualized shown Figure -dimension orem straightforward generalization orem]. orem]) exists constant  probability     )} )} log ) log log. condition orem hold,  fallback risk bound: orem]) exists constant probability  )}    ) Definition Let )}, constant holds )}  bound? ) convergence rate, bound) convergence rate main difference) fast convergence rate. slow regime max,  orem, starting) convergence rate. thus, smaller smaller threshold) obtaining fast convergence rate. upper Risk Bound For , ),   loss functions ),  ? ),  ? )]+ )}  ?  ?). hence) obtain, )) ) Let, ), ? ), constant , )} var, )} ) similarly, , , , ,?  constant,   ,?  ? , )} var? , )} ) Let, , (?, ?))     set loss functions hyposes  ,?) -dimension, ?). similarly(?   (?)  (?(?     sets loss functions correspond hyposes(?) dimensions(?) respectively. note-dimension holds(?)  lemma,?) (?)  proof See Appendix full version paper. apply orem hyposis space  loss function, , )) obtain exists constant,?)  ,?  probability    kdh,? ,?)  ,   ,?) ,?  ) obtain ckdh,?    ,?) ,?) ,?    ) Assumption Lemma(?) ?  ) substitute) obtain exists constant,?)  ,?  probability   ckd,?  ? ,?)  ,?) ,?  bound,? lemma obtain final risk bound, summarized orem: orem Suppose Assumptions hold. ,? defined), defined Lemma,?) (?)  suppose  probability ,?)  ,?   ckdh,?  ) ,?)  ,?)  ,? constant. according bound converges? rate. assumption hold easy obtain bound? replaced case upper bound converges rate. provide furr analysis risk bound). (?,  ?   ?   constant    ?  (?, )} var?  (?, )} ) similarly,?  constant,    , ,? ? , )} var? , )}   lemma,?  max ,?  proof See Appendix full version paper.  loss function depends Lemma, ,?  max?  ,? constant,? depends too. thus, ingoring left-hand logarithmic term), optimal larger minimizes ,?  show minimum exists. definition loss function  var? , )} lim  ?? ,?) ? , )} refore large holds ,?  limit. consequently limc??  ,?  function) ,? continuous finite exists point  minimizes. when Privileged ERM provably regular ERM show demonstrates difference emprical risk minimization space empirical risk minimization privileged information joint  space. particular, show small training sizes conditions orems) learning rate regular ERM space learning rate privileged ERM joint  space. classification setting loss functions loss.   } infinite family distributions examples space. distributions non-zero support points, denoted assume points lie-dimensional line, shown Figure). figure) shows probability mass point distribution (?). hyposis space consists hyposes) sign h0t ?sign ). hyposis h01 generalization error ?. hyposis space hyposis h03 slightly worse h01 generalization error verified fixed (?) constant (defined equation?)  )   ?). ) Note inequality) tight arbitrary small. -dimension suppose suﬃciently small  ?, function (?, defined Definition. order risk bound) condition ) satisfied.  small, condition) satisfied large range. hence), distributions (?) satisfy ?,  ?   obtain R01) converges R01 rate lower bound shows R01 converges R01? rate) space) space figure spaces. lemma Suppose . exp(?20n 256, probability R01 r01?   )/(20n).  combining upper lower bounds obtain convergence rate R01) R01 proof lower bound appears Appendix full version paper. suppose teacher constructed distribution (?) examples space way.  (?) non-zero support points, denoted? ? ? ? lie-dimensional line, shown Figure). figure) shows probability mass point space. assume joint distribution, non-zero support points? ? ? ?  hyposis space consists hyposes) sign?   ?sign?  ). hyposis  generalization error  uniformly hyposis uniformly hyposis generalization error ?. h01 uniformly verified (?)  constant (defined equation))    ?)  . ) Note inequality) tight arbitrary small. moreover, verified minimizes ,?  .  holds,?  . easy satisfies Assumptions assumption satisfied   also, verified Assumption satisfied   satisfies Lemma. -dimension  orem Lemma, .712 R01 converges R01? rate. bounds ,? independent convergence rate holds distribution  obtained ? upper bound) converges R01 rate  , upper bound) converges R01 rate This improvement due teacher construction (?) learner choice hyposis h03 caused large prevented convergence rate large range. constructed (?)   hyposis dichotomy bad hyposis h03 construction  h03 uniformly generalization error significantly larger h03 example, hyposis h03 uniformly better, generalization error. conclusions formulated algorithm empirical risk minimization privileged information derived risk bound. risk bound outlines conditions correcting space that, satisfied, fast learning decision space, original learning problem decision space hard. showed privileged information provably significantly improves learning rate.  paper showed good correcting space improve learning rate. but, good correcting space, achieve learning rate faster? anor intersting problem analyze Privileged ERM learner completely trust teacher. condition translates constraint),  ?  ) term),  ? ),  hyperparameter. finally, important direction develop risk bounds svm+ (which regularized version Privileged erm) show provably svm. classical supervised machine learning paradigm learner labeled training set examples goal find decision function small generalization error unknown test examples. learning problem easy. learner space decision functions generalization error) training size increases, decision function found learner converges quickly optimal one. however learning problem hard learner space decision functions large convergence learning) rate slow. hard learning problem XOR space decision functions-dimensional hyperplanes. obvious question ?can accelerate learning rate learner additional information learning problem??. during years paradigms learning additional information proposed that, conditions, provably accelerate learning rate. for example, semi-supervised learning additional information unlabeled training examples. paper recently proposed Learning Using Privileged Information (lupi) paradigm], additional information kind. let decision space. LUPI \\x0cparadigm, addition standard training data,  teacher supplies learner privileged information correcting space  privileged information training examples test examples. LUPI paradigm requires, training set find decision function small generalization error unknown test examples  question accelerating learning rate, reformulated terms LUPI paradigm, ?what kind additional information teacher provide learner order accelerate learning rate??. paraphrased, question essentially ?who good teacher??. paper outline conditions additional information provided teacher fast learning rate hard problems. LUPI paradigm emerges number applications, time series prediction, protein classification human computation. experiments] domains demonstrated clear advantage LUPI paradigm supervised learning. lupi paradigm implemented svm+ algorithm], turn based wellknown SVM algorithm]. present version svm+ classification, version regression found]. let) sign decision function    correcting function. optimization problem svm+ min?  ?   kwk22? k22.       ?         objective function svm+ hyperparameters,  term? k22) intended restrict capacity-dimension) function space let),  hinge loss decision function, ? ?   ]+ loss correcting function ?   optimization problem) rewritten min? ) kwk22? k22  .        )). optimization problem simplified generalized version): min,??? .         ) \\x0cwhere arbitrary bounded loss functions, space decision functions space correcting functions. let constant (that defined later]+ max, , ))  ? ),  ? ) loss composite hyposis, , ). paper study relaxation): min, ,???  refer learning algorithm defined optimization problem) empirical risk minimization privileged information, abbreviated Privileged erm. basic assumption Privileged ERM achieve small loss ? correcting space achieve small loss), decision space. this assumption reﬂects human learning process, teacher tells learner important examples small loss correcting space) learner account order find good decision rule. regular empirical risk minimization (erm) finds hyposis minimizes training error while regular ERM directly minimizes training error privileged ERM minimizes training error indirectly, minimization training error correcting function relaxation constraint). let decision function terms generalization error) hyposis space Suppose training oracle loss?  fixed losses   find satisfies system inequalities:    ?  ) denote learning algorithm defined) oracleerm. straightforward generalization proof Proposition] shows generalization error hyposis found OracleERM converges rate. this rate faster worst-case convergence rate regular ERM]. paper realistic setting, oracle available. our subsequent derivations rely heavily definition: Definition decision function uniformly correcting function , non-zero probability,    given space decision functions space correcting functions define     uniformly ?}. note    correcting functions good Our results based assumptions: Assumption  this assumption restrictive, means optimization problem) Privileged ERM feasible solution training size infinity. assumption exists correcting function  , non-zero probability?    put anor way, assume existence correcting function mimics losses  let learning rate Privileged ERM ran joint  space space decision correcting functions  develop upper bound risk decision function found Privileged erm. under assumptions bound converges rate This implies correcting space good, Privileged ERM joint  space fast learning rate), Privileged ERM fast learning rate. ) decision space. that true ?decision space hard regular ERM decision space slow learning rate. ). illustrate result artificial learning problem, regular ERM decision space learn rate faster correcting space good Privileged ERM learns decision space rate. paper structure. Section give additional definitions. Section review existing risk bounds derive results. section proof risk bound Privileged erm. Section show Privileged ERM provably regular erm. conclude give directions future research Section due space constraints, proofs supplementary material. previous work attempt oretical analysis LUPI Vapnik Vashist]. addition analysis learning oracle (mentioned above), considered algorithm, close, Privileged erm. developed risk bound (proposition]) decision function found algorithm. this bound applies Privileged erm. bound] tailored classification setting-loss functions decision correcting space. contrast, bound holds bounded loss functions loss functions different. bound] depends generalization error correcting function found Privileged erm. vapnik Vashist] concluded bound convergence rate bound imply bound convergence rate decision function found algorithm. Definitions triple, sampled distribution unknown learner. denote marginal distribution, marginal distribution? ). distribution nature distribution constructed teacher. spaces decision correcting functions chosen learner. Let(?) ?   ? )} generalization errors decision function correcting function respectively. assume loss functions range]. this assumption satisfied bounded loss function simply dividing maximal value. denote arg minh) arg min??? (?) decision correction function minimal generalization error. loss functions  also, denote loss, R01)} generalization error.  loss arg minh?hpr01) decision function minimal generalization error. Let \\x0crn0, , , ? , , ) empirical generalization errors hyposis, . loss function arg min?? , empirical risk minimizer denote  arg min?? , minimizer generalization error. loss function note general   let, ,    uniformly ?}. Assumption,  additional technical assumption: Assumption exists constant  inf? ),  ? ,  , (?) (?)   this assumption satisfied, example, classification setting  loss functions probability density function, underlying distribution bounded points nonzero probability. case inf, , , }. lemma (proved Appendix full version paper) shows suﬃciently large optimization problems) asymptotically (when  equivalent: Lemma Suppose Assumptions hold true. exists finite   , ?). moreover,  subsequent derivations assume finite) equivalent. later show choose optimizes forthcoming risk bound. risk bounds presented paper based-dimension function classes. while definition-dimension binary functions well-known learning community, real-valued functions review here. let set realvalued functions ,    .  )— }. set ) shattered exists function     -dimension defined-dimension set), maximal size set ) shattered Review existing excess risk bounds fast convergence rates derive risk bounds generic excess risk bounds developed Massart Nedelec] generalized Gine Koltchinskii] Koltchinkii]. paper version bounds]. let space hyposes  } real-valued loss function ),  ). let ) Hyposis space small) Hyposis space large Figure Visualization hyposis spaces. horisontal axis measures distance terms variance) hyposis \\x0chyposis vertical axis minimal error hyposes fixed distance  note error function displayed graphs non-continuous. large hyposis space graph) caused hyposis significantly nearly-optimal error. arg minf)}, fbn arg minf constant var),  )}  ),  )}. ) This condition generalization tsybakov low-noise condition] arbitrary loss functions arbitrary hyposis spaces. constant) characterizes error surface hyposis space Suppose),  )} small, optimal.  variance left hand side), small. but differs significantly variance left hand side), large. thus, variance left hand side) measure distance hyposis spaces large small visualized shown Figure let-dimension orem straightforward generalization orem]. orem]) exists constant  probability     )} )} log ) let log log. condition orem hold,  fallback risk bound: orem]) exists constant probability  )}    ) Definition Let )}, constant holds )} for bound? ) convergence rate, bound) convergence rate main difference) fast convergence rate. slow regime max,  orem, starting) convergence rate. thus, smaller smaller threshold) obtaining fast convergence rate. Upper Risk Bound For , ),   loss functions ),  ? ),  ? )]+ )}  ?  ?). hence) obtain, )) ) Let, ), ? ), constant , )} var, )} ) similarly, , , , ,?  constant,   ,?  ? , )} var? , )} ) Let, , (?, ?))     set loss functions hyposes  ,?) -dimension, ?). similarly(?   (?)  (?(?     sets loss functions correspond hyposes(?) dimensions(?) respectively. note-dimension holds(?)  lemma,?) (?)  proof See Appendix full version paper. apply orem hyposis space  loss function, , )) obtain exists constant,?)  ,?  probability    kdh,? ,?)  ,   ,?) ,?  using) obtain ckdh,?    ,?) ,?) ,?    ) Assumption Lemma(?) ?  ) substitute) obtain exists constant,?)  ,?  probability   ckd,?  ? ,?)  ,?) ,?  bound,? Lemma obtain final risk bound, summarized orem: orem Suppose Assumptions hold. let,? defined), defined Lemma,?) (?)  suppose  probability ,?)  ,?   ckdh,?  ) ,?)  ,?)  ,? constant. According bound converges? rate. Assumption hold easy obtain bound? replaced case upper bound converges rate. provide furr analysis risk bound). let (?,  ?   ?   constant    ?  (?, )} var?  (?, )} ) similarly,?  constant,    , ,? ? , )} var? , )}   Lemma,?  max ,?  proof See Appendix full version paper.  since loss function depends Lemma, ,?  max?  ,? constant,? depends too. thus, ingoring left-hand logarithmic term), optimal larger minimizes ,?  show minimum exists. definition loss function  var? , )} lim  ?? ,?) ? , )} refore large holds ,?  limit. Consequently limc??  ,?  since function) ,? continuous finite exists point  minimizes. When Privileged ERM provably regular ERM show demonstrates difference emprical risk minimization space empirical risk minimization privileged information joint  space. particular, show small training sizes conditions orems) learning rate regular ERM space learning rate privileged ERM joint  space. classification setting loss functions loss. let  } infinite family distributions examples space. all distributions non-zero support points, denoted assume points lie-dimensional line, shown Figure). figure) shows probability mass point distribution (?). hyposis space consists hyposes) sign h0t ?sign ). hyposis h01 generalization error ?. hyposis space hyposis h03 slightly worse h01 generalization error verified fixed (?) constant (defined equation?)  )   ?). ) Note inequality) tight arbitrary small. -dimension suppose suﬃciently small  ?, function (?, defined Definition. order risk bound) condition ) satisfied. but small, condition) satisfied large range. hence), distributions (?) satisfy ?,  ?   obtain R01) converges R01 rate lower bound shows R01 converges R01? rate) space) space figure spaces. lemma Suppose . let exp(?20n 256, probability R01 r01?   )/(20n).  combining upper lower bounds obtain convergence rate R01) R01 proof lower bound appears Appendix full version paper. suppose teacher constructed distribution (?) examples space way.  (?) non-zero support points, denoted? ? ? ? lie-dimensional line, shown Figure). figure) shows probability mass point space. assume joint distribution, non-zero support points? ? ? ?  hyposis space consists hyposes) sign?   ?sign?  ). hyposis  generalization error however uniformly hyposis uniformly hyposis generalization error ?. h01 uniformly verified (?)  constant (defined equation))    ?)  . ) Note inequality) tight arbitrary small. moreover, verified minimizes ,?  . for holds,?  . easy satisfies Assumptions assumption satisfied   also, verified Assumption satisfied   satisfies Lemma. -dimension  hence orem Lemma, .712 R01 converges R01? rate. since bounds ,? independent convergence rate holds distribution  obtained ? upper bound) converges R01 rate  , upper bound) converges R01 rate This improvement due teacher construction (?) learner choice hyposis h03 caused large prevented convergence rate large range. constructed (?)   hyposis dichotomy bad hyposis h03 with construction  h03 uniformly generalization error significantly larger h03 for example, hyposis h03 uniformly better, generalization error. Conclusions formulated algorithm empirical risk minimization privileged information derived risk bound. our risk bound outlines conditions correcting space that, satisfied, fast learning decision space, original learning problem decision space hard. showed privileged information provably significantly improves learning rate.  paper showed good correcting space improve learning rate. but, good correcting space, achieve learning rate faster? anor intersting problem analyze Privileged ERM learner completely trust teacher. this condition translates constraint),  ?  ) term),  ? ),  hyperparameter. finally, important direction develop risk bounds svm+ (which regularized version Privileged erm) show provably svm.',\n",
       " 'PP3987': 'reinforcement learning handle policy search problems unknown environments. policy gradient methods, train parameterized stochastic policies climbing gradient average reward. advantage methods easily deal continuous state-action continuing (not episodic) tasks. policy gradient methods successfully applied practical tasks]. domain control, policy constructed controller exploration strategy. controller represented domain-appropriate pre-structured parametric function. exploration strategy required seek parameters controller. directly perturbing parameters controller, conventional exploration strategies perturb resulting control signal. however, significant problem sampling strategy high variance gradient estimates leads slow convergence. recently, parameter-based exploration] strategies search controller parameter space direct parameter perturbation proposed, demonstrated work eﬃciently conventional strategies]. anor approach speeding policy gradient methods replace gradient natural gradient-called natural policy gradient]; motivated intuition change policy parameterization inﬂuence result policy update. combination parameterbased exploration strategies natural policy gradient expected result improvements convergence rate; however, algorithm proposed. however, natural policy gradients parameter-based exploration strategies disadvantage computational cost high. natural policy gradient requires computation inverse Fisher information matrix (fim) policy distribution; prohibitively expensive, high-dimensional policy. unfortunately, parameter-based exploration strategies tend higher dimensions control-based ones. refore, expected method diﬃcult apply realistic control tasks. paper, propose reinforcement learning method combines natural policy gradient parameter-based exploration. derive eﬃcient algorithm estimating natural policy gradient exploration strategy implementation. algorithm calculates natural policy gradient inverse exact FIM Monte Carloestimated gradient. resulting algorithm, called natural policy gradients parameter-based exploration (npgpe), computational cost similar conventional policy gradient algorithms. numerical experiments show proposed method outperforms policy gradient methods, including current state--art NAC] control-based exploration. policy Search Framework standard reinforcement learning framework agent interacts Markov decision process. section, review estimation policy gradients describe difference controland parameter-based exploration.  Markov Decision Process Notation discrete time agent observes state selects action receives instantaneous reward resulting state transition environment. state action defined continuous spaces paper. state chosen transition probability reward randomly expectation agent advance. objective reinforcement learning agent construct policy maximizes agent performance. parameterized policy , defined probability distribution action space state parameters assume  unique well-defined stationary distribution—?). assumption, natural performance measure infinite horizon tasks average reward ?(?) —?)  , )dads.  Policy Gradients \\x0cpolicy gradient methods update policies estimating gradient average reward.  policy parameters. state-action  ?(?  assumed , differentiable.  exact gradient average reward (see]) ?(?) —?)  , ?)?? log ? )dads. ) natural gradient] basis information geometry, studies Riemannian geometric structure manifold probability distributions. result information geometry states FIM defines Riemannian metric tensor space probability distributions] direction steepest descent Riemannian manifold natural gradient, conventional gradient premultiplied inverse matrix Riemannian metric tensor]. thus, natural gradient computed gradient fim, converge faster conventional gradient. kakade] applied natural gradient policy search; called natural policy gra?  ?(?)   ?(?) dient. fim invertible, natural policy gradient  policy gradient premultiplied inverse matrix FIM  paper, employ FIM proposed Kakade], defined —?)  , ?)?? log , ?)?? log , dads. figure Illustration main difference control-based exploration parameter-based exploration. controller , represented single-layer perceptron. controlbased exploration strategy (left) perturbs resulting control signal, parameter-based exploration strategy (right) perturbs parameters controller.  Learning Samples calculation) requires knowledge underlying transition probabilities—?). gpomdp algorithm] computes Monte Carlo approximation): agent interacts environment, producing observation, action, reward sequence ..., mild technical assumptions, policy gradient approximation ?(?)   log  called eligibility trace], log  called characteristic eligibility], denotes discount factor  ).   estimation approaches true gradient variance increases set  log    log  ?). refore, natural policy experiments). define  gradient approximation  ?(?)    )  log  ?). estimate natural policy gradient, heuristic sug  gested Kakade  (?? log  ?)?? log  ) online estimate fim, small positive constant.  parameter-based Exploration control tasks, attempt (deterministic stochastic) controller , exploration strategy,  denotes control  parameters controller. objective learning seek suitable values parameters exploration strategy required carry stochastic sampling current parameters. typical exploration strategy model, call control-based exploration, normal distribution control space (figure1 (left)). case, action agent control, policy represented  exp   ))   )) , ?   covariance matrix agent seeks . control time generated  ),  ?). ] showed approximation error proportional??  —), sub-dominant eigenvalue Markov chain One feature Gaussian unit] agent potentially control degree exploratory behavior. control-based exploration strategy samples output controller. however, structures parameter space control space identical. refore, sampling strategy generates controls generated current controller, exploration variances decrease. property leads large variance gradient estimates. reason policy improvement stuck. address issue, Sehnke. ] introduced exploration strategy policy gradient methods called policy gradients parameter-based exploration (pgpe). approach, action agent parameters controller, policy represented           , exp  ?  —?—  covariance matrix agent seeks .  controller included dynamics environment, control time generated   ,     gpomdp-based methods estimate policy gradients partially observable settings.,  excludes observation current state. exploration strategy policy, directly perturbs parameters (figure1 (right)), samples generated current parameters small exploration variances. note advantage framework gradient estimated directly sampling parameters controller, implementation policy gradient algorithms require diﬃcult derive complex controllers. sehnke. ] demonstrated PGPE yield faster convergence control-based exploration strategy challenging episodic tasks. however, parameter-based exploration higher dimension control-based one. refore, computational cost inverse calculated), natural policy gradients find limited applications. natural Policy Gradients parameter-based Exploration section, propose algorithm called natural policy gradients parameter-based exploration (npgpe) eﬃcient estimation natural policy gradient.  Implementation gaussian-based Exploration Strategy employ policy representation model—? multivariate normal distribution parameters , represents Cholesky decomposition covariance  upper triangular matrix  sun. ] noted matrix advantages implementation: makes explicit independent parameters addition, diagonal elements square roots determining covariance matrix  refore, positive semidefinite. remainder eigenvalues text, ]-dimensional column vector consisting elements upper-right elements.,  here sub-matrix row column Inverse Fisher Information Matrix Previous natural policy gradient methods] empirical fim, estimated sample path. methods highly ineﬃcient—?) invert empirical fim, matrix elements. avoid problem directly computing exact fim. algorithm Natural Policy Gradient Method parameter-based Exploration require: : policy parameters, ): controller, step size, discount rate, baseline. observe initialize ... draw ), compute action obtain observation reward execute   log  log  —?)     —?) {triu diag    log   —?)       )? end substituting —?) ), rewrite policy gradient obtain   wds.   ?(?) —?) —?)?  log—?   furrmore, FIM distribution   dwds  —?) —?)?  log—?)?  log—?)    .  —?)?  log—?)?  log—?) because independent—? real fim. sun. ] proved precise FIM Gaussian distribution,  block-diagonal matrix diag ..., block identical   block        ?  vkt? denotes-dimensional column vector nonzero element element one?  ]-dimensional identity matrix. furr, Akimoto. ] derived inverse matrix diagonal block fim.  block-diagonal matrix upper triangular, easy verify inverse matrix FIM       ? ? vkt    ? ?  ? ? ) Natural Policy Gradient  log  —?) now, derive eligibility premultiplied inverse matrix FIM   log —?) manner]. characteristic eligibility.     —?)    ). log    —?)  —?)   characteristic obviously log log eligibility.     —?) vit triu diag log  npg) npg) vpg) vpg step paramter-based control-based paramter empirical optimum control return paramter-based control-based state state \\x0cfigure Performance npg) compared npg), vpg), vpg) linear quadratic regulation task averaged 100 trials. left: empirical optimum denotes return optimum gain. center right: Illustration main difference controland parameter-based exploration. sampling area state-control space (center) state-parameter space (right) plotted. triu denotes upper triangular matrix, element identical    , element orwise symmetric matrix.  dimension ); characteristic eligibility. expressed   —?) ?  diag  log according), diag vkt Cvk  cvk?  —?) refore block  log   —?)  log  —?)  log         diag?  vkt? ?    ?  vkt  ?     log   obtain log —?)  —?)     —?) triu diag log ) refore, time complexity computing  log  log  log  log  —?)       ...,     —?). significant imis order computation log provement current natural policy gradient estimation) parameter-based exploration, complexity note simple forms exploration distribution used. exploration strategy represented independent normal distribution parameter natural policy gradient estimated) time. limited form ignores relationship parameters, practical high-dimensional controllers.  Algorithm parameterized class controllers ), exploration strategy—?). online version based GPOMDP algorithm implementation shown Algorithm  generated  , practice, parameters controller   normal random numbers. now reduce variance gradient estimation, employ variance reduction techniques] adapt reinforcement baseline figure Simulator two-link arm robot. experiments section, evaluate performance proposed NPGPE method. eﬃciency parameter-based exploration reported episodic tasks]. compare parameterand control-based exploration strategies natural gradient conventional ?vanilla? gradients simple continuing task linear control problem. demonstrate npgpe usefulness physically realistic locomotion task two-link arm robot simulator.  Implementation compare exploration strategies. parameterbased exploration strat? egy—?) presented Section. control-based exploration strategy—?  vector control represented normal distribution control space, generated controller represents Cholesky decomposition covariance matrix  upper triangular matrix parameters policy,  ]-dimensional column vector consisting elements upper-right elements Linear Quadratic Regulator linear control problem serve benchmark delayed reinforcement tasks]. dynamics environment     reward ?s2t u2t experiment, set states constrained lie range], truncated. agent chooses action lie range], action executed environment truncated. controller represented ,    optimal parameter   riccati equation. clarification, write NPG employs natural policy gradient VPG employs ?vanilla? policy gradient. refore, npg) vpg) denote parameterbased exploration strategy, npg) vpg) denote control-based exploration strategy. proposed NPGPE method npg). figure2 (left) shows performance compared methods. algorithm parameter-based exploration performance control-based exploration continuing task. natural policy gradient improved convergence speed, combination parameter-based exploration outperformed methods. reason acceleration learning case fact samples generated parameter-based exploration strategy effective search. figure2 (center right) show plots sampling area state-control space state-parameter space, respectively. control-based exploration maintains sampling area control space, sampling uniform parameter space agent visits frequently. refore, parameter-based exploration realize eﬃcient sampling control-based exploration.  \\x0clocomotion Task two-link Arm Robot applied algorithm robot shown Figure3 Kimura. ]. objective learning find control rules move forward. joints controlled servo motors react npg) npg) nac) 104 105 106 step 107 102 weight weight gain return gain 103 104 105 step 106 107 102 103 104 105 step 106 107 Figure Performance npg) compared npg) nac) locomotion task averaged 100 trials. left: Mean performance compared methods. center: Parameters controller npg). right: Parameters controller npg). parameters controller normalized gain weight /gain denotes parameter joint. arrows center denote changing points relation important parameters. angular-position commands. time step, agent observes angular position motors, observation normalized], selects action. reward distance body movement caused previous action. robot moves backward, agent receives negative reward. state vector expressed control motor generated exp(?  )). dimension parameters policies parameterand control-based exploration strategy, respectively. compared npg., npgpe, npg) nac). nac state--art policy gradient algorithm] combines natural policy gradients, actor-critic framework, leastsquares temporal-difference-learning. nac computes inverse matrix estimate natural steepest ascent direction. nac(d3w time complexity iteration, prohibitively expensive, apply NAC control-based exploration. figure4 (left) shows results. initially, npg) outperformed nac); however, reaches good solutions fewer steps. furrmore, stage, nac) matches npg). figure4 (center right) show path relation parameters controller. npg) slower npg) adapt relation early stage; however, seek relations important parameters (indicated arrows figures) faster, npg) stuck ineﬃcient sampling. conclusions This paper proposed natural policy gradient method combined parameter-based exploration cope high-dimensional reinforcement learning domains. proposed algorithm, npgpe, simple quickly calculates estimation natural policy gradient. moreover, experimental results demonstrate significant improvement control domain. future works focus developing actor-critic versions NPGPE encourage performance improvements early stage, combining gradient methods natural conjugate gradient methods]. addition, comparison direct parameter perturbation methods finite difference gradient methods], cma], NES] gain understanding properties eﬃcacy combination parameter-based exploration strategies natural policy gradient. furrmore, application algorithm real-world problems required assess utility. acknowledgments This work suported Japan Society Promotion Science 9031). Reinforcement learning handle policy search problems unknown environments. policy gradient methods, train parameterized stochastic policies climbing gradient average reward. advantage methods easily deal continuous state-action continuing (not episodic) tasks. policy gradient methods successfully applied practical tasks]. domain control, policy constructed controller exploration strategy. controller represented domain-appropriate pre-structured parametric function. exploration strategy required seek parameters controller. instead directly perturbing parameters controller, conventional exploration strategies perturb resulting control signal. however, significant problem sampling strategy high variance gradient estimates leads slow convergence. recently, parameter-based exploration] strategies search controller parameter space direct parameter perturbation proposed, demonstrated work eﬃciently conventional strategies]. anor approach speeding policy gradient methods replace gradient natural gradient-called natural policy gradient]; motivated intuition change policy parameterization inﬂuence result policy update. combination parameterbased exploration strategies natural policy gradient expected result improvements convergence rate; however, algorithm proposed. however, natural policy gradients parameter-based exploration strategies disadvantage computational cost high. natural policy gradient requires computation inverse Fisher information matrix (fim) policy distribution; prohibitively expensive, high-dimensional policy. unfortunately, parameter-based exploration strategies tend higher dimensions control-based ones. refore, expected method diﬃcult apply realistic control tasks. paper, propose reinforcement learning method combines natural policy gradient parameter-based exploration. derive eﬃcient algorithm estimating natural policy gradient exploration strategy implementation. our algorithm calculates natural policy gradient inverse exact FIM Monte Carloestimated gradient. resulting algorithm, called natural policy gradients parameter-based exploration (npgpe), computational cost similar conventional policy gradient algorithms. numerical experiments show proposed method outperforms policy gradient methods, including current state--art NAC] control-based exploration. Policy Search Framework standard reinforcement learning framework agent interacts Markov decision process. section, review estimation policy gradients describe difference controland parameter-based exploration.  Markov Decision Process Notation discrete time agent observes state selects action receives instantaneous reward resulting state transition environment. state action defined continuous spaces paper. state chosen transition probability reward randomly expectation agent advance. objective reinforcement learning agent construct policy maximizes agent performance. parameterized policy , defined probability distribution action space state parameters assume  unique well-defined stationary distribution—?). under assumption, natural performance measure infinite horizon tasks average reward ?(?) —?)  , )dads.  Policy Gradients \\x0cpolicy gradient methods update policies estimating gradient average reward.  policy parameters. state-action  ?(?  assumed , differentiable.  exact gradient average reward (see]) ?(?) —?)  , ?)?? log ? )dads. ) natural gradient] basis information geometry, studies Riemannian geometric structure manifold probability distributions. result information geometry states FIM defines Riemannian metric tensor space probability distributions] direction steepest descent Riemannian manifold natural gradient, conventional gradient premultiplied inverse matrix Riemannian metric tensor]. thus, natural gradient computed gradient fim, converge faster conventional gradient. kakade] applied natural gradient policy search; called natural policy gra?  ?(?)   ?(?) dient. FIM invertible, natural policy gradient  policy gradient premultiplied inverse matrix FIM  paper, employ FIM proposed Kakade], defined —?)  , ?)?? log , ?)?? log , dads. Figure Illustration main difference control-based exploration parameter-based exploration. controller , represented single-layer perceptron. while controlbased exploration strategy (left) perturbs resulting control signal, parameter-based exploration strategy (right) perturbs parameters controller.  Learning Samples calculation) requires knowledge underlying transition probabilities—?). GPOMDP algorithm] computes Monte Carlo approximation): agent interacts environment, producing observation, action, reward sequence ..., under mild technical assumptions, policy gradient approximation ?(?)   log  called eligibility trace], log  called characteristic eligibility], denotes discount factor  ).   estimation approaches true gradient variance increases set  log    log  ?). refore, natural policy experiments). define  gradient approximation  ?(?)    )  log  ?). estimate natural policy gradient, heuristic sug  gested Kakade  (?? log  ?)?? log  ) online estimate fim, small positive constant.  parameter-based Exploration control tasks, attempt (deterministic stochastic) controller , exploration strategy,  denotes control  parameters controller. objective learning seek suitable values parameters exploration strategy required carry stochastic sampling current parameters. typical exploration strategy model, call control-based exploration, normal distribution control space (figure1 (left)). case, action agent control, policy represented  exp   ))   )) , ?   covariance matrix agent seeks . control time generated  ),  ?). ] showed approximation error proportional??  —), sub-dominant eigenvalue Markov chain One feature Gaussian unit] agent potentially control degree exploratory behavior. control-based exploration strategy samples output controller. however, structures parameter space control space identical. refore, sampling strategy generates controls generated current controller, exploration variances decrease. this property leads large variance gradient estimates. this reason policy improvement stuck. address issue, Sehnke. ] introduced exploration strategy policy gradient methods called policy gradients parameter-based exploration (pgpe). approach, action agent parameters controller, policy represented           , exp  ?  —?—  covariance matrix agent seeks .  controller included dynamics environment, control time generated   ,     gpomdp-based methods estimate policy gradients partially observable settings.,  excludes observation current state. because exploration strategy policy, directly perturbs parameters (figure1 (right)), samples generated current parameters small exploration variances. note advantage framework gradient estimated directly sampling parameters controller, implementation policy gradient algorithms require diﬃcult derive complex controllers. sehnke. ] demonstrated PGPE yield faster convergence control-based exploration strategy challenging episodic tasks. however, parameter-based exploration higher dimension control-based one. refore, computational cost inverse calculated), natural policy gradients find limited applications. natural Policy Gradients parameter-based Exploration section, propose algorithm called natural policy gradients parameter-based exploration (npgpe) eﬃcient estimation natural policy gradient.  Implementation gaussian-based Exploration Strategy employ policy representation model—? multivariate normal distribution parameters , represents Cholesky decomposition covariance  upper triangular matrix  Sun. ] noted matrix advantages implementation: makes explicit independent parameters addition, diagonal elements square roots determining covariance matrix  refore, positive semidefinite. remainder eigenvalues text, ]-dimensional column vector consisting elements upper-right elements.,  here sub-matrix row column Inverse Fisher Information Matrix Previous natural policy gradient methods] empirical fim, estimated sample path. such methods highly ineﬃcient—?) invert empirical fim, matrix elements. avoid problem directly computing exact fim. Algorithm Natural Policy Gradient Method parameter-based Exploration require: : policy parameters, ): controller, step size, discount rate, baseline. observe Initialize ... Draw ), compute action obtain observation reward Execute   log  log  —?)     —?) {triu diag    log   —?)       )? end substituting —?) ), rewrite policy gradient obtain   wds.   ?(?) —?) —?)?  log—?   furrmore, FIM distribution   dwds  —?) —?)?  log—?)?  log—?)    .  —?)?  log—?)?  log—?) Because independent—? real fim. sun. ] proved precise FIM Gaussian distribution,  block-diagonal matrix diag ..., block identical   block        ?  vkt? denotes-dimensional column vector nonzero element element one?  ]-dimensional identity matrix. furr, Akimoto. ] derived inverse matrix diagonal block fim. because block-diagonal matrix upper triangular, easy verify inverse matrix FIM       ? ? vkt    ? ?  ? ? ) Natural Policy Gradient  log  —?) now, derive eligibility premultiplied inverse matrix FIM   log —?) manner]. characteristic eligibility.     —?)    ). log    —?)  —?)   characteristic obviously log log eligibility.     —?) vit triu diag log  npg) npg) vpg) vpg step paramter-based control-based paramter empirical optimum control return paramter-based control-based state state \\x0cfigure Performance npg) compared npg), vpg), vpg) linear quadratic regulation task averaged 100 trials. left: empirical optimum denotes return optimum gain. center right: Illustration main difference controland parameter-based exploration. sampling area state-control space (center) state-parameter space (right) plotted. triu denotes upper triangular matrix, element identical    , element orwise symmetric matrix. let dimension ); characteristic eligibility. expressed   —?) ?  diag  log According), diag vkt Cvk  Cvk?  —?) refore block  log   —?)  log  —?)  log         diag?  vkt? ?    ?  vkt  ?     log   obtain log —?) because —?)     —?) triu diag log ) refore, time complexity computing  log  log  log  log  —?)       ...,     —?). this significant imis order computation log provement current natural policy gradient estimation) parameter-based exploration, complexity note simple forms exploration distribution used. when exploration strategy represented independent normal distribution parameter natural policy gradient estimated) time. this limited form ignores relationship parameters, practical high-dimensional controllers.  Algorithm for parameterized class controllers ), exploration strategy—?). online version based GPOMDP algorithm implementation shown Algorithm  generated  , practice, parameters controller   normal random numbers. now reduce variance gradient estimation, employ variance reduction techniques] adapt reinforcement baseline Figure Simulator two-link arm robot. Experiments section, evaluate performance proposed NPGPE method. eﬃciency parameter-based exploration reported episodic tasks]. compare parameterand control-based exploration strategies natural gradient conventional ?vanilla? gradients simple continuing task linear control problem. demonstrate npgpe usefulness physically realistic locomotion task two-link arm robot simulator.  Implementation compare exploration strategies. parameterbased exploration strat? egy—?) presented Section. control-based exploration strategy—?  vector control represented normal distribution control space, generated controller represents Cholesky decomposition covariance matrix  upper triangular matrix parameters policy,  ]-dimensional column vector consisting elements upper-right elements Linear Quadratic Regulator linear control problem serve benchmark delayed reinforcement tasks]. dynamics environment     reward ?s2t u2t experiment, set states constrained lie range], truncated. when agent chooses action lie range], action executed environment truncated. controller represented ,    optimal parameter   Riccati equation. for clarification, write NPG employs natural policy gradient VPG employs ?vanilla? policy gradient. refore, npg) vpg) denote parameterbased exploration strategy, npg) vpg) denote control-based exploration strategy. our proposed NPGPE method npg). figure2 (left) shows performance compared methods. algorithm parameter-based exploration performance control-based exploration continuing task. natural policy gradient improved convergence speed, combination parameter-based exploration outperformed methods. reason acceleration learning case fact samples generated parameter-based exploration strategy effective search. figure2 (center right) show plots sampling area state-control space state-parameter space, respectively. because control-based exploration maintains sampling area control space, sampling uniform parameter space agent visits frequently. refore, parameter-based exploration realize eﬃcient sampling control-based exploration.  \\x0clocomotion Task two-link Arm Robot applied algorithm robot shown Figure3 Kimura. ]. objective learning find control rules move forward. joints controlled servo motors react npg) npg) nac) 104 105 106 step 107 102 weight weight gain return gain 103 104 105 step 106 107 102 103 104 105 step 106 107 Figure Performance npg) compared npg) nac) locomotion task averaged 100 trials. left: Mean performance compared methods. center: Parameters controller npg). right: Parameters controller npg). parameters controller normalized gain weight /gain denotes parameter joint. arrows center denote changing points relation important parameters. angular-position commands. time step, agent observes angular position motors, observation normalized], selects action. reward distance body movement caused previous action. when robot moves backward, agent receives negative reward. state vector expressed control motor generated exp(?  )). dimension parameters policies parameterand control-based exploration strategy, respectively. compared npg., npgpe, npg) nac). nac state--art policy gradient algorithm] combines natural policy gradients, actor-critic framework, leastsquares temporal-difference-learning. nac computes inverse matrix estimate natural steepest ascent direction. because nac(d3w time complexity iteration, prohibitively expensive, apply NAC control-based exploration. figure4 (left) shows results. initially, npg) outperformed nac); however, reaches good solutions fewer steps. furrmore, stage, nac) matches npg). figure4 (center right) show path relation parameters controller. npg) slower npg) adapt relation early stage; however, seek relations important parameters (indicated arrows figures) faster, npg) stuck ineﬃcient sampling. Conclusions This paper proposed natural policy gradient method combined parameter-based exploration cope high-dimensional reinforcement learning domains. proposed algorithm, npgpe, simple quickly calculates estimation natural policy gradient. moreover, experimental results demonstrate significant improvement control domain. future works focus developing actor-critic versions NPGPE encourage performance improvements early stage, combining gradient methods natural conjugate gradient methods]. addition, comparison direct parameter perturbation methods finite difference gradient methods], cma], NES] gain understanding properties eﬃcacy combination parameter-based exploration strategies natural policy gradient. furrmore, application algorithm real-world problems required assess utility. acknowledgments This work suported Japan Society Promotion Science 9031).',\n",
       " 'PP4002': 'real-life relational datasets, approach matches exceeds state art accuracy dense models, time order magnitude speedup Conditional random fields (crfs]) successful modeling complex systems, applications speech tagging] heart motion abnormality detection]. key advantage CRFs probabilistic graphical models (pgms]) stems observation applications, variables unknown test time denote variables ors, called evidence test time. PGM formulations model joint distribution), CRFs directly model conditional distributions). discriminative approach adopted CRFs approximation quality learned conditional distribution), representational power model ?wasted? modeling). however, approximation cost increased computational complexity structure] parameter learning] compared generative models. particular, unlike Bayesian networks junction trees) likelihood CRF structure decompose combination small subcomponent scores, making existing approaches structure learning inapplicable, and) computing optimal parameters closed form, CRFs resort gradient-based methods. moreover, computing gradient log-likelihood respect CRF parameters requires inference current model \\x0cevery training datapoint. high-treewidth models, approximate inference-hard]. overcome extra computational challenges posed conditional random fields, practitioners resort approximations process:    crf structure hand, leading suboptimal structures. approximate inference parameter learning results suboptimal parameters. approximate inference test time results suboptimal results]. replacing CRF conditional likelihood objective tractable. ]) results suboptimal models (both terms learned structure parameters). approximation techniques lack quality guarantees, combining system serves furr compound errors. well-known avoid approximations CRF parameter learning restrict models low treewidth, dependencies variables tree-like structure. models, parameter learning inference exactly1 structure learning involves approximations. important dependencies variables however, captured single tree-like structure, low-treewidth CRFs rarely practice. paper, argue commitment single CRF structure irrespective evidence makes tree-like CRFs inferior option. show tree CRFs evidence-dependent structure, learned generalization chow-liu algorithm) yield results equal significantly densely-connected CRFs real-life datasets) order magnitude faster dense models. specifically, contributions follows:     formally define CRFs evidence-specific) structure. observe that, structures, CRF feature weights learned exactly. generalize chow-liu algorithm] learn evidence-specific structures tree crfs. generalize tree CRFs evidence-specific structure (ess-crfs) relational setting. demonstrate empirically superior performance ess-crfs densely connected models terms accuracy runtime real-life relational models. conditional random fields conditional random field pairwise features2 defines conditional dis tribution) exp wijk fijk functions called features, feature weights) normalization constant (which depends evidence), set edges model. reﬂect fact depends weights write). apply CRF model, defines set features typical feature pixels image segment tend similar colors  —colori ?colorj (?) indicator function. features \\x0cand training data consists fully observed assignments optimal feature weights maximize conditional log-likelihood (cllh) data:   wijk fijk) logz))?. ?= arg max logp) arg max problem) closed form solution, unique global optimum found gradient-based optimization technique fact]: Fact Conditional log-likelihood), abbreviated cllh, concave moreover, log fijk) ) [fijk)] ?wijk denotes expectation respect distribution) Convexity negative CLLH objective closed-form expression gradient lets convex optimization techniques-bfgs] find unique optimum  however, gradient) conditional distribution computing) requires inference model datapoint. time complexity exact inference exponential treewidth graph defined edges]. refore, exact evaluation CLLH objective)and gradient) exact inference test time feasible models low-treewidth unfortunately, restricting space models low treewidth severely decreases expressive power crfs. complex dependencies real-life distributions adequately captured single tree-like structure, models practice high treewidth, making exact inference infeasible. instead, approximate inference techniques, Here rest paper, ?exact parameter learning? ?with arbitrary accuracy polynomial time? standard convex optimization techniques. contrast closed form exact parameter learning generative low-treewidth models representing joint distribution). paper, case pairwise dependencies, features depend variables (but depend arbitrary variables). approach principle extended CRFs higher order dependencies, chow-liu algorithm structure learning replaced algorithm learns low-treewidth junction trees]. belief propagation] sampling] parameter learning test time. approximate inference-hard], approximate inference algorithms result quality guarantees. greater expressive power models obtained expense worse quality estimated parameters inference. here, show alternative increase expressive power tree-like structured CRFs sacrificing optimal weights learning exact inference test time. practice, approach suited relational propositional settings, higher parameters dimensionality propositional case. however, present detail propositional case ory convey key \\x0chigh-level ideas. evidence-specific structure CRFs Observe that, evidence set edges CRF formulation) viewed supergraph conditional model edge, ?disabled? sense: edge features identically zero, frsk (xrx values ofx wijk fijk wijk fijk) evidence model) edges equivalent) removed notion effective CRF structure, captures extra sparsity: Definition Given CRF model) evidence effective conditional model structure set edges features identically zero, . fijk ) low treewidth values inference parameter learning effective structure tractable, priori structure high treewidth. unfortunately, practice treewidth) smaller treewidth low-treewidth effective structures rarely used, treewidth global property graph (even computing treewidth-complete]), feature design local process. fact, ability learn optimal weights set mutually correlated features understanding inter-feature dependencies key advantage CRFs PGM formulations. achieving low treewidth effective structures requires elaborate feature design, making model construction diﬃcult. instead, work, separate construction low-treewidth effective structures feature design weight learning, combine advantages exact inference discriminative weights learning, high expressive power high-treewidth models, local feature design. observe CRF definition) written equivalently, exp wijk ,  fijk)) ) Even) equivalent) structure model explicitly encoded multiplicative component features. addition feature values effective structure model controlled indicator functions(?). indicator functions provide control treewidth effective structures independently features. traditionally, assumed priori structure CRF model fixed. however, assumption necessary. work, assume structure determined evidence parameters). resulting model, call CRF evidence-specific structure (ess-crf), defines conditional distribution \\x0cfollows) exp wijk, )) fijk)) ) dependence structure forms. provide algorithm constructing evidence-specific CRF structures shortly. ess-crfs important advantage traditional parametrization) parameters determine model structure decoupled feature weights result, problem structure learning., optimizing decoupled feature selection (choosing feature weights learning (optimizing). decoupling makes easier guarantee effective structure model low treewidth relegating global computation structure construction algorithm). fixed choice structure construction algorithm (?, structure parameters long (?, guaranteed return low-treewidth structures, learning optimal feature weights inference test time exactly, Fact directly extends feature weights ess-crfs: Algorithm Standard CRF approach Define features fijk), implicitly defining high-treewidth CRF structure Optimize weights maximize conditional LLH) training data. approximate inference compute CLLH objective) gradient). foreach test data Use conditional model) define conditional distribution). approximate inference compute marginals assignment algorithm CRF evidence-specific structures approach Define features fijk). choose structure learning alg. , guaranteed return low-treewidth structures. define learn data parameters structure construction algorithm (?, ?). optimize weights maximize conditional LLH log training data. exact inference compute CLLH objective) gradient). foreach test data Use conditional model) define conditional distribution). exact inference compute marginals assignment observation Conditional log-likelihood logp) ess-crfs) concave also, logp, )) fijk) [fijk)] ) ?wijk summarize, standard CRF workﬂow (alg. ), propose ess-crfs (alg. ). standard approach approximations (with little, any, guarantees result quality) stage (lines), ess-crf approach structure selection (line involves approximation. next, present simple effective algorithm learning evidence-specific tree structures, based existing algorithm generative models. existing structure learning algorithms similarly adapted learn evidence-specific models higher treewidth. conditional chow-liu algorithm tractable evidence-specific structures Learning PGM structure data cases intractable. Markov random fields (mrfs), special case CRFs evidence, learning structure-hard. ]). however, simple class mrfs, tree-structured models, eﬃcient algorithm exists] finds structure. section, adapt algorithm (called chow-liu algorithm) learning evidence-specific structures crfs. pairwise Markov random fields graphical models define distribution normalized product low-dimensional potentials:   Notice pairwise MRFs special case CRFs fij log wij unlike tree crfs, however, likelihood tree MRF structures decomposes contributions individual edges: llh (?, mutual information(?) entropy. refore, shown], structure obtained taking maximum spanning tree fully connected graph, weight edge pairwise marginals low dimensionality, marginals mutual informations estimated data accurately, makes chow-liu algorithm learning tree-structured models. concrete evidence write conditional version tree structure likelihood) evidence: llh   exact conditional distributions available, chow-liu algorithm find optimal conditional structure. unfortunately, estimating conditional distributions fixed accuracy general requires amount data exponential dimensionality]. however, plug approximate conditionals(? learned Algorithm Conditional chow-liu algorithm learning evidence-specific tree structures Parameter learning stage.  found. -bfgs(?) ) log uij foreach  arg max)?dtrain Constructing structures test time foreach dtest foreach set edge weight rij ipb? ,  maximum spanning tree, algorithm Relational ess-crf algorithm parameter learning stage learn structure parameters conditional chow-liu algorithm (alg. let defined)  find. -bfgs gradient) arg maxw log data standard density estimation technique3 particular, features fijk CRF model, train logistic regression model(?  uij Zij, uij exp uijk fijk ) \\x0cessentially, logistic regression model small CRF variables. exact optimal weights found eﬃciently standard convex optimization techniques. resulting evidence-specific structure learning algorithm, summarized Alg alg returns tree, quality estimators), quality resulting structures. importantly, alg. means choice ess-crf approach. edge scores. ], edge selection procedures. ] higher treewidth junction trees, components chow-liu algorithm alg.  relational CRFs evidence-specific sructure Traditional (also called propositional) PGMs suited dealing relational data, variable entity type, entities related types links. usually, entity types link types. example, webpages internet linked hyperlinks, social networks link people friendship relationships. relational data violates. data assumption traditional pgms, huge dimensionalities relational datasets preclude learning meaningful propositional models. instead, formulations relational PGMs proposed] work relational data, including relational crfs. key property formulations model defined template potentials defined abstract level variable types replicated concrete entities. concretely, relational CRFs variable assigned type set types. binary relation specific type link variables, specifies types input arguments, set features fkr (?, feature weights wkr write inst, types match input types relation link type data (for example, hyperlink webpages). conditional distribution generalized propositional CRF) copying template potentials instance relation, exp ?inst Observe meaningful difference relational CRF) propositional formulation) shares parameters edges. accounting parameter sharing, straightforward adapt ess-crf formulation relational setting. define relationaless-crf conditional distribution) exp,  ?inst \\x0cnotice approximation error(?) source proximations approach. chow?liu CRF ess?crf Train set size chow?liu CRF ess?crf ess?crf structure reg. classification error Test LLH Test LLH ess?crf structure reg.  WebKB classification Error TRAFFIC TEMPERATURE SVM RMN ess?crf M3N Train set size Figure left: test LLH temperature. middle: traffic. right: classification errors webkb. structure learning algorithm (?, guaranteed return low-treewidth structures, learn optimal feature weights perform inference test time exactly: Observation Relational ess-crf log-likelihood concave respect moreover, ?logp) fkr) ) fkr)  )) ?wkr ?inst Conditional chow-liu algorithm (alg. extended relational setting templated logistic regression weights estimating edge conditionals. resulting algorithm shown alg.  observe test phase alg. alg.  relational setting, learn—) parameters, dataset size, structure selection feature weights, opposed parameters propositional case. thus, relational ess-crfs typically prone overfitting propositional ones. experiments tested ess-crf approach propositional relational data. large number parameters needed propositional case )), approach practical cases abundant data. experiments propositional data serve prove concept, verifying ess-crf successfully learn model single tree baseline. contrast propositional settings, relational cases low parameter space dimensionality eliminates overfitting problem. result, relational datasets ess-crf attractive approach practice. experiments show ess-crfs comfortably outperforming state art high-treewidth discriminative models real-life relational datasets.  Propositional models compare ess-crfs fixed tree crfs, tree structure learned chow-liu algorithm temperature sensor network data discretized variables) San Francisco TRAFFIC data selected variables). cases, variables evidence rest unknowns results fig.  found regularize conditional chow-liu (alg. choosing test time edges selected training. fig. plot results regularized (red) unregularized (blue). limit plentiful data ess-crf outperform fixed tree baseline. however, space models larger ess-crf, overfitting important issue regularization important.  Relational models Face recognition. evaluate ess-crfs relational models. model, called faces, aims improve face recognition collections related images information similarity faces addition standard single-face features. key idea people images similar, person. model variable denoting label, face blob. pairwise features), based blob color similarity, close faces appearance. single-variable features encode information output off--shelf standalone face classifier face location image (see] details). model semi-supervised way: test time, PGM instantiated jointly train test entities, values train entities fixed ground truth, inference finds (approximately) labels test entities. faces accuracy FACES accuracy mln+ sum ess?crf mln+ max Accuracy ess?crf Accuracy FACES accuracy mln+ max mln+ sum MLN sum Accuracy mln+ ess?crf MLN MLN max MLN sum time, seconds 2500 1000 1500 2000 time, seconds FACES time CONVERGENCE mln+ sum mln+ max MLN sum MLN max 1500 500 ess?crf 300 Inference 200 Inference 150 100 mln+ sum mln+ max MLN sum Parameter learning ess?crf Parameter learning time, seconds FACES time CONVERGENCE MLN max 250 2000 1000 100 time, seconds FACES time CONVERGENCE time, seconds 3000 MLN max 500 time, seconds mln+ sum MLN sum MLN max mln+ max Inference Parameter learning ess?crf Figure Results FACES datasets. top: evolution classification accuracy inference progresses time. stars show moment ESSCRF finishes running. horizontal dashed line resulting accuracy. FACES sum-product max-product gave accuracy. bottom: time convergence. compare ess-crfs dense relational PGM encoded Markov logic network (mln]) features. state art MLN implementations Alchemy package-sat sampling algorithm discriminative parameter learning, belief propagation] inference. mln, threshold pairwise features indicating likelihood label agreement set threshold prevent) oversmoothing) long inference times. also, prevent oversmoothing mln, found scale pairwise feature weights learned training, weakening smoothing effect single edge model4 denote models adjusted weights mln+. thresholding weights adjustment ess-crfs. figure shows results separate datasets: FACES 1720 images, unique people 100 training images fold, FACES 245 images, unique people training images, FACES 352 images, unique people training images. sumproduct maxproduct inference, denoted sum max correspondingly fig.  ess-crf choice made difference. ) ess-crf model superior (faces equal (faces accuracy dense MLN model, extra heuristic weights tweaking mln) ess-crf order magnitude faster. FACES model, ess-crf superior high-treewidth alternative. hypertext data. WebKB data (see] details), task label webpages computer science departments course, faculty, student, project, text link structure. compare ess-crfs high-treewidth relational Markov networks (rmns]), max-margin Markov \\x0cnetworks (m3ns]) standalone SVM classifier. relational PGMs single-variable features encoding webpage text, pairwise features encoding link structure. baseline SVM classifier single-variable features. rmns ess-crfs trained maximize conditional likelihood labels, M3Ns maximize margin likelihood correct assignment incorrect ones, explicitly targeting classification. results fig.  observe ess-crf matches accuracy high-treewidth rmns, showing smaller expressive power tree models fully compensated exact parameter learning inference. ess-crf faster rmn, taking sec. train sec. test single core.7ghz Opteron cpu. rmn M3N models 1500 sec. train 700mhz Pentium iii. accounting CPU speed difference, speedup significant. ess-crf achieve accuracy m3ns, objective directly related classification problem opposed density estimation. still, RMN results match M3N accuracy faster tractable ESS models replacing CRF conditional likelihood objective max-margin objective, important direction future work. because number pairwise relations model grows quadratically number variables, ?per-variable force smoothing? grows dataset size, adjust. Related work conclusions Related work. cornerstones ess-crf approach, models sparse evidence instantiated, multiple tractable models avoid restrictions expressive power inherent low-treewidth models, discussed existing literature. first, context-specific independence (csi]) long speeding inference] regularizing model parameters]. however, CSI treated local property model, made reasoning resulting treewidth evidencespecific models impossible. thus, full potential exact inference models CSI remained unused. work step fully exploiting potential. multiple tractable models, trees, widely components mixtures. ]), including mixtures trees], approximate distributions rich inherent structure. unlike mixture models, approach selecting single structure evidence advantage allowing eﬃcient exact decoding probable assignment unknowns Viterbi algorithm]. mixture models approach, joint optimization structure weights notation) infeasible due local optima objective. one-shot structure learning algorithm, empirically demonstrated, works practice. faster expectation maximization] standard train mixture models. learning CRF structure general-hard, hardness results generative models. ]). moreover, CRF structure \\x0clearning furr complicated fact CRF structure likelihood decompose scores local graph components, scores generative models]. existing work CRF structure learning local guarantees. practice, hardness CRF structure learning leads high popularity heuristics: chain skip-chain] structures used, grid-like structures. approaches learn structure data broadly divided categories. first, CRF structure defined sparsity pattern feature weights, regularization penalty achieve sparsity weight learning]. type approaches greedily adds features CRF model maximize improvement (approximate) model likelihood. ]). finally, approximate CRF structure score combination local scores, algorithm learning generative structures (where score decomposes). ess-crf falls category approaches. negative oretical results learnability simplest CRF structures local scores], approaches work practice]. learning weights straightforward tractable crfs, log-likelihood concave] gradient) mature convex optimization techniques. far, exact weights learning special hand-crafted structures, chains], work arbitrary trees. dense structures, computing gradient) intractable approximate inference general models-hard]. result, approximate inference techniques, belief propagation] Gibbs sampling] employed, guarantees quality result. alternatively, approximation objective. ]) used, yielding suboptimal weights. experiments showed exact weight learning tractable models advantage approximation quality eﬃciency dense structures. conclusions future work. summarize, shown propositional relational settings, tractable CRFs evidence-specific structures, class models expressive power greater single tree-structured model, constructed relying globally optimal results eﬃcient algorithms (logistic regression, chow-liu algorithm, exact inference tree-structured models-bfgs convex differentiable functions). traditional CRF workﬂow (alg. involves approximation quality guaranteed multiple stages process, approach, ess-crf (alg. ), source approximation, conditional structure scores. demonstrated real-life relational datasets approach matches exceeds accuracy state art dense discriminative models, time provide factor magnitude speedup. important future work directions generalizing ess-crf larger treewidths maxmargin weights learning classification. acknowledgements. work supported NSF Career iis-0644225 ARO MURI W911NF0710287 w911nf0810242. ben Taskar sharing WebKB data. faces model data developed jointly Denver Dash Matthai philipose. real-life relational datasets, approach matches exceeds state art accuracy dense models, time order magnitude speedup Conditional random fields (crfs]) successful modeling complex systems, applications speech tagging] heart motion abnormality detection]. key advantage CRFs probabilistic graphical models (pgms]) stems observation applications, variables unknown test time denote variables ors, called evidence test time. while PGM formulations model joint distribution), CRFs directly model conditional distributions). discriminative approach adopted CRFs approximation quality learned conditional distribution), representational power model ?wasted? modeling). however, approximation cost increased computational complexity structure] parameter learning] compared generative models. particular, unlike Bayesian networks junction trees) likelihood CRF structure decompose combination small subcomponent scores, making existing approaches structure learning inapplicable, and) computing optimal parameters closed form, CRFs resort gradient-based methods. moreover, computing gradient log-likelihood respect CRF parameters requires inference current model \\x0cevery training datapoint. for high-treewidth models, approximate inference-hard]. overcome extra computational challenges posed conditional random fields, practitioners resort approximations process:    crf structure hand, leading suboptimal structures. approximate inference parameter learning results suboptimal parameters. approximate inference test time results suboptimal results]. replacing CRF conditional likelihood objective tractable. ]) results suboptimal models (both terms learned structure parameters). not approximation techniques lack quality guarantees, combining system serves furr compound errors. well-known avoid approximations CRF parameter learning restrict models low treewidth, dependencies variables tree-like structure. for models, parameter learning inference exactly1 structure learning involves approximations. important dependencies variables however, captured single tree-like structure, low-treewidth CRFs rarely practice. paper, argue commitment single CRF structure irrespective evidence makes tree-like CRFs inferior option. show tree CRFs evidence-dependent structure, learned generalization chow-liu algorithm) yield results equal significantly densely-connected CRFs real-life datasets) order magnitude faster dense models. more specifically, contributions follows:     Formally define CRFs evidence-specific) structure. observe that, structures, CRF feature weights learned exactly. generalize chow-liu algorithm] learn evidence-specific structures tree crfs. generalize tree CRFs evidence-specific structure (ess-crfs) relational setting. demonstrate empirically superior performance ess-crfs densely connected models terms accuracy runtime real-life relational models. conditional random fields conditional random field pairwise features2 defines conditional dis tribution) exp wijk fijk functions called features, feature weights) normalization constant (which depends evidence), set edges model. reﬂect fact depends weights write). apply CRF model, defines set features typical feature pixels image segment tend similar colors  —colori ?colorj (?) indicator function. given features \\x0cand training data consists fully observed assignments optimal feature weights maximize conditional log-likelihood (cllh) data:   wijk fijk) logz))?. ?= arg max logp) arg max problem) closed form solution, unique global optimum found gradient-based optimization technique fact]: Fact Conditional log-likelihood), abbreviated cllh, concave moreover, log fijk) ) [fijk)] ?wijk denotes expectation respect distribution) Convexity negative CLLH objective closed-form expression gradient lets convex optimization techniques-bfgs] find unique optimum  however, gradient) conditional distribution computing) requires inference model datapoint. time complexity exact inference exponential treewidth graph defined edges]. refore, exact evaluation CLLH objective)and gradient) exact inference test time feasible models low-treewidth unfortunately, restricting space models low treewidth severely decreases expressive power crfs. complex dependencies real-life distributions adequately captured single tree-like structure, models practice high treewidth, making exact inference infeasible. instead, approximate inference techniques, Here rest paper, ?exact parameter learning? ?with arbitrary accuracy polynomial time? standard convex optimization techniques. this contrast closed form exact parameter learning generative low-treewidth models representing joint distribution). paper, case pairwise dependencies, features depend variables (but depend arbitrary variables). our approach principle extended CRFs higher order dependencies, chow-liu algorithm structure learning replaced algorithm learns low-treewidth junction trees]. belief propagation] sampling] parameter learning test time. approximate inference-hard], approximate inference algorithms result quality guarantees. greater expressive power models obtained expense worse quality estimated parameters inference. here, show alternative increase expressive power tree-like structured CRFs sacrificing optimal weights learning exact inference test time. practice, approach suited relational propositional settings, higher parameters dimensionality propositional case. however, present detail propositional case ory convey key \\x0chigh-level ideas. evidence-specific structure CRFs Observe that, evidence set edges CRF formulation) viewed supergraph conditional model edge, ?disabled? sense: edge features identically zero, frsk (xrx values ofx wijk fijk wijk fijk) evidence model) edges equivalent) removed notion effective CRF structure, captures extra sparsity: Definition Given CRF model) evidence effective conditional model structure set edges features identically zero, . fijk ) low treewidth values inference parameter learning effective structure tractable, priori structure high treewidth. unfortunately, practice treewidth) smaller treewidth low-treewidth effective structures rarely used, treewidth global property graph (even computing treewidth-complete]), feature design local process. fact, ability learn optimal weights set mutually correlated features understanding inter-feature dependencies key advantage CRFs PGM formulations. achieving low treewidth effective structures requires elaborate feature design, making model construction diﬃcult. instead, work, separate construction low-treewidth effective structures feature design weight learning, combine advantages exact inference discriminative weights learning, high expressive power high-treewidth models, local feature design. observe CRF definition) written equivalently, exp wijk ,  fijk)) ) Even) equivalent) structure model explicitly encoded multiplicative component features. addition feature values effective structure model controlled indicator functions(?). indicator functions provide control treewidth effective structures independently features. traditionally, assumed priori structure CRF model fixed. however, assumption necessary. work, assume structure determined evidence parameters). resulting model, call CRF evidence-specific structure (ess-crf), defines conditional distribution \\x0cfollows) exp wijk, )) fijk)) ) dependence structure forms. provide algorithm constructing evidence-specific CRF structures shortly. ess-crfs important advantage traditional parametrization) parameters determine model structure decoupled feature weights result, problem structure learning., optimizing decoupled feature selection (choosing feature weights learning (optimizing). such decoupling makes easier guarantee effective structure model low treewidth relegating global computation structure construction algorithm). for fixed choice structure construction algorithm (?, structure parameters long (?, guaranteed return low-treewidth structures, learning optimal feature weights inference test time exactly, Fact directly extends feature weights ess-crfs: Algorithm Standard CRF approach Define features fijk), implicitly defining high-treewidth CRF structure Optimize weights maximize conditional LLH) training data. use approximate inference compute CLLH objective) gradient). foreach test data Use conditional model) define conditional distribution). use approximate inference compute marginals assignment algorithm CRF evidence-specific structures approach Define features fijk). choose structure learning alg. , guaranteed return low-treewidth structures. define learn data parameters structure construction algorithm (?, ?). Optimize weights maximize conditional LLH log training data. use exact inference compute CLLH objective) gradient). foreach test data Use conditional model) define conditional distribution). use exact inference compute marginals assignment observation Conditional log-likelihood logp) ess-crfs) concave also, logp, )) fijk) [fijk)] ) ?wijk summarize, standard CRF workﬂow (alg. ), propose ess-crfs (alg. ). standard approach approximations (with little, any, guarantees result quality) stage (lines), ess-crf approach structure selection (line involves approximation. next, present simple effective algorithm learning evidence-specific tree structures, based existing algorithm generative models. many existing structure learning algorithms similarly adapted learn evidence-specific models higher treewidth. Conditional chow-liu algorithm tractable evidence-specific structures Learning PGM structure data cases intractable. even Markov random fields (mrfs), special case CRFs evidence, learning structure-hard. ]). however, simple class mrfs, tree-structured models, eﬃcient algorithm exists] finds structure. section, adapt algorithm (called chow-liu algorithm) learning evidence-specific structures crfs. pairwise Markov random fields graphical models define distribution normalized product low-dimensional potentials:   Notice pairwise MRFs special case CRFs fij log wij unlike tree crfs, however, likelihood tree MRF structures decomposes contributions individual edges: llh (?, mutual information(?) entropy. refore, shown], structure obtained taking maximum spanning tree fully connected graph, weight edge pairwise marginals low dimensionality, marginals mutual informations estimated data accurately, makes chow-liu algorithm learning tree-structured models. given concrete evidence write conditional version tree structure likelihood) evidence: llh   exact conditional distributions available, chow-liu algorithm find optimal conditional structure. unfortunately, estimating conditional distributions fixed accuracy general requires amount data exponential dimensionality]. however, plug approximate conditionals(? learned Algorithm Conditional chow-liu algorithm learning evidence-specific tree structures Parameter learning stage.  found. -bfgs(?) ) log uij foreach  arg max)?dtrain Constructing structures test time foreach dtest foreach set edge weight rij ipb? ,  maximum spanning tree, Algorithm Relational ess-crf algorithm parameter learning stage Learn structure parameters conditional chow-liu algorithm (alg. Let defined)  Find. -bfgs gradient) arg maxw log data standard density estimation technique3 particular, features fijk CRF model, train logistic regression model(?  uij Zij, uij exp uijk fijk ) \\x0cessentially, logistic regression model small CRF variables. exact optimal weights found eﬃciently standard convex optimization techniques. resulting evidence-specific structure learning algorithm, summarized Alg alg returns tree, quality estimators), quality resulting structures. importantly, alg. means choice ess-crf approach. edge scores. ], edge selection procedures. ] higher treewidth junction trees, components chow-liu algorithm alg.  Relational CRFs evidence-specific sructure Traditional (also called propositional) PGMs suited dealing relational data, variable entity type, entities related types links. usually, entity types link types. for example, webpages internet linked hyperlinks, social networks link people friendship relationships. relational data violates. data assumption traditional pgms, huge dimensionalities relational datasets preclude learning meaningful propositional models. instead, formulations relational PGMs proposed] work relational data, including relational crfs. key property formulations model defined template potentials defined abstract level variable types replicated concrete entities. more concretely, relational CRFs variable assigned type set types. binary relation specific type link variables, specifies types input arguments, set features fkr (?, feature weights wkr write inst, types match input types relation link type data (for example, hyperlink webpages). conditional distribution generalized propositional CRF) copying template potentials instance relation, exp ?inst Observe meaningful difference relational CRF) propositional formulation) shares parameters edges. accounting parameter sharing, straightforward adapt ess-crf formulation relational setting. define relationaless-crf conditional distribution) exp,  ?inst \\x0cnotice approximation error(?) source proximations approach. chow?liu CRF ess?crf Train set size chow?liu CRF ess?crf ess?crf structure reg. Classification error Test LLH Test LLH ess?crf structure reg.  WebKB classification Error TRAFFIC TEMPERATURE SVM RMN ess?crf M3N Train set size Figure left: test LLH temperature. middle: traffic. right: classification errors webkb. given structure learning algorithm (?, guaranteed return low-treewidth structures, learn optimal feature weights perform inference test time exactly: Observation Relational ess-crf log-likelihood concave respect moreover, ?logp) fkr) ) fkr)  )) ?wkr ?inst Conditional chow-liu algorithm (alg. extended relational setting templated logistic regression weights estimating edge conditionals. resulting algorithm shown alg.  observe test phase alg. alg.  relational setting, learn—) parameters, dataset size, structure selection feature weights, opposed parameters propositional case. thus, relational ess-crfs typically prone overfitting propositional ones. experiments tested ess-crf approach propositional relational data. with large number parameters needed propositional case )), approach practical cases abundant data. experiments propositional data serve prove concept, verifying ess-crf successfully learn model single tree baseline. contrast propositional settings, relational cases low parameter space dimensionality eliminates overfitting problem. result, relational datasets ess-crf attractive approach practice. our experiments show ess-crfs comfortably outperforming state art high-treewidth discriminative models real-life relational datasets.  Propositional models compare ess-crfs fixed tree crfs, tree structure learned chow-liu algorithm TEMPERATURE sensor network data discretized variables) San Francisco TRAFFIC data selected variables). cases, variables evidence rest unknowns results fig.  found regularize conditional chow-liu (alg. choosing test time edges selected training. fig. plot results regularized (red) unregularized (blue). one limit plentiful data ess-crf outperform fixed tree baseline. however, space models larger ess-crf, overfitting important issue regularization important.  Relational models Face recognition. evaluate ess-crfs relational models. model, called faces, aims improve face recognition collections related images information similarity faces addition standard single-face features. key idea people images similar, person. our model variable denoting label, face blob. pairwise features), based blob color similarity, close faces appearance. single-variable features encode information output off--shelf standalone face classifier face location image (see] details). model semi-supervised way: test time, PGM instantiated jointly train test entities, values train entities fixed ground truth, inference finds (approximately) labels test entities. FACES accuracy FACES accuracy mln+ sum ess?crf mln+ max Accuracy ess?crf Accuracy FACES accuracy mln+ max mln+ sum MLN sum Accuracy mln+ ess?crf MLN MLN max MLN sum time, seconds 2500 1000 1500 2000 time, seconds FACES time CONVERGENCE mln+ sum mln+ max MLN sum MLN max 1500 500 ess?crf 300 Inference 200 Inference 150 100 mln+ sum mln+ max MLN sum Parameter learning ess?crf Parameter learning time, seconds FACES time CONVERGENCE MLN max 250 2000 1000 100 time, seconds FACES time CONVERGENCE time, seconds 3000 MLN max 500 time, seconds mln+ sum MLN sum MLN max mln+ max Inference Parameter learning ess?crf Figure Results FACES datasets. top: evolution classification accuracy inference progresses time. stars show moment ESSCRF finishes running. horizontal dashed line resulting accuracy. for FACES sum-product max-product gave accuracy. bottom: time convergence. compare ess-crfs dense relational PGM encoded Markov logic network (mln]) features. state art MLN implementations Alchemy package-sat sampling algorithm discriminative parameter learning, belief propagation] inference. for mln, threshold pairwise features indicating likelihood label agreement set threshold prevent) oversmoothing) long inference times. also, prevent oversmoothing mln, found scale pairwise feature weights learned training, weakening smoothing effect single edge model4 denote models adjusted weights mln+. thresholding weights adjustment ess-crfs. figure shows results separate datasets: FACES 1720 images, unique people 100 training images fold, FACES 245 images, unique people training images, FACES 352 images, unique people training images. sumproduct maxproduct inference, denoted sum max correspondingly fig.  for ess-crf choice made difference. one) ess-crf model superior (faces equal (faces accuracy dense MLN model, extra heuristic weights tweaking mln) ess-crf order magnitude faster. one FACES model, ess-crf superior high-treewidth alternative. hypertext data. for WebKB data (see] details), task label webpages computer science departments course, faculty, student, project, text link structure. compare ess-crfs high-treewidth relational Markov networks (rmns]), max-margin Markov \\x0cnetworks (m3ns]) standalone SVM classifier. all relational PGMs single-variable features encoding webpage text, pairwise features encoding link structure. baseline SVM classifier single-variable features. rmns ess-crfs trained maximize conditional likelihood labels, M3Ns maximize margin likelihood correct assignment incorrect ones, explicitly targeting classification. results fig.  observe ess-crf matches accuracy high-treewidth rmns, showing smaller expressive power tree models fully compensated exact parameter learning inference. ess-crf faster rmn, taking sec. train sec. test single core.7ghz Opteron cpu. rmn M3N models 1500 sec. train 700mhz Pentium iii. even accounting CPU speed difference, speedup significant. ess-crf achieve accuracy m3ns, objective directly related classification problem opposed density estimation. still, RMN results match M3N accuracy faster tractable ESS models replacing CRF conditional likelihood objective max-margin objective, important direction future work. Because number pairwise relations model grows quadratically number variables, ?per-variable force smoothing? grows dataset size, adjust. Related work conclusions Related work. two cornerstones ess-crf approach, models sparse evidence instantiated, multiple tractable models avoid restrictions expressive power inherent low-treewidth models, discussed existing literature. first, context-specific independence (csi]) long speeding inference] regularizing model parameters]. however, CSI treated local property model, made reasoning resulting treewidth evidencespecific models impossible. thus, full potential exact inference models CSI remained unused. our work step fully exploiting potential. multiple tractable models, trees, widely components mixtures. ]), including mixtures trees], approximate distributions rich inherent structure. unlike mixture models, approach selecting single structure evidence advantage allowing eﬃcient exact decoding probable assignment unknowns Viterbi algorithm]. both mixture models approach, joint optimization structure weights notation) infeasible due local optima objective. our one-shot structure learning algorithm, empirically demonstrated, works practice. faster expectation maximization] standard train mixture models. learning CRF structure general-hard, hardness results generative models. ]). moreover, CRF structure \\x0clearning furr complicated fact CRF structure likelihood decompose scores local graph components, scores generative models]. existing work CRF structure learning local guarantees. practice, hardness CRF structure learning leads high popularity heuristics: chain skip-chain] structures used, grid-like structures. all approaches learn structure data broadly divided categories. first, CRF structure defined sparsity pattern feature weights, regularization penalty achieve sparsity weight learning]. type approaches greedily adds features CRF model maximize improvement (approximate) model likelihood. ]). finally, approximate CRF structure score combination local scores, algorithm learning generative structures (where score decomposes). ess-crf falls category approaches. although negative oretical results learnability simplest CRF structures local scores], approaches work practice]. learning weights straightforward tractable crfs, log-likelihood concave] gradient) mature convex optimization techniques. far, exact weights learning special hand-crafted structures, chains], work arbitrary trees. for dense structures, computing gradient) intractable approximate inference general models-hard]. result, approximate inference techniques, belief propagation] Gibbs sampling] employed, guarantees quality result. alternatively, approximation objective. ]) used, yielding suboptimal weights. our experiments showed exact weight learning tractable models advantage approximation quality eﬃciency dense structures. conclusions future work. summarize, shown propositional relational settings, tractable CRFs evidence-specific structures, class models expressive power greater single tree-structured model, constructed relying globally optimal results eﬃcient algorithms (logistic regression, chow-liu algorithm, exact inference tree-structured models-bfgs convex differentiable functions). whereas traditional CRF workﬂow (alg. involves approximation quality guaranteed multiple stages process, approach, ess-crf (alg. ), source approximation, conditional structure scores. demonstrated real-life relational datasets approach matches exceeds accuracy state art dense discriminative models, time provide factor magnitude speedup. important future work directions generalizing ess-crf larger treewidths maxmargin weights learning classification. acknowledgements. this work supported NSF Career iis-0644225 ARO MURI W911NF0710287 w911nf0810242. Ben Taskar sharing WebKB data. faces model data developed jointly Denver Dash Matthai philipose.',\n",
       " 'PP4045': 'data clustering fundamental problem fields, machine learning, data mining computer vision]. unfortunately, universally accepted definition cluster, diverse forms clusters real applications. generally agreed objects belonging cluster satisfy internal coherence condition, objects belonging cluster satisfy condition. existing clustering methods partition-based-means], spectral clustering, aﬃnity propagation]. methods implicitly share assumption: data point belong cluster. assumption greatly simplifies problem, judge wher data point outlier not, challenging. however, assumption results bad performance methods exists large number outliers, \\x0cfrequently met real-world applications. criteria judge wher objects belong cluster typically expressed pairwise relations, encoded weights aﬃnity graph. however, applications, high order relations appropriate, choice, naturally results hyperedges hypergraphs. example, clustering set points lines, pairwise relations meaningful, pair data points trivially defines line. however, data points, wher collinear conveys important information. graph-based clustering problem studied, researchers deal hypergraph-based clustering existing graph-based clustering methods. direction transform hypergraph graph, edge-weights mapped weights original hypergraph. zien. . ] proposed approaches called ?clique expansion? ?star expansion?, respectively, purpose. rodriguez] showed relationship spectral properties Laplacian matrix resulting graph minimum cut original hypergraph. agarwal. ] proposed ?clique averaging? method reported results ?clique expansion? method. anor direction generalize graph-based clustering method hypergraphs. zhou. ] generalized well-known ?normalized cut? method] defined hypergraph normalized cut criterion-partition vertices. shashua. ] cast clustering problem high order relations nonnegative factorization problem closest hyper-stochastic version input aﬃnity tensor. based game ory, Bulo Pelillo] proposed hypergraph-based clustering problem multi-player non-cooperative ?clustering game? solve replicator equation, fact generalization previous work]. formulation solid oretical foundation, possesses appealing properties, achieved state-art results. method fact specific case proposed method, discuss point Section paper, propose unified method clustering-ary aﬃnity relations, applicable graph-based hypergraph-based clustering problems. method motivated intuitive observation: cluster objects, exist-ary aﬃnity relations, (sometimes all-ary aﬃnity relations agree criterion. example, line clustering problem, points line, triplets, triplets satisfy criterion lie line. ensemble large number aﬃnity relations produced outliers robust noises, yielding robust mechanism clustering. formulation Clustering-ary aﬃnity relations intuitively clustering special kind edge-weighted hypergraph-graph. formally-graph triplet,   finite set vertices, vertex representing object, set hyper \\x0cedges, hyperedge representing-ary aﬃnity relation, weighting function associates real (can negative) hyperedge, larger weights representing stronger aﬃnity relations. -ary aﬃnity relations duplicate objects, hyperedges vertices. hyperedges duplicated vertices, simply set weights zeros. hyperedge involves vertices, represented-tuple    weighted adjacency array graph      super-symmetry array, denoted defined          else, Note edge     duplicate entries array subset vertices, edge set denoted cluster, hyperedges large weights. simplest measure reﬂect ensemble phenomenon sum entries hyperedges vertices expressed:     ) ,??  Suppose indicator vector subset yvi orwise expressed)    yv1   yvk ) ,??  obviously increases number vertices increases. summands average entries expressed: Sav)  ,??  ,??  .      yv1   yvk         xv1   xvk) ,??  natural constraint intuitively, true cluster, Sav large. thus, clustering problem corresponds problem maximizing Sav essence, combinatorial optimization problem, neir objects select. problem-hard, reduce complexity, relax continuous range,   constant, keeping constraint problem becomes: max) ,??     xvi) subject  ,    standard simplex note Sav) abbreviated) simplify formula. adoption -norm) intuitive probabilistic meaning, represents probability cluster object, makes solution sparse, means automatically select objects form cluster, ignoring objects. relation Clustering game. ], Bulo Pelillo proposed cast hypergraph-based clustering problem clustering game, leads similar formulation). fact, formulation special case)  setting means probability choosing strategy (from game ory perspective) choosing object (from perspective) upper bound, fact prior, represents noninformative prior. point essential applications, avoids phenomenon components dominate. example, weight hyperedge extremely large, cluster select vertices hyperedge, desirable. fact, offers tool control number objects cluster. component exceed cluster objects] represents smallest integer larger equal constraint , solution totally]. algorithm Formulation) local maxima. large maxima correspond true clusters small maxima form meaningless subsets. section, analyze properties maximizer critical algorithm design, introduce algorithm calculate  formulation) constrained optimization problem, adding Lagrangian multipliers           obtain Lagrangian function:   , )       reward vertex denoted), defined follows)     ,??  Since super-symmetry array)  xvt kri) proportional gradient Any local maximizer satisfy karush-kuhn-tucker (kkt) condition., firstorder conditions local optimality. ,    )??      )      Since all?nonnegative equivalent     equivalent hence, KKT conditions rewritten:  )  vertices set divided disjoint subsets , ?)}  ?}. equation) characterizes properties solution), furr summarized orem. orem  solution), exists constant ) rewards vertices belonging? larger rewards vertices belonging? equal rewards vertices belonging? smaller proof: Since KKT condition condition), solution satisfy). set non-zero components) ) set components smaller ). update increase), values components belonging) decrease values components belonging) increase. orem solution) ), ), ). contrary, ), ), solution). fact, case, increase decrease increase). , ) define rij)     ,??  xvt?   )rij) ))? ) \\x0csince), select proper increase). formula) constraint  min   ), rij)  min  increase) reaches maximum; rij ) min   increase) reaches maximum. ) According analysis, ), ), update increase). procedure iterates) ), ), ). prior (initialization), algorithm compute local maximizer) summarized Algorithm successively chooses ?best? vertex ?worst? vertex update components significant maxima formulation) correspond true clusters, multiple initializations (priors) obtain initialization basin attraction significant maximum. informative priors fact easily eﬃciently constructed neighborhood vertex (vertices hyperedges connecting vertex), neighbors vertex generally higher probabilities belong cluster. algorithm Compute local maximizer prior) input: Weighted adjacency array prior); repeat Compute reward) vertex Compute)); Find vertex)) largest reward vertex)) smallest reward; Compute update) formula) obtain); local maximizer output: local maximizer  algorithm Construct prior) vertex input: Hyperedge set) sort hyperedges) descending order weights;   )— Add vertices hyperedge—  break; end For vertex set component xvj— output: prior). vertex set hyperedges connected denoted). construct prior), Algorithm constraint initializations nonzero components. cover basin attractions maxima, expect initializations locate uniformly space  ?}. vertex, construct prior, thus, construct priors total. priors, Algorithm obtain maxima. significant maxima) maxima, significant maximum multiple times. way, robustly obtain multiple clusters simultaneously, clusters overlap, desirable properties applications. note clustering game approach] utilizes noninformative prior, vertices equal probability. thus, obtain multiple clusters simultaneously. clustering game approach) means drop points point initially included, selected. however, method automatically add drop points, anor key difference clustering game approach. iteration \\x0calgorithm components makes update rewards update) eﬃcient. )) increases, sizes)) decrease quickly) converges local maximum quickly. suppose maximal number hyperedges vertex time complexity Algorithm(thk), number iterations. total time complexity method(nthk), ran Algorithm initializations. experiments evaluate method types experiments. addresses problem line clustering, addresses problem illumination-invariant face clustering, addresses problem aﬃne-invariant point set matching. compare method clique averaging] algorithm matching game approach]. experiments, clique averaging approach number clusters advance; however, clustering game approach method automatically reveal number clusters, yields advantages applications.  Line Clustering experiment, problem clustering lines point sets. pairwise similarity measures useless case, points needed characterizing property. dissimilarity measure triplets points distance fitting line. , dissimilarity measure points}, similarity function}) exp scaling parameter, controls sensitivity similarity measure deformation. randomly generate lines region line points, points perturbed Gaussian noise, ?). randomly add outliers point set. fig. ) illustrates point set lines shown red, blue green colors, respectively, outliers shown magenta color. evaluate performance, ran algorithms data set trials varying parameter values, performance measured-measure. fix number outliers, vary scaling parameter, result shown fig. ). method, set . obviously, method affected scaling parameter clustering game approach sensitive note fact controls weights hyperedge graph graph-based algorithms notoriously sensitive weights graph. instead, setting proper method overcomes problem. fig. ), observe?, clustering game approach performance. thus, fix?, change noise parameter , results clustering game approach, clique averaging algorithm method shown blue, green red colors fig. ), respectively. figure shows, noise small, matching game approach outperforms clique averaging algorithm, noise large, clique averaging algorithm outperforms matching game approach. matching game approach robust outliers, \\x0cclique averaging algorithm robust noises. method result, select coherent clusters matching game approach, control size clusters, avoiding problem points selected clusters. fig. ) fig. ), vary number outliers 100, results demonstrate method clustering game approach robust outliers, clique averaging algorithm sensitive outliers, partition-based method point assigned cluster. illustrate inﬂuence fix , test performance method result shown fig. ), note axis/?. stressed Section clustering game approach fact special case method thus, result result clustering game approach fig. ) conditions. obviously/? approaches real number points cluster, result better. note result appears/? , due fact outliers fall line clusters, fig. ).  illumination-invariant face clustering shown variability images Labmertian surface fixed pose, variable lighting conditions surface point shadowed, constitutes dimensional linear subspace]. leads natural measure dissimilarity images, clustering. fact, generalization-lines problem ksubspaces problem. assume images consideration form columns matrix, s24 normalize column norm, +??  serves natural measure dissimilarity, ith singular matrix. experiments Yale Face Database extended version], individuals, illumination conditions. lighting conditions, images severely shadowed, delete images experiments subset (about images individual). considered cases faces random individuals (randomly choose faces individual), outliers. case outliers consists additional faces individual. combinations, ran trials obtain average-measures (mean standard deviation), result reported Table note algorithm, individually tune parameters obtain results. results show partition-based clustering method (clique averaging) sensitive outliers, performs outliers. clustering game approach method perform well, outliers, method performs better. figure Results clustering lines noises outliers. performance clique averaging algorithm], matching game approach] method shown green dashed, blue dotted read solid curves, respectively. figure viewed color. table Experiments illuminant-invariant face clustering Classes Outliers Clique Averaging Cluster \\x0cing Game Our Method             aﬃne-invariant Point Set Matching important problem object recognition fact object viewpoints, resulting differently deformed images. consequently, invariance viewpoints desirable property well-known near-planar object vision tasks. viewpoint modeled aﬃne transformations. subsection, show matching planar point sets viewpoints formulated hypergraph clustering problem algorithm suitable tasks. suppose point sets points, respectively. point match point candidate matches. aﬃne transformation correct matches, mii? mjj mkk? ?ijk —det)—, Sijk  area triangle formed points?   area triangle formed points   det) determinant regard candidate match   —det point, exp(? ijk serves natural similarity measure   points (candidate matches), mii mjj mkk scaling parameter, correct matching configuration naturally form cluster. note problem, candidate matches incorrect matches, considered outliers. experiments shapes mpeg shape database]. shape, uniformly sample contour points. shapes sampled point sets demonstrated fig.  regard original contour point sets randomly add Gaussian noise, transform randomly generated aﬃne matrices form. fig. ) shows pair red blue, respectively. points (candidate matches) belong cluster, partition-based clustering method, clique aver7 aging method, used. thus, compare method matching game approach measure performance methods counting matches agree ground truths. —det)— unknown, estimate range sample values range, conduct experiment —det)—. fig. ), fix noise parameter , test robustness methods varying scaling parameter obviously, method robust matching game approach sensitive. fig. ), increase , adjust reach performances methods. expected, method robust noise benefiting parameter set fig. ) fig. ). fig. ), fix , test performance method result verifies importance parameter figure shapes contour point sets experiment. figure Performance curves aﬃne-invariant point set matching problem. red solid curves demonstrate performance method, blue dotted curve illustrates performance matching game approach. discussion paper, characterized clustering ensemble aﬃnity relations relax clustering problem optimizing constrained homogenous function. showed clustering game approach turns special case method. proposed eﬃcient algorithm automatically reveal clusters data set, severe noises large number outliers. experimental results demonstrated superiority approach respect state--art counterparts. especially, method sensitive scaling parameter affects weights graph, desirable property applications. key issue hypergraph-based clustering high computational cost construction hypergraph, studying eﬃciently construct approximate hypergraph perform clustering incomplete hypergraph. acknowledgement This research CSIDM Project. csidm-200803 partially funded grant National Research Foundation (nrf) administered Media Development Authority (mda) singapore, work partially supported NSF Grants iis-0812118, BCS0924164 AFOSR Grant fa9550-0207. Data clustering fundamental problem fields, machine learning, data mining computer vision]. unfortunately, universally accepted definition cluster, diverse forms clusters real applications. but generally agreed objects belonging cluster satisfy internal coherence condition, objects belonging cluster satisfy condition. most existing clustering methods partition-based-means], spectral clustering, aﬃnity propagation]. methods implicitly share assumption: data point belong cluster. this assumption greatly simplifies problem, judge wher data point outlier not, challenging. however, assumption results bad performance methods exists large number outliers, \\x0cfrequently met real-world applications. criteria judge wher objects belong cluster typically expressed pairwise relations, encoded weights aﬃnity graph. however, applications, high order relations appropriate, choice, naturally results hyperedges hypergraphs. for example, clustering set points lines, pairwise relations meaningful, pair data points trivially defines line. however, data points, wher collinear conveys important information. graph-based clustering problem studied, researchers deal hypergraph-based clustering existing graph-based clustering methods. one direction transform hypergraph graph, edge-weights mapped weights original hypergraph. zien. . ] proposed approaches called ?clique expansion? ?star expansion?, respectively, purpose. rodriguez] showed relationship spectral properties Laplacian matrix resulting graph minimum cut original hypergraph. agarwal. ] proposed ?clique averaging? method reported results ?clique expansion? method. anor direction generalize graph-based clustering method hypergraphs. zhou. ] generalized well-known ?normalized cut? method] defined hypergraph normalized cut criterion-partition vertices. shashua. ] cast clustering problem high order relations nonnegative factorization problem closest hyper-stochastic version input aﬃnity tensor. based game ory, Bulo Pelillo] proposed hypergraph-based clustering problem multi-player non-cooperative ?clustering game? solve replicator equation, fact generalization previous work]. this formulation solid oretical foundation, possesses appealing properties, achieved state-art results. this method fact specific case proposed method, discuss point Section paper, propose unified method clustering-ary aﬃnity relations, applicable graph-based hypergraph-based clustering problems. our method motivated intuitive observation: cluster objects, exist-ary aﬃnity relations, (sometimes all-ary aﬃnity relations agree criterion. for example, line clustering problem, points line, triplets, triplets satisfy criterion lie line. ensemble large number aﬃnity relations produced outliers robust noises, yielding robust mechanism clustering. Formulation Clustering-ary aﬃnity relations intuitively clustering special kind edge-weighted hypergraph-graph. formally-graph triplet,   finite set vertices, vertex representing object, set hyper \\x0cedges, hyperedge representing-ary aﬃnity relation, weighting function associates real (can negative) hyperedge, larger weights representing stronger aﬃnity relations. -ary aﬃnity relations duplicate objects, hyperedges vertices. for hyperedges duplicated vertices, simply set weights zeros. each hyperedge involves vertices, represented-tuple    weighted adjacency array graph      super-symmetry array, denoted defined          else, Note edge     duplicate entries array for subset vertices, edge set denoted cluster, hyperedges large weights. simplest measure reﬂect ensemble phenomenon sum entries hyperedges vertices expressed:     ) ,??  Suppose indicator vector subset yvi orwise expressed)    yv1   yvk ) ,??  obviously increases number vertices increases. since summands average entries expressed: Sav)  ,??  ,??  .      yv1   yvk         xv1   xvk) ,??  natural constraint intuitively, true cluster, Sav large. thus, clustering problem corresponds problem maximizing Sav essence, combinatorial optimization problem, neir objects select. problem-hard, reduce complexity, relax continuous range,   constant, keeping constraint problem becomes: max) ,??     xvi) subject  ,    standard simplex note Sav) abbreviated) simplify formula. adoption -norm) intuitive probabilistic meaning, represents probability cluster object, makes solution sparse, means automatically select objects form cluster, ignoring objects. relation Clustering game. ], Bulo Pelillo proposed cast hypergraph-based clustering problem clustering game, leads similar formulation). fact, formulation special case)  setting means probability choosing strategy (from game ory perspective) choosing object (from perspective) upper bound, fact prior, represents noninformative prior. this point essential applications, avoids phenomenon components dominate. for example, weight hyperedge extremely large, cluster select vertices hyperedge, desirable. fact, offers tool control number objects cluster. since component exceed cluster objects] represents smallest integer larger equal because constraint , solution totally]. Algorithm Formulation) local maxima. large maxima correspond true clusters small maxima form meaningless subsets. section, analyze properties maximizer critical algorithm design, introduce algorithm calculate  since formulation) constrained optimization problem, adding Lagrangian multipliers           obtain Lagrangian function:   , )       reward vertex denoted), defined follows)     ,??  Since super-symmetry array)  xvt kri) proportional gradient Any local maximizer satisfy karush-kuhn-tucker (kkt) condition., firstorder conditions local optimality. that,    )??      )      Since all?nonnegative equivalent     equivalent hence, KKT conditions rewritten:  )  according vertices set divided disjoint subsets , ?)}  ?}. Equation) characterizes properties solution), furr summarized orem. orem  solution), exists constant ) rewards vertices belonging? larger rewards vertices belonging? equal rewards vertices belonging? smaller proof: Since KKT condition condition), solution satisfy). set non-zero components) ) set components smaller ). for update increase), values components belonging) decrease values components belonging) increase. according orem solution) ), ), ). contrary, ), ), solution). fact, case, increase decrease increase). that, ) define rij)     ,??  xvt?   )rij) ))? ) \\x0csince), select proper increase). according formula) constraint  min   since), rij)  min  increase) reaches maximum; rij ) min   increase) reaches maximum. ) According analysis, ), ), update increase). such procedure iterates) ), ), ). from prior (initialization), algorithm compute local maximizer) summarized Algorithm successively chooses ?best? vertex ?worst? vertex update components since significant maxima formulation) correspond true clusters, multiple initializations (priors) obtain initialization basin attraction significant maximum. such informative priors fact easily eﬃciently constructed neighborhood vertex (vertices hyperedges connecting vertex), neighbors vertex generally higher probabilities belong cluster. Algorithm Compute local maximizer prior) input: Weighted adjacency array prior); repeat Compute reward) vertex Compute)); Find vertex)) largest reward vertex)) smallest reward; Compute update) formula) obtain); local maximizer output: local maximizer  algorithm Construct prior) vertex input: Hyperedge set) Sort hyperedges) descending order weights;   )— Add vertices hyperedge—  break; end For vertex set component xvj— output: prior). for vertex set hyperedges connected denoted). construct prior), Algorithm because constraint initializations nonzero components. cover basin attractions maxima, expect initializations locate uniformly space  ?}. since vertex, construct prior, thus, construct priors total. from priors, Algorithm obtain maxima. significant maxima) maxima, significant maximum multiple times. way, robustly obtain multiple clusters simultaneously, clusters overlap, desirable properties applications. note clustering game approach] utilizes noninformative prior, vertices equal probability. thus, obtain multiple clusters simultaneously. clustering game approach) means drop points point initially included, selected. however, method automatically add drop points, anor key difference clustering game approach. iteration \\x0calgorithm components makes update rewards update) eﬃcient. )) increases, sizes)) decrease quickly) converges local maximum quickly. suppose maximal number hyperedges vertex time complexity Algorithm(thk), number iterations. total time complexity method(nthk), ran Algorithm initializations. Experiments evaluate method types experiments. addresses problem line clustering, addresses problem illumination-invariant face clustering, addresses problem aﬃne-invariant point set matching. compare method clique averaging] algorithm matching game approach]. experiments, clique averaging approach number clusters advance; however, clustering game approach method automatically reveal number clusters, yields advantages applications.  Line Clustering experiment, problem clustering lines point sets. pairwise similarity measures useless case, points needed characterizing property. dissimilarity measure triplets points distance fitting line. , dissimilarity measure points}, similarity function}) exp scaling parameter, controls sensitivity similarity measure deformation. randomly generate lines region line points, points perturbed Gaussian noise, ?). randomly add outliers point set. fig. ) illustrates point set lines shown red, blue green colors, respectively, outliers shown magenta color. evaluate performance, ran algorithms data set trials varying parameter values, performance measured-measure. fix number outliers, vary scaling parameter, result shown fig. ). for method, set . obviously, method affected scaling parameter clustering game approach sensitive note fact controls weights hyperedge graph graph-based algorithms notoriously sensitive weights graph. instead, setting proper method overcomes problem. from fig. ), observe?, clustering game approach performance. thus, fix?, change noise parameter , results clustering game approach, clique averaging algorithm method shown blue, green red colors fig. ), respectively. figure shows, noise small, matching game approach outperforms clique averaging algorithm, noise large, clique averaging algorithm outperforms matching game approach. this matching game approach robust outliers, \\x0cclique averaging algorithm robust noises. our method result, select coherent clusters matching game approach, control size clusters, avoiding problem points selected clusters. fig. ) fig. ), vary number outliers 100, results demonstrate method clustering game approach robust outliers, clique averaging algorithm sensitive outliers, partition-based method point assigned cluster. illustrate inﬂuence fix , test performance method result shown fig. ), note axis/?. stressed Section clustering game approach fact special case method thus, result result clustering game approach fig. ) conditions. obviously/? approaches real number points cluster, result better. note result appears/? , due fact outliers fall line clusters, fig. ).  illumination-invariant face clustering shown variability images Labmertian surface fixed pose, variable lighting conditions surface point shadowed, constitutes dimensional linear subspace]. this leads natural measure dissimilarity images, clustering. fact, generalization-lines problem ksubspaces problem. assume images consideration form columns matrix, s24 normalize column norm, +??  serves natural measure dissimilarity, ith singular matrix. experiments Yale Face Database extended version], individuals, illumination conditions. since lighting conditions, images severely shadowed, delete images experiments subset (about images individual). considered cases faces random individuals (randomly choose faces individual), outliers. case outliers consists additional faces individual. for combinations, ran trials obtain average-measures (mean standard deviation), result reported Table note algorithm, individually tune parameters obtain results. results show partition-based clustering method (clique averaging) sensitive outliers, performs outliers. clustering game approach method perform well, outliers, method performs better. Figure Results clustering lines noises outliers. performance clique averaging algorithm], matching game approach] method shown green dashed, blue dotted read solid curves, respectively. this figure viewed color. table Experiments illuminant-invariant face clustering Classes Outliers Clique Averaging Cluster \\x0cing Game Our Method             aﬃne-invariant Point Set Matching important problem object recognition fact object viewpoints, resulting differently deformed images. consequently, invariance viewpoints desirable property well-known near-planar object vision tasks. viewpoint modeled aﬃne transformations. subsection, show matching planar point sets viewpoints formulated hypergraph clustering problem algorithm suitable tasks. suppose point sets points, respectively. for point match point candidate matches. under aﬃne transformation correct matches, mii? mjj mkk? ?ijk —det)—, Sijk  area triangle formed points?   area triangle formed points   det) determinant regard candidate match   —det point, exp(? ijk serves natural similarity measure   points (candidate matches), mii mjj mkk scaling parameter, correct matching configuration naturally form cluster. note problem, candidate matches incorrect matches, considered outliers. experiments shapes mpeg shape database]. for shape, uniformly sample contour points. both shapes sampled point sets demonstrated fig.  regard original contour point sets randomly add Gaussian noise, transform randomly generated aﬃne matrices form. fig. ) shows pair red blue, respectively. since points (candidate matches) belong cluster, partition-based clustering method, clique aver7 aging method, used. thus, compare method matching game approach measure performance methods counting matches agree ground truths. since —det)— unknown, estimate range sample values range, conduct experiment —det)—. fig. ), fix noise parameter , test robustness methods varying scaling parameter obviously, method robust matching game approach sensitive. fig. ), increase , adjust reach performances methods. expected, method robust noise benefiting parameter set fig. ) fig. ). fig. ), fix , test performance method result verifies importance parameter figure shapes contour point sets experiment. figure Performance curves aﬃne-invariant point set matching problem. red solid curves demonstrate performance method, blue dotted curve illustrates performance matching game approach. Discussion paper, characterized clustering ensemble aﬃnity relations relax clustering problem optimizing constrained homogenous function. showed clustering game approach turns special case method. proposed eﬃcient algorithm automatically reveal clusters data set, severe noises large number outliers. experimental results demonstrated superiority approach respect state--art counterparts. especially, method sensitive scaling parameter affects weights graph, desirable property applications. key issue hypergraph-based clustering high computational cost construction hypergraph, studying eﬃciently construct approximate hypergraph perform clustering incomplete hypergraph. Acknowledgement This research CSIDM Project. csidm-200803 partially funded grant National Research Foundation (nrf) administered Media Development Authority (mda) singapore, work partially supported NSF Grants iis-0812118, BCS0924164 AFOSR Grant fa9550-0207.',\n",
       " 'PP4057': 'neuronal elements brain constitute intriguing complex network]. functional magnetic resonance imaging (fmri) applied study functional connectivity neural elements form complex network brain level. suggested ﬂuctuations blood oxygenation level-dependent (bold) signal rest reﬂecting neuronal baseline activity brain correspond functionally relevant networks]. analysis functional resting state networks (rsn) based analysis correlation temporal dynamics regions brain eir assessed voxels correlate signal \\x0cpredefined regions-called) seeds] unsupervised multivariate approaches independent component analysis (ica]. Figure proposed framework. pairwise mutual information) calculated 2x2x2 group voxels subjects resting state fmri activity. graph pairwise mutual information thresholded top 100,000-directed links kept. graphs analyzed infinite relational model (irm) assuming functional units subjects interactions ) individual. extracted interactions characterize individuals. models identify coherently behaving groups terms correlation give limited insight groups interact. furrmore, correlation optimal extracting order statistics easily fails establishing higher order interactions regions brain]. paper view analysis functional resting state networks. starting definition resting state functional coherent groups search functional units brain communicate parts brain coherent manner. consequently, define functional units interact remaining parts network. functional connectivity regions measured mutual information. mutual information) rooted information ory data detect functional relations regions order interaction]. reby, resting state fmri represented mutual information graph pairwise relations voxels constituting complex network. numerous studies analyzed graphs borrowing ideas study complex networks]. common procedures extract summary statistics networks compare random networks analyses demonstrated fmri derived graphs behave random]. paper propose relational modeling] order quantify functional coherent groups resting state networks. particular, investigate line modeling discriminate patients multiple sclerosis healthy individuals. multiple Sclerosis) inﬂammatory disease resulting widespread demyelinization subcortical spinal white matter. focal axonal demyelinization secondary axonal degeneration results variable delays disruption signal transmission cortico-cortical cortico-subcortical connections]. addition characteristic macroscopic white-matter lesions structural magnetic resonance imaging (mri), pathologyand advanced mri-studies shown demyelinated lesions cortical gray-matter white-matter normal structural MRI]. findings show demyelination disseminated brain affecting brain functional connectivity. structural MRI information extent white-matter lesions, information impact functional brain connectivity. widespread demyelinization brain., affecting brain anatomical functional ?wiring?) represents disease state suited relational modeling. here, relational modeling provide \\x0cglobal view communication functional network extracted functional units. furrmore, method facilitates examination brain networks simultaneously completely data driven manner. illustration proposed analysis figure methods data: clinically stable patients relapsing-remitting) secondary progressive multiple sclerosis; females; age years; range years) healthy individuals females; age years; range years) participated cross-sectional study. patients neurologically examined assigned score EDSS ranged (median edss; disease duration years; range years). -fmri performed subjects rest eyes closed Tesla Magnetom trio, siemens, erlangen, germany). gradient echo*-weighted echo planar imaging sequence whole-brain coverage (repetition time: 2490; isotropic voxels). -fmri session lasted min (482 brain volumes). scan session cardiac respiratory cycles monitored pulse oximeter pneumatic belt. preprocessing: After exclusion pre-saturation volumes remaining volume realigned volume rigid body transformation. realigned images normalized MNI template. order remove nuisance effects related residual movement physiological effects linear filter comprised motion related total physiological effects (cardiac, respiratory respiration volume time) constructed]. filtering, voxel masked] divided 5039 voxel groups consisting  voxels estimation pairwise.  Mutual Information Graphs mutual information voxel groups Pij, Pij, log thus, mutual information hinges estimation) joint density Pij). approaches exists estimation mutual information] ranging parametric non-parametric methods nearest neighbor density estimators] histogram methods. accuracy approaches relies number observations present. histogram approach. equiprobable rar equidistant bins] based percentiles derived individual distribution voxel group. ) pij, counts number-occurrences observations voxels voxel group bin voxels group bin time such, total 480 3840 samples populate 100 bins joint histogram. generate mutual information graphs subject total?5039? (5039 billion pairwise evaluated. thresholded graph keeping top 100, 000 pairwise links graph. such, graph size 5039 5039 total 200,000 directed links. 100, 000 100,000 undirected link) resulted graph link density 5039? (5039.0079 total number links 100, 000 million links (when counting links direction).  \\x0cinfinite Relational Modeling (irm) importance modeling brain connectivity interactions widely recognized literature fmri]. approaches dynamic causal modeling], structural equation models] dynamic Bayes nets] limited analysis interactions brain regions predefined regions interest. benefits current relational modeling approach regions defined completely data driven manner method establishes interaction low computational complexity admitting analysis large scale brain networks. functional connectivity graphs previously considered] discrimination schizophrenia. ] resting state networks defined based normalized graph cuts order derive functional units. normalized cuts suited separation voxels groups disconnected components method lacks ability coherent interaction groups. ] stochastic block model denoted relational model) proposed identification coherent groups nodes complex networks. here, node belongs class denote ith row clustering assignment matrix probability link node determined class assignments here‘ , denotes probability generating link node class node class Dirichlet process] propose nonparametric generalization model potentially infinite number classes. infinite relational model (irm). inference IRM jointly determines number latent classes class assignments class link probabilities. knowledge attempt explore IRM model fmri data. ] generative model infinite relational model (?) —? ) )—? ),  , beta(? ),  )) bernoulli , ) entitys tendency participate relations determined solely cluster assignment prior elements conjugate resulting integral,   )  )   ? ) analytical solution,   ) beta, ), ,  )) beta(? ),  ) )    , number links functional units , number non-links functional unit disregarding links node itself. vector length entries number voxel groups. assume graphs independent subjects)   ,   ) beta, ), ,  )) beta(? ),   result, posterior likelihood      ,   —?)   ) beta, ), ,  ))   beta(? ),   ?(?)      where number expressed functional units number voxel groups assigned functional unit expected ) )+?  )+? )+?  ) mcmc Sampling IRM model: proposed] Gibbs sampling scheme combination split-merge sampling] clustering assignment matrix split-merge sampling procedure proposed] restricted Gibbs sampling sweeps. initialized restricted Gibbs sampler sequential allocation procedure proposed]. MCMC sampling, posterior likelihood node assignment assignment remaining nodes needed Gibbs sampler calculating split-merge acceptance ratios].  )  beta )+?? )) beta(? ),?   )   beta )+?? )) orwise beta(? ),? ))  size functional unit disregarding assignment ith node. note posterior likelihood eﬃciently calculated parts) computation, , evaluation Beta function affected considered assignment change. scoring functional units terms stability: sampling obtain large amount potential solutions, however, visualization interpretation diﬃcult average samples requires extracted groups samples runs related. visualization selected single extracted sample ., MAP estimate) separate randomly initialized runs 500 iterations. facilitate interpretation displayed top extracted functional units reproducible separate runs. identify functional units analyzed nodes-occurred cluster extracted samples random starts)¿ score stot  stot   counts number times voxels group-occurred voxels group stot total number times voxels group-occurred voxels graph.   voxels cth group cluster samples voxels-occurred samples. results Discussion Following] calculated average shortest path length hli, average clustering coeﬃcient hci, degree distribution largest connected component., giant component) subject specific graph threshold define top 100, 000 links. table derived graphs erd?enyi random graphs. clustering coeﬃcient, degree distribution parameter giant component differ significantly random graphs. however, significant differences Normal group indicating global features affected disease. run, initialized IRM model randomly generated functional units.  set prior ,  , favoring orwise orwise priori higher functional unit link density relative link density. set log (where number voxel groups). model estimation treated% links equivalent number nonlinks missing random graphs. treating entries missing random maintaining counts observed values]. estimated models stable average extracted  functional units. figure area curve (auc) scores receiver operator characteristic predicting links subject prediction links based averaging final 100 samples. AUC scores random subjects high degree variability subjects terms model ability account links non-links graphs. found significant difference Normal group terms Table Median threshold values average shortest path hli, average clustering coeﬃcient hci, degree distribution exponent . )  giant component. largest connected component graphs relative complete graph) normal multiple-sclerosis group non-parametric test difference median groups. random graph erd?enyi random graph density constructed graphs. normal Random-value(normal-value(normal. random.0164.0163.9964 hli.4509.6764 hci.1116.0898.0079.9954 .001 .7448 .001.8587.8810.7928 .001 Figure AUC score runs subject Normal group (top) group (bottom). top distribution AUC scores groups (normal: blue: red). significant difference median distributions found ). model ability account network dynamics. thus, difference terms IRM model account structure networks Normal subjects. finally, link prediction surprisingly stable subject runs links non-links treated missing. high degree variability graphs extracted resting state fmri subjects relative variability subject. inference stochastic optimization procedure visualized sample highest likelihood. map estimate) runs figure display top reproducible extracted voxel groups., functional units) runs. fifteen functional units easily identified functionally relevant networks. selected functional units similar \\x0cnetworks previously identified resting-state fmri data ICA]. sensori-motor network represented functional units; posterior part default-mode network] functional units; fronto-parietal network functional units; visual system represented functional units. note striking similarity sensori-motor ica1, posterior part default network ICA2 fronto-parietal network ICA3 visual component ica4. contrary ICA current approach model interactions components consistent pattern revealed functional units highest connectivity show strongest connectivity. furrmore functional units symmetric connectivity profiles. functional unit strongly connected functional unit (sensori-motor system), strongly connect functional units, case (default-mode network). functional units attribute vascular noise units connected remaining functional units. panel figure tested difference medians connectivity extracted functional units. connections significant . healthy individuals show stronger connectivity selected functional units relative patients. functional units involved distributed brain comprise visual system (functional unit), sensori-motor network (functional unit), fronto-parietal network (functional unit). expected affects brain globally white-matter disseminated brain]. patients show stronger connectivity relative healthy individuals selected parts sensori-motor (functional unit) fronto-parietal network (functional units). interpretation finding communication increases fronto-parietal sensori-motor network eir maladaptive consequence disease part beneficial compensatory mechanism maintain motor function. figure Panel Visualization MAP model restarts. functional units red circles median unit link density lines median functional unit link density. gray scale line width code link density functional units logarithmic scale. panel Selected resting state components extracted group independent component analysis (ica) given. temporal concatenation subjects Infomax ICA algorithm] identify spatially independent components. subsequently individual component time series regression model obtain subject specific component maps]. displayed ICA maps based sample-tests corrected multiple comparisons  Gaussian random fields ory. panel AUC score relations extracted groups thresholded significance level based sided rank-sum test. blue link density larger Normal, yellow larger normal. high resolution version figure found supplementary material). table Leave classification performance based support vector machine (svm) linear kernel, linear discriminant analysis (lda-nearest neighbor (knn). significance level estimated comparing classification performance classifiers randomly permuted class labels, bold significant classification . svm LDA KNN Raw data PCA ICA   Degree IRM .002 .001 ) Discriminating Normal subjects: evaluated classification performance subject specific group link densities ) based leave cross-validation. considered standard classifiers, soft margin support vector machine (svm) linear kernel), linear discriminant analysis (lda) based pooled variance estimate (features projected principal component analysis dimensional feature space prior analysis-nearest neighbor (knn), compared classifier performances classifying normalized raw subject specific voxel time series. matrix subject voxel time data projected dominant dimensional subspace denoted (pca). comparison included group ICA] analysis performance node degree (degree) features previously successful classification schizophrenia]. IRM model Bayesian average predictions dominated MAP estimate figure classification analyses normalized feature. table classification results. group ICA proposed IRM model significantly classify random. irm model higher classification rate significant classifiers. finally, note contrary analysis based temporal correlation ICA PCA approaches classification benefit mutual information higher order dependencies account necessarily reﬂected correlation. such, brain region driven variance anor brain region captured mutual information necessarily captured correlation. conclusion functional units extracted IRM model correspond previously RSNs]. conventional models assessing functional connectivity-fmri data aim divide brain segregated networks IRM explicitly models relations functional units enabling visualization analysis interactions. classification models predict subject disease state revealed IRM model higher prediction rate discrimination based components extracted conventional group ICA approach]. irm readily extends directed graphs networks derived task related functional activation. lieve proposed method constitutes promising framework analysis functionally derived brain networks general. Neuronal elements brain constitute intriguing complex network]. functional magnetic resonance imaging (fmri) applied study functional connectivity neural elements form complex network brain level. suggested ﬂuctuations blood oxygenation level-dependent (bold) signal rest reﬂecting neuronal baseline activity brain correspond functionally relevant networks]. most analysis functional resting state networks (rsn) based analysis correlation temporal dynamics regions brain eir assessed voxels correlate signal \\x0cpredefined regions-called) seeds] unsupervised multivariate approaches independent component analysis (ica]. while Figure proposed framework. all pairwise mutual information) calculated 2x2x2 group voxels subjects resting state fmri activity. graph pairwise mutual information thresholded top 100,000-directed links kept. graphs analyzed infinite relational model (irm) assuming functional units subjects interactions ) individual. extracted interactions characterize individuals. models identify coherently behaving groups terms correlation give limited insight groups interact. furrmore, correlation optimal extracting order statistics easily fails establishing higher order interactions regions brain]. paper view analysis functional resting state networks. starting definition resting state functional coherent groups search functional units brain communicate parts brain coherent manner. consequently, define functional units interact remaining parts network. functional connectivity regions measured mutual information. mutual information) rooted information ory data detect functional relations regions order interaction]. reby, resting state fmri represented mutual information graph pairwise relations voxels constituting complex network. numerous studies analyzed graphs borrowing ideas study complex networks]. here common procedures extract summary statistics networks compare random networks analyses demonstrated fmri derived graphs behave random]. paper propose relational modeling] order quantify functional coherent groups resting state networks. particular, investigate line modeling discriminate patients multiple sclerosis healthy individuals. multiple Sclerosis) inﬂammatory disease resulting widespread demyelinization subcortical spinal white matter. focal axonal demyelinization secondary axonal degeneration results variable delays disruption signal transmission cortico-cortical cortico-subcortical connections]. addition characteristic macroscopic white-matter lesions structural magnetic resonance imaging (mri), pathologyand advanced mri-studies shown demyelinated lesions cortical gray-matter white-matter normal structural MRI]. findings show demyelination disseminated brain affecting brain functional connectivity. structural MRI information extent white-matter lesions, information impact functional brain connectivity. given widespread demyelinization brain., affecting brain anatomical functional ?wiring?) represents disease state suited relational modeling. here, relational modeling provide \\x0cglobal view communication functional network extracted functional units. furrmore, method facilitates examination brain networks simultaneously completely data driven manner. illustration proposed analysis figure Methods data: clinically stable patients relapsing-remitting) secondary progressive multiple sclerosis; females; age years; range years) healthy individuals females; age years; range years) participated cross-sectional study. patients neurologically examined assigned score EDSS ranged (median edss; disease duration years; range years). -fmri performed subjects rest eyes closed Tesla Magnetom trio, siemens, erlangen, germany). gradient echo*-weighted echo planar imaging sequence whole-brain coverage (repetition time: 2490; isotropic voxels). -fmri session lasted min (482 brain volumes). during scan session cardiac respiratory cycles monitored pulse oximeter pneumatic belt. preprocessing: After exclusion pre-saturation volumes remaining volume realigned volume rigid body transformation. realigned images normalized MNI template. order remove nuisance effects related residual movement physiological effects linear filter comprised motion related total physiological effects (cardiac, respiratory respiration volume time) constructed]. after filtering, voxel masked] divided 5039 voxel groups consisting  voxels estimation pairwise.  Mutual Information Graphs mutual information voxel groups Pij, Pij, log thus, mutual information hinges estimation) joint density Pij). several approaches exists estimation mutual information] ranging parametric non-parametric methods nearest neighbor density estimators] histogram methods. accuracy approaches relies number observations present. histogram approach. equiprobable rar equidistant bins] based percentiles derived individual distribution voxel group. ) pij, counts number-occurrences observations voxels voxel group bin voxels group bin time such, total 480 3840 samples populate 100 bins joint histogram. generate mutual information graphs subject total?5039? (5039 billion pairwise evaluated. thresholded graph keeping top 100, 000 pairwise links graph. such, graph size 5039 5039 total 200,000 directed links. 100, 000 100,000 undirected link) resulted graph link density 5039? (5039.0079 total number links 100, 000 million links (when counting links direction).  \\x0cinfinite Relational Modeling (irm) importance modeling brain connectivity interactions widely recognized literature fmri]. approaches dynamic causal modeling], structural equation models] dynamic Bayes nets] limited analysis interactions brain regions predefined regions interest. benefits current relational modeling approach regions defined completely data driven manner method establishes interaction low computational complexity admitting analysis large scale brain networks. functional connectivity graphs previously considered] discrimination schizophrenia. ] resting state networks defined based normalized graph cuts order derive functional units. while normalized cuts suited separation voxels groups disconnected components method lacks ability coherent interaction groups. ] stochastic block model denoted relational model) proposed identification coherent groups nodes complex networks. here, node belongs class denote ith row clustering assignment matrix probability link node determined class assignments here‘ , denotes probability generating link node class node class using Dirichlet process] propose nonparametric generalization model potentially infinite number classes. infinite relational model (irm). inference IRM jointly determines number latent classes class assignments class link probabilities. knowledge attempt explore IRM model fmri data. following] generative model infinite relational model (?) —? ) )—? ),  , beta(? ),  )) bernoulli , ) entitys tendency participate relations determined solely cluster assignment since prior elements conjugate resulting integral,   )  )   ? ) analytical solution,   ) beta, ), ,  )) beta(? ),  ) )    , number links functional units , number non-links functional unit disregarding links node itself. vector length entries number voxel groups. assume graphs independent subjects)   ,   ) beta, ), ,  )) beta(? ),   result, posterior likelihood      ,   —?)   ) beta, ), ,  ))   beta(? ),   ?(?)      Where number expressed functional units number voxel groups assigned functional unit expected ) )+?  )+? )+?  ) mcmc Sampling IRM model: proposed] Gibbs sampling scheme combination split-merge sampling] clustering assignment matrix split-merge sampling procedure proposed] restricted Gibbs sampling sweeps. initialized restricted Gibbs sampler sequential allocation procedure proposed]. for MCMC sampling, posterior likelihood node assignment assignment remaining nodes needed Gibbs sampler calculating split-merge acceptance ratios].  )  beta )+?? )) beta(? ),?   )   beta )+?? )) orwise beta(? ),? ))  size functional unit disregarding assignment ith node. note posterior likelihood eﬃciently calculated parts) computation, , evaluation Beta function affected considered assignment change. Scoring functional units terms stability: sampling obtain large amount potential solutions, however, visualization interpretation diﬃcult average samples requires extracted groups samples runs related. for visualization selected single extracted sample ., MAP estimate) separate randomly initialized runs 500 iterations. facilitate interpretation displayed top extracted functional units reproducible separate runs. identify functional units analyzed nodes-occurred cluster extracted samples random starts)¿ score stot  stot   counts number times voxels group-occurred voxels group stot total number times voxels group-occurred voxels graph.   voxels cth group cluster samples voxels-occurred samples. Results Discussion Following] calculated average shortest path length hli, average clustering coeﬃcient hci, degree distribution largest connected component., giant component) subject specific graph threshold define top 100, 000 links. table derived graphs erd?enyi random graphs. both clustering coeﬃcient, degree distribution parameter giant component differ significantly random graphs. however, significant differences Normal group indicating global features affected disease. for run, initialized IRM model randomly generated functional units.  set prior ,  , favoring orwise orwise priori higher functional unit link density relative link density. set log (where number voxel groups). model estimation treated% links equivalent number nonlinks missing random graphs. when treating entries missing random maintaining counts observed values]. estimated models stable average extracted  functional units. figure area curve (auc) scores receiver operator characteristic predicting links subject prediction links based averaging final 100 samples. while AUC scores random subjects high degree variability subjects terms model ability account links non-links graphs. found significant difference Normal group terms Table Median threshold values average shortest path hli, average clustering coeﬃcient hci, degree distribution exponent . )  giant component. largest connected component graphs relative complete graph) normal multiple-sclerosis group non-parametric test difference median groups. random graph erd?enyi random graph density constructed graphs. Normal Random-value(normal-value(normal. random.0164.0163.9964 hli.4509.6764 hci.1116.0898.0079.9954 .001 .7448 .001.8587.8810.7928 .001 Figure AUC score runs subject Normal group (top) group (bottom). top distribution AUC scores groups (normal: blue: red). significant difference median distributions found ). model ability account network dynamics. thus, difference terms IRM model account structure networks Normal subjects. finally, link prediction surprisingly stable subject runs links non-links treated missing. this high degree variability graphs extracted resting state fmri subjects relative variability subject. considering inference stochastic optimization procedure visualized sample highest likelihood. MAP estimate) runs figure display top reproducible extracted voxel groups., functional units) runs. fifteen functional units easily identified functionally relevant networks. selected functional units similar \\x0cnetworks previously identified resting-state fmri data ICA]. sensori-motor network represented functional units; posterior part default-mode network] functional units; fronto-parietal network functional units; visual system represented functional units. note striking similarity sensori-motor ica1, posterior part default network ICA2 fronto-parietal network ICA3 visual component ica4. contrary ICA current approach model interactions components consistent pattern revealed functional units highest connectivity show strongest connectivity. furrmore functional units symmetric connectivity profiles. functional unit strongly connected functional unit (sensori-motor system), strongly connect functional units, case (default-mode network). functional units attribute vascular noise units connected remaining functional units. panel figure tested difference medians connectivity extracted functional units. given connections significant . healthy individuals show stronger connectivity selected functional units relative patients. functional units involved distributed brain comprise visual system (functional unit), sensori-motor network (functional unit), fronto-parietal network (functional unit). this expected affects brain globally white-matter disseminated brain]. patients show stronger connectivity relative healthy individuals selected parts sensori-motor (functional unit) fronto-parietal network (functional units). interpretation finding communication increases fronto-parietal sensori-motor network eir maladaptive consequence disease part beneficial compensatory mechanism maintain motor function. Figure Panel Visualization MAP model restarts. given functional units red circles median unit link density lines median functional unit link density. gray scale line width code link density functional units logarithmic scale. panel Selected resting state components extracted group independent component analysis (ica) given. after temporal concatenation subjects Infomax ICA algorithm] identify spatially independent components. subsequently individual component time series regression model obtain subject specific component maps]. displayed ICA maps based sample-tests corrected multiple comparisons  Gaussian random fields ory. panel AUC score relations extracted groups thresholded significance level based sided rank-sum test. blue link density larger Normal, yellow larger normal. high resolution version figure found supplementary material). table Leave classification performance based support vector machine (svm) linear kernel, linear discriminant analysis (lda-nearest neighbor (knn). significance level estimated comparing classification performance classifiers randomly permuted class labels, bold significant classification . svm LDA KNN Raw data PCA ICA   Degree IRM .002 .001 ) Discriminating Normal subjects: evaluated classification performance subject specific group link densities ) based leave cross-validation. considered standard classifiers, soft margin support vector machine (svm) linear kernel), linear discriminant analysis (lda) based pooled variance estimate (features projected principal component analysis dimensional feature space prior analysis-nearest neighbor (knn), compared classifier performances classifying normalized raw subject specific voxel time series. matrix subject voxel time data projected dominant dimensional subspace denoted (pca). for comparison included group ICA] analysis performance node degree (degree) features previously successful classification schizophrenia]. for IRM model Bayesian average predictions dominated MAP estimate figure for classification analyses normalized feature. table classification results. group ICA proposed IRM model significantly classify random. IRM model higher classification rate significant classifiers. finally, note contrary analysis based temporal correlation ICA PCA approaches classification benefit mutual information higher order dependencies account necessarily reﬂected correlation. such, brain region driven variance anor brain region captured mutual information necessarily captured correlation. Conclusion functional units extracted IRM model correspond previously RSNs]. whereas conventional models assessing functional connectivity-fmri data aim divide brain segregated networks IRM explicitly models relations functional units enabling visualization analysis interactions. using classification models predict subject disease state revealed IRM model higher prediction rate discrimination based components extracted conventional group ICA approach]. irm readily extends directed graphs networks derived task related functional activation. lieve proposed method constitutes promising framework analysis functionally derived brain networks general.',\n",
       " 'PP4180': 'apprenticeship learning variant reinforcement learning, introduced Abbeel] (see]), designed address diﬃculty correctly reward function reinforcement learning problems. basic idea underlying apprenticeship learning learning agent, called apprentice, observe anor agent, called expert, behaving Markov Decision Process (mdp). goal apprentice learn policy good expert policy, relative unknown reward function. weaker requirement usual goal reinforcement learning, find policy maximizes reward. development apprenticeship learning framework prompted observation that, reward functions diﬃcult specify, demonstrations good behavior expert available. refore, observing \\x0cexpert, infer information true reward function needing. existing apprenticeship learning algorithms number limitations. one, typically assume true reward function expressed linear combination set features. however, cases apprentice unwilling unable assume rewards structure. additionally, formulations apprenticeship learning harder reinforcement learning; apprenticeship learning algorithms typically invoke reinforcement learning algorithms subroutines, performance guarantees depend strongly quality subroutines. consequently, apprenticeship learning algorithms suffer challenges large state spaces, exploration. exploitation trade-offs, etc., reinforcement work author student Princeton university. learning algorithms. fact contrary intuition demonstrations expert good expert make problem easier, harder. anor approach expert demonstrations received attention primarily empirical literature passively imitate expert classification algorithm (see, Section comprehensive survey). classification well-studied machine learning problem, leverage knowledge ?easier? problem order solve ?diﬃcult? one. however, formal analysis straightforward learning strategy main recent Ross Bagnell], discussed below). paper, setting apprentice classification algorithm passively imitate observed expert mdp, bound difference apprentice policy expert policy terms accuracy learned classifier. put differently, show apprenticeship learning reduced classification. idea reducing learning problem anor proposed Zadrozny Langford]. main contributions paper pair oretical results. first?show difference apprentice policy expert policy   , error learned classifier. secondly, interestingly, extend result prove difference policy values(?) expert policy close optimal. course, perfectly imitate expert, naturally near-optimal expert policy preferred. result implies furr: near-optimal experts easier imitate, sense fewer demonstration required achieve performance guarantee. important practical consequences. priori expert demonstrating good behavior, result implies fewer demonstrations collected case. yield substantial savings expert demonstrations expensive diﬃcult obtain. related Work Several authors reduced reinforcement learning simpler problems. bagnell] algorithm constructing good nonstationary policy sequence good ?onestep? policies. policies concerned maximizing reward collected single time step, learned observations expert. langford Zadrozny] reduced reinforcement learning sequence classification problems (see Blatt Hero]), problems unusual structure, authors provide small amount guidance data problems collected. kakade Langford] reduced reinforcement learning regression, required additional assumptions easily learning algorithm access entire state space. importantly, work makes standard reinforcement learning assumptions true rewards known, learning algorithm interact directly environment. paper interested settings reward function known, learning algorithm limited passively observing expert. concurrently work, Ross Bagnell] approach reducing imitation learning classification, analysis resembles ours. however, framework requires passive observation expert, focused improving sensitivity reduction horizon length, classification error. assume expert deterministic policy, assumption make. preliminaries finite-horizon mdp, horizon state space infinite, assume action space finite.  initial state distribution, transition function, , specifies next-state distribution state action  assumption make unknown reward function ) rmax states rmax finite upper bound reward state. big notation concealing polynomial dependence problem parameters. give exact bounds body paper. introduce notation definitions policies. policy stationary mapping states distributions actions. case, , denotes probability taking action state Let set stationary policies. policy nonstationary belongs set     times)      case, denotes probability taking action state time also, nonstationary, refers stationary policy equal tth component (stationary nonstationary) policy deterministic action distributions concentrated single action. deterministic policy stationary, ) action state nonstationary) action state time define function? ) nonstationary policy time usual manner: ? ?  ? ?  ?  ? ?    ? ) expected cumulative reward policy starting state time step Note functions nonstationary policy, time step policy defined (?) ? ) ?(? )], optimal policy  satisfies  arg max? (?). write denote (possibly nonstationary) expert policy, VtE) abbreviation? ). goal find nonstationary apprentice policy     note values policies respect unknown reward function. ? distribution state-action pairs time policy words, sample, drawn? drawing ?(? policy time steps generates trajectory   letting write DtE abbreviation?  minor abuse notation, write ? mean: draw state-action pair, ? discard details Justification Reduction Our goal reduce apprenticeship learning classification, describe reduction defined, justify utility reduction. classification problem, learning algorithm training set   , labeled  drawn independently distribution  space finite set labels. learning algorithm definition hyposis class set functions mapping objective learning algorithm find hyposis error) small. purposes, hyposis class pac-learnable exists learning algorithm that, training set size poly(  algorithm  that, probability runs poly(  steps outputs hyposis     inf poly(  error hyposis expression typically depend quantities, number labels-dimension], dependence germane discussion. existence pac-learnable hyposis classes reason reducing apprenticeship learning classification endeavor. suppose apprentice observes independent trajectories expert policy ith trajectory sequence si1 ai1   sih aih key note (sit ait viewed independent sample distribution DtE pac-learnable hyposis class set functions map1 time step ping state space finite action space poly(  apprentice PAC learning algorithm learn hyposis that, )   union)?dte probability   bound, inequality holds probability    small, this policy learned natural choice apprentice policy set classifiers imitate behavior expert. light preceding discussion, remainder paper make assumption apprentice policy. assumption apprentice policy deterministic policy satisfies)?dte)   time steps shown, apprentice policy satisfying Assumption small found high probability, provided expert policy well-approximated pac-learnable hyposis class apprentice trajectories expert. reasonable intuition policy assumption high policy remainder paper devoted confirming intuition. guarantee Any Expert error rate assumption small, apprentice policy closely imitates expert policy hope implies   case, orem shows.  orem assumption holds,    rmax typical classification problem, assumed training test examples drawn distribution. main challenge proving orem assumption hold classification problems reduced apprenticeship learning problem. because, state-action pair (sit ait appearing expert trajectory distributed DtE state-action pair visited apprentice policy follow distribution, behavior apprentice prior time step match expert behavior. strategy proving orem show differences apprentice policy degrade relative expert policy. proceeding, show Assumption implies condition, purposes, convenient. ) lemma   deterministic nonstationary policy. )?dte  ))     , prs?dte, ))  proof. fix ], suppose contradiction prs?dte, ))  bad orwise.    state good, ) good) prs?dte good) )?dte )?dte prs?dte bad) )?dte ) bad) prs?dte good)  prs?dte good))    prs?dte good?? ) bad)  inequality holds)?dte  inequality holds prs?dte good)  chain inequalities contradicts assumption \\x0clemma. lemmas main tools prove orem proofs lemmas, write denote trajectory,       also? denote probability measure induced trajectories policy(? denote sum rewards states trajectory. importantly, definitions (?) ?  lemma proves deterministic policy ?almost? agrees expert policy state time step, worse  lemma   deterministic nonstationary policy. states time steps))        rmax ,   ,   time steps proof. trajectory good ?consistent?   bad orwise.  zsa good bad Rmax good?? rmax good  rmax inequality holds because, union bound assigns fraction measure bad trajectories, maximum reward trajectory HRmax inequality holds good trajectories assigned measure??   deterministic. lemma proves slightly statement Lemma policy agrees expert policy ?almost? state time step, worse  lemma   nonstationary policy. time steps, ,         rmax prs?dte     time steps bad proof. trajectory good orwise.   ?? ?? ?? good bad?? good bad ?? bad bad   rmax?? bad   rmax inequality holds because, union bound assigns fraction measure bad trajectories, maximum reward trajectory HRmax inequality holds assumption rewards nonnegative. ready combine previous lemmas prove orem proof orem apprentice policy satisfies Assumption Lemma choose , prs?dte))      construct ?dummy? policy  follows: For time steps , , state))   states, )) lemma     rmax Lemma      rmax combining inequalities yields     rmax  chosen arbitrarily, set maximizes lower bound. guarantee Good Expert orem makes assumptions expert policy. however, cases reasonable assume expert near-optimal policy (indeed, not, question decision select expert). orem shows dependence classification error significantly expert near-optimal policy. orem assumption holds,     Rmax      suboptimality expert policy   note bound orem varies  interpret bound follows: goal learn apprentice policy expert policy value, double progress goal halving classification error rate. hand, orem suggests error rate reduced factor four. near-optimal expert policy yield weaker dependence expert policy optimal policy, state selects actions as1 as2 uniformly random. deterministic apprentice policy closely imitates expert eir set ) as1 ) as2 eir case classification error however, optimal, actions optimal actions state apprentice policy optimal well. strategy proving orem replace Lemma result namely, Lemma weaker dependence classification error  small. prove Lemma define policies. definitions respect arbitrary nonstationary base policy proof orem make choice base policy. fix deterministic nonstationary policy ,? satisfies,? ))     , states time steps Such policy exists letting  close zero, ,? deterministic policy ?almost? agrees state time step. course, depending choice policy ,? exist small set concern moment; proof orem base policy chosen small like. defined ,? define  follows: For states time steps, ,? )) ,? )    ,  orwise   =? ,? , actions orwise— ,   words, state time step distribution obtained proportionally redistributing probability assigned action,? ) distribution, actions. case, assigns probability action,? ) treated specially, clear proof Lemma immaterial distribution, defined cases; choose uniform distribution definiteness.  deterministic policy defined ) arg max?    , states time steps words) action state time assuming policy reafter. definition requires mixed policies. mixed policy consists finite set deterministic nonstationary policies, distribution policies; mixed policy drawing single policy distribution initial time step, policy exclusively reafter. formally, mixed policy defined set finite component policy deterministic ordered pairs {(?   nonstationary policy ) )   define mixed policy  ,?,+ follows: For component policy time step,? eir+ component policy choice; yields— component policies. probability ) assigned component policy  )  ) number times steps,?  established definitions, ready prove lemmas prove orem lemma   ,?,+   ,?,+ proof. proof backwards induction Clearly?? ? ) states function policy depends reward function Now suppose  ) states states)  induction,?,+  ,?,+  ??   ,    ,    ?    ,    ,    )  ?    ,? ), ?    ),   ,? )) ?    ,? ),  ?    ),  ,? ))   ,? )) ?    ,? ),    ,? )) ?   ,    ,   ?   ),   ,  ? ). equality holds policies straightforwardly definition?  rest derivation uses, order: inductive hyposis; definition  ,?,+ property,?  fact) action respect fact) definition  definition? ). action respect Lemma   ,?,+   ,?    proof.   ,?,+ mixed policy, linearity expectation  ,?,+  ,?,+ component policy   ) proba bility. refore  ,?,+      ,?         ,?    fact probability    assigned component policy identical ,? component policy   lemma   ,?       proof. combining Lemmas yields  ,?      algebraic manipulation  ,?        ,?           ,?     ?   ,?        line, divide ) changing direction inequality assumption  ready combine previous lemmas prove orem proof orem apprentice policy satisfies Assumption Lemma choose , prs?dte))      proof orem construct ?dummy? policy  follows: For time steps , , state))   states, )) lemma )     rmax     ??? rearranging yields Substituting       ???   rmax ) Now observe that, set base policy   definition valid choice  ???    rmax           rmax  rmax      lemma), order. letting proves orem. Apprenticeship learning variant reinforcement learning, introduced Abbeel] (see]), designed address diﬃculty correctly reward function reinforcement learning problems. basic idea underlying apprenticeship learning learning agent, called apprentice, observe anor agent, called expert, behaving Markov Decision Process (mdp). goal apprentice learn policy good expert policy, relative unknown reward function. this weaker requirement usual goal reinforcement learning, find policy maximizes reward. development apprenticeship learning framework prompted observation that, reward functions diﬃcult specify, demonstrations good behavior expert available. refore, observing \\x0cexpert, infer information true reward function needing. existing apprenticeship learning algorithms number limitations. for one, typically assume true reward function expressed linear combination set features. however, cases apprentice unwilling unable assume rewards structure. additionally, formulations apprenticeship learning harder reinforcement learning; apprenticeship learning algorithms typically invoke reinforcement learning algorithms subroutines, performance guarantees depend strongly quality subroutines. consequently, apprenticeship learning algorithms suffer challenges large state spaces, exploration. exploitation trade-offs, etc., reinforcement work author student Princeton university. learning algorithms. this fact contrary intuition demonstrations expert good expert make problem easier, harder. anor approach expert demonstrations received attention primarily empirical literature passively imitate expert classification algorithm (see, Section comprehensive survey). classification well-studied machine learning problem, leverage knowledge ?easier? problem order solve ?diﬃcult? one. however, formal analysis straightforward learning strategy main recent Ross Bagnell], discussed below). paper, setting apprentice classification algorithm passively imitate observed expert mdp, bound difference apprentice policy expert policy terms accuracy learned classifier. put differently, show apprenticeship learning reduced classification. idea reducing learning problem anor proposed Zadrozny Langford]. our main contributions paper pair oretical results. first?show difference apprentice policy expert policy   , error learned classifier. secondly, interestingly, extend result prove difference policy values(?) expert policy close optimal. course, perfectly imitate expert, naturally near-optimal expert policy preferred. but result implies furr: near-optimal experts easier imitate, sense fewer demonstration required achieve performance guarantee. this important practical consequences. priori expert demonstrating good behavior, result implies fewer demonstrations collected case. this yield substantial savings expert demonstrations expensive diﬃcult obtain. Related Work Several authors reduced reinforcement learning simpler problems. bagnell] algorithm constructing good nonstationary policy sequence good ?onestep? policies. policies concerned maximizing reward collected single time step, learned observations expert. langford Zadrozny] reduced reinforcement learning sequence classification problems (see Blatt Hero]), problems unusual structure, authors provide small amount guidance data problems collected. kakade Langford] reduced reinforcement learning regression, required additional assumptions easily learning algorithm access entire state space. importantly, work makes standard reinforcement learning assumptions true rewards known, learning algorithm interact directly environment. paper interested settings reward function known, learning algorithm limited passively observing expert. concurrently work, Ross Bagnell] approach reducing imitation learning classification, analysis resembles ours. however, framework requires passive observation expert, focused improving sensitivity reduction horizon length, classification error. assume expert deterministic policy, assumption make. Preliminaries finite-horizon mdp, horizon state space infinite, assume action space finite. let initial state distribution, transition function, , specifies next-state distribution state action  assumption make unknown reward function ) rmax states Rmax finite upper bound reward state. big notation concealing polynomial dependence problem parameters. give exact bounds body paper. introduce notation definitions policies. policy stationary mapping states distributions actions. case, , denotes probability taking action state Let set stationary policies. policy nonstationary belongs set     times)      case, denotes probability taking action state time also, nonstationary, refers stationary policy equal tth component (stationary nonstationary) policy deterministic action distributions concentrated single action. deterministic policy stationary, ) action state nonstationary) action state time define function? ) nonstationary policy time usual manner: ? ?  ? ?  ?  ? ?    ? ) expected cumulative reward policy starting state time step Note functions nonstationary policy, time step policy defined (?) ? ) ?(? )], optimal policy  satisfies  arg max? (?). write denote (possibly nonstationary) expert policy, VtE) abbreviation? ). our goal find nonstationary apprentice policy     note values policies respect unknown reward function. let? distribution state-action pairs time policy words, sample, drawn? drawing ?(? policy time steps generates trajectory   letting write DtE abbreviation?  minor abuse notation, write ? mean: draw state-action pair, ? discard Details Justification Reduction Our goal reduce apprenticeship learning classification, describe reduction defined, justify utility reduction. classification problem, learning algorithm training set   , labeled  drawn independently distribution  here space finite set labels. learning algorithm definition hyposis class set functions mapping objective learning algorithm find hyposis error) small. for purposes, hyposis class pac-learnable exists learning algorithm that, training set size poly(  algorithm  that, probability runs poly(  steps outputs hyposis    here inf poly(  error hyposis expression typically depend quantities, number labels-dimension], dependence germane discussion. existence pac-learnable hyposis classes reason reducing apprenticeship learning classification endeavor. suppose apprentice observes independent trajectories expert policy ith trajectory sequence si1 ai1   sih aih key note (sit ait viewed independent sample distribution DtE now pac-learnable hyposis class set functions map1 time step ping state space finite action space poly(  apprentice PAC learning algorithm learn hyposis that, )   and \\x0cunion)?dte probability   bound, inequality holds probability    small, This policy learned natural choice apprentice policy set classifiers imitate behavior expert. light preceding discussion, remainder paper make assumption apprentice policy. assumption apprentice policy deterministic policy satisfies)?dte)   time steps shown, apprentice policy satisfying Assumption small found high probability, provided expert policy well-approximated pac-learnable hyposis class apprentice trajectories expert. reasonable intuition policy Assumption high policy remainder paper devoted confirming intuition. Guarantee Any Expert error rate Assumption small, apprentice policy closely imitates expert policy hope implies   this case, orem shows.  orem Assumption holds,    Rmax typical classification problem, assumed training test examples drawn distribution. main challenge proving orem assumption hold classification problems reduced apprenticeship learning problem. this because, state-action pair (sit ait appearing expert trajectory distributed DtE state-action pair visited apprentice policy follow distribution, behavior apprentice prior time step match expert behavior. strategy proving orem show differences apprentice policy degrade relative expert policy. before proceeding, show Assumption implies condition, purposes, convenient. ) Lemma let  deterministic nonstationary policy. )?dte  ))     , prs?dte, ))  proof. fix ], suppose contradiction prs?dte, ))  bad orwise.    say state good, ) good) prs?dte good) )?dte )?dte prs?dte bad) )?dte ) bad) prs?dte good)  prs?dte good))    prs?dte good?? ) bad)  inequality holds)?dte  inequality holds prs?dte good)  this chain inequalities contradicts assumption \\x0clemma. lemmas main tools prove orem proofs lemmas, write denote trajectory,       also? denote probability measure induced trajectories policy(? denote sum rewards states trajectory. importantly, definitions (?) ?  lemma proves deterministic policy ?almost? agrees expert policy state time step, worse  lemma let  deterministic nonstationary policy. states time steps))        Rmax ,   ,   time steps proof. say trajectory good ?consistent?   bad orwise.  Zsa good bad Rmax good?? Rmax good  Rmax inequality holds because, union bound assigns fraction measure bad trajectories, maximum reward trajectory HRmax inequality holds good trajectories assigned measure??   deterministic. lemma proves slightly statement Lemma policy agrees expert policy ?almost? state time step, worse  lemma let  nonstationary policy. time steps, ,         Rmax prs?dte     time steps bad proof. say trajectory good orwise.   ?? ?? ?? good bad?? good bad ?? bad bad   Rmax?? bad   Rmax inequality holds because, union bound assigns fraction measure bad trajectories, maximum reward trajectory HRmax inequality holds assumption rewards nonnegative. ready combine previous lemmas prove orem Proof orem since apprentice policy satisfies Assumption Lemma choose , prs?dte))      now construct ?dummy? policy  follows: For time steps , , state))   states, )) Lemma     Rmax Lemma      Rmax Combining inequalities yields     Rmax  since chosen arbitrarily, set maximizes lower bound. Guarantee Good Expert orem makes assumptions expert policy. however, cases reasonable assume expert near-optimal policy (indeed, not, question decision select expert). orem shows dependence classification error significantly expert near-optimal policy. orem Assumption holds,     Rmax      suboptimality expert policy   note bound orem varies  interpret bound follows: goal learn apprentice policy expert policy value, double progress goal halving classification error rate. hand, orem suggests error rate reduced factor four. near-optimal expert policy yield weaker dependence expert policy optimal policy, state selects actions as1 as2 uniformly random. deterministic apprentice policy closely imitates expert eir set ) as1 ) as2 eir case classification error however, optimal, actions optimal actions state apprentice policy optimal well. our strategy proving orem replace Lemma result namely, Lemma weaker dependence classification error  small. prove Lemma define policies. definitions respect arbitrary nonstationary base policy proof orem make choice base policy. fix deterministic nonstationary policy ,? satisfies,? ))     , states time steps Such policy exists letting  close zero, ,? deterministic policy ?almost? agrees state time step. course, depending choice policy ,? exist small set concern moment; proof orem base policy chosen small like. having defined ,? define  follows: For states time steps, ,? )) ,? )    ,  orwise   =? ,? , actions orwise— ,   words, state time step distribution obtained proportionally redistributing probability assigned action,? ) distribution, actions. case, assigns probability action,? ) treated specially, clear proof Lemma immaterial distribution, defined cases; choose uniform distribution definiteness. let deterministic policy defined ) arg max?    , states time steps words) action state time assuming policy reafter. definition requires mixed policies. mixed policy consists finite set deterministic nonstationary policies, distribution policies; mixed policy drawing single policy distribution initial time step, policy exclusively reafter. more formally, mixed policy defined set finite component policy deterministic ordered pairs {(?   nonstationary policy ) )   define mixed policy  ,?,+ follows: For component policy time step,? eir+ component policy choice; yields— component policies. and probability ) assigned component policy  )  ) number times steps,?  having established definitions, ready prove lemmas prove orem lemma   ,?,+   ,?,+ proof. proof backwards induction Clearly?? ? ) states function policy depends reward function Now suppose  ) states states)  induction,?,+  ,?,+  ??   ,    ,    ?    ,    ,    )  ?    ,? ), ?    ),   ,? )) ?    ,? ),  ?    ),  ,? ))   ,? )) ?    ,? ),    ,? )) ?   ,    ,   ?   ),   ,  ? ). equality holds policies straightforwardly definition?  rest derivation uses, order: inductive hyposis; definition  ,?,+ property,?  fact) action respect fact) definition  definition? ). action respect Lemma   ,?,+   ,?    proof. since  ,?,+ mixed policy, linearity expectation  ,?,+  ,?,+ component policy   ) proba bility. refore  ,?,+      ,?         ,?    here fact probability    assigned component policy identical ,? component policy   lemma   ,?       proof. combining Lemmas yields  ,?      and algebraic manipulation  ,?        ,?           ,?     ?   ,?        line, divide ) changing direction inequality assumption  ready combine previous lemmas prove orem proof orem since apprentice policy satisfies Assumption Lemma choose , prs?dte))      proof orem construct ?dummy? policy  follows: For time steps , , state))   states, )) Lemma )     Rmax     ??? rearranging yields Substituting       ???   Rmax ) Now observe that, set base policy   definition valid choice and ???    Rmax           Rmax  Rmax      Lemma), order. letting proves orem.',\n",
       " 'PP4194': 'machine learning, model quality limited lack suﬃcient training data. data different, related tasks, available, exploit boost performance task transferring relevant information. multitask learning (mtl) considers problem inferring models tasks simultaneously, imposing regularity criteria shared representations order learning tasks. active research focus methods]) explored, providing empirical findings] oretical foundations]. recently, \\x0calso relationships tasks studied]) assuming cluster relationship] hierarchy] tasks. proposed method line research exploits externally provided hierarchical task relations. generality regularization-based MTL approaches makes extend simple cases classification regression Structured Output) learning problems authors contributed equally. work Technical University Berlin]. here, output form discrete class label real valued number, structured entity label sequence, tree, graph. main contributions paper explicitly extend regularization-based MTL formulation svm-struct formulation prediction]. learning methods computationally demanding, combining information tasks leads larger problems, renders interesting applications infeasible. hence, main contribution provide eﬃcient solver problems based bundle methods]. achieves faster convergence refore essential tool cope demands MTL setting. learning successfully applied analysis images, natural language, sequences. interest computational biology analysis dna, RNA protein sequences. field constitutes excellent application area MTL]. computational biology, supervised learning methods model biological processes order predict outcomes ultimately understand better. due complexity biological mechanisms, rich computational models developed, turn require reasonable amount training data. however, biomedical domain, obtaining labeled training examples experiments costly. thus, combining information related tasks cost-effective approach exploit label data. transferring label information tasks, makes sense assume hierarchical task relations. particular, computational biology, evolutionary processes impose task hierarchy]. instance, interested modeling common biological mechanism organisms task corresponds organism. setting, expect longer common evolutionary history organisms, beneficial share information tasks. work, chose challenging problem genome biology demonstrate approach practically feasible terms speed accuracy. initio gene finding], task build accurate model gene subsequently predict gene content newly sequenced genomes refine existing annotations. commonalities sequence features genes organisms, sequence differences made diﬃcult build universal gene finders achieve high accuracy crossorganism prediction. problem ideally suited application proposed-mtl approach. methods Regularization based supervised learning methods, SVM Logistic Regression play central role applications. general form, method consists loss function captures error respect training data    regularizer penalizes model complexity).  case Multitask Learning (mtl), interested obtaining models ..., based sets examples  couple individual tasks, additional regularization term introduced penalizes disagreement individual models]):  ...,  (xnt ynt )},      Special cases include  ]), hyper-parameter controlling strength coupling solutions tasks. tasks, number coupling terms hyper-parameters rise quadratically leading diﬃcult modelselection problem.  Hierarchical Multitask Learning (hmtl) case tasks correspond leaves tree related nodes. ], case taxonomically organized two-class classification tasks investigated, task corresponds species (taxon). idea mimic biological evolution assumed generate specialized molecular processes speciation event root leaf. implemented training examples nodes current subtree., tasks current node), similarity parent classifier induced regularization. thus, node solves optimization problem,    argmin  ——  )  parent node (with special case root node), loss function., hinge-loss). hyper-parameter  , determines contribution regularization origin. parent node parameters., strength coupling node parent). problem equivalently rewritten:    argmin——  , )  For tasks completely decouple learnt independently. parameters root node correspond globally model. refer cases base-line methods comparisons experimental section.  Structured Output Learning Extensions HMTL contrast binary classification, elements output space ., sequences, trees, graphs) structured output problems inherent structure makes sophisticated, problem-specific loss functions desirable. loss \\x0cbetween true label  predicted   measured loss function      widely approach predict label   linearly parametrized model input vector joint feature map    captures dependencies input output]): ) argmax, , .  common approaches estimate model parameters based structured output SVMs]) conditional random fields]). follow approach], estimating parameter vector amounts solving optimization problem    , ) min) ‘(maxhw,   ) regularizer loss function. ) max) k22 obtain structured output support vector machine, margin rescaling hinge-loss. turns combine structured output formulation hierarchical multitask learning straightforward way. extend regularizer) ?-parametrized convex combination multitask regularizer  original term. ) k22 omitting constant terms, arrive,? ) k22 , apply hierarchical multitask learning approach solve node optimization problem:    , ) min,? ) ‘(maxhw,   major diﬃculty remains: solving resulting optimization problems considerably larger single-task case.  Bundle Method Eﬃcient Optimization common approach obtain solution-called cutting-plane column-generation methods. considers growing subsets structures solves restricted optimization problems. algorithm implementing variant strategy based primal optimization appendix (similar]). cutting-plane column generation techniques converge slowly. moreover, size restricted optimization problems grows steadily solving expensive iteration. simple gradient descent order methods directly applied alternatives) continuous non-smooth. approach based bundle methods regularized risk minimization proposed]. case svms, furr relates OCAS method introduced]. order achieve fast convergence, variant methods adapted structured output learning suitable hierarchical multitask learning. objective function,?     , ) ‘(max,   ,? ) defined Section. direct optimization expensive computing involves computing maximum output space. hence, propose optimize estimate ), computed eﬃciently. define estimated empirical empirical loss ) loss ) max,  ,  (?,?)?   accordingly, define estimated objective function,? ). easy verify) ). set pairs ), )) defined suitably chosen, growing subset) . algorithm). general, bundle methods extensions cutting plane methods prox-function stabilize solution approximated function. framework regularized risk minimization, natural prox-function regularizer. apply approach objective) solve min,? ) max{hai proposed], set set cutting planes lower bound limited size. moreover, calculate aggregation cutting plane lower bounds estimated empirical loss solve primal optimization problem) dual space proposed], adopt elegant strategy] obtain aggregated cutting plane  dual solution    respect formulations reach minimum optimized min) maxhai min)  This aggregated plane additional cutting plane iteration step. refore monotonically increasing lower bound estimated empirical loss remove previously generated cutting planes compromising convergence (see] details). algorithm handle (non-)smooth convex loss function subgradient computed. eﬃciently hingeloss, squared hinge-loss, huberloss, logistic-loss. resulting optimization algorithm outlined Algorithm improvements possible: For instance, bypass updating empirical risk estimates line )  finally, Algorithm formulated primal space, easy)  reformulate dual variables making independent dimensionality  Taxonomically Constrained Model Selection Model selection multitask learning diﬃcult, requires hyper-parameter selection different, related tasks dependent manner. approach, Algorithm Bundle Methods Structured Output Algorithm max] details) imal size bundle set linesearch trade-off.   repeat .., argmaxy?? )  )} max,  )} max, ?? : argmin: (?,?)?      end ) Compute  compute) ,? ) max max {hai update)  ?  )  argmin? )   +??  ?   end ) ) ) )  node taxonomy corresponds solving optimization problem subject hyper-parameters (except root node, relevant). hence, direct optimization combinations dependent hyper-parameters model selection feasible cases. refore, propose perform local model selection optimize current node top bottom independently. corresponds taxonomy reducing parameter search space. clarify point, assume perfect binary tree tasks. length path root leaf log2). parameters path dependent. values chosen root inﬂuence optimal choice furr tree. candidate values parameter jointly optimizing interdependent parameters path corresponds optimizing grid log2) contrast log2) proposed local strategy. results Background demonstrate validity approach, applied computational biology problem gene finding. here, task identify genomic regions encoding genes (from RNAs and proteins produced). genomic sequence represented long strings letters (genome sizes range megabases gigabases). prokaryotes (mostly bacteria archaea) gene structures comparably simple. figure): protein coding region starts start codon (one specific-mers prokaryotes) number codon triplets nucleotides each) terminated stop codon (one specific-mers prokaryotes). genic regions transcribed rna, subsequently contained coding region translated protein. parts RNA translated called untranslated region (utr). genes separated anor intergenic regions. protein coding segment depleted stop codons making computational higher problem identifying coding regions straight forward. eukaryotes (animals, plants, etc.) however, coding region interrupted introns, removed RNA translated protein. introns ﬂanked specific sequence signals-called splice sites. figure). presence introns substantially complicates identification transcribed coding regions. particular, insuﬃcient identify regions depleted stop codons determine encoded protein sequence. accurately detect transcribed regions eukaryotic genomes, refore additional experimental data., sequencing RNA fragments). here, key problems computational gene finding) predicting (only) coding regions prokaryotes) predicting exon-intron structure (but coding region) eukaryotes. prokaryotic Gene Intergenic UTR Start Codon Coding region} ATG Stop Codon UTR Intergenic TAA Eukaryotic Gene Intergenic Exon Intron Exon Intron Exon Intergenic} UTR Coding region UTR Figure Panel shows structure prokaryotic gene. protein coding region ﬂanked start stop codon multiple nucleotides. utr denotes untranslated region. panel shows structure eukaryotic gene. transcribed region introns exons. introns ﬂanked splice sites removed rna. remaining sequence UTRs coding region. problem identifying genes posed label sequence learning task, assigns label (out intergenic, transcript start, untranslated region, coding start, coding exon, intron, coding stop, transcript stop) position genome. labels follow grammar dictated biological processes transcription translation (see Figure making suitable apply structured output learning techniques identify genes. biological processes cellular machineries recognize genes slowly evolved time, genes closely related species tend exhibit \\x0csimilar sequence characteristics. refore problems suited application multitask learning: sharing information species expected lead accurate gene predictions compared approaching problem species isolation. currently, genomes prokaryotic eukaryotic species sequenced, genes encoded, standard methods typically infer systematically exploiting reliable information related species. aspects problem. first, focusing eukaryotic gene finding single species, show proposed optimization algorithm quickly converges optimal solution. second, problem prokaryotic gene finding species, demonstrate hierarchical multitask structured output learning significantly improves gene prediction accuracy. supplement, data code found project website3  Eukaryotic Gene Finding Based RNASeq problem detecting exonic, intronic intergenic regions single eukaryotic genome. experimental data RNA sequencing (rna-seq) evidence exonic intronic regions simplicity, assume position genome numbers position experimentally determined exonic intronic, respectively. ideally, exons introns belonging gene constant number confirmations, values vary greatly genes. reality, measurements typically incomplete noisy, inference techniques greatly reconstruct complete gene structures. hmm hmsvm, method employs state model defining allowed transitions states. consists basic states: intergenic, exonic, intron start (donor splice site), intronic, intron end (acceptor splice site). states duplicated times model levels confirmation model mirrored simultaneous predictions genes strands genome (see supplement details). total, states, parameters scoring features derived exon intron confirmation computational splice site predictions (see supplement details). model 1000 parameters. trained model 700 training regions exon/intron structures total length.  million nucleotides (data nematode elegans). column generationbased algorithm (see appendix) Bundle method-based algorithm (algorithm recorded upper lower bounds objective run time. figure). algorithms http://bioweb-mtl similar amount computation iteration (mostly decoding steps), bundle-method showed faster convergence. assessed prediction accuracy three-fold cross-validation procedure individual test sequences consisted large genomic regions mbp) genes. evaluation procedure expected yield unbiased estimates similar whole-genome predictions. prediction accuracy compared recently proposed, widely method called Cuﬄinks]. observed method detects introns transcripts accurately Cuﬄinks data set analyzed. figure). cuﬄinks Our method?score objective Bundle Method Upper Bound Original Upper Bound Original Lower Bound Target Bundle Method Lower Bound iteration Intron Transcript Figure Left panel: Convergence bundle method-based solver versus column generation (log-scale). panel: Prediction accuracy eukaryotic gene finding method comparison state--art method, Cuﬄinks]. -score (harmonic precision recall) assessed based metrics: correctly predicted introns transcripts introns correct (see label).  Gene Finding Multiple Prokaryotic Genomes series experiments evaluated benefit applying SOMTL prokaryotic gene prediction. prediction method modeled prokaryotic genes Markov chain nucleotide level. noneless account biological fact genetic information encoded triplets, model-cycle exon states; details Figure Start Intergenic Stop Exonic3 Start Codon Stop Codon Exonic2 Exonic1 Figure Simple state model prokaryotic gene finding. suitable model prokaryotic gene prediction gene starts start codon. triplet nucleotides) ends stop codon length divisible properties enforced allowing transitions exonic states start stop codons, respectively. property enforced allowing transitions exon state Exonic3 stop codon state. data generation selected subset organisms publicly genomes broadly cover spectrum prokaryotic organisms. order show MTL beneficial distant species, selected representatives domains: bacteria archaea. relationship organisms captured taxonomy shown Figure created based information NCBI website4 organism, generated training annotated gene. genomic sequences cut neighboring genes (splitting intergenic regions equally), minimum distance nucleotides genes maintained. features learning derived nucleotide sequence transcoding numerical representation triplets. resulted binary vectors size non-zero entry. sub-sampled complete dataset examples organism \\x0ccreated datasets training examples, evaluation examples 200 test examples. ftp://ftp.ncbi.nlm.nih.gov/genomes/bacteria/ Figure Species taxonomic hierarchy prokaryotic gene finding. experimental setup For model selection grid parameter ranges [100, 250], .025] node taxonomy. figure). sub-sampling dataset performed times results subsequently averaged. compared MTL algorithm baseline methods, predictors tasks trained information transfer (independent) extreme case, global model fitted tasks based union data sets (union). performance measured-score, harmonic precision recall, precision recall determined nucleotide level. wher exonic nucleotide correctly predicted) singlegene regions. (note due per-nucleotide Markov restriction, however, method exploit gene examples sequence.) results Figure shows results proposed MTL method baseline methods (see Appendix table). observe generally pays combine information organisms, union performs independent. MTL improves naive combination method union-score increases percentage points tumefaciens. average, observe improvement percentage points MTL independent percentage points MTL union, confirming MTL transferring information tasks. addition, bundle method converges fast originally proposed cutting plane method. ?score Independent Union MTL ylo cis tili sla Figure Evaluation MTL baseline methods independent union. discussion introduced regularization-based approach learning setting hierarchical task relations empirically shown validity application computational biology. cope increased problem size encountered MTL setting, developed eﬃcient solver based bundle-methods demonstrated improved convergence behavior compared column generation techniques. applying-mtl algorithm problem prokaryotic gene finding, show sharing information tasks results improved accuracy learning tasks isolation. additionally, taxonomy, relates individual tasks, proved led accurate predictions obtained simply training examples toger. previously shown MTL algorithms excel scenarios limited training data relative complexity problem model]. experiment carried small data set, work required turn approach state--art prokaryotic gene finder. acknowledgments anonymous reviewers insightful comments. moreover, grateful Jonas behr, Jose leiva, Yasemin Altun klaus-robert?uller. work supported German Research Foundation (dfg) grant 1894. Machine learning, model quality limited lack suﬃcient training data. when data different, related tasks, available, exploit boost performance task transferring relevant information. multitask learning (mtl) considers problem inferring models tasks simultaneously, imposing regularity criteria shared representations order learning tasks. this active research focus methods]) explored, providing empirical findings] oretical foundations]. recently, \\x0calso relationships tasks studied]) assuming cluster relationship] hierarchy] tasks. our proposed method line research exploits externally provided hierarchical task relations. generality regularization-based MTL approaches makes extend simple cases classification regression Structured Output) learning problems authors contributed equally. this work Technical University Berlin]. here, output form discrete class label real valued number, structured entity label sequence, tree, graph. one main contributions paper explicitly extend regularization-based MTL formulation svm-struct formulation prediction]. learning methods computationally demanding, combining information tasks leads larger problems, renders interesting applications infeasible. hence, main contribution provide eﬃcient solver problems based bundle methods]. achieves faster convergence refore essential tool cope demands MTL setting. learning successfully applied analysis images, natural language, sequences. interest computational biology analysis dna, RNA protein sequences. this field constitutes excellent application area MTL]. computational biology, supervised learning methods model biological processes order predict outcomes ultimately understand better. due complexity biological mechanisms, rich computational models developed, turn require reasonable amount training data. however, biomedical domain, obtaining labeled training examples experiments costly. thus, combining information related tasks cost-effective approach exploit label data. when transferring label information tasks, makes sense assume hierarchical task relations. particular, computational biology, evolutionary processes impose task hierarchy]. for instance, interested modeling common biological mechanism organisms task corresponds organism. setting, expect longer common evolutionary history organisms, beneficial share information tasks. work, chose challenging problem genome biology demonstrate approach practically feasible terms speed accuracy. initio gene finding], task build accurate model gene subsequently predict gene content newly sequenced genomes refine existing annotations. despite commonalities sequence features genes organisms, sequence differences made diﬃcult build universal gene finders achieve high accuracy crossorganism prediction. this problem ideally suited application proposed-mtl approach. Methods Regularization based supervised learning methods, SVM Logistic Regression play central role applications. general form, method consists loss function captures error respect training data    regularizer penalizes model complexity).  case Multitask Learning (mtl), interested obtaining models ..., based sets examples  couple individual tasks, additional regularization term introduced penalizes disagreement individual models]):  ...,  (xnt ynt )},      Special cases include  ]), hyper-parameter controlling strength coupling solutions tasks. for tasks, number coupling terms hyper-parameters rise quadratically leading diﬃcult modelselection problem.  Hierarchical Multitask Learning (hmtl) case tasks correspond leaves tree related nodes. ], case taxonomically organized two-class classification tasks investigated, task corresponds species (taxon). idea mimic biological evolution assumed generate specialized molecular processes speciation event root leaf. this implemented training examples nodes current subtree., tasks current node), similarity parent classifier induced regularization. thus, node solves optimization problem,    argmin  ——  )  parent node (with special case root node), loss function., hinge-loss). hyper-parameter  , determines contribution regularization origin. parent node parameters., strength coupling node parent). problem equivalently rewritten:    argmin——  , )  For tasks completely decouple learnt independently. parameters root node correspond globally model. refer cases base-line methods comparisons experimental section.  Structured Output Learning Extensions HMTL contrast binary classification, elements output space ., sequences, trees, graphs) structured output problems inherent structure makes sophisticated, problem-specific loss functions desirable. loss \\x0cbetween true label  predicted   measured loss function      widely approach predict label   linearly parametrized model input vector joint feature map    captures dependencies input output]): ) argmax, , .  common approaches estimate model parameters based structured output SVMs]) conditional random fields]). here follow approach], estimating parameter vector amounts solving optimization problem    , ) min) ‘(maxhw,   ) regularizer loss function. for) max) k22 obtain structured output support vector machine, margin rescaling hinge-loss. turns combine structured output formulation hierarchical multitask learning straightforward way. extend regularizer) ?-parametrized convex combination multitask regularizer  original term. when) k22 omitting constant terms, arrive,? ) k22 , thus apply hierarchical multitask learning approach solve node optimization problem:    , ) min,? ) ‘(maxhw,   major diﬃculty remains: solving resulting optimization problems considerably larger single-task case.  Bundle Method Eﬃcient Optimization common approach obtain solution-called cutting-plane column-generation methods. here considers growing subsets structures solves restricted optimization problems. algorithm implementing variant strategy based primal optimization appendix (similar]). cutting-plane column generation techniques converge slowly. moreover, size restricted optimization problems grows steadily solving expensive iteration. simple gradient descent order methods directly applied alternatives) continuous non-smooth. our approach based bundle methods regularized risk minimization proposed]. case svms, furr relates OCAS method introduced]. order achieve fast convergence, variant methods adapted structured output learning suitable hierarchical multitask learning. objective function,?     , ) ‘(max,   and,? ) defined Section. direct optimization expensive computing involves computing maximum output space. hence, propose optimize estimate ), computed eﬃciently. define estimated empirical empirical loss ) loss ) max,  ,  (?,?)?   accordingly, define estimated objective function,? ). easy verify) ). set pairs ), )) defined suitably chosen, growing subset) . algorithm). general, bundle methods extensions cutting plane methods prox-function stabilize solution approximated function. framework regularized risk minimization, natural prox-function regularizer. apply approach objective) solve min,? ) max{hai proposed], set set cutting planes lower bound limited size. moreover, calculate aggregation cutting plane lower bounds estimated empirical loss solve primal optimization problem) dual space proposed], adopt elegant strategy] obtain aggregated cutting plane  dual solution    respect formulations reach minimum optimized min) maxhai min)  This aggregated plane additional cutting plane iteration step. refore monotonically increasing lower bound estimated empirical loss remove previously generated cutting planes compromising convergence (see] details). algorithm handle (non-)smooth convex loss function subgradient computed. this eﬃciently hingeloss, squared hinge-loss, huberloss, logistic-loss. resulting optimization algorithm outlined Algorithm improvements possible: For instance, bypass updating empirical risk estimates line )  finally, Algorithm formulated primal space, easy)  reformulate dual variables making independent dimensionality  Taxonomically Constrained Model Selection Model selection multitask learning diﬃcult, requires hyper-parameter selection different, related tasks dependent manner. for approach, Algorithm Bundle Methods Structured Output Algorithm max] details) imal size bundle set linesearch trade-off.   repeat .., argmaxy?? )  )} max,  )} max, ?? : argmin: (?,?)?      end ) Compute  compute) ,? ) max max {hai Update)  ?  )  argmin? )   +??  ?   end ) ) ) )  node taxonomy corresponds solving optimization problem subject hyper-parameters (except root node, relevant). hence, direct optimization combinations dependent hyper-parameters model selection feasible cases. refore, propose perform local model selection optimize current node top bottom independently. this corresponds taxonomy reducing parameter search space. clarify point, assume perfect binary tree tasks. length path root leaf log2). parameters path dependent. values chosen root inﬂuence optimal choice furr tree. given candidate values parameter jointly optimizing interdependent parameters path corresponds optimizing grid log2) contrast log2) proposed local strategy. Results Background demonstrate validity approach, applied computational biology problem gene finding. here, task identify genomic regions encoding genes (from RNAs and proteins produced). genomic sequence represented long strings letters (genome sizes range megabases gigabases). prokaryotes (mostly bacteria archaea) gene structures comparably simple. figure): protein coding region starts start codon (one specific-mers prokaryotes) number codon triplets nucleotides each) terminated stop codon (one specific-mers prokaryotes). genic regions transcribed rna, subsequently contained coding region translated protein. parts RNA translated called untranslated region (utr). genes separated anor intergenic regions. protein coding segment depleted stop codons making computational higher problem identifying coding regions straight forward. eukaryotes (animals, plants, etc.) however, coding region interrupted introns, removed RNA translated protein. introns ﬂanked specific sequence signals-called splice sites. figure). presence introns substantially complicates identification transcribed coding regions. particular, insuﬃcient identify regions depleted stop codons determine encoded protein sequence. accurately detect transcribed regions eukaryotic genomes, refore additional experimental data., sequencing RNA fragments). here, key problems computational gene finding) predicting (only) coding regions prokaryotes) predicting exon-intron structure (but coding region) eukaryotes. Prokaryotic Gene Intergenic UTR Start Codon Coding region} ATG Stop Codon UTR Intergenic TAA Eukaryotic Gene Intergenic Exon Intron Exon Intron Exon Intergenic} UTR Coding region UTR Figure Panel shows structure prokaryotic gene. protein coding region ﬂanked start stop codon multiple nucleotides. utr denotes untranslated region. panel shows structure eukaryotic gene. transcribed region introns exons. introns ﬂanked splice sites removed rna. remaining sequence UTRs coding region. problem identifying genes posed label sequence learning task, assigns label (out intergenic, transcript start, untranslated region, coding start, coding exon, intron, coding stop, transcript stop) position genome. labels follow grammar dictated biological processes transcription translation (see Figure making suitable apply structured output learning techniques identify genes. because biological processes cellular machineries recognize genes slowly evolved time, genes closely related species tend exhibit \\x0csimilar sequence characteristics. refore problems suited application multitask learning: sharing information species expected lead accurate gene predictions compared approaching problem species isolation. currently, genomes prokaryotic eukaryotic species sequenced, genes encoded, standard methods typically infer systematically exploiting reliable information related species. aspects problem. first, focusing eukaryotic gene finding single species, show proposed optimization algorithm quickly converges optimal solution. second, problem prokaryotic gene finding species, demonstrate hierarchical multitask structured output learning significantly improves gene prediction accuracy. supplement, data code found project website3  Eukaryotic Gene Finding Based RNASeq problem detecting exonic, intronic intergenic regions single eukaryotic genome. experimental data RNA sequencing (rna-seq) evidence exonic intronic regions for simplicity, assume position genome numbers position experimentally determined exonic intronic, respectively. ideally, exons introns belonging gene constant number confirmations, values vary greatly genes. but reality, measurements typically incomplete noisy, inference techniques greatly reconstruct complete gene structures. HMM hmsvm, method employs state model defining allowed transitions states. consists basic states: intergenic, exonic, intron start (donor splice site), intronic, intron end (acceptor splice site). states duplicated times model levels confirmation model mirrored simultaneous predictions genes strands genome (see supplement details). total, states, parameters scoring features derived exon intron confirmation computational splice site predictions (see supplement details). overall model 1000 parameters. trained model 700 training regions exon/intron structures total length.  million nucleotides (data nematode elegans). column generationbased algorithm (see appendix) Bundle method-based algorithm (algorithm recorded upper lower bounds objective run time. figure). whereas algorithms http://bioweb-mtl similar amount computation iteration (mostly decoding steps), bundle-method showed faster convergence. assessed prediction accuracy three-fold cross-validation procedure individual test sequences consisted large genomic regions mbp) genes. this evaluation procedure expected yield unbiased estimates similar whole-genome predictions. prediction accuracy compared recently proposed, widely method called Cuﬄinks]. observed method detects introns transcripts accurately Cuﬄinks data set analyzed. figure). Cuﬄinks Our method?score objective Bundle Method Upper Bound Original Upper Bound Original Lower Bound Target Bundle Method Lower Bound iteration Intron Transcript Figure Left panel: Convergence bundle method-based solver versus column generation (log-scale). right panel: Prediction accuracy eukaryotic gene finding method comparison state--art method, Cuﬄinks]. -score (harmonic precision recall) assessed based metrics: correctly predicted introns transcripts introns correct (see label).  Gene Finding Multiple Prokaryotic Genomes series experiments evaluated benefit applying SOMTL prokaryotic gene prediction. prediction method modeled prokaryotic genes Markov chain nucleotide level. noneless account biological fact genetic information encoded triplets, model-cycle exon states; details Figure Start Intergenic Stop Exonic3 Start Codon Stop Codon Exonic2 Exonic1 Figure Simple state model prokaryotic gene finding. suitable model prokaryotic gene prediction gene starts start codon. triplet nucleotides) ends stop codon length divisible properties enforced allowing transitions exonic states start stop codons, respectively. property enforced allowing transitions exon state Exonic3 stop codon state. data generation selected subset organisms publicly genomes broadly cover spectrum prokaryotic organisms. order show MTL beneficial distant species, selected representatives domains: bacteria archaea. relationship organisms captured taxonomy shown Figure created based information NCBI website4 for organism, generated training annotated gene. genomic sequences cut neighboring genes (splitting intergenic regions equally), minimum distance nucleotides genes maintained. features learning derived nucleotide sequence transcoding numerical representation triplets. this resulted binary vectors size non-zero entry. sub-sampled complete dataset examples organism \\x0ccreated datasets training examples, evaluation examples 200 test examples. ftp://ftp.ncbi.nlm.nih.gov/genomes/bacteria/ Figure Species taxonomic hierarchy prokaryotic gene finding. experimental setup For model selection grid parameter ranges [100, 250], .025] node taxonomy. figure). sub-sampling dataset performed times results subsequently averaged. compared MTL algorithm baseline methods, predictors tasks trained information transfer (independent) extreme case, global model fitted tasks based union data sets (union). performance measured-score, harmonic precision recall, precision recall determined nucleotide level. wher exonic nucleotide correctly predicted) singlegene regions. (note due per-nucleotide Markov restriction, however, method exploit gene examples sequence.) results Figure shows results proposed MTL method baseline methods (see Appendix table). observe generally pays combine information organisms, union performs independent. indeed MTL improves naive combination method union-score increases percentage points tumefaciens. average, observe improvement percentage points MTL independent percentage points MTL union, confirming MTL transferring information tasks. addition, bundle method converges fast originally proposed cutting plane method. ?score Independent Union MTL ylo cis tili sla Figure Evaluation MTL baseline methods independent union. Discussion introduced regularization-based approach learning setting hierarchical task relations empirically shown validity application computational biology. cope increased problem size encountered MTL setting, developed eﬃcient solver based bundle-methods demonstrated improved convergence behavior compared column generation techniques. applying-mtl algorithm problem prokaryotic gene finding, show sharing information tasks results improved accuracy learning tasks isolation. additionally, taxonomy, relates individual tasks, proved led accurate predictions obtained simply training examples toger. previously shown MTL algorithms excel scenarios limited training data relative complexity problem model]. experiment carried small data set, work required turn approach state--art prokaryotic gene finder. Acknowledgments anonymous reviewers insightful comments. moreover, grateful Jonas behr, Jose leiva, Yasemin Altun klaus-robert?uller. this work supported German Research Foundation (dfg) grant 1894.',\n",
       " 'PP4228': 'one main goals scene understanding semantic segmentation images: label diverse set object properties, multiple scales, time identifying spatial extent properties hold. instance, image segmented things (man-made objects, people animals), amorphous regions stuff grass sky, main geometric properties ground plane vertical planes buildings scene. optimal identification properties requires inference spatial supports levels granularity, regions overlap. appears understood successful extraction properties requires models make inferences adaptive spatial \\x0cneighborhoods span patches individual pixels. incorporating segmentation information inform labeling process recently increasingly active research area. initially inferences restricted super-pixel segmentations, recent trends emphasize joint models capabilities represent uncertainty segmentation process]. diﬃculty selection segments adequate spatial support reliable labeling, major diﬃculty design models segmentation labeling layers learned jointly. paper, present joint image segmentation labeling model (jsl) which, bag possibly overlapping figure-ground (binary) segment hyposes, extracted independently multiple image locations scales, constructs joint probability distribution compatible image interpretations tilings) assembled segments, labels. learning, present procedure based Maximum likelihood, partition function tilings labelings increasingly accurately approximated iteration, including incorrect configurations model rates probable. prevents supported, part, mcext-025481, CNCSIS uefiscu, pnii/2009. cpmc figure-ground Segments Labelings Tilings JSL FGTiling Sky) Bldg-objfg-obj Water Sky) Bldg-obj-obj Water Sky) Bldg-obj-obj Water Figure Overview joint segment composition categorization framework. image extract bag figure-ground segmentations, constrained spatial locations scales, CPMC algorithm] retain figure segments algorithms segment bagging). segments composed image interpretations (tilings) FGTiling]. brief, segments nodes consistency graph segments spatially overlap connected edge. valid compositions (tilings) obtained computing maximal cliques consistency graph. multiple tilings generated image. tilings \\x0cconsist subsets segments induce residual regions pixels belonging segments selected tiling. labeling (jsl), configurations scored based category-dependent properties measured midlevel category-independent properties measured dependency graph subset consistency graph connecting spatially neighboring segments share boundary. model parameters [??    jointly learned Maximum Likelihood based incremental Saddle Point partition function approximation. notice segment appearing tilings image constrained label (red vertical edges). cyclic behavior leads stable optimization process. method jointly learns midlevel, category-independent parameters segment composition model, category-sensitive parameters labeling model segments. knowledge model joint image segmentation labeling, accommodates inference learning, common, consistent probabilistic framework. show procedure matches state art Stanford], VOC2010 dataset% accuracy test set achieved. framework reviewed fig.   Related Work One approach recognize elements image accurately partition regions based low mid-level statistical regularities, label regions, pursued Barnard. ]. labeling problem reduced small number classification problems. however, existing mid-level segmentation algorithms generate unique, accurate segmentation image, multiple images, set generic parameters]. achieve recognition, tasks require multiple overlapping spatial supports provided segmentations. segmenting object parts regions finer granularity, labels decided locally, level pixels] superpixels], based measurements collected neighborhoods limited spatial support. inconsistent label configurations resolved smoothing neighboring responses, encouraging consistency labels belonging regions similar low-level properties]. models effective local appearance statistics discriminative, case amorphous stuff (water, grass), inference harder constrain shape recognition, requires longer-range interactions groups measurements. introduce constraints estimating categories occur image global classifiers, bias inference label distribution]. complementary research trend segment recognize categories based features extracted competing image regions larger spatial support (extended regions). extended regions rectangles produced bounding box detectors]. responses combined single pixel superpixel layer, obtain final labeling. extended regions arise multiple full-image segmentations]. computing segmentations multiple times parameters, chances increase Probabilistic Segmentation Labeling Let segments accurate. multiple segmentations aggregated inclusion hierarchy], obtained independently. work. ] generative models drive sequential-segmentation process, formulated Data Driven Markov Chain Monte Carlo inference. recently, Gould. ] proposed model segmentation labeling region hyposes generated sequential procedure, uniform label swaps pixels contained inside individual segment proposals accepted reduce global energy function. kumar Koller] proposed improved joint inference dual-decomposition. approach segmentation labeling layered rar simultaneous, learning segmentation labeling parameters performed jointly (rar separately), probabilistic framework.  set (bag) segments image case, segments obtained publicly CPMC algorithm], represent figureground hyposes, computed independently applying constraints spatial locations scales image Subsets segments bag form power set— elements. focus restriction power set image, tiling set), property segments contained subset tiling) spatially overlap subset maximal)          . , overlap}. tiling) segments labeled category labels. call labeling mapping obtained assigning labels segments tiling   — ,   label segment— (one label corresponds segment Let) set labelings image) sum valid segment compositions (tilings) image), label space each. define joint probability distribution tilings labelings, ), exp ) ) ) exp ), normalizer partition function) ), ), parameters model. constrained probability distribution defined sets: set segments tiling index set labels segments, cardinality.  defined , )  parameters    additive decomposition viewed sum term), encoding mid-level, category independent score tiling anor category-dependent score), encoding potential labeling) tiling components, defined interactions unary pairwise terms. potential labeling), ?lli ) ?lli ?nsl \\x0cwith ?lli ?lli unary pairwise, label-dependent potentials, Nsli label relevant neighborhood experiments Nsli unary pairwise terms linear Some figure-ground segments) spatially overlap. call segmentation assembled non-overlapping figure-ground segments tiling, tiling toger set labels segments labeling (rar labeled tiling). parameters. ?lli  ?lli ?lli encodes segment exhibit regularities typical objects belonging class potential tiling defined  ) ?nst  unary pairwise, label-independent potential functions, Nsti local image neighborhood. nsti share boundary part overlap}. terms linear parameters, similar components category dependent potential).  encodes segment exhibits generic object regularities (details segmentation model, found]). inference: Given image inference optimal tiling labeling? ?  ? ?  argmax  Our inference methodology sec.  learning: During learning optimize parameters maximize likelihood) ground truth model: argmax  argmax  log )  ground truth labeled tilings image learning methodology, including incremental saddle point approximation partition function presented sec.  inference Tilings Labelings Given image bag multiple figure-ground segments extracted CPMC], inference performed composing number plausible tilings subsets segments, labeling tiling spatial inference methods. inference algorithm computing (sampling) tilings associates segment node consistency graph edge exists pairs nodes \\x0ccorresponding segments spatially overlap. cliques consistency graph correspond alternative segmentations image constructed basic segments. algorithm] eﬃciently find number plausible maximal weighted cliques, scored). maximum— distinct maximal cliques (tilings) returned, segment contained inference labels segments tiling performed number reliable methods work tree-reweighted belief propagation trw]. maximum) computed selecting labeling highest probability) tilings generated segmentation algorithm. set— figure-ground segments, total complexity inference steps required sample tilings], maxsi {—nsti —}, complexity inference trw (with complexity, say, computed tiling, steps select highest scoring labeling. — 200 joint inference labelings tilings takes seconds image implementation produces set plausible segmentation labeling hyposes learning, next. incremental Saddle Point Learning Fundamental maximum likelihood learning tractable, stable suﬃciently accurate estimate partition function). number terms ), exponential number figure-ground segments number labels. reviewed sec. approximate tilings distribution image number configurations bounded number figureground segments. replaces exponential set terms partition function) sum tilings) set size—. turn, tiling labeled exponentially ways? sum partition function), running labelings tiling. possibility deal exponential sum models loopy dependencies pseudo-marginal Approximation (pma) estimates ) loopy computes gradients expectations estimated marginals. kumar. ] found approximation perform learning conditional random fields pixel labeling. requires inference tilings optimization iteration. 484 iterations required convergence VOC dataset, strategy case 140 times longer learning strategy based incremental saddle-point approximations presented (below), requires hours learning. run time, PMA produce satisfactory results model (sec. ). anor possibility approximate exponential sum labels largest term, obtained probable configuration saddle-point approximation). however, approach behave erratically result ﬂips MAP configurations approximate partition function (sec. ). ensure stability learning accuracy, incremental saddle point approximation partition function. obtained accumulating incorrect (?offending?) labelings rated probable current model, learning iteration) denotes set partition function image computed learning iteration) , argmax  ground truth labeling image set) configurations compute (analytic) gradient quasi-newton methods optimize). learning progresses, labelings added partition function estimate accurate. learning procedure stops eir) label configurations incrementally generated, case exact partition function unbiased estimates parameters obtained) subset configuration space considered partition function approximation ?offending? configurations set generated previous learning (and inference) iteration. case biased estimate obtained. extent inevitable learning models loopy dependencies exponential state spaces. practice, datasets worked, learning algorithm converged iterations. experiments (sec. ), show learning significantly stable standard saddle-point approximations. experiments evaluate quality semantic segmentation produced models datasets: Stanford Background Dataset], VOC2010 Pascal Segmentation Challenge]. stanford Background Dataset 715 images comprises domains annotation: semantic classes geometric classes. task label pixel image types properties. dataset mid-level segmentation annotations individual objects, initially learn parameters segmentation model (see sec. ]). evaluation dataset performed cross-validation folds]. evaluation criterion pixel (labeling) accuracy. voc2010 dataset accepted challenging object-class segmentation benchmarks. dataset annotation individual objects, learn mid-level segmentation parameters (?). unlike stanford, pixels annotated, VOC objects classes ground truth labels. evaluation criterion VOC score: average per-class overlap pixels labeled class respective ground truth annotation3. quality segments tilings: generate bag figureground segments image publicly CPMC code]. cpmc algorithm generates large pool bag) figure-ground segmentations, scores mid-level properties, returns overlap measure segments]. Stanford Geometry Stanford Semantics VOC2010 Object Classes max. pixel accuracy max. voc score Method JSL Gould. ] \\x0csemantic Geometry Table left: Study maximum achievable labeling accuracy tiling set, Stanford voc2010. study tiling closest segmentation ground truth assigns ?perfect? pixel labels based ground truth. contrast, labeling accuracy obtain automatically Stanford geometry Stanford semantic voc2010. shows potential bottlenecks reaching maximum values training (ranking) labeling, rar spatial segment layouts tiling configurations produced. average number segments tiling Stanford voc. right: Mean pixel accuracies Stanford Labeling dataset. obtain results comparable state--art challenging full-image labeling problem. results significant, tilings (image segmentations) made average segments image. method competitive object segmentation datasets voc2010, object granularity higher regions large spatial support decisive effective recognition (table). top ranked. online version pre-trained models voc, tend discard background regions, VOC none. Stanford experiments, retrain CPMC segment ranker stanford segment layout annotations. generated segment bags 200 segments Stanford dataset, 100 segments VOC dataset. model sample tilings methodology] (see) sec. ). table left) labeling performance upper-bounds datasets figure-ground segments tilings produced. upper bounds high problems, quality segments tilings limit final labeling performance, compared current state--art. furr detail figure-ground segment pool quality (cpmc) assembly complete image interpretations (fgtiling), refer]. labeling performance: tiling component model) unary pairwise parameters (?) voc2010, unary parameters (?) stanford. detail features]. discuss features labeling component model) section. voc2010 Stanford meta-features unary, category-dependent terms. type meta-feature produced output regressors trained specific image features next) predict overlap input segments putative categories. metafeature regressor) category. type meta-feature obtained object detector] segment presented. detectors operate bounding boxes, determine segment class scores bounding box overlapping bounding box enclosing segment. target semantic concepts Stanford VOC2010 datasets widely different, label-dependent unary terms based features. cases pairwise features connecting segments (nsl encodes full connectivity), belonging \\x0ctiling. pairwise features simply square matrix values set]. way, model learn avoid patterns label-occurrence. stanford Background dataset, train types unary meta-features class, semantic geometric classes. unary meta-feature output regressor trained publicly features Hoiem. ], features Gould. ]. feature vectors transformed randomized feature map approximates gaussian-rbf kernel]. methodology work linear models randomized feature map, exploit non-linear kernel embeddings. summarizing, Stanford geometry, parameters, unary parameters classes, metafeatures bias pairwise parameters), Stanford semantic labels parameters,  unary classes, metafeatures bias, pair-wise, upper triangle 8x8 matrix). person person pottedplant bird horse bicycle bicycle person sofa bird train bird chair Figure (best viewed color) Semantic segmentation results method images VOC2010 test set: images algorithm performs satisfactorily, examples algorithm works well. notice identifying multiple objects class framework. stanford dataset, background regions grass sky shapeless locally discriminative. cases methods relying pixel-level descriptors obtain good results. baseline]). turn, outdoor datasets stuff challenging method relies segmentations (tilings) average segments image (table left). results obtain comparable Gould. ], visible table right. evaluation criterion methods: pixel accuracy. voc2010 dataset, performance evaluated VOC score, average per-class overlap pixels labeled class respective ground truth class. unary meta-features well. output SVM regressors trained] publicly features]. regressors predict class scores directly segments, based features: bag words gray-level SIFT] color SIFT] defined foreground background individual segment, pyramid HOGs parameters. multiple chi-square kernels, exp(?? )) combined]. unary meta-feature \\x0cuse outputs deformable part model detectors]. summarizing, category-dependent unary parameters,  classes, metafeatures bias), 210 category-dependent pairwise parameters (upper triangle 21x21 matrix). results, match slightly improve recent winners 2010 VOC challenge, reported table particular, method produces highest VOC score average classes, scores individual classes. images fig. show algorithm produces correct labelings. notice boundaries produced tilings align boundaries individual objects, multiple nearby objects class. impact segmentation labeling methods: evaluate inference method] (using code provided authors), VOC 2010 dataset, input segments potentials jsl. inference time++ implementation] comparable MATLAB implementations FGtiling jsl. score obtained] model% higher score obtained authors piece-wise training difclasses Background Aeroplane Bicycle Bird Boat Bottle Bus Car JSL CHD BSS Classes Cat Chair Cow DiningTable Dog Horse Motorbike Person JSL CHD BSS Classes PottedPlant Sheep Sofa Train/monitor Average JSL CHD BSS Table Per class results averages obtained method (jsl) top-scoring methods VOC2010 segmentation challenge (chd: cvc-harmony-det], bss: bonn-svrsegm]). compared VOC2010 participants, proposed method obtains scores classes, superior class average, standard measure ranking. top scores class marked bold. results methods found]. note JSL meta-features) CHD trained additional bounding box data images training set object detection. additional training data class average obtained BSS]. 160 100 learning iteration \\x0cwithout incremental. labelings ?logz 120 VOC score incremental 140 labelings total labelings Learning iteration learning iteration Figure left: negative log) end iteration, standard (non-incremental) incremental saddle-point approximations partition function. stable accurate incremental saddle-point approximation partition function, algorithm successfully learn. results obtained training voc2010 ?trainval? (train+validation) dataset. center: VOC2010 labeling score function learning iteration (training voc2010 ?trainval?). right: Number labeling configurations added partition function expansion learning proceeds voc2010. configurations added iterations. ferent pool segments% lower score jsl. suggests layered strategy based selecting compact set representative segmentations, labeling accurate sequentially searching segments labels. practice, proposed JSL framework depend fgtiling/cpmc provide segmentations. instead, segmentation method. tested JSL framework (learning inference) Stanford dataset, segmentations produced Ultrametric Contour Map (ucm) hierarchical segmentation method]. obtain similar number segments CPMC (200 image), selected segmentation levels. features parameters computed before. bag segments image derived UCM segmentations, segmentations tiling configurations image. case, scores semantic geometric classes, respectively, showing robustness JSL input segmentations (see table right). learning performance: learning experiments, model parameters initialized null vector, learning proceeds, unary terms set one. figure left center, shows comparisons learning incremental saddle point approximation partition function, VOC 2010 dataset. accumulating labelings incrementally, learning algorithm exhibits erratic behavior overfits? small number labelings estimate partition function produce results consecutive iterations. figure right, shows number total labelings added learning iteration. learning parameters VOC 2010 PMA 180 hours produced VOC score%. stopping learning PMA hours (slightly hrs required incremental saddle point approximation) results VOC score%. conclusion presented joint image segmentation labeling model (jsl) which, bag figure-ground image segment hyposes, constructs joint probability distribution compatible image interpretations assembled segments, labeling. process interpreted sampling maximal cliques graph connecting segments spatially overlap, sampling labels segments, conditioned choice tiling. propose joint learning procedure based Maximum Likelihood partition function tilings labelings increasingly accurately approximated training, including incorrect configurations model rates probable. ensures mistakes carried uncorrected future training iterations, produces stable accurate learning schedules. show models learned eﬃciently match state art Stanford dataset, VOC2010% accuracy test set achieved. One main goals scene understanding semantic segmentation images: label diverse set object properties, multiple scales, time identifying spatial extent properties hold. for instance, image segmented things (man-made objects, people animals), amorphous regions stuff grass sky, main geometric properties ground plane vertical planes buildings scene. optimal identification properties requires inference spatial supports levels granularity, regions overlap. appears understood successful extraction properties requires models make inferences adaptive spatial \\x0cneighborhoods span patches individual pixels. incorporating segmentation information inform labeling process recently increasingly active research area. while initially inferences restricted super-pixel segmentations, recent trends emphasize joint models capabilities represent uncertainty segmentation process]. one diﬃculty selection segments adequate spatial support reliable labeling, major diﬃculty design models segmentation labeling layers learned jointly. paper, present joint image segmentation labeling model (jsl) which, bag possibly overlapping figure-ground (binary) segment hyposes, extracted independently multiple image locations scales, constructs joint probability distribution compatible image interpretations tilings) assembled segments, labels. for learning, present procedure based Maximum likelihood, partition function tilings labelings increasingly accurately approximated iteration, including incorrect configurations model rates probable. this prevents supported, part, mcext-025481, CNCSIS uefiscu, pnii/2009. CPMC figure-ground Segments Labelings Tilings JSL FGTiling Sky) Bldg-objfg-obj Water Sky) Bldg-obj-obj Water Sky) Bldg-obj-obj Water Figure Overview joint segment composition categorization framework. given image extract bag figure-ground segmentations, constrained spatial locations scales, CPMC algorithm] retain figure segments algorithms segment bagging). segments composed image interpretations (tilings) FGTiling]. brief, segments nodes consistency graph segments spatially overlap connected edge. valid compositions (tilings) obtained computing maximal cliques consistency graph. multiple tilings generated image. tilings \\x0cconsist subsets segments induce residual regions pixels belonging segments selected tiling. for labeling (jsl), configurations scored based category-dependent properties measured midlevel category-independent properties measured dependency graph subset consistency graph connecting spatially neighboring segments share boundary. model parameters [??    jointly learned Maximum Likelihood based incremental Saddle Point partition function approximation. notice segment appearing tilings image constrained label (red vertical edges). cyclic behavior leads stable optimization process. method jointly learns midlevel, category-independent parameters segment composition model, category-sensitive parameters labeling model segments. knowledge model joint image segmentation labeling, accommodates inference learning, common, consistent probabilistic framework. show procedure matches state art Stanford], VOC2010 dataset% accuracy test set achieved. our framework reviewed fig.   Related Work One approach recognize elements image accurately partition regions based low mid-level statistical regularities, label regions, pursued Barnard. ]. labeling problem reduced small number classification problems. however, existing mid-level segmentation algorithms generate unique, accurate segmentation image, multiple images, set generic parameters]. achieve recognition, tasks require multiple overlapping spatial supports provided segmentations. segmenting object parts regions finer granularity, labels decided locally, level pixels] superpixels], based measurements collected neighborhoods limited spatial support. inconsistent label configurations resolved smoothing neighboring responses, encouraging consistency labels belonging regions similar low-level properties]. models effective local appearance statistics discriminative, case amorphous stuff (water, grass), inference harder constrain shape recognition, requires longer-range interactions groups measurements. one introduce constraints estimating categories occur image global classifiers, bias inference label distribution]. complementary research trend segment recognize categories based features extracted competing image regions larger spatial support (extended regions). extended regions rectangles produced bounding box detectors]. responses combined single pixel superpixel layer, obtain final labeling. extended regions arise multiple full-image segmentations]. computing segmentations multiple times parameters, chances increase Probabilistic Segmentation Labeling Let segments accurate. multiple segmentations aggregated inclusion hierarchy], obtained independently. work. ] generative models drive sequential-segmentation process, formulated Data Driven Markov Chain Monte Carlo inference. recently, Gould. ] proposed model segmentation labeling region hyposes generated sequential procedure, uniform label swaps pixels contained inside individual segment proposals accepted reduce global energy function. kumar Koller] proposed improved joint inference dual-decomposition. our approach segmentation labeling layered rar simultaneous, learning segmentation labeling parameters performed jointly (rar separately), probabilistic framework.  set (bag) segments image case, segments obtained publicly CPMC algorithm], represent figureground hyposes, computed independently applying constraints spatial locations scales image Subsets segments bag form power set— elements. focus restriction power set image, tiling set), property segments contained subset tiling) spatially overlap subset maximal)          . , overlap}. each tiling) segments labeled category labels. call labeling mapping obtained assigning labels segments tiling   — ,   label segment— (one label corresponds segment Let) set labelings image) sum valid segment compositions (tilings) image), label space each. define joint probability distribution tilings labelings, ), exp ) ) ) exp ), normalizer partition function) ), ), parameters model. constrained probability distribution defined sets: set segments tiling index set labels segments, cardinality.  defined , )  parameters    additive decomposition viewed sum term), encoding mid-level, category independent score tiling anor category-dependent score), encoding potential labeling) tiling components, defined interactions unary pairwise terms. potential labeling), ?lli ) ?lli ?nsl \\x0cwith ?lli ?lli unary pairwise, label-dependent potentials, Nsli label relevant neighborhood experiments Nsli unary pairwise terms linear Some figure-ground segments) spatially overlap. call segmentation assembled non-overlapping figure-ground segments tiling, tiling toger set labels segments labeling (rar labeled tiling). parameters. ?lli  ?lli for ?lli encodes segment exhibit regularities typical objects belonging class potential tiling defined  ) ?nst  unary pairwise, label-independent potential functions, Nsti local image neighborhood. nsti share boundary part overlap}. both terms linear parameters, similar components category dependent potential). for encodes segment exhibits generic object regularities (details segmentation model, found]). inference: Given image inference optimal tiling labeling? ?  ? ?  argmax  Our inference methodology sec.  learning: During learning optimize parameters maximize likelihood) ground truth model: argmax  argmax  log )  ground truth labeled tilings image our learning methodology, including incremental saddle point approximation partition function presented sec.  Inference Tilings Labelings Given image bag multiple figure-ground segments extracted CPMC], inference performed composing number plausible tilings subsets segments, labeling tiling spatial inference methods. inference algorithm computing (sampling) tilings associates segment node consistency graph edge exists pairs nodes \\x0ccorresponding segments spatially overlap. cliques consistency graph correspond alternative segmentations image constructed basic segments. algorithm] eﬃciently find number plausible maximal weighted cliques, scored). maximum— distinct maximal cliques (tilings) returned, segment contained inference labels segments tiling performed number reliable methods work tree-reweighted belief propagation trw]. maximum) computed selecting labeling highest probability) tilings generated segmentation algorithm. given set— figure-ground segments, total complexity inference steps required sample tilings], maxsi {—nsti —}, complexity inference trw (with complexity, say, computed tiling, steps select highest scoring labeling. for— 200 joint inference labelings tilings takes seconds image implementation produces set plausible segmentation labeling hyposes learning, next. Incremental Saddle Point Learning Fundamental maximum likelihood learning tractable, stable suﬃciently accurate estimate partition function). number terms ), exponential number figure-ground segments number labels. reviewed sec. approximate tilings distribution image number configurations bounded number figureground segments. this replaces exponential set terms partition function) sum tilings) set size—. turn, tiling labeled exponentially ways? sum partition function), running labelings tiling. one possibility deal exponential sum models loopy dependencies pseudo-marginal Approximation (pma) estimates ) loopy computes gradients expectations estimated marginals. kumar. ] found approximation perform learning conditional random fields pixel labeling. however requires inference tilings optimization iteration. with 484 iterations required convergence VOC dataset, strategy case 140 times longer learning strategy based incremental saddle-point approximations presented (below), requires hours learning. run time, PMA produce satisfactory results model (sec. ). anor possibility approximate exponential sum labels largest term, obtained probable configuration saddle-point approximation). however, approach behave erratically result ﬂips MAP configurations approximate partition function (sec. ). ensure stability learning accuracy, incremental saddle point approximation partition function. this obtained accumulating incorrect (?offending?) labelings rated probable current model, learning iteration) denotes set partition function image computed learning iteration) , argmax  ground truth labeling image set) configurations compute (analytic) gradient quasi-newton methods optimize). learning progresses, labelings added partition function estimate accurate. our learning procedure stops eir) label configurations incrementally generated, case exact partition function unbiased estimates parameters obtained) subset configuration space considered partition function approximation ?offending? configurations set generated previous learning (and inference) iteration. case biased estimate obtained. this extent inevitable learning models loopy dependencies exponential state spaces. practice, datasets worked, learning algorithm converged iterations. experiments (sec. ), show learning significantly stable standard saddle-point approximations. Experiments evaluate quality semantic segmentation produced models datasets: Stanford Background Dataset], VOC2010 Pascal Segmentation Challenge]. Stanford Background Dataset 715 images comprises domains annotation: semantic classes geometric classes. task label pixel image types properties. dataset mid-level segmentation annotations individual objects, initially learn parameters segmentation model (see sec. ]). evaluation dataset performed cross-validation folds]. evaluation criterion pixel (labeling) accuracy. VOC2010 dataset accepted challenging object-class segmentation benchmarks. this dataset annotation individual objects, learn mid-level segmentation parameters (?). unlike stanford, pixels annotated, VOC objects classes ground truth labels. evaluation criterion VOC score: average per-class overlap pixels labeled class respective ground truth annotation3. quality segments tilings: generate bag figureground segments image publicly CPMC code]. cpmc algorithm generates large pool bag) figure-ground segmentations, scores mid-level properties, returns overlap measure segments]. Stanford Geometry Stanford Semantics VOC2010 Object Classes max. pixel accuracy max. voc score Method JSL Gould. ] \\x0csemantic Geometry Table left: Study maximum achievable labeling accuracy tiling set, Stanford voc2010. study tiling closest segmentation ground truth assigns ?perfect? pixel labels based ground truth. contrast, labeling accuracy obtain automatically Stanford geometry Stanford semantic voc2010. this shows potential bottlenecks reaching maximum values training (ranking) labeling, rar spatial segment layouts tiling configurations produced. average number segments tiling Stanford voc. right: Mean pixel accuracies Stanford Labeling dataset. obtain results comparable state--art challenging full-image labeling problem. results significant, tilings (image segmentations) made average segments image. method competitive object segmentation datasets voc2010, object granularity higher regions large spatial support decisive effective recognition (table). top ranked. online version pre-trained models voc, tend discard background regions, VOC none. for Stanford experiments, retrain CPMC segment ranker stanford segment layout annotations. generated segment bags 200 segments Stanford dataset, 100 segments VOC dataset. model sample tilings methodology] (see) sec. ). table left) labeling performance upper-bounds datasets figure-ground segments tilings produced. upper bounds high problems, quality segments tilings limit final labeling performance, compared current state--art. for furr detail figure-ground segment pool quality (cpmc) assembly complete image interpretations (fgtiling), refer]. labeling performance: tiling component model) unary pairwise parameters (?) voc2010, unary parameters (?) stanford. detail features]. discuss features labeling component model) section. VOC2010 Stanford meta-features unary, category-dependent terms. one type meta-feature produced output regressors trained specific image features next) predict overlap input segments putative categories. metafeature regressor) category. type meta-feature obtained object detector] segment presented. detectors operate bounding boxes, determine segment class scores bounding box overlapping bounding box enclosing segment. since target semantic concepts Stanford VOC2010 datasets widely different, label-dependent unary terms based features. cases pairwise features connecting segments (nsl encodes full connectivity), belonging \\x0ctiling. pairwise features simply square matrix values set]. way, model learn avoid patterns label-occurrence. Stanford Background dataset, train types unary meta-features class, semantic geometric classes. unary meta-feature output regressor trained publicly features Hoiem. ], features Gould. ]. each feature vectors transformed randomized feature map approximates gaussian-rbf kernel]. using methodology work linear models randomized feature map, exploit non-linear kernel embeddings. summarizing, Stanford geometry, parameters, unary parameters classes, metafeatures bias pairwise parameters), Stanford semantic labels parameters,  unary classes, metafeatures bias, pair-wise, upper triangle 8x8 matrix). person person pottedplant bird horse bicycle bicycle person sofa bird train bird chair Figure (best viewed color) Semantic segmentation results method images VOC2010 test set: images algorithm performs satisfactorily, examples algorithm works well. notice identifying multiple objects class framework. Stanford dataset, background regions grass sky shapeless locally discriminative. cases methods relying pixel-level descriptors obtain good results. baseline]). turn, outdoor datasets stuff challenging method relies segmentations (tilings) average segments image (table left). results obtain comparable Gould. ], visible table right. evaluation criterion methods: pixel accuracy. VOC2010 dataset, performance evaluated VOC score, average per-class overlap pixels labeled class respective ground truth class. unary meta-features well. output SVM regressors trained] publicly features]. regressors predict class scores directly segments, based features: bag words gray-level SIFT] color SIFT] defined foreground background individual segment, pyramid HOGs parameters. multiple chi-square kernels, exp(?? )) combined]. unary meta-feature \\x0cuse outputs deformable part model detectors]. summarizing, category-dependent unary parameters,  classes, metafeatures bias), 210 category-dependent pairwise parameters (upper triangle 21x21 matrix). results, match slightly improve recent winners 2010 VOC challenge, reported table particular, method produces highest VOC score average classes, scores individual classes. images fig. show algorithm produces correct labelings. notice boundaries produced tilings align boundaries individual objects, multiple nearby objects class. impact segmentation labeling methods: evaluate inference method] (using code provided authors), VOC 2010 dataset, input segments potentials jsl. inference time++ implementation] comparable MATLAB implementations FGtiling jsl. score obtained] model% higher score obtained authors piece-wise training difclasses Background Aeroplane Bicycle Bird Boat Bottle Bus Car JSL CHD BSS Classes Cat Chair Cow DiningTable Dog Horse Motorbike Person JSL CHD BSS Classes PottedPlant Sheep Sofa Train/monitor Average JSL CHD BSS Table Per class results averages obtained method (jsl) top-scoring methods VOC2010 segmentation challenge (chd: cvc-harmony-det], bss: bonn-svrsegm]). compared VOC2010 participants, proposed method obtains scores classes, superior class average, standard measure ranking. top scores class marked bold. results methods found]. note JSL meta-features) CHD trained additional bounding box data images training set object detection. using additional training data class average obtained BSS]. 160 100 learning iteration \\x0cwithout incremental. labelings ?logz 120 VOC score incremental 140 labelings total labelings Learning iteration learning iteration Figure left: negative log) end iteration, standard (non-incremental) incremental saddle-point approximations partition function. without stable accurate incremental saddle-point approximation partition function, algorithm successfully learn. results obtained training voc2010 ?trainval? (train+validation) dataset. center: VOC2010 labeling score function learning iteration (training voc2010 ?trainval?). right: Number labeling configurations added partition function expansion learning proceeds voc2010. most configurations added iterations. ferent pool segments% lower score jsl. this suggests layered strategy based selecting compact set representative segmentations, labeling accurate sequentially searching segments labels. practice, proposed JSL framework depend fgtiling/cpmc provide segmentations. instead, segmentation method. tested JSL framework (learning inference) Stanford dataset, segmentations produced Ultrametric Contour Map (ucm) hierarchical segmentation method]. obtain similar number segments CPMC (200 image), selected segmentation levels. features parameters computed before. bag segments image derived UCM segmentations, segmentations tiling configurations image. case, scores semantic geometric classes, respectively, showing robustness JSL input segmentations (see table right). learning performance: learning experiments, model parameters initialized null vector, learning proceeds, unary terms set one. figure left center, shows comparisons learning incremental saddle point approximation partition function, VOC 2010 dataset. without accumulating labelings incrementally, learning algorithm exhibits erratic behavior overfits? small number labelings estimate partition function produce results consecutive iterations. figure right, shows number total labelings added learning iteration. learning parameters VOC 2010 PMA 180 hours produced VOC score%. stopping learning PMA hours (slightly hrs required incremental saddle point approximation) results VOC score%. Conclusion presented joint image segmentation labeling model (jsl) which, bag figure-ground image segment hyposes, constructs joint probability distribution compatible image interpretations assembled segments, labeling. process interpreted sampling maximal cliques graph connecting segments spatially overlap, sampling labels segments, conditioned choice tiling. propose joint learning procedure based Maximum Likelihood partition function tilings labelings increasingly accurately approximated training, including incorrect configurations model rates probable. this ensures mistakes carried uncorrected future training iterations, produces stable accurate learning schedules. show models learned eﬃciently match state art Stanford dataset, VOC2010% accuracy test set achieved.',\n",
       " 'PP4248': 'loss functions multiclass prediction problems. show multiclass loss expressed ?proper composite loss?, composition proper loss link function. extend existing results binary losses multiclass losses. determine stationarity condition, Bregman representation, order-sensitivity, existence uniqueness composite representation multiclass losses. subsume existing results ?classification calibration? relating properness show simple integral representation binary proper losses extended multiclass losses. introduction motivation paper understand intrinsic structure properties suitable loss functions problem multiclass prediction, includes multiclass probability estimation. suppose data sample? ] observation ] class. assume sample drawn iid distribution ]. observation predict probability belonging class ]. multiclass classification requires learner predict    class find arg maxi? ] loss measures quality prediction.  ? ]   ]} denote-simplex. multiclass probability estimation, + classification, loss] + partial losses components),  proper losses suitable probability estimation. studied detail ?binary case?) nice integral representation], characterization] differentiable. classification calibrated losses analog proper losses problem classification]. relationship classification calibration properness determined] results multiclass analogue now. design losses multiclass prediction received recent attention] papers developed connection proper losses, restrict consideration margin losses (which imply symmetry conditions). glasmachers] shown learning algorithms behave losses satisfy conditions earlier papers requirements stronger needed. contributions are: relate properness, classification calibration, notion] rename ?prediction calibrated? ; provide characterization multiclass properness; study composite proper losses composition proper loss invertible link) presenting uniqueness existence results; show results aid design proper losses; present (somewhat surprising) negative result integral representation proper multiclass losses. results characterisations. full proofs provided extended version]. formal Setup Suppose set, ] set labels. suppose data? ] label  data follow joint distribution denote respectively, expectation conditional expectation respect conditional risk loss function , ) )   )  ] means drawn multinomial distribution parameter typical learning problem make estimate  full risk)). minimizing) equivalent minimizing)  ),    transpose). suﬃces conditional risk; confer]. loss + proper, ,  strictly proper inequality strict conditional Bayes risk infq? ). function concave]. proper, ). strictly proper losses induce Fisher consistent  estimators probabilities: strictly proper, arg minq). order differentiate losses project-simplex subset denote                }, projection-simplex              inverse. losses defined simplex argument estimator) represents probability vector. desirable anor set predictions. losses + suppose exists invertible function   written composition loss defined simplex  )  )  )). function  composite loss.  proper, proper composite loss, proper loss link notation. kth unit vector vector components kth -vector,  derivative function denoted Hessian     ? ] ]}     relating Properness Classification Calibration Properness attractive property loss task class probability estimation. interested classifying (predicting  ] requires less. relate classification calibration analog properness classification problems) properness. suppose   cover subsets representing class)    observe sets subsets dimension  subsets partition parts, subspace intersection subspaces delimited precedent )-subspace side make properties). lemma Suppose   ]. hold:  exists ).  suppose  ) )  subspace dimension   suppose   )  exists   ] ) ). classification calibrated losses developed studied definitions names]. generalise notion ccalibration proposed] generalisation notion classification calibration]. definition Suppose + loss   -calibrated  ] ) ). -calibrated -calibrated Definition means probability vector predicts doesn belong subset. doesn predict class) real probability vector loss \\x0clarger. classification calibration sense] corresponds calibrated losses cmid   cmid -calibration induces fisher-consistent estimates case classification. furrmore cmid -calibrated ], continuous bounded below? equivalent infinite sample consistent defined]?. continuous) closed, ) infq). result generalises correspondence binary classification calibration properness, orem] multiclass losses). proposition continuous loss + strictly proper-calibrated   particular, continuous strictly proper loss cmid -calibrated. estimator conditional probability vector constructs minimizing empirical average continuous strictly proper loss, build estimator label (corresponding largest probability Fisher consistent problem classification. binary case, classification calibrated implication holds]: min) )) min)) ) Tewari Bartlett] characterised) holds multiclass case. reason assume equivalence classification calibration) holds give names notions. classification calibration notion linked Fisher consistency defined before) call prediction calibrated notion Tewari Bartlett (equivalent)). definition Suppose + loss. ) }), convex hull image prediction calibrated exists prediction function pred ] inf inf ). ‘ ,ppred) ¡maxi‘ Observe class predicted) directly (which equivalent loss invertible). suppose + prediction calibrated pred)) arg maxi cmid -calibrated everywhere. introducing reference ?link?  (which corresponds actual link proper composite loss) show pred function canonically expressed terms arg maxi   proposition Suppose + loss.  ) arg minv,    proper. prediction calibrated pred(? )) arg maxi characterizing Properness present simple (but new) consequences properness.   monotone)    confer]. proposition Suppose + loss. proper, monotone. proposition strictly proper invertible. present paper extensibility results binary losses multiclass losses. proposition shows characterisation properness general (not necessarily differentiable) multiclass case reduced binary case. binary case, classes denoted loss denoted project-simplex]:  , projection (?,    proposition Suppose + loss. define ,   (?)      (?)    (strictly) proper (strictly) proper,   proposition shows order check loss proper check properness line. easy characterization properness differentiable binary (?) (?) losses, + proper  ?? ?  ]). checked lines defined   extend characterisations properness multiclass case Proposition lambert] proved binary case, properness equivalent fact furr prediction reality, larger loss (?order sensitivity?). result relied total order multiclass case, exist total order. yet, compare predictions line true real class probability. result generalization binary case equivalence properness order sensitivity. proposition Suppose + loss. (strictly) proper,   , )) , )) inequality strict ?order sensitivity? tells properness: true class probability minimizes risk prediction moves true class probability line risk increases. property appears convenient optimization purposes: reaches local minimum argument risk loss strictly proper global minimum. loss proper, local minimum global minimum constant open set. observe typically minimising full risk(?)) functions  order sensitivity imply optimisation problem behaved; convexity , ensure convexity functional optimisation problem. order sensitivity line leads characterisation differentiable proper losses. binary case, condition fact derivative minimum ensures minimum. corollary Suppose + loss    differentiable. ) ‘(?? )) ?? ). proper ,    )  )   loss, Bayes risk) infq? , infq?  ) concave. proper) ). rar working loss + work simpler conditional Bayes risk  ) definitions]. suppose concave. limt)? exists, called directional derivative direction denoted). analogy usual definition subdifferential, superdifferential ) )   ),  ) )  ),  vector  ) called supergradient proposition restatement Bregman representation proper losses] differentiable case, orem] general case. proposition Suppose + loss. proper exists concave function exists supergradient)  ,  )  ). unique). fact defined simplex problem. indeed, superdifferential )   ),  ) )  ),      differentiable   ) (?? )),   ?(?? )) (?? )  )). concave differentiable function exists unique proper loss Bayes risk equal differentiable differentiable). property form proper losses Bayes risk. suppose concave. proper losses Bayes risk equal   ) )  ).  This result suggests information lost representing proper loss Bayes risk (when differentiable). proposition elucidates showing proper losses Bayes risk equal everywhere. proposition Two proper losses conditional Bayes risk function everywhere. differentiable, everywhere. differentiable    differentiable  ). proposition Suppose + proper loss. continuous differentiable continuous  , differentiable   proper Composite representation: Uniqueness Existence helpful define loss set rar confer]. composite losses (see definition) constructing losses: proper loss  + invertible link  defines   +      question: loss + proper composite representation (whereby written   representation unique? binary case study uniqueness representation loss proper composite loss. proposition Suppose    + proper composite loss proper loss differentiable link function differentiable invertible. proper loss unique. furrmore unique  ) exists )  choose differentiable, invertible continuous obtain    uniquely defined invertible. proposition Suppose + differentiable binary loss ) expressed proper composite loss conditions hold: decreasing (increasing increasing (decreasing) ) strictly increasing (decreasing) continuous. observe condition alway satisfied convex. suppose  function. loss defined   ),  + called binary margin loss. binary margin losses classification problems. show proposition applies corollary Suppose  differentiable  ) )  ) expressed proper composite loss   ) strictly monotonic continuous monotonic.  convex concave defined monotonic. binary margin losses composite proper losses. build smooth margin loss expressed  proper composite loss.  ) arctan ). ) )+? ) 2x2 invertible. generalize results multiclass case. proposition Suppose proper composite representations        proper losses  continuous invertible.  everywhere. continuous composite representation, proper loss decomposition) unique  everywhere). invertible composite representation, representation unique. ) loss + denote,    ], )} superprediction set (confer. ]). introduce set hyperplanes        hyperplane supports set            strictly convex part exists unique  exists hyper? plane supporting smooth  exists unique hyperplane supporting invertible, express definitions terms rar   + strictly convex, strictly convex part. ) Proposition Suppose continuous invertible loss. strictly proper composite representation convex, smooth strictly convex part. + Proposition Suppose + continuous loss. proper composite representation, convex smooth. invertible, strictly convex part. designing Proper Losses build family conditional Bayes risks. suppose) concave functions {li1  build concave function equal functions edge simplex  , pi1 pi2 Li1 (pi1 pi2 )). equivalent choosing binary loss function, knowing observation class result construction.  exists infinity solutions simply add concave function equal edge). lemma Suppose family concave functions {li1  pi1 pi2     (pi1 pi2 )li1 pi1 pi2 pi1 pi2 concave  , pi1 pi2 Li1 (pi1 pi2 using family Bayes risks, build family proper losses. lemma Suppose family binary proper losses   +  )   proper-class loss   (pi1 , pi1 pi2 orwise Observe easier work Bayes risk correspondence Bayes risks proper losses. integral Representations Proper Losses Unlike natural generalisation results proper binary proper multiclass losses above, result carry over: integral representation proper losses]. binary case exists family ?extremal? loss functions (cost-weighted generalisations loss) parametrised , defined   (?) ?   ? . shown], extremal functions, proper binary loss expressed weighted integral) constant) ?l00). representation special case representation Choquet ory] characterises point set expressed weighted combination ?extremal points? set. representation diﬃculty set extremal points larger rules \\x0cexistence nice small set ?primitive? proper losses rest section makes statement precise. convex cone set points closed linear combinations positive coeﬃcients. ,    point extremal implies    , represented non-trivial combination points set extremal points denoted suppose bounded closed convex set) set convex functions bounded) compact respect topology uniform convergence. orem] shows extremal points convex cone)   ),  dense. topology uniform convergence) means function ) sequence functions ) limi??    supu)—. result show set extremal Bayes risks dense set Bayes risks order simplify analysis, restrict attention fair proper losses. loss fair partial loss vertex simplex ]). proper loss fair Bayes risk vertex simplex case Bayes risk called fair). lose generality studying fair proper losses proper loss sum fair proper loss constant vector. set fair proper losses defined form closed convex cone, denoted set concave functions vertices simplex denoted closed convex cone. proposition Suppose fair proper loss exists sequence extremal fair proper losses converges proof Proposition requires lemma relies correspondence proper loss Bayes risk (proposition) fact continuous functions equal equal everywhere. lemma bayes risk extremal conversely, proper losses Bayes risk equal extremal correspondence uniform convergence sequence Bayes risk functions convergence proper losses. lemma Suppose  suppose proper losses.  converges uniformly converges bronshtein] Johansen] showed construct set extremal convex functions dense). trivial change sign leads family extremal proper fair Bayes risks dense set Bayes risks topology uniform convergence. means small set extremal (?primitive?) losses construct proper fair loss linear combinations convex polytope compact convex intersection finite set half-spaces refore convex hull vertices.  finite family Figure Complexity extremal concave functions aﬃne functions defined dimensions (corresponds). graph extremal condefine convex polyhedral function cave function dimensions. lines slope) maxi). set changes. pattern lines arbitrarily complex.  )}} covering polytopes. orem] shows defined, extremal conditions satisfied: polytopes face  implies vertex vertex belongs distinct polytopes set dense). result straightforward exhibit sets extremal fair Bayes risks)  examples)  cpii   ] Conclusion considered loss functions multiclass prediction problems made main contributions: extended existing results binary losses multiclass prediction problems including characterisations proper losses relationship properness classification calibration; related notion prediction calibration classification calibration; developed existence uniqueness results proper composite losses (which binary case) characterise loss proper composite representation terms geometry superprediction set; showed attractive (simply parametrised) integral representation binary proper losses extended multiclass case. results suggest order design losses multiclass prediction problems helpful composite representation, design proper part Bayes risk suggested binary case]. proper composite representation]. acknowledgements work performed whilst Elodie Vernet visiting ANU nicta, supported Australian Research Council nicta, backing australia ability. loss functions multiclass prediction problems. show multiclass loss expressed ?proper composite loss?, composition proper loss link function. extend existing results binary losses multiclass losses. determine stationarity condition, Bregman representation, order-sensitivity, existence uniqueness composite representation multiclass losses. subsume existing results ?classification calibration? relating properness show simple integral representation binary proper losses extended multiclass losses. Introduction motivation paper understand intrinsic structure properties suitable loss functions problem multiclass prediction, includes multiclass probability estimation. suppose data sample? ] observation ] class. assume sample drawn iid distribution ]. given observation predict probability belonging class ]. multiclass classification requires learner predict    class find arg maxi? ] loss measures quality prediction. let ? ]   ]} denote-simplex. for multiclass probability estimation, + for classification, loss] + partial losses components),  proper losses suitable probability estimation. studied detail ?binary case?) nice integral representation], characterization] differentiable. classification calibrated losses analog proper losses problem classification]. relationship classification calibration properness determined] most results multiclass analogue now. design losses multiclass prediction received recent attention] papers developed connection proper losses, restrict consideration margin losses (which imply symmetry conditions). glasmachers] shown learning algorithms behave losses satisfy conditions earlier papers requirements stronger needed. our contributions are: relate properness, classification calibration, notion] rename ?prediction calibrated? ; provide characterization multiclass properness; study composite proper losses composition proper loss invertible link) presenting uniqueness existence results; show results aid design proper losses; present (somewhat surprising) negative result integral representation proper multiclass losses. many results characterisations. full proofs provided extended version]. Formal Setup Suppose set, ] set labels. suppose data? ] label  data follow joint distribution denote respectively, expectation conditional expectation respect conditional risk loss function , ) )   )  ] means drawn multinomial distribution parameter typical learning problem make estimate  full risk)). minimizing) equivalent minimizing)  ),    transpose). thus suﬃces conditional risk; confer]. loss + proper, ,  strictly proper inequality strict conditional Bayes risk infq? ). this function concave]. proper, ). strictly proper losses induce Fisher consistent  estimators probabilities: strictly proper, arg minq). order differentiate losses project-simplex subset denote                }, projection-simplex              inverse. losses defined simplex argument estimator) represents probability vector. however desirable anor set predictions. one losses + suppose exists invertible function   written composition loss defined simplex  that)  )  )). such function  composite loss.  proper, proper composite loss, proper loss link notation. kth unit vector vector components kth -vector,  derivative function denoted Hessian let    ? ] ]}     Relating Properness Classification Calibration Properness attractive property loss task class probability estimation. however interested classifying (predicting  ] requires less. relate classification calibration analog properness classification problems) properness. suppose   cover subsets representing class)    observe sets subsets dimension  subsets partition parts, subspace intersection subspaces delimited precedent )-subspace side make properties). lemma Suppose   ]. hold: for exists ).  suppose  ) )  subspace dimension   suppose   ) for exists   ] ) ). Classification calibrated losses developed studied definitions names]. below generalise notion ccalibration proposed] generalisation notion classification calibration]. definition Suppose + loss   -calibrated  ] ) ). -calibrated -calibrated Definition means probability vector predicts doesn belong subset. doesn predict class) real probability vector loss \\x0clarger. classification calibration sense] corresponds calibrated losses cmid   cmid -calibration induces fisher-consistent estimates case classification. furrmore cmid -calibrated ], continuous bounded below? equivalent infinite sample consistent defined]?. this continuous) closed, ) infq). result generalises correspondence binary classification calibration properness, orem] multiclass losses). proposition continuous loss + strictly proper-calibrated   particular, continuous strictly proper loss cmid -calibrated. thus estimator conditional probability vector constructs minimizing empirical average continuous strictly proper loss, build estimator label (corresponding largest probability Fisher consistent problem classification. binary case, classification calibrated implication holds]: min) )) min)) ) Tewari Bartlett] characterised) holds multiclass case. since reason assume equivalence classification calibration) holds give names notions. classification calibration notion linked Fisher consistency defined before) call prediction calibrated notion Tewari Bartlett (equivalent)). definition Suppose + loss. let) }), convex hull image prediction calibrated exists prediction function pred ] inf inf ). ‘ ,ppred) ¡maxi‘ Observe class predicted) directly (which equivalent loss invertible). suppose + prediction calibrated pred)) arg maxi cmid -calibrated everywhere. introducing reference ?link?  (which corresponds actual link proper composite loss) show pred function canonically expressed terms arg maxi   Proposition Suppose + loss. let ) arg minv,    proper. prediction calibrated pred(? )) arg maxi Characterizing Properness present simple (but new) consequences properness.   monotone)    confer]. proposition Suppose + loss. proper, monotone. Proposition strictly proper invertible. present paper extensibility results binary losses multiclass losses. proposition shows characterisation properness general (not necessarily differentiable) multiclass case reduced binary case. binary case, classes denoted loss denoted project-simplex]:  , projection (?,    proposition Suppose + loss. define ,   (?)      (?)    (strictly) proper (strictly) proper,   this proposition shows order check loss proper check properness line. one easy characterization properness differentiable binary (?) (?) losses, + proper  ?? ?  ]). however checked lines defined   extend characterisations properness multiclass case Proposition lambert] proved binary case, properness equivalent fact furr prediction reality, larger loss (?order sensitivity?). result relied total order multiclass case, exist total order. yet, compare predictions line true real class probability. result generalization binary case equivalence properness order sensitivity. proposition Suppose + loss. (strictly) proper,   , )) , )) inequality strict ?order sensitivity? tells properness: true class probability minimizes risk prediction moves true class probability line risk increases. this property appears convenient optimization purposes: reaches local minimum argument risk loss strictly proper global minimum. loss proper, local minimum global minimum constant open set. but observe typically minimising full risk(?)) functions  order sensitivity imply optimisation problem behaved; convexity , ensure convexity functional optimisation problem. order sensitivity line leads characterisation differentiable proper losses. binary case, condition fact derivative minimum ensures minimum. corollary Suppose + loss    differentiable. let) ‘(?? )) ?? ). proper ,    )  )   loss, Bayes risk) infq? , infq?  ) concave. proper) ). rar working loss + work simpler conditional Bayes risk  ) definitions]. suppose concave. limt)? exists, called directional derivative direction denoted). analogy usual definition subdifferential, superdifferential ) )   ),  ) )  ),  vector  ) called supergradient proposition restatement Bregman representation proper losses] differentiable case, orem] general case. Proposition Suppose + loss. proper exists concave function exists supergradient)  ,  )  ). unique). fact defined simplex problem. indeed, superdifferential )   ),  ) )  ),      differentiable   ) (?? )),   ?(?? )) (?? )  )). hence concave differentiable function exists unique proper loss Bayes risk equal differentiable differentiable). property form proper losses Bayes risk. suppose concave. proper losses Bayes risk equal   ) )  ).  This result suggests information lost representing proper loss Bayes risk (when differentiable). proposition elucidates showing proper losses Bayes risk equal everywhere. proposition Two proper losses conditional Bayes risk function everywhere. differentiable, everywhere. differentiable    differentiable  ). proposition Suppose + proper loss. continuous differentiable continuous  , differentiable   Proper Composite representation: Uniqueness Existence helpful define loss set rar confer]. composite losses (see definition) constructing losses: proper loss  + invertible link  defines   +      question: loss + proper composite representation (whereby written   representation unique? binary case study uniqueness representation loss proper composite loss. proposition Suppose    + proper composite loss proper loss differentiable link function differentiable invertible. proper loss unique. furrmore unique  ) exists )  choose differentiable, invertible continuous obtain    uniquely defined invertible. proposition Suppose + differentiable binary loss ) expressed proper composite loss conditions hold: decreasing (increasing increasing (decreasing) ) strictly increasing (decreasing) continuous. Observe condition alway satisfied convex. suppose  function. loss defined   ),  + called binary margin loss. binary margin losses classification problems. show proposition applies Corollary Suppose  differentiable  ) )  ) expressed proper composite loss   ) strictly monotonic continuous monotonic.  convex concave defined monotonic. however binary margin losses composite proper losses. one build smooth margin loss expressed  proper composite loss. consider ) arctan ). ) )+? ) 2x2 invertible. generalize results multiclass case. proposition Suppose proper composite representations        proper losses  continuous invertible.  everywhere. continuous composite representation, proper loss decomposition) unique  everywhere). invertible composite representation, representation unique. ) given loss + denote,    ], )} superprediction set (confer. ]). introduce set hyperplanes        hyperplane supports set            strictly convex part exists unique  exists hyper? plane supporting smooth  exists unique hyperplane supporting invertible, express definitions terms rar   + strictly convex, strictly convex part. ) Proposition Suppose continuous invertible loss. strictly proper composite representation convex, smooth strictly convex part. + Proposition Suppose + continuous loss. proper composite representation, convex smooth. invertible, strictly convex part. Designing Proper Losses build family conditional Bayes risks. suppose) concave functions {li1  build concave function equal functions edge simplex  , pi1 pi2 Li1 (pi1 pi2 )). this equivalent choosing binary loss function, knowing observation class result construction.  exists infinity solutions simply add concave function equal edge). lemma Suppose family concave functions {li1  pi1 pi2     (pi1 pi2 )li1 pi1 pi2 pi1 pi2 concave  , pi1 pi2 Li1 (pi1 pi2 Using family Bayes risks, build family proper losses. lemma Suppose family binary proper losses   +  )   proper-class loss   (pi1 , pi1 pi2 orwise Observe easier work Bayes risk correspondence Bayes risks proper losses. Integral Representations Proper Losses Unlike natural generalisation results proper binary proper multiclass losses above, result carry over: integral representation proper losses]. binary case exists family ?extremal? loss functions (cost-weighted generalisations loss) parametrised , defined   (?) ?   ? . shown], extremal functions, proper binary loss expressed weighted integral) constant) ?l00). this representation special case representation Choquet ory] characterises point set expressed weighted combination ?extremal points? set. although representation diﬃculty set extremal points larger rules \\x0cexistence nice small set ?primitive? proper losses rest section makes statement precise. convex cone set points closed linear combinations positive coeﬃcients. that,    point extremal implies    that, represented non-trivial combination points set extremal points denoted suppose bounded closed convex set) set convex functions bounded) compact respect topology uniform convergence. orem] shows extremal points convex cone)   ),  dense. topology uniform convergence) this means function ) sequence functions ) limi??    supu)—. result show set extremal Bayes risks dense set Bayes risks order simplify analysis, restrict attention fair proper losses. loss fair partial loss vertex simplex ]). proper loss fair Bayes risk vertex simplex case Bayes risk called fair). one lose generality studying fair proper losses proper loss sum fair proper loss constant vector. set fair proper losses defined form closed convex cone, denoted set concave functions vertices simplex denoted closed convex cone. proposition Suppose fair proper loss exists sequence extremal fair proper losses converges proof Proposition requires lemma relies correspondence proper loss Bayes risk (proposition) fact continuous functions equal equal everywhere. lemma Bayes risk extremal conversely, proper losses Bayes risk equal extremal correspondence uniform convergence sequence Bayes risk functions convergence proper losses. lemma Suppose  suppose proper losses.  converges uniformly converges bronshtein] Johansen] showed construct set extremal convex functions dense). with trivial change sign leads family extremal proper fair Bayes risks dense set Bayes risks topology uniform convergence. this means small set extremal (?primitive?) losses construct proper fair loss linear combinations convex polytope compact convex intersection finite set half-spaces refore convex hull vertices. let finite family Figure Complexity extremal concave functions aﬃne functions defined now dimensions (corresponds). graph extremal condefine convex polyhedral function cave function dimensions. lines slope) maxi). set changes. pattern lines arbitrarily complex.  )}} covering polytopes. orem] shows defined, extremal conditions satisfied: polytopes face  implies vertex vertex belongs distinct polytopes set dense). using result straightforward exhibit sets extremal fair Bayes risks)  two examples)  cpii   ] Conclusion considered loss functions multiclass prediction problems made main contributions: extended existing results binary losses multiclass prediction problems including characterisations proper losses relationship properness classification calibration; related notion prediction calibration classification calibration; developed existence uniqueness results proper composite losses (which binary case) characterise loss proper composite representation terms geometry superprediction set; showed attractive (simply parametrised) integral representation binary proper losses extended multiclass case. our results suggest order design losses multiclass prediction problems helpful composite representation, design proper part Bayes risk suggested binary case]. proper composite representation]. acknowledgements work performed whilst Elodie Vernet visiting ANU nicta, supported Australian Research Council nicta, backing australia ability.',\n",
       " 'PP4325': 'not vapnik-chervonenkis classes created equal. observed Massart?elec], showed that, binary classification rates sample size margin condition, classes admit rates order ors (log. classes called ?rich? ]. noted gin Koltchinskii], fine complexity notion defines ?richness? fact embodied alexander capacity function Somewhat surprisingly, supremum function (called disagreement coeﬃcient Hanneke]) plays key role risk bounds active learning. contribution paper twofold. first, prove lower bounds passive learning based alexander capacity function, matching upper bounds] constants. second, prove lower bounds number label requests active learning terms capacity function. proof techniques information-oretic nature provide unified tool study active passive learning framework. active passive learning. arbitrary measurable space. , random variable taking values , unknown distribution   denotes marginal distribution here, instance feature, predictor variable) binary response label). classical results statistical learning assume availability. sample framework, learner \\x0cpassive control sample chosen. classical setting studied, question recently received attention: gain data obtained sequentially, learner allowed modify design distribution predictor variable receiving pair , learner actively information obtained facilitate faster learning? paradigms literature) design distribution Dirac delta function depends) design distribution restriction original distribution measurable set. rich literature approaches, mention results here. paradigm) closely related learning membership queries], generalized binary search], coding noiseless feedback]. goal actively choose observed  suﬃciently ?informative? classification task. paradigm, sample longer information distribution aﬃliation january, 2012: Department Electrical Computer engineering, Duke university. precise, capacity function depends underlying probability distribution.  (see] furr discussion references). setting) called selective sampling], term active learning used. paradigm, aim sequentially choose subsets based observations prior ith example, label requested  sequence assumed, form view point learner, sampled conditional distribution  recent years, interesting algorithms active learning selective sampling appeared literature, notably: algorithm Balcan. ], explicitly maintains ?disagreement? set ?version space? empirical risk minimization (erm) based algorithm Dasgupta. ], maintains set implicitly syntic real examples; importance-weighted active learning algorithm Beygelzimer. ], constructs design distribution careful reweighting feature space. insightful analysis carried Hanneke], distilled role-called disagreement coeﬃcient governing performance active learning algorithms. finally, Koltchinskii] analyzed active learning procedures localized Rademacher complexities alexander capacity function, discuss next. alexander capacity function. denote class candidate classifiers, classifier measurable function }. suppose dimension finite-dim) loss risk) probability error) risk globally minimized Bayes classifier ? defined ? }  regression function. define margin inf? ) —. problem satisfies massart noise condition. define excess risk classifier   equality .   ], define    ) ))        . ) )} set consists classifiers ?-close (?) sense, set consists points exists classifier  disagrees Bayes classifier  alexander capacity function] defined (?) ?  , (?) measures relative size terms disagreement region compared clearly, (?) bounded/? however, cases (?)   function originally introduced Alexander, context exponential inequalities empirical processes indexed classes functions, gin Koltchinskii] generalized alexander results. particular, proved (see, 1213]) that-classpof binary-valued functions-dim) ERM solution fbn arg minf massart noise condition satisfies log ) nh2 probability  constants upper bound) suggests importance alexander capacity function passive learning, leaving open question necessity. contribution lower bound matches upper bound) constant, showing that, fact, dependence capacity unavoidable. recently, Koltchinskii] made important connection hanneke disagreement coeﬃcient alexander capacity function. massart noise condition, Koltchinskii showed (see, Corollary]) that, achieving excess loss confidence number queries issued active learning algorithm bounded log/?) log log/?) log log/?) log log) sup?? ] (?) hanneke disagreement coeﬃcient. similar bounds based disagreement coeﬃcient appeared]. contribution paper lower bound expected number queries based alexander capacity (?). comparison lower bounds. passive learning, Massart?elec] proved lower bounds which, fact, correspond (?) /?  (?) endpoints complexity scale capacity function. capacity function hand, authors emphasize ?rich? classes yield larger lower bound. orem unified construction complexities (?). pac framework, lower bound/? /?) log/?)) back]. results noisy version problem), lower bound fact/?) log/?) /?) log/?)) classes (?) /?). active learning, Castro Nowak] derived lower bounds, disagreement coeﬃcient tsybakov-type noise condition. setting scope paper. hanneke] proved lower bound number label requests specifically algorithm terms disagreement coeﬃcient. contrast, lower bounds orem valid \\x0cfor algorithm terms alexander capacity function. finally, result? ?ainen] (strengned]) lower bound ?(?  inf closer construction lower bound reveals achieved specific margin ?/?. analysis unsatisfying, free parameter, necessarily coupled desired accuracy point view put Massart?elec, 2329], argue non-asymptotic analysis parameters problem made explicit. feel understanding problem. setup main results suppose instance space countably infinite set. also, log(?)  loge (?) throughout. definition function class margin parameter , denote class conditional probability distributions , that) Bayes classifier  ) regression function satisfies Massart condition margin  denote space probability measures introduce alexander capacity function) picture. explicitly dependence (?)  write   ?). denote set admissible capacity functions , .,  exist     (?)       ]. loss generality, assume (?)   definition    pair, def. (?, denote set joint distributions,  , form   ). moreover, admissible function    (?, denote subset),     (?). finally, type learning schemes dealing with. definition -step learning scheme consists objects: conditional proba) bility distributions   mapping   this definition covers passive case ?(?    ) active case user-controlled design distribution feature time information. learning process takes place sequentially follows: time step   random feature drawn accord) ing label drawn samples collected, learner computes candidate classifier fbn   quantify performance scheme, concept induced measure, generalizes set]. specifically,   ), define probability measure   Definition subset). accuracy parameter  , confidence parameter  -step learning scheme (?, )-learn) (fbn   remark leaving precision makes exposition bit cleaner light fact that, massart noise condition margin hkf ? kl1 (?)  ? . massart?elec, 2352]). preliminaries way, state main results paper: orem (lower bounds passive learning).   suﬃciently large   ], exist probability measure   class-dim) properties) Fix  ). exists-step passive learning scheme, )learns(?,  , log   log (?) =?  ) exists-step passive learning scheme, )-learns(?,    =?  orem (lower bounds active learning).   suﬃciently large   ], exist probability measure   class-dim) property: Fix  ). exists-step active learning scheme, )-learns(?,  , log =?    log (?)  (?) ) Kh2 Kh2 Remark lower bound) well-known back]. mention naturally arises construction. fact, smooth transition), extra log (?) factor disappearing approaches active learning lower bound, conjecture log (?) , fact, optimal, extra factor log log/?) ) arises passive learning algorithm black box. remainder paper organized follows: Section describes required informationoretic tools, Section prove orems proofs number technical lemmas found Supplementary material. information-oretic framework Let probability distributions common measurable space Given convex function ,   ) ?-divergence2?  (pkq) ?   arbitrary ?-finite measure dominates For special case}, distributions bernoulli) bernoulli) random deviate standard term -divergence? reserved generic classifier. instance,  easy show (pkq) depend choice dominating measure. variable, denote ?-divergence (pkq)      ) Two choices interest: ) log ordinary kullback? leibler) divergence(pkq), ) log reverse divergence(qkp), denote Dre (pkq). write?) binary divergence. approach makes fundamental data processing inequality holds ?divergence]: probability distributions random variable  conditional probability distribution random variable  kqz  (pkq) (resp., marginal distribution distribution (resp). arbitrary-step learning scheme Let fix finite set    assume associate probability measure ?pym (?, Bayes classifier  define induced measure) pym  moreover, probability distribution ,? ,  words,? joint distribution,      ingredient approach standard].    arbitrary?packing subset (that, kfi kl1 (?)  ). suppose satisfies)     arg min kfbn (?)   lemma easily proved triangle inequality:  lemma definitions,? ingredient approach application data processing inequality) judicious choice , uniformly distributed ) induced measure,?  lemma (see]): Lemma probability measure distributed independent divergence-generating function mapping  (pkq) nondecreasing interval]. assuming     (pkq)    ?))    ) proof. define indicator random variable    lemma hand, factored  refore, (pkq)   kqz ))    step data processing inequality), due fact binary, assumed monotonicity property ), arrive). next, choose divergence-generating function auxiliary distribution choice inspection right-hand side) suggests usual (log lower bounds] obtained ) behaves log large hand, ) behaves log small lower bounds form log  observations naturally lead respective choices ) log ) log divergence(pkq) reverse divergence Dre (pkq(qkp). choice obvious choice satisfying conditions lemma product marginals      ) log left-hand side(pkq kpm  mutual information joint distribution hand, hard show right-hand side) lower-bounded  log log combining   log log commonly variant) well-known fano inequality, Lemma, 1250, 1571]. steps, ) log lead bound log log log log     Dre kpm -called lautum information], inequality holds  however, convenient choose follows. fix arbitrary conditional distribution ,  learning scheme define probability measure  lemma     }—. (pkq(pym)kqy))eps) Dre (pym)kqy  Dre (pkq moreover, scheme passive. ) Dre (pkq) dre (pym)kqy))  ) holds Dre replaced Proofs orems Combinatorial preliminaries.  onsider-dimensional Boolean cube   }, ]}.    define Hamming dispk tance (?,  hamming weight   number nonzero coordinates.  denote subset consisting binary strings Hamming weight interested large separated well-balanced subsets end, lemma: Lemma suppose. suﬃciently large, exists set) (?,   properties) log log) distinct   (iii) ],    Proof orem loss generality, Let (?)  increase ensure ), probability measure puts mass remaining mass  (?)  (recall (?)  /?.) class indicator functions subsets cardinality-dim) focus subclass For   define  , ) ] orwise?     ], denote probability distribution bernoulli) random variable. now,  associate conditional probability measure? ? )   ? ? ]} easy? belongs). moreover,   ?   kl1 (?) ? ) ))  (?,   hence, choice     ?  ? (?,   }. implies ?  ], refore     ])/?  (?). established that,   probability measure   ? element(?, ?). finally  set Lemma?    distinct  ?   kl1 (?)  (?,  hence, ?-packing (?)-norm. position apply lemmas Section  )     fixed enumeration elements  denote pym) conditional probability measure? measure  pym }, bayes classifier. -step passive learning scheme, )-learns(?, define probability measure   constructed). addition,  , define auxiliary measure    ,  constructed) ? ? ]} applying Lemma ) log write(pkq?    log log apply Lemma defining   log log) easily proved fact(pym?)   ?)]  ? (pkq?  ?)  (?)   ?)]  ) refore, combining eqs. ) fact  obtain   log (?)  log ?)  (?)   ?)]   ) This bound valid ], optimal choice calculated closed form:   (?)  turn reverse divergence. first, suppose lemma Dre (pkq1??  ) log/?)  log hand, fact Dre (pym)kq1??   ) applying. ), write Dre (pkq1??       log ) conclude log  log log ) For vacuous bound  cases orem ) For fixed inequality log  log kh2 ,  choosing  eqs. ), obtain). ) For) optimal setting  /?  ). transition smooth determined   (?)  proof orem work construction proof orem first    uniform distribution convexity, pym max max(pym)kpym)) log(pkq)    upper bounded log obtain  applying Lemma ) log refore  log  log next, auxiliary measure??  ) Dre (pkq1?? log  ) Dre (pym)kq1?? ))eq1??   )eq1??    ) eq1??    eq1??  log (?)  ?? )  log (?) ) Lemma) definition) balance condipk tion) satisfied) fact  Applying Lemma ) log (?) log  log  log Combining) bound log kh2 , ). Not vapnik-chervonenkis classes created equal. this observed Massart?elec], showed that, binary classification rates sample size margin condition, classes admit rates order ors (log. classes called ?rich? ]. noted gin Koltchinskii], fine complexity notion defines ?richness? fact embodied alexander capacity function Somewhat surprisingly, supremum function (called disagreement coeﬃcient Hanneke]) plays key role risk bounds active learning. contribution paper twofold. first, prove lower bounds passive learning based alexander capacity function, matching upper bounds] constants. second, prove lower bounds number label requests active learning terms capacity function. our proof techniques information-oretic nature provide unified tool study active passive learning framework. active passive learning. let arbitrary measurable space. let, random variable taking values , unknown distribution   denotes marginal distribution here, instance feature, predictor variable) binary response label). classical results statistical learning assume availability. sample framework, learner \\x0cpassive control sample chosen. classical setting studied, question recently received attention: gain data obtained sequentially, learner allowed modify design distribution predictor variable receiving pair that, learner actively information obtained facilitate faster learning? two paradigms literature) design distribution Dirac delta function depends) design distribution restriction original distribution measurable set. rich literature approaches, mention results here. paradigm) closely related learning membership queries], generalized binary search], coding noiseless feedback]. goal actively choose observed  suﬃciently ?informative? classification task. paradigm, sample longer information distribution Aﬃliation january, 2012: Department Electrical Computer engineering, Duke university. precise, capacity function depends underlying probability distribution.  (see] furr discussion references). setting) called selective sampling], term active learning used. paradigm, aim sequentially choose subsets based observations prior ith example, label requested  sequence assumed, form view point learner, sampled conditional distribution  recent years, interesting algorithms active learning selective sampling appeared literature, notably: algorithm Balcan. ], explicitly maintains ?disagreement? set ?version space? empirical risk minimization (erm) based algorithm Dasgupta. ], maintains set implicitly syntic real examples; importance-weighted active learning algorithm Beygelzimer. ], constructs design distribution careful reweighting feature space. insightful analysis carried Hanneke], distilled role-called disagreement coeﬃcient governing performance active learning algorithms. finally, Koltchinskii] analyzed active learning procedures localized Rademacher complexities alexander capacity function, discuss next. alexander capacity function. let denote class candidate classifiers, classifier measurable function }. suppose dimension finite-dim) loss risk) probability error) risk globally minimized Bayes classifier ? defined ? }  regression function. define margin inf? ) —. problem satisfies massart noise condition. define excess risk classifier   equality . given  ], define    ) ))        . ) )} set consists classifiers ?-close (?) sense, set consists points exists classifier  disagrees Bayes classifier  alexander capacity function] defined (?) ?  , (?) measures relative size terms disagreement region compared clearly, (?) bounded/? however, cases (?)   function originally introduced Alexander, context exponential inequalities empirical processes indexed classes functions, gin Koltchinskii] generalized alexander results. particular, proved (see, 1213]) that-classpof binary-valued functions-dim) ERM solution fbn arg minf massart noise condition satisfies log ) nh2 probability  constants upper bound) suggests importance alexander capacity function passive learning, leaving open question necessity. our contribution lower bound matches upper bound) constant, showing that, fact, dependence capacity unavoidable. recently, Koltchinskii] made important connection hanneke disagreement coeﬃcient alexander capacity function. under massart noise condition, Koltchinskii showed (see, Corollary]) that, achieving excess loss confidence number queries issued active learning algorithm bounded log/?) log log/?) log log/?) log log) sup?? ] (?) hanneke disagreement coeﬃcient. similar bounds based disagreement coeﬃcient appeared]. contribution paper lower bound expected number queries based alexander capacity (?). Comparison lower bounds. for passive learning, Massart?elec] proved lower bounds which, fact, correspond (?) /?  (?) endpoints complexity scale capacity function. without capacity function hand, authors emphasize ?rich? classes yield larger lower bound. our orem unified construction complexities (?). PAC framework, lower bound/? /?) log/?)) back]. results noisy version problem), lower bound fact/?) log/?) /?) log/?)) classes (?) /?). for active learning, Castro Nowak] derived lower bounds, disagreement coeﬃcient tsybakov-type noise condition. this setting scope paper. hanneke] proved lower bound number label requests specifically algorithm terms disagreement coeﬃcient. contrast, lower bounds orem valid \\x0cfor algorithm terms alexander capacity function. finally, result? ?ainen] (strengned]) lower bound ?(?  inf closer construction lower bound reveals achieved specific margin ?/?. such analysis unsatisfying, free parameter, necessarily coupled desired accuracy this point view put Massart?elec, 2329], argue non-asymptotic analysis parameters problem made explicit. feel understanding problem. Setup main results suppose instance space countably infinite set. also, log(?)  loge (?) throughout. definition given function class margin parameter , denote class conditional probability distributions , that) Bayes classifier  ) regression function satisfies Massart condition margin let denote space probability measures introduce alexander capacity function) picture. whenever explicitly dependence (?)  write   ?). denote set admissible capacity functions , .,  exist     (?)       ]. without loss generality, assume (?)   definition given   pair, def. (?, denote set joint distributions,  , form   ). moreover, admissible function    (?, denote subset),     (?). finally, type learning schemes dealing with. definition -step learning scheme consists objects: conditional proba) bility distributions   mapping   This definition covers passive case ?(?    ) active case user-controlled design distribution feature time information. learning process takes place sequentially follows: time step   random feature drawn accord) ing label drawn after samples collected, learner computes candidate classifier fbn   quantify performance scheme, concept induced measure, generalizes set]. specifically,   ), define probability measure   Definition let subset). given accuracy parameter  , confidence parameter  -step learning scheme (?, )-learn) (fbn   Remark leaving precision makes exposition bit cleaner light fact that, massart noise condition margin hkf ? kl1 (?)  ? . massart?elec, 2352]). with preliminaries way, state main results paper: orem (lower bounds passive learning). given  suﬃciently large   ], exist probability measure   class-dim) properties) Fix  ). exists-step passive learning scheme, )learns(?,  , log   log (?) =?  ) exists-step passive learning scheme, )-learns(?,    =?  orem (lower bounds active learning). given  suﬃciently large   ], exist probability measure   class-dim) property: Fix  ). exists-step active learning scheme, )-learns(?,  , log =?    log (?)  (?) ) Kh2 Kh2 Remark lower bound) well-known back]. mention naturally arises construction. fact, smooth transition), extra log (?) factor disappearing approaches active learning lower bound, conjecture log (?) , fact, optimal, extra factor log log/?) ) arises passive learning algorithm black box. remainder paper organized follows: Section describes required informationoretic tools, Section prove orems proofs number technical lemmas found Supplementary material. information-oretic framework Let probability distributions common measurable space Given convex function ,   ) ?-divergence2?  (pkq) ?   arbitrary ?-finite measure dominates For special case}, distributions bernoulli) bernoulli) random deviate standard term -divergence? reserved generic classifier. for instance,  easy show (pkq) depend choice dominating measure. variable, denote ?-divergence (pkq)      ) Two choices interest: ) log ordinary kullback? leibler) divergence(pkq), ) log reverse divergence(qkp), denote Dre (pkq). write?) binary divergence. our approach makes fundamental data processing inequality holds ?divergence]: probability distributions random variable  conditional probability distribution random variable  kqz  (pkq) (resp., marginal distribution distribution (resp). consider arbitrary-step learning scheme Let fix finite set    assume associate probability measure ?pym (?, Bayes classifier for define induced measure) pym  moreover, probability distribution ,? ,  words,? joint distribution,      ingredient approach standard]. let   arbitrary?packing subset (that, kfi kl1 (?)  ). suppose satisfies)    now arg min kfbn (?)   lemma easily proved triangle inequality:  lemma with definitions,? ingredient approach application data processing inequality) judicious choice let, uniformly distributed ) induced measure,?  lemma (see]): Lemma consider probability measure distributed independent let divergence-generating function that mapping  (pkq) nondecreasing interval]. assuming     (pkq)    ?))    ) proof. define indicator random variable    Lemma hand, factored  refore, (pkq)   kqz ))    step data processing inequality), due fact binary, assumed monotonicity property using), arrive). next, choose divergence-generating function auxiliary distribution Choice inspection right-hand side) suggests usual (log lower bounds] obtained ) behaves log large hand, ) behaves log small lower bounds form log  observations naturally lead respective choices ) log ) log divergence(pkq) reverse divergence Dre (pkq(qkp). choice one obvious choice satisfying conditions lemma product marginals     with ) log left-hand side(pkq kpm  mutual information joint distribution hand, hard show right-hand side) lower-bounded  log log combining   log log commonly variant) well-known fano inequality, Lemma, 1250, 1571]. steps, ) log lead bound log log log log     Dre kpm -called lautum information], inequality holds  however, convenient choose follows. fix arbitrary conditional distribution ,  given learning scheme define probability measure  lemma for    }—. (pkq(pym)kqy))eps) Dre (pym)kqy  Dre (pkq moreover, scheme passive. ) Dre (pkq) Dre (pym)kqy))  ) holds Dre replaced Proofs orems Combinatorial preliminaries. given onsider-dimensional Boolean cube   }, ]}. for   define Hamming dispk tance (?,  Hamming weight   number nonzero coordinates. for denote subset consisting binary strings Hamming weight interested large separated well-balanced subsets end, lemma: Lemma suppose. suﬃciently large, exists set) (?,   properties) log log) distinct   (iii) ],    Proof orem without loss generality, Let (?)  increase ensure ), probability measure puts mass remaining mass  (?)  (recall (?)  /?.) let class indicator functions subsets cardinality-dim) focus subclass For   define  , ) ] orwise?    for ], denote probability distribution bernoulli) random variable. now,  associate conditional probability measure? ? )   ? ? ]} easy? belongs). moreover,   ?   kl1 (?) ? ) ))  (?,   hence, choice     ?  ? (?,   }. this implies ?  ], refore     ])/?  (?). established that,   probability measure   ? element(?, ?). finally  set Lemma?    distinct  ?   kl1 (?)  (?,  hence, ?-packing (?)-norm. now position apply lemmas Section let )     fixed enumeration elements for denote pym) conditional probability measure? measure  pym }, Bayes classifier. now-step passive learning scheme, )-learns(?, define probability measure   constructed). addition,  , define auxiliary measure    ,  constructed) ? ? ]} applying Lemma ) log write(pkq?    log log next apply Lemma defining   log log) easily proved fact(pym?)   ?)]  ? (pkq?  ?)  (?)   ?)]  ) refore, combining eqs. ) fact  obtain   log (?)  log ?)  (?)   ?)]   ) This bound valid ], optimal choice calculated closed form:   (?)  turn reverse divergence. first, suppose lemma Dre (pkq1??  ) log/?)  log hand, fact Dre (pym)kq1??   ) applying. ), write Dre (pkq1??       log ) conclude log  log log ) For vacuous bound  now cases orem ) For fixed inequality log  log kh2 ,  choosing  eqs. ), obtain). ) For) optimal setting  /?  ). transition smooth determined   (?)  proof orem work construction proof orem first    uniform distribution convexity, pym max max(pym)kpym)) log(pkq)    upper bounded log obtain  applying Lemma ) log refore  log  log next, auxiliary measure??  ) Dre (pkq1?? log  ) Dre (pym)kq1?? ))eq1??   )eq1??    ) eq1??    eq1??  log (?)  ?? )  log (?) ) Lemma) definition) balance condipk tion) satisfied) fact  Applying Lemma ) log (?) log  log  log Combining) bound log kh2 , ).',\n",
       " 'PP4346': 'according recently inﬂuential statistical approach perception, brain represents interpretation stimulus, uncertainty. words, ideally brain represent full posterior distribution interpretations stimulus, statistically optimal inference learning, hyposis supported increasing number psychophysical electrophysiological results]. although generally accepted humans maintain complex posterior representation, outstanding diﬃculty approach full posterior distribution general complex, highly correlated (due explaining effects), multimodal (multiple interpretations), high-dimensional. approach address problem neural circuits neuronal activity represent parameters variational approximation real posterior]. approach approximate full posterior, number neurons explodes number variables example, approximation Gaussian distribution requires parameters represent covariance matrix variables. anor approach identify neurons variables interpret neural activity samples posterior]. interpretation consistent range experimental observations, including neural variability (which result uncertainty posterior) spontaneous activity (corresponding samples prior absence stimulus]. advantage sampling number neurons scales linearly number variables, represent arbitrarily complex posterior distributons samples. part issue: collecting suﬃcient number samples form complex, high-dimensional representation time-costly. modeling studies shown small number samples suﬃcient perform low-dimensional tasks (intuitively, taking lowdimensional marginal posterior accumulates samples dimensions]. however, sensory data inherently high-dimensional. such, order faithfully represent visual scenes potentially objects object parts, requires high-dimensional latent space represent high number potential causes, returns problem sampling approaches face high dimensions. goal line research pursued address questions: find sophisticated representation posterior high-dimensional hidden spaces? goal believed shared brain, find biologically plausible solution reaching? paper propose approach approximate inference learning addresses drawbacks sampling neural processing model, maintains beneficial posterior representation neural plausibility. show sampling combined preselection candidate units. selection connects sampling inﬂuential models neural processing emphasize feed-forward processing] more), consistent popular view neural pro \\x0ccessing learning interplay feed-forward recurrent stages processing]. combined approach emerges naturally interpreting feedforward selection sampling approximations exact inference probabilistic framework perception. select Sample Approach Approximate Inference Inference learning neural circuits regarded task inferring true hidden stimulus. inferring objects visual scene based image projected retina. refer sensory stimulus image) data point   refer hidden objects   denoting hidden variablepor hidden unit data distribution modeled generative data model    denoting parameters model1 assume data distribution optimally modeled generative distribution optimal parameters posterior probability represents optimal inference data point parameters set data points   maximum likelihood parameters argmax?  ?)}. standard procedure find maximum likelihood solution expectation maximization). iteratively optimizes lower bound data likelihood inferring posterior distribution hidden variables current parameters-step), adjusting parameters maximize likelihood data averaged posterior-step). -step updates typically depend small number expectation values posterior) ,?) ) ) elementary function hidden variables) sst case standard sparse coding). non-trivial generative model, computation case continuous variables sum replaced integral. hierarchical model, prior distribution subdivided hierarchically sets variables. expectation values) computationally demanding part optimization. exact computation intractable well-known algorithms]) rely estimations. iterations neural processing assumption neural activity represents posterior hidden variables-step), synaptic plasticity implements model parameters-step). prominent models neural processing ground approximations expectation values) show combined. selection. feedforward processing frequently discussed important component neural processing]. perspective early component neural activity preselection candidate units hyposes sensory stimulus] more), goal reducing computational demand orwise complex computation. context probabilistic approaches, recently shown preselection \\x0ccan formulated variational approximation exact inference]. variational distribution case truncated sum hidden states) ) )  ;     ) )    orwise. subset represents preselected latent states. data point) eqn. results good approximations posterior posterior mass. applications posterior mass concentrated small volumes state space, approximation quality stay high small sets approximation compute eﬃciently expectation values needed-step)  ) ,?)  )iqn;?) )  eqn. represents reduction required computational resources involves summations integrations) smaller state space requirement set selected prior computation expectation values, final improvement eﬃciency relies selections eﬃciently computable. such, selection function carefully chosen order define eﬃciently selects candidate units contributed data point) defined: ) indices highest values (compare fig. ). sparse coding models, instance, exploit posterior mass lies close low dimensional subspaces define sets], found deriving eﬃciently computable upper-bounds probabilities) ] derivations based taking limits data noise]. complex models] (sec. ) discussion suitable selection functions. precise form limited inﬂuence final approximation accuracy values approximation) size sets chosen generously easily regions large posterior mass. larger precise selection. equal entire state space, selection required approximations) fall back case exact inference. sampling. alternative approximate expectation values. sampling posterior distribution, samples compute average) ,?)  )  ?).  challenging aspect approach eﬃciently draw samples posterior. high-dimensional sample space, Markov Chain Monte Carlo (mcmc). class methods draws samples posterior distribution subsequent sample drawn relative current state, resulting sequence samples form Markov chain. limit large number samples, Monte Carlo methods oretically represent probability distribution. however, number samples \\x0crequired high-dimensional spaces large (fig. , sampling). map estimate exact preselection(  selected units) select sample smax smax sampling) ) )  selected units Wdh Wdh Figure Simplified illustration posterior mass respective regions approximation approach compute expectation values. graphical model showing connection Wdh observed variables hidden variables hidden variables/units selected form set graphical model resulting selection hidden variables weights Wdh (black). select sample. preselection deterministic approach stochastic nature sampling, formulation approximation expectation values) straight-forward combination approaches: data point) approximate expectation) variational distribution; defined preselection). second, approximate expectations. ; sampling. combined approach) ; ) ,?)  )iqn;?)  ) denote samples truncated \\x0cdistribution drawing distribution entire state space, approximation) requires samples potentially small subspace (fig. ). subspace original probability mass concentrated smaller volume, MCMC algorithms perform eﬃciently, results smaller space explore, shorter burn times, reduced number required samples. compared selection alone, select sample approach represent increase eﬃciency number samples required good approximation number states sparse coding: Example Application systematically investigate computational eﬃciency, performance, biological plausibility select sample approach comparison selection sampling sparse coding model images. choice sparse coding model numerous advantages. first, non-trivial model extremely well-studied machine learning research, eﬃcient algorithms exist]). second, standard (albeit simplistic) model organization receptive fields primary visual cortex]. discrete variant model Binary Sparse Coding (bsc], compare]), binary hidden variables orwise features standard sparse coding versions. generative model BSC expressed—?)    ,  )  denotes basis vectors parameterizes sparsity above). -step updates BSC learning algorithm (see. ) iqn )new)  — ) expectation values needed-step hsiqn sst compare learning inference algorithms: BSCexact algorithm approximations obtained exact posterior expectations) ?). refer exact algorithm BSCexact directly computable, expectation values BSCexact require sums entire state space., terms. large numbers latent dimensions, BSCexact intractable. bscselect algorithm eﬃciently scales \\x0cnumber hidden dimensions obtained applying preselection. pfor BSC model) (for }. note addition states) include states non-zero unit (all singletons). including avoids iterations initial phases learning leave basis functions unmodified (see]). selection function) define use) (wdh  component large) strongly) basis function (see fig. ). note) related deterministic ica-like selection hidden state) limit case noise (compare]). furr restrictions state space require modified-step equations (see]), considered here. bscsample alternative non-deterministic approach derived Gibbs sampling. gibbs sampling MCMC algorithm systematically explores sample space repeatedly drawing samples conditional distributions individual hidden dimensions. words, transition probability current sample candidate sample current(snew case binary sample space, equates selecting random axis ,   toggling bit (reby changing binary state dimension), leaving remaining axes unchanged. specifically, posterior probability computed candidate sample expressed   introduced parameter smoothing posterior distribution. ensure mixing behavior MCMC chains wide range (note model parameter learning), define  temperature parameter set manually selected good mixing achieved. samples drawn manner approximate expectation values). bscs learning algorithm combining selection sampling obtained applying). note inserting BSC generative model) results: bernoullikn;   ) bernoullikn  BernoulliKn;     remainder Bernoulli distribution cancels out. define binary vector consisting entries selected   basis functions selected, observe dimensions, distribution equal posterior. bsc model hidden dimensions: ?, bernoulli?  ?     bernoulli?   ; instead drawing samples; draw samples exact posterior. bsc generative model dimensions. sampling procedure BSCsample applied simply ignoring non-selected dimensions parameters. data points, latent dimensions selected averaging data points update model parameters. selection ), defining), indices highest values randomly selected dimensions (drawn uniform distribution non-selected dimensions). randomly selected dimensions fulfill purpose inclusion singleton states BSCselect preselection Gibbs sampling selected dimensions define approximation required expectation values) result algorithm referred bscs expectation values complexity. collecting number operations compute BSC cases, arrive) denotes number hidden states contribute calculation expectation values. approaches preselection (bscselect bscs calculations expectation values performed reduced latent space; refore replaced BSCexact number scales exponentially exact BSCselect case, scales exponentially number preselected hidden variables: select however, sampling based approaches (bscsample bscs number directly corresponds number samples evaluated obtained empirically. show later 200 reasonable choice interval investigate paper  ). numerical Experiments compare select sample approach selection sampling applied individually data sets: artifical images natural image patches. experiments sampling approaches, draw independent chains initialized random states order increase mixing samples. also, samples drawn chain, burn samples, retained samples. artificial data. set experiments investigate select sample approach convergence properties artificial data sets ground truth available. experiments run small scale problem, compute exact data likelihood step algorithms (bscexact BSCselect BSCsample bscs compare convergence ground truth likelihood. (?) step \\x0cbscsample BSCselect BSCexact step step bscs step Figure Experiments artificial bars data,  dotted line ground truth log-likelihood value. random selection 2000 training data points) learned basis functions Wdh successful training run. development log-likelihood period steps investigated algorithms. data experiments consisted images generated creating basis functions form horizontal vertical bars pixel grid. bar randomly assigned eir positive (wdh }) negative (whgt0 }). 2000) data points generated linearly combining basis functions (see]). sparseness resulted, average, active bars data point. model, added Gaussian noise) data (fig. ). applied algorithms dataset monitored exact likelihood period steps (fig. ). calculation exact likelihood requires)) operations, feasible small scale problem. models preselection (bscselect bscs set effectively halving number hidden variables participating calculation expectation values. BSCsample bscs drew 200 samples posterior) data point, number states evaluated totaled sample 200 2400 200 1200, respectively. ensure mixing behavior annealing temperature set. experiment basis functions initialized data Gaussian noise, prior probability ?init data noise variance data. algorithms recover correct set bases functions% trials, sparseness prior data noise high accuracy. comparing computational costs algorithms shows benefits preselection small scale problem: BSCexact evaluates expectation values full set 4096 hidden states, BSCselect considers states. pure sampling based approaches performs 2400 evaluations bscs requires 1200 evaluations. image patches. test select sample approach natural image data challenging scale, include biological plausibility demonstration applicability larger scale problems. extracted, 000 patches size 676 pixels van Hateren image database] preprocessed Difference Gaussians (dog) \\x0cfilter, approximates sensitivity center center-off neurons found early stages mammalian visual processing. filter parameters chosen]. experiments ran 100 iterations ensure proper convergence. annealing temperature set.  104 states 400 100 .53e7 105.51e7 106.49e7(?) 200 107.47e7 states Figure Experiments image patches , 800. random selection patches (after DoG preprocessing). random selection learned basis functions (number samples set 200). end approx. log-likelihood 100-steps. number samples data point. number states evaluated approaches. series experiments investigate effect number drawn samples performance algorithm measured approximate data likelihood) entire range values \\x0cand. observe bscs 200 samples hidden dimension (total states 200 suﬃcient: final likelihood 100 steps begins saturate. particularly, increasing number samples increase likelihood%. fig. report curve, trend observed values anor set experiments, number samples (200 pure sampling case (bscsample order monitor likelihood behavior. observed consistent trends: algorithm observed converge high-likelihood solution, initialized solutions high likelihood, likelihood decreases. demonstrates gains select sample pure sampling: bscs 200 000 samples robustly reach high-likelihood solutions, regime BSCsample algorithm poorly converge high-likelihood solution, 200 800 160, 000 samples (fig. ). large scale experiment image patches. comparison results shows eﬃcient algorithm obtained combination preselection sampling, select sample approach (bscs minimal effect performance algorithm depicted fig.  eﬃciency applications larger scale problems individual approximation approaches. demonstrate eﬃciency combined approach applied bscs image dataset, high number observed hidden dimensions. extracted database 500, 000 patches size 600 pixels. bscs applied number hidden units set 600. conditions previous experiments (notably 200 , 000 samples 100 iterations) obtain set gabor-like basis functions (see fig. ) states (fig. ). knowledge, presented results illustrate largest application sparse coding complete representation posterior. discussion introduced eﬃcient method unsupervised learning probabilistic models maintains complex representation posterior problems consistent restricted set images 900 images man-made structures (see Fig). brightest pixels clamped max remaining% (reducing inﬂuences light-reﬂections) 1012 states BSCselect bscs 200 104 100 Figure large-scale application bscs image patches 1600 pixels 1600 hidden dimensions). random selection inferred basis functions shown (see Suppl basis functions model parameters). comparison computational complexity: BSCselect scales exponentially bscs scales linearly. note large difference real-world scales. furrmore, approach biologically plausible models brain make sense environment large-scale sensory inputs. specifically, method implemented neural networks mechanisms, independently suggested context statistical framework perception: feed-forward preselection], sampling]. showed seemingly contrasting approaches combined based interpretation approximate inference methods, resulting considerable increase computational eﬃciency., figs. ). sparse coding model natural images standard model neural response properties] order investigate, numerically analytically, applicability eﬃciency method. comparisons approach exact inference, selection alone, sampling showed favorable scaling number observed hidden dimensions. knowledge, sparse coding implementation reached comparable problem size , 000) assumed Laplace prior MAP estimation posterior]. however, MAP estimations, basis functions rescaled (compare]) data noise prior parameters inferred (instead regularizer hand-set). method require artificial mechanisms rich posterior representation. representations are, furrmore, crucial inferring parameters data noise sparsity (learned experiments), correctly act faced uncertain input]. concretely, sparse coding model binary latent variables. allowed systematic comparison exact low-dimensional problems, extension continuous case straight-forward. model, selection step results simple, local neurally plausible integration input data). combination Gibbs sampling, neurally plausible neurons individually sample state based current state neurons, transmitted recurrent connections]. idea combining sampling feed-forward mechanisms previously explored, contexts goals. work Beal] variational approximations proposal distributions importance sampling, Zhu] guided metropolis-hastings algorithm data-driven proposal. distribution. approaches selecting subspaces prior sampling diﬃcult link neural feed-forward sweeps]. expect select sample strategy widely applicable machine learning models posterior probability masses expected concentrated small sub-space latent space. sophisticated preselection mechanisms sampling schemes lead furr reduction computational efforts, details depend general model input data. acknowledgements. acknowledge funding German Research Foundation (dfg) project 1196), German Federal Ministry Education Research (bmbf), project 01gq0840 (jas, ass), Swartz Foundation Swiss National Science Foundation). furrmore, support Physics dept. center Scientific Computing (csc) Frankfurt acknowledged. According recently inﬂuential statistical approach perception, brain represents interpretation stimulus, uncertainty. words, ideally brain represent full posterior distribution interpretations stimulus, statistically optimal inference learning, hyposis supported increasing number psychophysical electrophysiological results]. Although generally accepted humans maintain complex posterior representation, outstanding diﬃculty approach full posterior distribution general complex, highly correlated (due explaining effects), multimodal (multiple interpretations), high-dimensional. one approach address problem neural circuits neuronal activity represent parameters variational approximation real posterior]. although approach approximate full posterior, number neurons explodes number variables example, approximation Gaussian distribution requires parameters represent covariance matrix variables. anor approach identify neurons variables interpret neural activity samples posterior]. this interpretation consistent range experimental observations, including neural variability (which result uncertainty posterior) spontaneous activity (corresponding samples prior absence stimulus]. advantage sampling number neurons scales linearly number variables, represent arbitrarily complex posterior distributons samples. part issue: collecting suﬃcient number samples form complex, high-dimensional representation time-costly. modeling studies shown small number samples suﬃcient perform low-dimensional tasks (intuitively, taking lowdimensional marginal posterior accumulates samples dimensions]. however, sensory data inherently high-dimensional. such, order faithfully represent visual scenes potentially objects object parts, requires high-dimensional latent space represent high number potential causes, returns problem sampling approaches face high dimensions. goal line research pursued address questions: find sophisticated representation posterior high-dimensional hidden spaces? goal believed shared brain, find biologically plausible solution reaching? paper propose approach approximate inference learning addresses drawbacks sampling neural processing model, maintains beneficial posterior representation neural plausibility. show sampling combined preselection candidate units. such selection connects sampling inﬂuential models neural processing emphasize feed-forward processing] more), consistent popular view neural pro \\x0ccessing learning interplay feed-forward recurrent stages processing]. our combined approach emerges naturally interpreting feedforward selection sampling approximations exact inference probabilistic framework perception. Select Sample Approach Approximate Inference Inference learning neural circuits regarded task inferring true hidden stimulus. inferring objects visual scene based image projected retina. refer sensory stimulus image) data point   refer hidden objects   denoting hidden variablepor hidden unit data distribution modeled generative data model    denoting parameters model1 assume data distribution optimally modeled generative distribution optimal parameters posterior probability represents optimal inference data point parameters set data points   maximum likelihood parameters argmax?  ?)}. standard procedure find maximum likelihood solution expectation maximization). iteratively optimizes lower bound data likelihood inferring posterior distribution hidden variables current parameters-step), adjusting parameters maximize likelihood data averaged posterior-step). -step updates typically depend small number expectation values posterior) ,?) ) ) elementary function hidden variables) sst case standard sparse coding). for non-trivial generative model, computation case continuous variables sum replaced integral. for hierarchical model, prior distribution subdivided hierarchically sets variables. expectation values) computationally demanding part optimization. exact computation intractable well-known algorithms]) rely estimations. iterations neural processing assumption neural activity represents posterior hidden variables-step), synaptic plasticity implements model parameters-step). here prominent models neural processing ground approximations expectation values) show combined. selection. feedforward processing frequently discussed important component neural processing]. one perspective early component neural activity preselection candidate units hyposes sensory stimulus] more), goal reducing computational demand orwise complex computation. context probabilistic approaches, recently shown preselection \\x0ccan formulated variational approximation exact inference]. variational distribution case truncated sum hidden states) ) )  ;     ) )    orwise. subset represents preselected latent states. given data point) eqn. results good approximations posterior posterior mass. since applications posterior mass concentrated small volumes state space, approximation quality stay high small sets this approximation compute eﬃciently expectation values needed-step)  ) ,?)  )iqn;?) )  eqn. represents reduction required computational resources involves summations integrations) smaller state space requirement set selected prior computation expectation values, final improvement eﬃciency relies selections eﬃciently computable. such, selection function carefully chosen order define eﬃciently selects candidate units contributed data point) defined: ) indices highest values (compare fig. ). for sparse coding models, instance, exploit posterior mass lies close low dimensional subspaces define sets], found deriving eﬃciently computable upper-bounds probabilities) ] derivations based taking limits data noise]. for complex models] (sec. ) discussion suitable selection functions. often precise form limited inﬂuence final approximation accuracy values approximation) size sets chosen generously easily regions large posterior mass. larger precise selection. for equal entire state space, selection required approximations) fall back case exact inference. sampling. alternative approximate expectation values. sampling posterior distribution, samples compute average) ,?)  )  ?).  challenging aspect approach eﬃciently draw samples posterior. high-dimensional sample space, Markov Chain Monte Carlo (mcmc). this class methods draws samples posterior distribution subsequent sample drawn relative current state, resulting sequence samples form Markov chain. limit large number samples, Monte Carlo methods oretically represent probability distribution. however, number samples \\x0crequired high-dimensional spaces large (fig. , sampling). MAP estimate exact preselection(  selected units) select sample smax smax sampling) ) )  selected units Wdh Wdh Figure Simplified illustration posterior mass respective regions approximation approach compute expectation values. Graphical model showing connection Wdh observed variables hidden variables hidden variables/units selected form set Graphical model resulting selection hidden variables weights Wdh (black). select sample. although preselection deterministic approach stochastic nature sampling, formulation approximation expectation values) straight-forward combination approaches: data point) approximate expectation) variational distribution; defined preselection). second, approximate expectations. ; sampling. combined approach) ; ) ,?)  )iqn;?)  ) denote samples truncated \\x0cdistribution instead drawing distribution entire state space, approximation) requires samples potentially small subspace (fig. ). subspace original probability mass concentrated smaller volume, MCMC algorithms perform eﬃciently, results smaller space explore, shorter burn times, reduced number required samples. compared selection alone, select sample approach represent increase eﬃciency number samples required good approximation number states Sparse coding: Example Application systematically investigate computational eﬃciency, performance, biological plausibility select sample approach comparison selection sampling sparse coding model images. choice sparse coding model numerous advantages. first, non-trivial model extremely well-studied machine learning research, eﬃcient algorithms exist]). second, standard (albeit simplistic) model organization receptive fields primary visual cortex]. here discrete variant model Binary Sparse Coding (bsc], compare]), binary hidden variables orwise features standard sparse coding versions. generative model BSC expressed—?)    ,  )  denotes basis vectors parameterizes sparsity above). -step updates BSC learning algorithm (see. ) iqn )new)  — ) expectation values needed-step hsiqn sst compare learning inference algorithms: BSCexact algorithm approximations obtained exact posterior expectations) ?). refer exact algorithm BSCexact although directly computable, expectation values BSCexact require sums entire state space., terms. for large numbers latent dimensions, BSCexact intractable. bscselect algorithm eﬃciently scales \\x0cnumber hidden dimensions obtained applying preselection. pfor BSC model) (for }. note addition states) include states non-zero unit (all singletons). including avoids iterations initial phases learning leave basis functions unmodified (see]). selection function) define use) (wdh  component large) strongly) basis function (see fig. ). note) related deterministic ica-like selection hidden state) limit case noise (compare]). furr restrictions state space require modified-step equations (see]), considered here. bscsample alternative non-deterministic approach derived Gibbs sampling. gibbs sampling MCMC algorithm systematically explores sample space repeatedly drawing samples conditional distributions individual hidden dimensions. words, transition probability current sample candidate sample current(snew case binary sample space, equates selecting random axis ,   toggling bit (reby changing binary state dimension), leaving remaining axes unchanged. specifically, posterior probability computed candidate sample expressed   introduced parameter smoothing posterior distribution. ensure mixing behavior MCMC chains wide range (note model parameter learning), define  temperature parameter set manually selected good mixing achieved. samples drawn manner approximate expectation values). bscs learning algorithm combining selection sampling obtained applying). first note inserting BSC generative model) results: BernoulliKn;   ) BernoulliKn  BernoulliKn;     remainder Bernoulli distribution cancels out. define binary vector consisting entries selected   basis functions selected, observe dimensions, distribution equal posterior. BSC model hidden dimensions: ?, bernoulli?  ?     bernoulli?   ; Instead drawing samples; draw samples exact posterior. BSC generative model dimensions. sampling procedure BSCsample applied simply ignoring non-selected dimensions parameters. for data points, latent dimensions selected averaging data points update model parameters. for selection ), defining), indices highest values randomly selected dimensions (drawn uniform distribution non-selected dimensions). randomly selected dimensions fulfill purpose inclusion singleton states BSCselect preselection Gibbs sampling selected dimensions define approximation required expectation values) result algorithm referred bscs expectation values complexity. collecting number operations compute BSC cases, arrive) denotes number hidden states contribute calculation expectation values. for approaches preselection (bscselect bscs calculations expectation values performed reduced latent space; refore replaced for BSCexact number scales exponentially exact BSCselect case, scales exponentially number preselected hidden variables: select however, sampling based approaches (bscsample bscs number directly corresponds number samples evaluated obtained empirically. show later 200 reasonable choice interval investigate paper  ). Numerical Experiments compare select sample approach selection sampling applied individually data sets: artifical images natural image patches. for experiments sampling approaches, draw independent chains initialized random states order increase mixing samples. also, samples drawn chain, burn samples, retained samples. artificial data. our set experiments investigate select sample approach convergence properties artificial data sets ground truth available. experiments run small scale problem, compute exact data likelihood step algorithms (bscexact BSCselect BSCsample bscs compare convergence ground truth likelihood. (?) step \\x0cbscsample BSCselect BSCexact step step bscs step Figure Experiments artificial bars data,  dotted line ground truth log-likelihood value. Random selection 2000 training data points) Learned basis functions Wdh successful training run. Development log-likelihood period steps investigated algorithms. Data experiments consisted images generated creating basis functions form horizontal vertical bars pixel grid. each bar randomly assigned eir positive (wdh }) negative (whgt0 }). 2000) data points generated linearly combining basis functions (see]). using sparseness resulted, average, active bars data point. according model, added Gaussian noise) data (fig. ). applied algorithms dataset monitored exact likelihood period steps (fig. ). although calculation exact likelihood requires)) operations, feasible small scale problem. for models preselection (bscselect bscs set effectively halving number hidden variables participating calculation expectation values. for BSCsample bscs drew 200 samples posterior) data point, number states evaluated totaled sample 200 2400 200 1200, respectively. ensure mixing behavior annealing temperature set. experiment basis functions initialized data Gaussian noise, prior probability ?init data noise variance data. all algorithms recover correct set bases functions% trials, sparseness prior data noise high accuracy. comparing computational costs algorithms shows benefits preselection small scale problem: BSCexact evaluates expectation values full set 4096 hidden states, BSCselect considers states. pure sampling based approaches performs 2400 evaluations bscs requires 1200 evaluations. image patches. test select sample approach natural image data challenging scale, include biological plausibility demonstration applicability larger scale problems. extracted, 000 patches size 676 pixels van Hateren image database] preprocessed Difference Gaussians (dog) \\x0cfilter, approximates sensitivity center center-off neurons found early stages mammalian visual processing. filter parameters chosen]. for experiments ran 100 iterations ensure proper convergence. annealing temperature set.  104 states 400 100 .53e7 105.51e7 106.49e7(?) 200 107.47e7 states Figure Experiments image patches , 800. Random selection patches (after DoG preprocessing). Random selection learned basis functions (number samples set 200). End approx. log-likelihood 100-steps. number samples data point. Number states evaluated approaches. series experiments investigate effect number drawn samples performance algorithm measured approximate data likelihood) entire range values \\x0cand. observe bscs 200 samples hidden dimension (total states 200 suﬃcient: final likelihood 100 steps begins saturate. particularly, increasing number samples increase likelihood%. fig. report curve, trend observed values anor set experiments, number samples (200 pure sampling case (bscsample order monitor likelihood behavior. observed consistent trends: algorithm observed converge high-likelihood solution, initialized solutions high likelihood, likelihood decreases. this demonstrates gains select sample pure sampling: bscs 200 000 samples robustly reach high-likelihood solutions, regime BSCsample algorithm poorly converge high-likelihood solution, 200 800 160, 000 samples (fig. ). large scale experiment image patches. comparison results shows eﬃcient algorithm obtained combination preselection sampling, select sample approach (bscs minimal effect performance algorithm depicted fig.  this eﬃciency applications larger scale problems individual approximation approaches. demonstrate eﬃciency combined approach applied bscs image dataset, high number observed hidden dimensions. extracted database 500, 000 patches size 600 pixels. bscs applied number hidden units set 600. using conditions previous experiments (notably 200 , 000 samples 100 iterations) obtain set gabor-like basis functions (see fig. ) states (fig. ). knowledge, presented results illustrate largest application sparse coding complete representation posterior. Discussion introduced eﬃcient method unsupervised learning probabilistic models maintains complex representation posterior problems consistent restricted set images 900 images man-made structures (see Fig). brightest pixels clamped max remaining% (reducing inﬂuences light-reﬂections) 1012 states BSCselect bscs 200 104 100 Figure large-scale application bscs image patches 1600 pixels 1600 hidden dimensions). random selection inferred basis functions shown (see Suppl basis functions model parameters). Comparison computational complexity: BSCselect scales exponentially bscs scales linearly. note large difference real-world scales. furrmore, approach biologically plausible models brain make sense environment large-scale sensory inputs. specifically, method implemented neural networks mechanisms, independently suggested context statistical framework perception: feed-forward preselection], sampling]. showed seemingly contrasting approaches combined based interpretation approximate inference methods, resulting considerable increase computational eﬃciency., figs. ). sparse coding model natural images standard model neural response properties] order investigate, numerically analytically, applicability eﬃciency method. comparisons approach exact inference, selection alone, sampling showed favorable scaling number observed hidden dimensions. knowledge, sparse coding implementation reached comparable problem size , 000) assumed Laplace prior MAP estimation posterior]. however, MAP estimations, basis functions rescaled (compare]) data noise prior parameters inferred (instead regularizer hand-set). our method require artificial mechanisms rich posterior representation. such representations are, furrmore, crucial inferring parameters data noise sparsity (learned experiments), correctly act faced uncertain input]. concretely, sparse coding model binary latent variables. this allowed systematic comparison exact low-dimensional problems, extension continuous case straight-forward. model, selection step results simple, local neurally plausible integration input data). combination Gibbs sampling, neurally plausible neurons individually sample state based current state neurons, transmitted recurrent connections]. idea combining sampling feed-forward mechanisms previously explored, contexts goals. work Beal] variational approximations proposal distributions importance sampling, Zhu] guided metropolis-hastings algorithm data-driven proposal. distribution. both approaches selecting subspaces prior sampling diﬃcult link neural feed-forward sweeps]. expect select sample strategy widely applicable machine learning models posterior probability masses expected concentrated small sub-space latent space. using sophisticated preselection mechanisms sampling schemes lead furr reduction computational efforts, details depend general model input data. acknowledgements. acknowledge funding German Research Foundation (dfg) project 1196), German Federal Ministry Education Research (bmbf), project 01gq0840 (jas, ass), Swartz Foundation Swiss National Science Foundation). furrmore, support Physics dept. Center Scientific Computing (csc) Frankfurt acknowledged.',\n",
       " 'PP4354': 'when objects images same Although people recognize categorize objects successfully effortlessly, object recognition machine learning incredibly diﬃcult problem people success puzzle cognitive scientists. solve problem, object recognition techniques typically generate set features predefined procedure., SIFT descriptors] textons]) learn features., deep belief networks]) images. general goal methods extract features images identifying objects generated images transformations occurred producing., viewpoint changes). strategy people typically perceive object transformed image., translations). however, transformations ignored: perceived identity objects depends orientation features respect scene., . differ orientation), objects paper, terminology scene, image, object. entire visual input observer scene. scene set images. image part visual input generated single object, ambiguous objects generate image. object item world generates image visual input. not. developing proper object recognition fully understanding people depends explaining people determine orientation objects respect scene. importance orientation object recognition leads question: objects project image viewing conditions., degree rotations), people infer object image? psychology, main ories people solve problem: invariant feature hyposis], essentially strategy current object recognition techniques (use features preserve object identity transformations generate images object), reference frame hyposis, posits objects embedded coordinate axes]. coordinate axes set orientation scale objects, identified objects. produce image, coordinate axes. situations orientation image reference frame simply orientation retina; however, case rotate heads retinal image rotates) rotated object., person lying bench document rotated desk). thus, reference frame image ambiguous additional information. however, anor object scene orientation unambiguous (like), orientation ambiguous image inferred demonstrate people orientation images scene determine orientation ambiguous image participants solve arithmetic problems, operator image ambiguous numbers ﬂanking operator eir oriented upright rotated degrees. solution people adopt indicative reference frame inferred operator (multiplication implies upright reference frame addition implies diagonal reference frame). experimental method explore reference frame inference wide range contexts. real life, typically view scenes multiple reference frames. example, books bookshelf upright, books tilted diagonally (for support), books lie ﬂat. work investigating people infer number reference frames, orientations, images belong reference frame. solve problem, note image scene belongs single reference frame, reference frames form partition images scene (where block partition corresponds reference frame). standard nonparametric Bayesian model partitions, formulate ideal observer model infer multiple reference frames parameters. model predicts people sensitive cues inferring reference \\x0cframes scene: proximity ambiguous image unambiguous ﬂanking images conﬂicting orientations, difference number objects aligned competing reference frames. confirm people sensitive cues method above. summary article follows. first, Section summarizes relevant psychological research orientation affects objects perceived ambiguous images. next, Section develops method online testing reference frame people infer image establishes eﬃcacy. section presents ideal observer model reference frame inference scenes multiple reference frames. model predicts ambiguous image proximity reference frames affect inferred reference frame Section confirms people act accordance prediction behavioral experiment. model predicts number aligned objects reference frame affect reference frame inferred ambiguous image. section confirms prediction behavioral experiment. section concludes paper highlights directions future research. orientation psychological ories object representation Though perceived object images depend orientation (like), examples perceived object depend orientation], including. square. diamond, effects orientation object recognition]. led psychologists people represent objects reference frame set coordinate axes Figure) shows reference frames predict image interpreted view ambiguity reference frame essentially strength intrinsic axes]. coordinate axes properties., scale), focus orientation article. ) Figure Reference frames. ) ambiguity image resolved reference frames: horizontal orientation (solid axes) rotated degrees (dashed axes). ) images unambiguous, ) reference frame ambiguous objects inﬂuenced objects \\x0cunambiguous reference frames. ) group objects eir establishes reference frame group. coordinate axes aligned document axes coordinate axes diagonal document axes. objects rotationally invariant, object generates observed image identifiable orientation (see Figure)). dependence object perception orientation established norm demonstrated familiar objects, faces, handwriting objects]. central reference frame hyposis ability perceptual system infer reference frame image. reference frame consistent observed image, psychologists explored people infer reference frame image. reference frame inference strongly inﬂuenced top-down axis retinal image axis gravity (given proprioceptive vestibular senses], scene inﬂuence inferred reference frame. objects grouped toger world tend affected transformation generate images., text poster poster rotated), inferred reference frame ambiguous image inﬂuenced orientations images surrounding. figures) phenomenological demonstrations alignment orientations objects scene bias inferred reference frame image reference frame ambiguous (and strong corroborating empirical evidence principle Figure) biased interpreted based surrounding context images Figure) interpreted eir tilted diﬃcult interpret ors tilted simultaneously]. thus, reference frame shared objects group. wealth research reference frame inference scenes single reference frame, knowledge, research people determine reference frame ambiguously oriented images reference frame scene (and consistent images). exploring cues inﬂuence human reference frame inference scenes multiple reference frames, develop method testing human reference frame inference. testing reference frame inference arithmetic test factors inﬂuence reference frame people infer image, people solve arithmetic problem operation. people view response multiplication answer, reference frame aligned horizontal vertical axes page. alternatively, people view response addition answer, reference frame aligned axes diagonal page (and thus, relative reference frame, treated method previous techniques., explicitly image orientation recording frequency orientation chosen eir compatible conﬂicting tested hyposis]) due ability wide range contexts demonstrate robust importance reference slightly terminology previous work refer principle alignment rar symmetry avoid ambiguity word symmetry (which symmetry referring). although ambiguous images, method works ambiguous images teaching participant addition orientation image multiplication. ) Axis Oriented) Diagonal Oriented Axis Oriented Diagonal Oriented Frequency Frequency Response Response Figure Effect orientations objects reference frame. ) aligned axes implies operator ) aligned diagonal implies operator diagonal orientation. ) Frequency answers) participants. participants respond, solution product meaning reference frame aligned axes page. ) Frequency answers) participants. participants respond, meaning reference frame aligned diagonals page. frame inference seemingly unrelated cognitive behavior (solving arithmetic problem). confirm validity reproducing previously found effect inﬂuence orientation images scene]. reference frame image ambiguous, factor inﬂuences inferred reference frame orientation images grouped with, images identifiable orientation. thus, people solve arithmetic problem, operator paired numbers aligned top-down axes page (figure)), respond, result multiplication. alternatively, people solve problem numbers aligned diagonally, infer diagonal axes reference frame respond, result addition (figure)). test method, recruited participants online, answered arithmetic problem exchange small monetary reward. participants counterbalanced axis diagonally oriented conditions (figures) respectively) participants gave eir addition) multiplication) solution. changing orientation numbers, solutions arithmetic problems participants Figures) identical numbers identical operator image. figures) show responses groups participants answered arithmetic problem) differed predicted.208, yates? chisquare correction). thus, participants solve arithmetic problems effective method testing reference frame inference perceived orientations inﬂuence higher level cognition. modeling reference frame inference Before describing model reference frame inference multiple reference frames, present probabilistic model scenes multiple images single reference frame.  Reference frame inference scenes reference frame assume vocabulary objects ahead time size rotations. scene., Figure) scene) consists set images., images Figure)). image scene, model visual properties spatial location (xi1 xi2 visual properties image generated unknown object rotated orientation scene reference frame.  binary image-object alignment matrix) encodes object-rotation pairs consistent observed image, image object rotated degrees consistent model assumes spatial locations images independent identically distributed draws Gaussian distribution shared parameters center point reference frame, spread objects center point. unobserved objects orientation reference frame drawn independent discrete distributions parameters prior objects reference frame orientations, respectively. generative model defines statistical model: iid—?  discrete(?)   discrete(?) —?,  gaussian (?,  model assumes types objects, rotations degrees), model captures sensitivity participants demonstration (figure). figure), oriented degrees. , non-zero object produce image consistent \\x0cwith observed image implies operator consistent participant responses (figure)). oriented degrees (figure, non-zero reason before. implies operator consistent participant responses (figure)).  Extending model scenes multiple reference frames Although model defined previous section succeeds inferring reference frame ambiguous image images grouped with, handle scenes multiple reference frames, scenes Figure extend model partitioning images scene reference frames, image scene belongs reference frame reference frame block partition. perspective, inferring multiple reference frames scene images equivalent partitioning scene clustering images. insight grouping images reference frames finding partition scene, extend model select reference frames scene (with unknown number reference frames). first, generate partition images scene Chinese restaurant process (crp] parameter exchangeable distribution partitions. crp defined sequential construction   current number reference frames number objects assigned reference frame denotes reference frame object assigned assigned reference frame previous objects increments initialize, object starts reference frame). assignment vector denotes reference frame image block partition (reference frame) rotation embedded spatial layout scene center position spread (each generated gaussian-inverse Wishart distribution shared parameters). thus, defined generative model set images scene: iid  —?  crp(?)  giw iid iid  discrete(?)   discrete(?)  rci rci gaussian GIW signifies gaussian-inverse-wishart distribution, hyperparameters model. gibbs sampling inference], cluster assignments image updated parameters cluster begin assigning image reference frame iterating. observed image, resample set existing clusters newly drawn clusters. values resampled, discard empty clusters update parameters remaining clusters drawing posterior distribution objects assigned reference frame set images locations reference frame  Predictions human reference frame inference What factors inﬂuence reference frame assigned ambiguous image ideal observer model? factors predicts inﬂuence image inferred reference frame Operator Position Operator Position Operator Position Operator Position Figure Trials Experiment showing positions operators main factor experiment. factors randomized trials numbers problem (always single digits), numbers rotated, diagonal numbers operator aligned (positive diagonal shown figure, numbers operator aligned negative diagonal well), rotation operator. ) Participant responses proximity experiment Percent grouped left Percent grouped left Model predictions proximity experiment Operator Position Operator Position Figure Proximity effects) Human results) Model results. closer operator left number, left number orientation. proximity close image unambiguous \\x0cimages images reference frame coupled spatial location) alignment difference number images assigned reference frame. general paradigm test predictions operator ﬂanked number orientations side (see examples Figure). clear numbers reference frame, ambiguous reference frame operator assigned. compare factors inﬂuences reference frames inferred scene people model behavioral experiments. experiment Proximity effects reference frame inference When reference frame image ambiguous conﬂicting neighboring reference frames, model predicts proximity distance ambiguous image conﬂicting reference frames affect reference frame adopted ambiguous image. explore question method presented above, participants asked solve arithmetic problem operator ambiguous numbers conﬂicting reference frames (orientations). deduce reference frame inferred operator image answer participants. manipulate proximity changing location operator closer numbers shown Figure  Methods total 134 participants completed experiment online Amazon Mechanical Turk exchange usd. participants give correct solution arithmetic problem (neir addition multiplication solution) leaving 130 participants analysis. participants asked maximize window answering arithmetic problem. factors manipulated subjects preliminary testing demonstrated strong effect trial order selected reference frame (probably reference frames rarely change world). primary factor interest experiment position operator scored (far left) (far right), counterbalanced participants (without position). problem viewed simulated aperture minimize effect monitor reference frame). Figure trials operator position. factors randomized participants: numbers problem (randomly chosen single digit numbers), number rotated (left right), diagonal numbers operator aligned (positive diagonal, shown Figure negative diagonal), rotation operator ?).  Results Discussion Figure) shows participants infer orientation left number operator closer left number. results confirm hyposis: closer operator image unambiguous reference frame, participants infer \\x0creference frame operator). probit regression analysis corroborates result regression coeﬃcient significantly). model results generated Gibbs sampling previously described) shown Figure). trial, ran sampler burn iterations, recorded 750 samples, thinned samples selecting samples. left 150 samples formed estimate proportion times operator grouped left reference frame. parameters initialized: .001, [264, 261], 1000i (scenes 550?550 pixels bottom-left corner origin), identity matrix, 110. discrete distributions encoding priors objects orientations, uniform possibilities. model human results exhibit qualitative behavior: distance operator left number decreased, probability operator orientation left number increased. experiment Alignment effects reference frame inference Our model predicts difference number unambiguous images assigned conﬂicting reference frames affect reference frame adopted operator image. experiment, test prediction method above, manipulate number extra oriented unambiguous objects competing reference frames (see Figure)).  Methods total people participated online Amazon Mechanical Turk exchange usd. participants gave incorrect answer, leaving participants analysis. instructions design identical previous experiment, extra factors manipulating context left number left vice versa) operator positions). figure) illustrates trials context manipulations operator position.  Results Discussion Figure) shows participants infer operator orientation orientation whichever side objects closer, replicating effect Experiment.8728.0005 model results generated procedure parameter values Experiment (except account increased number objects) Figure) shows similarity participant results. conclusions future directions paper, introduced study people infer reference frame images scenes multiple reference frames. presented implicit method testing reference frame inference, ideal observer model predicts people sensitive scene cues) Percent grouped left) 5l1r Alignment effects participant responses 5l1r 1l5r) 1l5r Percent grouped left Operator Position Alignment effects model responses 5l1r 1l5r Operator Position Figure Alignment effects. operator orientation side objects. 5l1r denotes objects left reference frame object right, 1l5r opposite arrangement. ) Example stimuli) Human results) Model results. behavioral evidence supporting predictions. objects people perceive depend orientation images scene, results improve understanding configuration objects scenes affects object perception. plan extend model capture cues identified perceptual psychologists. step include bias-down axis input image] nonuniform distribution rotations (estimating ?). capture elongation cue (that orientation spread images scene biases orientation reference frame images scene]) coupling covariance matrix (?) rotation) reference frame. currently, model assumes positions images reference frame Gaussian distributed; however, people strong expectations arrangement images scene]. plan compare people bias sophisticated scene segmentation model]. interested cues depend structure images orientation agent world, axes symmetry] gravitational axes]. anor direction future work address assumption model: How people learn set objects wher objects orientation-invariant? potential solution combine model previous work presented nonparametric Bayesian model learning features transformations allowed undergo]. hopefully, incorporating model feature learning method yield inferred features and, turn, create feature generation object recognition techniques providing understanding people perceive objects ambiguous image data. finally, plan explore presented principles scale realistic scenes objects complex orientations. paradigm principled starting point investigating reference frames identified scenes multiple reference frames. easily extended complex scenes associating orientations rotations depth) ambiguous image arithmetic operators. hope leads understanding object identification reference frame identification. acknowledgements Karen schloss, Stephen palmer, Anna rafferty, David Whitney Computational Cognitive Science Lab Berkeley discussions AFOSR grant-9550-0232 support. When objects images same Although people recognize categorize objects successfully effortlessly, object recognition machine learning incredibly diﬃcult problem people success puzzle cognitive scientists. solve problem, object recognition techniques typically generate set features predefined procedure., SIFT descriptors] textons]) learn features., deep belief networks]) images. general goal methods extract features images identifying objects generated images transformations occurred producing., viewpoint changes). this strategy people typically perceive object transformed image., translations). however, transformations ignored: perceived identity objects depends orientation features respect scene., . differ orientation), objects paper, terminology scene, image, object. entire visual input observer scene. scene set images. image part visual input generated single object, ambiguous objects generate image. object item world generates image visual input. not. developing proper object recognition fully understanding people depends explaining people determine orientation objects respect scene. importance orientation object recognition leads question: objects project image viewing conditions., degree rotations), people infer object image? psychology, main ories people solve problem: invariant feature hyposis], essentially strategy current object recognition techniques (use features preserve object identity transformations generate images object), reference frame hyposis, posits objects embedded coordinate axes]. coordinate axes set orientation scale objects, identified objects. though produce image, coordinate axes. situations orientation image reference frame simply orientation retina; however, case rotate heads retinal image rotates) rotated object., person lying bench document rotated desk). thus, reference frame image ambiguous additional information. however, anor object scene orientation unambiguous (like), orientation ambiguous image inferred demonstrate people orientation images scene determine orientation ambiguous image participants solve arithmetic problems, operator image ambiguous numbers ﬂanking operator eir oriented upright rotated degrees. solution people adopt indicative reference frame inferred operator (multiplication implies upright reference frame addition implies diagonal reference frame). this experimental method explore reference frame inference wide range contexts. real life, typically view scenes multiple reference frames. for example, books bookshelf upright, books tilted diagonally (for support), books lie ﬂat. yet work investigating people infer number reference frames, orientations, images belong reference frame. solve problem, note image scene belongs single reference frame, reference frames form partition images scene (where block partition corresponds reference frame). using standard nonparametric Bayesian model partitions, formulate ideal observer model infer multiple reference frames parameters. model predicts people sensitive cues inferring reference \\x0cframes scene: proximity ambiguous image unambiguous ﬂanking images conﬂicting orientations, difference number objects aligned competing reference frames. confirm people sensitive cues method above. summary article follows. first, Section summarizes relevant psychological research orientation affects objects perceived ambiguous images. next, Section develops method online testing reference frame people infer image establishes eﬃcacy. section presents ideal observer model reference frame inference scenes multiple reference frames. model predicts ambiguous image proximity reference frames affect inferred reference frame Section confirms people act accordance prediction behavioral experiment. model predicts number aligned objects reference frame affect reference frame inferred ambiguous image. section confirms prediction behavioral experiment. section concludes paper highlights directions future research. Orientation psychological ories object representation Though perceived object images depend orientation (like), examples perceived object depend orientation], including. square. diamond, effects orientation object recognition]. this led psychologists people represent objects reference frame set coordinate axes Figure) shows reference frames predict image interpreted view ambiguity reference frame essentially strength intrinsic axes]. though coordinate axes properties., scale), focus orientation article. ) Figure Reference frames. ) ambiguity image resolved reference frames: horizontal orientation (solid axes) rotated degrees (dashed axes). ) images unambiguous, ) reference frame ambiguous objects inﬂuenced objects \\x0cunambiguous reference frames. ) group objects eir this establishes reference frame group. coordinate axes aligned document axes coordinate axes diagonal document axes. for objects rotationally invariant, object generates observed image identifiable orientation (see Figure)). dependence object perception orientation established norm demonstrated familiar objects, faces, handwriting objects]. central reference frame hyposis ability perceptual system infer reference frame image. reference frame consistent observed image, psychologists explored people infer reference frame image. though reference frame inference strongly inﬂuenced top-down axis retinal image axis gravity (given proprioceptive vestibular senses], scene inﬂuence inferred reference frame. objects grouped toger world tend affected transformation generate images., text poster poster rotated), inferred reference frame ambiguous image inﬂuenced orientations images surrounding. figures) phenomenological demonstrations alignment orientations objects scene bias inferred reference frame image reference frame ambiguous (and strong corroborating empirical evidence principle Figure) biased interpreted based surrounding context images Figure) interpreted eir tilted diﬃcult interpret ors tilted simultaneously]. thus, reference frame shared objects group. although wealth research reference frame inference scenes single reference frame, knowledge, research people determine reference frame ambiguously oriented images reference frame scene (and consistent images). before exploring cues inﬂuence human reference frame inference scenes multiple reference frames, develop method testing human reference frame inference. Testing reference frame inference arithmetic test factors inﬂuence reference frame people infer image, people solve arithmetic problem operation. people view response multiplication answer, reference frame aligned horizontal vertical axes page. alternatively, people view response addition answer, reference frame aligned axes diagonal page (and thus, relative reference frame, treated method previous techniques., explicitly image orientation recording frequency orientation chosen eir compatible conﬂicting tested hyposis]) due ability wide range contexts demonstrate robust importance reference slightly terminology previous work refer principle alignment rar symmetry avoid ambiguity word symmetry (which symmetry referring). Although ambiguous images, method works ambiguous images teaching participant addition orientation image multiplication. ) Axis Oriented) Diagonal Oriented Axis Oriented Diagonal Oriented Frequency Frequency Response Response Figure Effect orientations objects reference frame. ) aligned axes implies operator ) aligned diagonal implies operator diagonal orientation. ) Frequency answers) participants. most participants respond, solution product meaning reference frame aligned axes page. ) Frequency answers) participants. most participants respond, meaning reference frame aligned diagonals page. frame inference seemingly unrelated cognitive behavior (solving arithmetic problem). confirm validity reproducing previously found effect inﬂuence orientation images scene]. when reference frame image ambiguous, factor inﬂuences inferred reference frame orientation images grouped with, images identifiable orientation. thus, people solve arithmetic problem, operator paired numbers aligned top-down axes page (figure)), respond, result multiplication. alternatively, people solve problem numbers aligned diagonally, infer diagonal axes reference frame respond, result addition (figure)). test method, recruited participants online, answered arithmetic problem exchange small monetary reward. participants counterbalanced axis diagonally oriented conditions (figures) respectively) participants gave eir addition) multiplication) solution. changing orientation numbers, solutions arithmetic problems participants Figures) identical numbers identical operator image. figures) show responses groups participants answered arithmetic problem) differed predicted.208, yates? chisquare correction). thus, participants solve arithmetic problems effective method testing reference frame inference perceived orientations inﬂuence higher level cognition. Modeling reference frame inference Before describing model reference frame inference multiple reference frames, present probabilistic model scenes multiple images single reference frame.  Reference frame inference scenes reference frame assume vocabulary objects ahead time size rotations. each scene., Figure) scene) consists set images., images Figure)). for image scene, model visual properties spatial location (xi1 xi2 visual properties image generated unknown object rotated orientation scene reference frame.  binary image-object alignment matrix) encodes object-rotation pairs consistent observed image, image object rotated degrees consistent model assumes spatial locations images independent identically distributed draws Gaussian distribution shared parameters center point reference frame, spread objects center point. unobserved objects orientation reference frame drawn independent discrete distributions parameters prior objects reference frame orientations, respectively. generative model defines statistical model: iid—?  discrete(?)   discrete(?) —?,  gaussian (?,  model assumes types objects, rotations degrees), model captures sensitivity participants demonstration (figure). Figure), oriented degrees. , non-zero object produce image consistent \\x0cwith observed image implies operator consistent participant responses (figure)). when oriented degrees (figure, non-zero reason before. implies operator consistent participant responses (figure)).  Extending model scenes multiple reference frames Although model defined previous section succeeds inferring reference frame ambiguous image images grouped with, handle scenes multiple reference frames, scenes Figure extend model partitioning images scene reference frames, image scene belongs reference frame reference frame block partition. from perspective, inferring multiple reference frames scene images equivalent partitioning scene clustering images. with insight grouping images reference frames finding partition scene, extend model select reference frames scene (with unknown number reference frames). first, generate partition images scene Chinese restaurant process (crp] parameter exchangeable distribution partitions. CRP defined sequential construction   current number reference frames number objects assigned reference frame denotes reference frame object assigned assigned reference frame previous objects increments initialize, object starts reference frame). this assignment vector denotes reference frame image each block partition (reference frame) rotation embedded spatial layout scene center position spread (each generated gaussian-inverse Wishart distribution shared parameters). thus, defined generative model set images scene: iid  —?  crp(?)  giw iid iid  discrete(?)   discrete(?)  rci rci Gaussian GIW signifies gaussian-inverse-wishart distribution, hyperparameters model. Gibbs sampling inference], cluster assignments image updated parameters cluster begin assigning image reference frame iterating. for observed image, resample set existing clusters newly drawn clusters. after values resampled, discard empty clusters update parameters remaining clusters drawing posterior distribution objects assigned reference frame set images locations reference frame  Predictions human reference frame inference What factors inﬂuence reference frame assigned ambiguous image ideal observer model? two factors predicts inﬂuence image inferred reference frame Operator Position Operator Position Operator Position Operator Position Figure Trials Experiment showing positions operators main factor experiment. factors randomized trials numbers problem (always single digits), numbers rotated, diagonal numbers operator aligned (positive diagonal shown figure, numbers operator aligned negative diagonal well), rotation operator. ) Participant responses proximity experiment Percent grouped left Percent grouped left Model predictions proximity experiment Operator Position Operator Position Figure Proximity effects) Human results) Model results. closer operator left number, left number orientation. proximity close image unambiguous \\x0cimages images reference frame coupled spatial location) alignment difference number images assigned reference frame. general paradigm test predictions operator ﬂanked number orientations side (see examples Figure). clear numbers reference frame, ambiguous reference frame operator assigned. compare factors inﬂuences reference frames inferred scene people model behavioral experiments. Experiment Proximity effects reference frame inference When reference frame image ambiguous conﬂicting neighboring reference frames, model predicts proximity distance ambiguous image conﬂicting reference frames affect reference frame adopted ambiguous image. explore question method presented above, participants asked solve arithmetic problem operator ambiguous numbers conﬂicting reference frames (orientations). this deduce reference frame inferred operator image answer participants. manipulate proximity changing location operator closer numbers shown Figure  Methods total 134 participants completed experiment online Amazon Mechanical Turk exchange usd. four participants give correct solution arithmetic problem (neir addition multiplication solution) leaving 130 participants analysis. participants asked maximize window answering arithmetic problem. all factors manipulated subjects preliminary testing demonstrated strong effect trial order selected reference frame (probably reference frames rarely change world). primary factor interest experiment position operator scored (far left) (far right), counterbalanced participants (without position). problem viewed simulated aperture minimize effect monitor reference frame). see Figure trials operator position. factors randomized participants: numbers problem (randomly chosen single digit numbers), number rotated (left right), diagonal numbers operator aligned (positive diagonal, shown Figure negative diagonal), rotation operator ?).  Results Discussion Figure) shows participants infer orientation left number operator closer left number. results confirm hyposis: closer operator image unambiguous reference frame, participants infer \\x0creference frame operator). probit regression analysis corroborates result regression coeﬃcient significantly). model results generated Gibbs sampling previously described) shown Figure). for trial, ran sampler burn iterations, recorded 750 samples, thinned samples selecting samples. this left 150 samples formed estimate proportion times operator grouped left reference frame. parameters initialized: .001, [264, 261], 1000i (scenes 550?550 pixels bottom-left corner origin), identity matrix, 110. discrete distributions encoding priors objects orientations, uniform possibilities. model human results exhibit qualitative behavior: distance operator left number decreased, probability operator orientation left number increased. Experiment Alignment effects reference frame inference Our model predicts difference number unambiguous images assigned conﬂicting reference frames affect reference frame adopted operator image. experiment, test prediction method above, manipulate number extra oriented unambiguous objects competing reference frames (see Figure)).  Methods total people participated online Amazon Mechanical Turk exchange usd. participants gave incorrect answer, leaving participants analysis. instructions design identical previous experiment, extra factors manipulating context left number left vice versa) operator positions). figure) illustrates trials context manipulations operator position.  Results Discussion Figure) shows participants infer operator orientation orientation whichever side objects closer, replicating effect Experiment.8728.0005 model results generated procedure parameter values Experiment (except account increased number objects) Figure) shows similarity participant results. Conclusions future directions paper, introduced study people infer reference frame images scenes multiple reference frames. presented implicit method testing reference frame inference, ideal observer model predicts people sensitive scene cues) Percent grouped left) 5l1r Alignment effects participant responses 5l1r 1l5r) 1l5r Percent grouped left Operator Position Alignment effects model responses 5l1r 1l5r Operator Position Figure Alignment effects. operator orientation side objects. 5l1r denotes objects left reference frame object right, 1l5r opposite arrangement. ) Example stimuli) Human results) Model results. behavioral evidence supporting predictions. because objects people perceive depend orientation images scene, results improve understanding configuration objects scenes affects object perception. plan extend model capture cues identified perceptual psychologists. step include bias-down axis input image] nonuniform distribution rotations (estimating ?). capture elongation cue (that orientation spread images scene biases orientation reference frame images scene]) coupling covariance matrix (?) rotation) reference frame. currently, model assumes positions images reference frame Gaussian distributed; however, people strong expectations arrangement images scene]. plan compare people bias sophisticated scene segmentation model]. interested cues depend structure images orientation agent world, axes symmetry] gravitational axes]. anor direction future work address assumption model: How people learn set objects wher objects orientation-invariant? potential solution combine model previous work presented nonparametric Bayesian model learning features transformations allowed undergo]. hopefully, incorporating model feature learning method yield inferred features and, turn, create feature generation object recognition techniques providing understanding people perceive objects ambiguous image data. finally, plan explore presented principles scale realistic scenes objects complex orientations. our paradigm principled starting point investigating reference frames identified scenes multiple reference frames. easily extended complex scenes associating orientations rotations depth) ambiguous image arithmetic operators. our hope leads understanding object identification reference frame identification. acknowledgements Karen schloss, Stephen palmer, Anna rafferty, David Whitney Computational Cognitive Science Lab Berkeley discussions AFOSR grant-9550-0232 support.',\n",
       " 'PP4387': 'this study considers wher recurrent networks spiking neurons generative model stationary patterns temporal sequences. precisely, derive model learns adapt spontaneously spike sequences conform closely empirical distribution actual spike sequences caused inputs impinging sensory layer network. generative model model joint distribution percepts hidden world. world complex temporal relationships, model recognize predict temporal patterns. behavioural studies]) support assumption brain performing approximate Bayesian inference. recently, evidence hyposis found electro-physiological work]. abstract Bayesian models proposed account phenomenon]. however, remains open question wher optimization abstract Bayesian models translated plausible learning rules synapses networks spiking neurons. paper, show derivation spike-based plasticity rules statistical learning principles yields learning dynamics generative spiking network model \\x0care akin ?????? ???????? figure network spiking neurons, divided observed latent pools neurons. spike-time Dependent Plasticity (stdp]. learning rule derived variational optimization process. typically, optimization recurrent Bayesian networks involves forward backward propagation steps. propose plasticity rule approximates backward steps introduction delayed updates synaptic weights dynamics. ory supported simulations demonstrate learning mechanism capture hidden observed spiking patterns. spike Response Model (srm], spikes generated stochastically depending neuronal membrane potential. srm generalized linear model (glm). closely related integrate-and-fire model, successfully explain neuronal spike trains]. model, membrane potential neuron time expressed)  ) bias represents constant external input neuron) spike train jth neuron defined tfj {t1j   set spike timings. diagonal elements synaptic matrix fixed negative, implements reset membrane potential spike simple account neuronal refractoriness]. time constant 10ms]. spike generation process stochastic time-dependent firing intensity) depends membrane potential) exp)) ) exponential dependence firing intensity membrane potential agrees experimental results]. set equations) captures simplified dynamics spiking neuron stochastic spike timing. sections, introduce oretical framework approximations paper. basic learning mechanism introduced derived, simulation illustrating proposed learning rule learn spatio-temporal features input spike trains reproduce spontaneous activity. principled Framework network consisting distinct sets neurons, observed neurons called visible neurons latent neurons called hidden), illustrated Figure activities observed neurons represent quantity interest modelled, latent neurons fulfill mediating role representing hidden observed spike train. learning context neuronal network consists changing synaptic \\x0cstrengths neurons. postulate underlying principle learning relies learning distributions spike trains evoked eir sensory inputs complicated sequences cognitive events. statistics, learning distributions involves minimizing measure distance model (that, neuronal network) target distribution. observations). principled measure distance distributions pempirical kullback-leibler divergence] defined pempirical(pempirical) DXpempirical) log ) individual represent entire spike trains. measure inte gration spike trains. learning mechanism minimize divergence distribution defined network) observed spike timings distribution pempirical evoked unknown external process. note minimizing divergence entails maximizing likelihood observed spike trains generated model. order derive learning dynamics model section, evaluate gradient likelihood) respect free parameters model. synaptic eﬃcacies biases joint likelihood spike train observed latent neurons neuronal model written] log [log   ) i2v Since neuronal network including latent units (that, neurons receiving external inputs), actual observation likelihood effective quantity obtained integrating latent spike trains DXH ) gradient) expectation conditioned observed neurons? history: log log DXH) log DXf). diﬃcult evaluate conditions entire latent spike train entire observed spike train. words, posterior distribution spiketimings latent neurons depends past future observed neurons? spike train.  Weak Coupling Approximation order render model tractable, introduce approximation dynamics based weak coupling approximation], amounts replacing)  ) Gaussian process inverse variance) ) intrinsic noise added regularize simulations assume). note) function network state synaptic eﬃcacies. network model defines joint distribution observed input spike trains membrane potentials) log) exp? ) i2v \\x0ci2v terms depending model parameters latent states dropped contribute gradients interested) drift Gaussian process membrane potentials read equation). ) variance due external input obtained noting) exp/?  exp)/? ))/?  thus, weak coupling regime exp)/? ))/? ) weak coupling approximation amounts replacing spikes latent neurons intensities Gaussian noise. note approximated model, latent variables longer latent spike trains, membrane potentials. however, emphasize end intensities substituted spikes below.  Variational Approximation Posterior Membrane Potential variational approach statistics method approximate complex distribution family simpler distributions variational methods applied spiking neural networks contexts, connectivity external source inference]. following, interpret neural activity plasticity toger approximate form variational learning. approximate posterior Gaussian process) log? ) variational parameters representing drift ith membrane potential time posterior process normalization constant. note parameters) posterior process network dynamics noise). order finite-divergence prior posterior processes]. finding good approximation variational parameters) amounts minimizing quantity) exp))] i2v i2v? )) i2v? ) Although) written analytically terms instantaneous covariance posterior process, adopt simpler mean-field approximation.  (hui). write hui) dshi) plays role ?drift? derivative note  ) Heaviside step function. result-divergence  )? ) exp(? ))] i2v) drifts) variational approximation updated gradient descent) exp(? ))] k2v )) )? ) Figure Posterior firing intensity simple networks) network) From top neurons, simulated field approximation. bottom: observed spike train, firing intensity latent neurons posterior inverse variance. green neuron direct connection observed neuron, stronger modulation firing rate latent neurons. ) network pools neurons, observed latent pools. ) Simulation results. top bottom: observed spike trains, spike trains latent pool firing intensities latent neurons realizations network. rate latent pool increases spikes observed neurons. note spiking implementation model rates mamatical rate model. key points note). first, absence observations, approximating) simply), posterior prior processes equal. second, first, fourth terms) backward terms, correspond corrections ?belief? past states generated inputs. implies order estimate drift) posterior membrane potential neuron time observations time third, fourth term equation) contribution gradient fact inverse variance) defined equation) function network state. important feature model, implies amount noise dynamics adapted explain observed spike trains.  Towards spike-time Dependent Plasticity learn parameters network, synaptic weights neural ?biases? gradient descent learning rate )  )    )  computation note posterior drift) known, purely locally. long ?backward window? would, course, biologically implausible. however-line approximations backward terms provide reasonable approximation taking small backwards filters 50ms. mechanistically, applications operate small delay, required calculate backwards correction term. biology delays exist, weights switched time stimulation induces change] More precisely, small backward window amounts approximating gradient posterior drift) cutting time integrals finite time horizon., equation) replace integral size ?backward window? approximate gradient. expression) written delayed update equation) exp(? ))] k2v) resulting update variable learning equation. simulation shown Figure conceptual illustration posterior firing intensity) propagates information backward observed latent neurons, process essential learning temporal patterns. note firing rate presynaptic neuron information directly site synapse access spike arrivals (but underlying firing rate). however, spike arrivals provide reasonable estimate rate. Figure show simulation network pools spiking neurons updates based spike times (rar rates) qualitatively information rate formula derived above. equations) refore replace pre-synaptic firing intensity) temporally filtered spike trains constitute good approximation).  STDP Window From learning equation synaptic weightr), extract stdp-like learning window rewriting plasticity rules) ) expected change time posterior. before, replace firing intensity trial spikes. assuming spike observed neuron evaluated) plot weight change occur latent neuron fires. equation). show resulting spike-time Dependent Plasticity simple network neurons Figure note shape) remarkably reminiscent experimentally found measurements STDP]. particular, shape STDP curve depends type neuron connections excitatory excitatory excitatory inhibitory inhibitory inhibitory neurons (figure). simulations order demonstrate method ability capture stationary temporal patterns, performed simulations tasks. involves formation temporal chain, involves stationary pattern generator. simulations discretetime (euler method) version equations) 1ms. backward window size 50ms, learning rate used.                                                     figure spike-time Dependent Plasticity simple network composed neurons. weight change) (vertical axis) function spike timing neuron top latent neuron), bottom (observed) neuron produces spike (horizontal axis). shown permutations excitatory) inhibitory) neuron types, left learning windows network downward upward synapses, respectively. task consisted learning periodic chain, pools observed neurons successively activated shown Figure. time lag introduced pattern force network form temporal hidden representations capable capturing time dependencies obvious observable instantaneous clues blank moment, network pattern actively latent neurons. learning, spontaneously patterns observable neurons developed clear resemblance patterns provided training, slightly larger amount noise present, shown Figure. noise level model network reduced, noise-free ?cleared concept? observed patterns generated (figure) demonstrates recurrent network learned task. learning configured network sequence task understood study connectivity pattern latent neurons. latent neuron active sequence (figure). reordered labels neurons structure connectivity matrix visble. subsets latent neurons active ?subpatterns? sequence task, latent neurons active observable units quiescent (figure). lateral connectivity latent neurons asymmetry forward direction chain. task aimed learning randomly generate statinonary patterns 10ms. successfull learning task requires learning stationary patterns stochastic transitions figure shows results task. discussion Some models recently proposed stdp-like learning rules derive ?first principles? ]). however, models eir diﬃculty dealing recurrent latent dynamics, account non-factorial latent representations. work, proposed plausible derivation synaptic plasticity network consisting spiking neurons, capture time dependencies observed spike trains process combinatorial features. generative model comprising latent observed neurons, mechanism utilizes implicit (that, short-term delayed) backward iterations arise naturally variational inference. plasticity mechanism emerges closely resembles familiar STDP mechanism found experimental studies. simulations show plasticity rules capable learning temporal stationary pattern generator. future work attempt furr elucidate biological plausibility approach, connection spike-time Dependent plasticity. acknowledgments Support provided SNF grant (crsik0 122697), \\x0cerc grant (268689) SystemsX IPhD grant. figure Simulation results. sequence task, 20ms-periodic sequence network observed neurons latent neurons% inhibitory neurons (chosen randomly). connections observed neurons set order illustrate latent-latent recurrent connections. ) sample periodic input pattern. note long waiting time sequence wait    ) Simulations network 20ms clamped data. ) Latent neurons sample. ) Sample simulation network parameters noise, order show underlying dynamics. achieved transformation) ) random jump task: learning produce patterns (4ms long) 10ms. ) sample input pattern) One realization network 20ms clamped data. ) Sample latent pattern. ) Sample simulation network parameters noise. note decreasing level noise impairment performance task. ) learned synaptic matrix task; latent neurons-ordered order show role latentto-latent synapses dynamics role latent-observed synapses represent pattern features. This study considers wher recurrent networks spiking neurons generative model stationary patterns temporal sequences. more precisely, derive model learns adapt spontaneously spike sequences conform closely empirical distribution actual spike sequences caused inputs impinging sensory layer network. generative model model joint distribution percepts hidden world. since world complex temporal relationships, model recognize predict temporal patterns. behavioural studies]) support assumption brain performing approximate Bayesian inference. more recently, evidence hyposis found electro-physiological work]. various abstract Bayesian models proposed account phenomenon]. however, remains open question wher optimization abstract Bayesian models translated plausible learning rules synapses networks spiking neurons. paper, show derivation spike-based plasticity rules statistical learning principles yields learning dynamics generative spiking network model \\x0care akin ?????? ???????? figure network spiking neurons, divided observed latent pools neurons. spike-time Dependent Plasticity (stdp]. our learning rule derived variational optimization process. typically, optimization recurrent Bayesian networks involves forward backward propagation steps. propose plasticity rule approximates backward steps introduction delayed updates synaptic weights dynamics. ory supported simulations demonstrate learning mechanism capture hidden observed spiking patterns. Spike Response Model (srm], spikes generated stochastically depending neuronal membrane potential. SRM generalized linear model (glm). closely related integrate-and-fire model, successfully explain neuronal spike trains]. model, membrane potential neuron time expressed)  ) bias represents constant external input neuron) spike train jth neuron defined tfj {t1j   set spike timings. diagonal elements synaptic matrix fixed negative, implements reset membrane potential spike simple account neuronal refractoriness]. time constant 10ms]. spike generation process stochastic time-dependent firing intensity) depends membrane potential) exp)) ) exponential dependence firing intensity membrane potential agrees experimental results]. set equations) captures simplified dynamics spiking neuron stochastic spike timing. sections, introduce oretical framework approximations paper. basic learning mechanism introduced derived, simulation illustrating proposed learning rule learn spatio-temporal features input spike trains reproduce spontaneous activity. Principled Framework network consisting distinct sets neurons, observed neurons called visible neurons latent neurons called hidden), illustrated Figure activities observed neurons represent quantity interest modelled, latent neurons fulfill mediating role representing hidden observed spike train. learning context neuronal network consists changing synaptic \\x0cstrengths neurons. postulate underlying principle learning relies learning distributions spike trains evoked eir sensory inputs complicated sequences cognitive events. statistics, learning distributions involves minimizing measure distance model (that, neuronal network) target distribution. observations). principled measure distance distributions pempirical kullback-leibler divergence] defined pempirical(pempirical) DXpempirical) log ) individual represent entire spike trains. measure inte gration spike trains. our learning mechanism minimize divergence distribution defined network) observed spike timings distribution pempirical evoked unknown external process. note minimizing divergence entails maximizing likelihood observed spike trains generated model. order derive learning dynamics model section, evaluate gradient likelihood) respect free parameters model. synaptic eﬃcacies biases joint likelihood spike train observed latent neurons neuronal model written] log [log   ) i2v Since neuronal network including latent units (that, neurons receiving external inputs), actual observation likelihood effective quantity obtained integrating latent spike trains DXH ) gradient) expectation conditioned observed neurons? history: log log DXH) log DXf). this diﬃcult evaluate conditions entire latent spike train entire observed spike train. words, posterior distribution spiketimings latent neurons depends past future observed neurons? spike train.  Weak Coupling Approximation order render model tractable, introduce approximation dynamics based weak coupling approximation], amounts replacing)  ) Gaussian process inverse variance) ) intrinsic noise added regularize simulations assume). note) function network state synaptic eﬃcacies. our network model defines joint distribution observed input spike trains membrane potentials) log) exp? ) i2v \\x0ci2v terms depending model parameters latent states dropped contribute gradients interested) drift Gaussian process membrane potentials read equation). ) variance due external input obtained noting) exp/?  exp)/? ))/?  thus, weak coupling regime exp)/? ))/? ) weak coupling approximation amounts replacing spikes latent neurons intensities Gaussian noise. note approximated model, latent variables longer latent spike trains, membrane potentials. however, emphasize end intensities substituted spikes below.  Variational Approximation Posterior Membrane Potential variational approach statistics method approximate complex distribution family simpler distributions variational methods applied spiking neural networks contexts, connectivity external source inference]. following, interpret neural activity plasticity toger approximate form variational learning. approximate posterior Gaussian process) log? ) variational parameters representing drift ith membrane potential time posterior process normalization constant. note parameters) posterior process network dynamics noise). this order finite-divergence prior posterior processes]. finding good approximation variational parameters) amounts minimizing quantity) exp))] i2v i2v? )) i2v? ) Although) written analytically terms instantaneous covariance posterior process, adopt simpler mean-field approximation.  (hui). write hui) dshi) plays role ?drift? derivative note  ) Heaviside step function. result-divergence  )? ) exp(? ))] i2v) drifts) variational approximation updated gradient descent) exp(? ))] k2v )) )? ) Figure Posterior firing intensity simple networks) network) From top neurons, simulated field approximation. bottom: observed spike train, firing intensity latent neurons posterior inverse variance. green neuron direct connection observed neuron, stronger modulation firing rate latent neurons. ) network pools neurons, observed latent pools. ) Simulation results. from top bottom: observed spike trains, spike trains latent pool firing intensities latent neurons realizations network. rate latent pool increases spikes observed neurons. note spiking implementation model rates mamatical rate model. key points note). first, absence observations, approximating) simply), posterior prior processes equal. second, first, fourth terms) backward terms, correspond corrections ?belief? past states generated inputs. this implies order estimate drift) posterior membrane potential neuron time observations time third, fourth term equation) contribution gradient fact inverse variance) defined equation) function network state. this important feature model, implies amount noise dynamics adapted explain observed spike trains.  Towards spike-time Dependent Plasticity learn parameters network, synaptic weights neural ?biases? gradient descent learning rate )  )    )  computation note posterior drift) known, purely locally. long ?backward window? would, course, biologically implausible. however-line approximations backward terms provide reasonable approximation taking small backwards filters 50ms. mechanistically, applications operate small delay, required calculate backwards correction term. biology delays exist, weights switched time stimulation induces change] More precisely, small backward window amounts approximating gradient posterior drift) cutting time integrals finite time horizon., equation) replace integral size ?backward window? approximate gradient. expression) written delayed update equation) exp(? ))] k2v) resulting update variable learning equation. simulation shown Figure conceptual illustration posterior firing intensity) propagates information backward observed latent neurons, process essential learning temporal patterns. note firing rate presynaptic neuron information directly site synapse access spike arrivals (but underlying firing rate). however, spike arrivals provide reasonable estimate rate. indeed Figure show simulation network pools spiking neurons updates based spike times (rar rates) qualitatively information rate formula derived above. equations) refore replace pre-synaptic firing intensity) temporally filtered spike trains constitute good approximation).  STDP Window From learning equation synaptic weightr), extract stdp-like learning window rewriting plasticity rules) ) expected change time posterior. before, replace firing intensity trial spikes. assuming spike observed neuron evaluated) plot weight change occur latent neuron fires. equation). show resulting spike-time Dependent Plasticity simple network neurons Figure note shape) remarkably reminiscent experimentally found measurements STDP]. particular, shape STDP curve depends type neuron connections excitatory excitatory excitatory inhibitory inhibitory inhibitory neurons (figure). Simulations order demonstrate method ability capture stationary temporal patterns, performed simulations tasks. involves formation temporal chain, involves stationary pattern generator. both simulations discretetime (euler method) version equations) 1ms. backward window size 50ms, learning rate used.        ??                                             figure spike-time Dependent Plasticity simple network composed neurons. weight change) (vertical axis) function spike timing neuron top latent neuron), bottom (observed) neuron produces spike (horizontal axis). shown permutations excitatory) inhibitory) neuron types, left learning windows network downward upward synapses, respectively. task consisted learning periodic chain, pools observed neurons successively activated shown Figure. time lag introduced pattern force network form temporal hidden representations capable capturing time dependencies obvious observable instantaneous clues blank moment, network pattern actively latent neurons. after learning, spontaneously patterns observable neurons developed clear resemblance patterns provided training, slightly larger amount noise present, shown Figure. noise level model network reduced, noise-free ?cleared concept? observed patterns generated (figure) demonstrates recurrent network learned task. learning configured network sequence task understood study connectivity pattern latent neurons. latent neuron active sequence (figure). reordered labels neurons structure connectivity matrix visble. subsets latent neurons active ?subpatterns? sequence task, latent neurons active observable units quiescent (figure). lateral connectivity latent neurons asymmetry forward direction chain. task aimed learning randomly generate statinonary patterns 10ms. successfull learning task requires learning stationary patterns stochastic transitions figure shows results task. Discussion Some models recently proposed stdp-like learning rules derive ?first principles? ]). however, models eir diﬃculty dealing recurrent latent dynamics, account non-factorial latent representations. work, proposed plausible derivation synaptic plasticity network consisting spiking neurons, capture time dependencies observed spike trains process combinatorial features. using generative model comprising latent observed neurons, mechanism utilizes implicit (that, short-term delayed) backward iterations arise naturally variational inference. plasticity mechanism emerges closely resembles familiar STDP mechanism found experimental studies. simulations show plasticity rules capable learning temporal stationary pattern generator. future work attempt furr elucidate biological plausibility approach, connection spike-time Dependent plasticity. acknowledgments Support provided SNF grant (crsik0 122697), \\x0cerc grant (268689) SystemsX IPhD grant. Figure Simulation results. sequence task, 20ms-periodic sequence network observed neurons latent neurons% inhibitory neurons (chosen randomly). connections observed neurons set order illustrate latent-latent recurrent connections. ) sample periodic input pattern. note long waiting time sequence wait    ) Simulations network 20ms clamped data. ) Latent neurons sample. ) Sample simulation network parameters noise, order show underlying dynamics. this achieved transformation) ) random jump task: learning produce patterns (4ms long) 10ms. ) sample input pattern) One realization network 20ms clamped data. ) Sample latent pattern. ) Sample simulation network parameters noise. note decreasing level noise impairment performance task. ) learned synaptic matrix task; latent neurons-ordered order show role latentto-latent synapses dynamics role latent-observed synapses represent pattern features.',\n",
       " 'PP4414': 'kernel methods long provided powerful tools generalizing linear statistical approaches nonlinear settings, embedding sample high dimensional feature space, reproducing kernel Hilbert space (rkhs]. product feature mappings computed explicitly, positive definite kernel function, permits eﬃcient computation deal explicitly feature representation. recently, RKHS feature map represent probability distributions, rar mapping single points: refer representations probability distributions kernel means. choice kernel, feature mapping rich expectation uniquely identifies distribution: RKHSs termed characteristic]. kernel means characteristic RKHSs applied successfully number statistical tasks, including sample problem], independence tests], conditional independence tests]. advantage kernel approach tests apply immediately domain kernels defined. propose general nonparametric framework Bayesian inference, expressed terms kernel means. goal Bayesian inference find posterior observation)? )? ) ) density function prior, conditional density likelihood framework, posterior, prior, likelihood expressed kernel means: update prior posterior called Kernel bayes? rule (kbr). implement kbr, kernel means learned nonparametrically training data: prior likelihood means expressed terms samples prior joint probabilities, posterior kernel weighted sample. resulting updates straightforward matrix operations. leads main advantage KBR approach: absence specific parametric model analytic form prior likelihood densities, perform Bayesian inference making suﬃcient observations system. alternatively, parametric model, complex require time-consuming sampling techniques inference. contrast, KBR simple implement, amenable well-established approximation techniques yield computational cost linear training sample size]. furr establish rate consistency estimated posterior kernel true posterior, function training sample size. proposed kernel realization bayes? rule extension approach] state-space models. earlier work applies heuristic, however, kernel previous hidden state observation assumed combine additively update hidden state estimate. recently, method belief propagation kernel means proposed]: unlike present work, directly estimates conditional densities assuming prior uniform. alternative kernel means nonparametric density estimates. classical approaches include finite distribution estimates partitioned domain kernel density estimation, perform poorly high dimensional data. alternatively, direct estimates density ratio estimating conditional. ]. contrast density estimation approaches, KBR makes easy compute posterior expectations RKHS product) perform conditioning marginalization, requiring numerical integration.  Kernel expression bayes? rule Positive definite kernel probabilities begin review basic concepts tools statistics RKHS]. Panset-valued) positive definite kernel symmetric kernel ???   arbitrary points  ] positive definite kernel uniquely defines Hilbert space (rkhs) consisting functions)   (reproducing property).  real numbers     , measure spaces, random variable probability paper, assumed positive definite kernels measurable spaces measurable bounded, boundedness defined supx?? , positive definite kernel measurable space RKHS kernel defined -valued random variable)dpx). ) For notational simplicity, dependence shown. kernel depends distribution (and kernel), written mpx whichever equivalent notations clearest context. reproducing property  ) Let positive definite kernels respective RKHS (uncentered) covariance operator defined relation   , ihy  ihy noted identified tensor product space product kernel]. identification standard: tensor product isomorphic space linear maps correspondence     ]. define CXX hf2 CXX)]  introduce notion characteristic rkhs, essential kernels manipulate probability measures. bounded measurable positive definite kernel called characteristic)] (?, implies probabilities uniquely determined kernel means]. property, problems statistical inference cast terms inference kernel means. widely characteristic kernel Gaussian kernel, exp yk2? )). empirical estimates kernel covariance operator straightforward obtain. . sample    law empirical kernel covariance operator) (?, (?, ) written tensor product form. -consistent norm.  Kernel bayes? rule derive kernel implementation bayes? rule.  prior distribution. ). following, denote probabilities. )? ), respectively. goal obtain estimator kernel posterior mqx). orem fundamental manipulating conditional probabilities positive definite kernels. orem]).   holds cxx cxy CXX injective, relation expressed cxx CXY Using. ), obtain expression kernel orem]). assume CXX injective, mqy kernel means respectively.   (cxx   mqy CXX  ) discussed], operator CXX implements forward filtering prior conditional density. ). note, however, assumptions  injectivity CXX \\x0chold general; easily provide counterexamples. following, noneless derive population expression bayes? rule strong assumptions, prototype empirical estimator expressed terms Gram matrices, prove consistency subject smoothness conditions distributions. deriving kernel realization bayes? rule, orem obtain kernel representation joint probability CXX    ) equation covariance operator .  ), ? ? point measure applications Bayesian inference, probability conditioned computed. plugging point measure . ), population expression CXX] kernel conditional probability). , random variable law replacing. ), obtain CZW). kernel posterior obtain. step derive covariance operators. ). recalling  identified covariance operator CZW   . ) obtain operators. ), kernel expression bayes? rule. argument rigorously implemented empirical estimates kernel means covariances.   . sample law assume consistent estimator  (?) (?,     sample defines estimator (which generated weights. negative values allowed empirical estimators CZW identified respectively. . ), (?) (?)    CXX CXX identity coeﬃcient Tikhonov regularization operator inversion. propositions express estimators Gram matrices. proofs simple matrix manipulation shown Supplementary material. following, denote Gram matrices )), respectively.  input sample express  weighted sample express kernel prior  (iii) regularization constants. computation:  compute Gram matrices )), vector    compute  compute diag ?). output: matrix conditioning kernel posterior) estimated weighted sample figure Kernel bayes? rule Algorithm bzw Proposition gram matrix expressions bzw  (?, (?, (?,  respectively, common coeﬃcient      ) prop. implies probabilities estimated weighted samples   respectively, common weights. weights negative, anor type Tikhonov regularization computing. ). bzw) Proposition  gram matrix expression ktx diag diagonal matrix elements .  (?,     (?,     ) call eqs. ) kernel bayes? rule (kbr., expression bayes? rule terms kernel means. algorithm implement KBR summarized fig.  aim estimate, expectation function respect posterior, based. ) estimator, ihx     weighted sample represent posterior, KBR similarity Monte Carlo methods importance sampling sequential Monte Carlo]). kbr method, however, generate samples posterior, updates weights sample matrix operations. provide experimental comparisons KBR sampling methods sec. .  Consistency KBR estimator demonstrate consistency KBR estimator. ). show rate derived assumptions, leave detailed discussions proofs Supplementary material. assume sample size prior infinity sample size likelihood infinity,  -consistent. oretical results, assume Hilbert spaces separable. following) denotes range orem  , random vector law  . )? ), estimator    khx??     . assume (cxx.         ) ?    ) estimator. ). ) condition (cxx requires prior smooth.  direct empirical kernel. sample size typically  orem implies -consistency. slow rate, practice convergence faster oretical guarantee. bayesian inference Kernel bayes? rule Bayesian inference, tasks interest include finding properties posterior (map value, moments), computing expectation function posterior. demonstrate kernel obtained KBR solving problems. first, orem obtain consistent estimator posterior expectation  covers wide class functions characteristic kernels (see experiments sec. ). next, point estimate] proposes preimage arg minx kkx (?, ktx)k2hx represents posterior effectively point. approach present paper point estimates considered. case Gaussian kernel, fixed point method sequentially optimize]. kbr prior likelihood expressed terms samples. unlike methods Bayesian inference, exact knowledge densities needed, samples obtained. typical situations KBR approach advantageous: relation variables diﬃcult realize simple parametric model, obtain samples variables. nonparametric state-space model sec. ).   prior and likelihood hard obtain explicitly, sampling possible) population genetics, branching processes likelihood model split species, explicit density hard obtain. approximate Bayesian Computation (abc) popular sampling method situations]. ) nonparametric Bayesian inference. ]), prior typically form process density. kbr approach give alternative ways Bayesian computation problems. show experimental comparisons KBR approach ABC sec. . standard sampling method MCMC sequential applicable, computation time consuming, real-time applications feasible. kbr, expectation posterior obtained simply product.  computed. kbr \\x0capproach noneless weakness common nonparametric methods: data point appears training sample, reliability output low. thus, suﬃcient diversity training sample reliably estimate posterior. kbr computation, Gram matrix inversion necessary, cost sample size attempted directly. substantial cost reductions achieved low rank matrix approximations incomplete Cholesky decomposition], approximates Gram matrix form  matrix computing costs(nr2 Woodbury identity, KBR approximately computed cost(nr2 kernel choice model selection key effectiveness kbr, kernel methods. kbr involves model parameters: kernel parameters), regularization parameters strategy parameter selection depends posterior inference problem. applied supervised setting, standard cross-validation). general approach requires constructing related supervised problem. suppose prior marginal posterior density) averaged equal marginal density compare discrepancy kernel leads application-fold approach. average estimators] namely, partition, disjoint subsets kernel] data posterior estimated data   prior (?,  application nonparametric state-space model. state-space model,  observable hidden state. assume conditional probabilities explicitly, estimate simple parametric models. rar, assume sample    observable hidden variables training phase. problem considered], give principled approach based kbr. conditional probability transition observation process) represented covariance (?,  operators computed training sample; bxy (?, (?, bxx defined similarly. note data., consistency achieved mixing property Markov model. simplicity, focus filtering problem, smoothing prediction similarly. filtering, estimate current hidden state observations  sequential estimate     derived KBR give sketch below; Supplementary material detailed derivation). suppose estimator kernel       form)     coeﬃcients time applying orem twice) (?, kernel   estimated   )  ) ?transfer? matrix defined notation) ) diag) ,...,?  ) kernel bayes? rule yields)    ) ) eqs.   ) describe update rule    contrast], estimates previous hidden state observation assumed combine additively, derivation based applying kbr. sequential filtering, substantial reduction computational cost achieved low rank approximations matrices training phase: rank computation costs step filtering. bayesian computation likelihood. likelihood and prior obtained analytic form sampling possible, ABC approach] popular Bayesian computation. abc rejection method generates sample follows) generate prior) generate, accept orwise reject). step), distance tolerance acceptance. situation above, KBR approach method) generate prior) generate sample ), (iii) compute Gram matrices   ). distribution sample ABC approaches true posterior  empirical posterior estimate KBR converges true  computational eﬃciency abc, however, arbitrarily low small rarely accepted Step). finally, ABC generates sample, statistic posterior approximated. case kbr, statistics posterior (such confidence intervals) harder obtain, consistency guaranteed expectations RKHS functions. sec. , provide experimental comparisons addressing trade-off computational time accuracy ABC kbr.      \\x0cexperiments Nonparametric inference posterior First compare KBR standard kernel density estimation (kde). . sample  . ) conditional. ) estimated KhX )hhy  KhX  khx HhY . sample prior posterior) represented weighted sample  importance weight). compare estimates obtained KBR KDE, Gaussian kernels methods. note Gaussian kernel, function) belong consistency KBR method rigorously guaranteed. orem). gaussian kernels, however, approximate continuous function compact subset arbitrary accuracy]. expect posterior estimated effectively. ave. MSE runs) KBR kde]) experiments, dimensionality KBR) ranging form. distribution KBR (med dist) kde) kde (best, randomly generated run. prior , VXX), VXX-component sample sizes 200. bandwidth parameter KDE set chosen ways, square cross-validation] performance, set    }. kbr, methods choose devi2 Dimension ation parameter Gaussian kernel: median pairwise distances data-fold Figure KBR. kde. sec.  fig. shows MSE estimates 1000 random points , accuracy methods decrease larger dimensionality, KBR significantly outperforms kde. bayesian computation likelihood compare KBR ABC terms estimation accuracy computational time. compute estimation accuracy rigorously, Gaussian distributions true prior likelihood. samples model sec.  evaluated points performed runs covariance. cpu time Error dim.) . Square Errors KBR ABC 200 600 400 800 1000 2000 For abc, rejection method; CPU time (sec) advanced sampling schemes], implementation straightforward. parameters Figure Estimation accuracy comfor acceptance used, accuracy computational time KBR abc. putational time shown fig toger total sizes generated samples. KBR method, sample sizes likelihood prior varied. regularization parameters kbr, Gaussian kernels incomplete Cholesky decomposition employed. results KBR achieves accurate results ABC computational time.  Filtering problems KBR filter proposed sec. applied. alternative strategies state-space models complex dynamics involve extended Kalman filter (ekf) unscented Kalman filter (ukf]). works nonparametric state-space model HMM nonparametric estimation conditional. kde partitions] and, recently, kernel method]. following, KBR method compared linear nonlinear Kalman filters. kbr regularization parameters kernel parameters., deviation parameter Gaussian kernel). validation approach applied selecting dividing training sample two. reduce search space, set Gaussian kernel deviation  median pairwise distances training samples]), leaving parameters tuned.  KBR EKF UKF KBF EKF UKF Mean square errors Mean square errors 200 400 600 800 Training sample size 1000 200 400 600 Training data size Data) 800 Data) Figure Comparisons KBR Filter ekf. (average MSEs SEs runs.)    KBR (gauss.210 .015.222 .009 KBR.146 .003.210 .008 Kalman dim.) .980 .083.935 .064 Kalman (quat.) .557 .023.541 .022 Table Average MSEs SEs camera angle estimates runs). syntic data sets kbr, ekf, ukf, assuming EKF UKF exact dynamics. dynamics hidden state  sin ))(cos sin (mod   independent noise. note dynamics nonlinear observation ). dynamics defined follows) (noisy rotation) ) (noisy oscillatory rotation) . results shown fig.  cases, EKF UKF show unrecognizably small difference. dynamics) weak nonlinearity, KBR shows slightly worse MSE EKF ukf. dataset) strong nonlinearity, KBR outperforms 200 nonlinear Kalman filters, true dynamics. next, applied KBR filter camera rotation problem angle camera hidden variable movie frames room camera observed. 3600 frames rgb pixels ]1200 1800 frames training, half test. details data]. make data noisy adding Gaussian noise,  experiments cover settings. first, assume hidden state included), general matrix. case, Kalman filter estimating relations linear assumption, KBR filter Gaussian kernels setting, exploit fact ): Kalman filter, represented quanternion, KBR filter kernel table shows Frobenius norms estimated matrix true one. kbr filter significantly outperforms Kalman filter, \\x0ckbr advantage extracting complex nonlinear dependence observation hidden state. conclusion proposed general, framework implementing Bayesian inference, prior, likelihood, posterior expressed kernel means reproducing kernel Hilbert spaces. model expressed terms set training samples, inference consists small number straightforward matrix operations. approach suited cases simple parametric models analytic forms density available, samples easily obtained. addressed applications: Bayesian inference likelihood, sequential filtering nonparametric state-space model. future studies include comparisons sampling approaches advanced Monte carlo, applications inference problems nonparametric Bayesian models Bayesian reinforcement learning. acknowledgements. supported part JSPS KAKENHI) 22300098. due difference noise model, results directly comparable]. Kernel methods long provided powerful tools generalizing linear statistical approaches nonlinear settings, embedding sample high dimensional feature space, reproducing kernel Hilbert space (rkhs]. product feature mappings computed explicitly, positive definite kernel function, permits eﬃcient computation deal explicitly feature representation. more recently, RKHS feature map represent probability distributions, rar mapping single points: refer representations probability distributions kernel means. with choice kernel, feature mapping rich expectation uniquely identifies distribution: RKHSs termed characteristic]. kernel means characteristic RKHSs applied successfully number statistical tasks, including sample problem], independence tests], conditional independence tests]. advantage kernel approach tests apply immediately domain kernels defined. propose general nonparametric framework Bayesian inference, expressed terms kernel means. goal Bayesian inference find posterior observation)? )? ) ) density function prior, conditional density likelihood framework, posterior, prior, likelihood expressed kernel means: update prior posterior called Kernel bayes? rule (kbr). implement kbr, kernel means learned nonparametrically training data: prior likelihood means expressed terms samples prior joint probabilities, posterior kernel weighted sample. resulting updates straightforward matrix operations. this leads main advantage KBR approach: absence specific parametric model analytic form prior likelihood densities, perform Bayesian inference making suﬃcient observations system. alternatively, parametric model, complex require time-consuming sampling techniques inference. contrast, KBR simple implement, amenable well-established approximation techniques yield computational cost linear training sample size]. furr establish rate consistency estimated posterior kernel true posterior, function training sample size. proposed kernel realization bayes? rule extension approach] state-space models. this earlier work applies heuristic, however, kernel previous hidden state observation assumed combine additively update hidden state estimate. more recently, method belief propagation kernel means proposed]: unlike present work, directly estimates conditional densities assuming prior uniform. alternative kernel means nonparametric density estimates. classical approaches include finite distribution estimates partitioned domain kernel density estimation, perform poorly high dimensional data. alternatively, direct estimates density ratio estimating conditional. ]. contrast density estimation approaches, KBR makes easy compute posterior expectations RKHS product) perform conditioning marginalization, requiring numerical integration.  Kernel expression bayes? rule Positive definite kernel probabilities begin review basic concepts tools statistics RKHS]. given Panset-valued) positive definite kernel symmetric kernel ???   arbitrary points  ] positive definite kernel uniquely defines Hilbert space (rkhs) consisting functions)   (reproducing property).  real numbers     let, measure spaces, random variable probability throughout paper, assumed positive definite kernels measurable spaces measurable bounded, boundedness defined supx?? , let positive definite kernel measurable space RKHS kernel defined -valued random variable)dpx). ) For notational simplicity, dependence shown. since kernel depends distribution (and kernel), written mpx whichever equivalent notations clearest context. from reproducing property  ) Let positive definite kernels respective RKHS (uncentered) covariance operator defined relation   , ihy  ihy noted identified tensor product space product kernel]. identification standard: tensor product isomorphic space linear maps correspondence     ]. define CXX hf2 CXX)]  introduce notion characteristic rkhs, essential kernels manipulate probability measures. bounded measurable positive definite kernel called characteristic)] (?, implies probabilities uniquely determined kernel means]. with property, problems statistical inference cast terms inference kernel means. widely characteristic kernel Gaussian kernel, exp yk2? )). empirical estimates kernel covariance operator straightforward obtain. given. sample    law empirical kernel covariance operator) (?, (?, ) written tensor product form. -consistent norm.  Kernel bayes? rule derive kernel implementation bayes? rule. let prior distribution. ). following, denote probabilities. )? ), respectively. our goal obtain estimator kernel posterior mqx). orem fundamental manipulating conditional probabilities positive definite kernels. orem]).   holds CXX CXY CXX injective, relation expressed CXX CXY Using. ), obtain expression kernel orem]). assume CXX injective, mqy kernel means respectively.   (cxx   mqy CXX  ) discussed], operator CXX implements forward filtering prior conditional density. ). note, however, assumptions  injectivity CXX \\x0chold general; easily provide counterexamples. following, noneless derive population expression bayes? rule strong assumptions, prototype empirical estimator expressed terms Gram matrices, prove consistency subject smoothness conditions distributions. deriving kernel realization bayes? rule, orem obtain kernel representation joint probability CXX    ) equation covariance operator .  ), ? ? point measure applications Bayesian inference, probability conditioned computed. plugging point measure . ), population expression CXX] kernel conditional probability). let, random variable law replacing. ), obtain CZW). this kernel posterior obtain. step derive covariance operators. ). recalling  identified covariance operator CZW   . ) obtain operators. ), kernel expression bayes? rule. argument rigorously implemented empirical estimates kernel means covariances. let  . sample law assume consistent estimator  (?) (?,     sample defines estimator (which generated weights. negative values allowed empirical estimators CZW identified respectively. from. ), (?) (?)    CXX CXX identity coeﬃcient Tikhonov regularization operator inversion. propositions express estimators Gram matrices. proofs simple matrix manipulation shown Supplementary material. following, denote Gram matrices )), respectively.  input sample express  weighted sample express kernel prior  (iii) regularization constants. computation:  compute Gram matrices )), vector    compute  compute diag ?). output: matrix given conditioning kernel posterior) estimated weighted sample figure Kernel bayes? rule Algorithm bzw Proposition Gram matrix expressions bzw  (?, (?, (?,  respectively, common coeﬃcient      ) prop. implies probabilities estimated weighted samples   respectively, common weights. since weights negative, anor type Tikhonov regularization computing. ). bzw) Proposition for Gram matrix expression ktx diag diagonal matrix elements .  (?,     (?,     ) call eqs. ) kernel bayes? rule (kbr., expression bayes? rule terms kernel means. algorithm implement KBR summarized fig.  aim estimate, expectation function respect posterior, based. ) estimator, ihx     weighted sample represent posterior, KBR similarity Monte Carlo methods importance sampling sequential Monte Carlo]). KBR method, however, generate samples posterior, updates weights sample matrix operations. provide experimental comparisons KBR sampling methods sec. .  Consistency KBR estimator demonstrate consistency KBR estimator. ). show rate derived assumptions, leave detailed discussions proofs Supplementary material. assume sample size prior infinity sample size likelihood infinity,  -consistent. oretical results, assume Hilbert spaces separable. following) denotes range orem let , random vector law  . )? ), estimator    khx??     . assume (cxx.        for ) ?    ) estimator. ). ) condition (cxx requires prior smooth.  direct empirical kernel. sample size typically  orem implies -consistency. while slow rate, practice convergence faster oretical guarantee. Bayesian inference Kernel bayes? rule Bayesian inference, tasks interest include finding properties posterior (map value, moments), computing expectation function posterior. demonstrate kernel obtained KBR solving problems. first, orem obtain consistent estimator posterior expectation  this covers wide class functions characteristic kernels (see experiments sec. ). next, point estimate] proposes preimage arg minx kkx (?, ktx)k2hx represents posterior effectively point. approach present paper point estimates considered. case Gaussian kernel, fixed point method sequentially optimize]. KBR prior likelihood expressed terms samples. thus unlike methods Bayesian inference, exact knowledge densities needed, samples obtained. typical situations KBR approach advantageous: relation variables diﬃcult realize simple parametric model, obtain samples variables. nonparametric state-space model sec. ).   prior and likelihood hard obtain explicitly, sampling possible) population genetics, branching processes likelihood model split species, explicit density hard obtain. approximate Bayesian Computation (abc) popular sampling method situations]. ) nonparametric Bayesian inference. ]), prior typically form process density. KBR approach give alternative ways Bayesian computation problems. show experimental comparisons KBR approach ABC sec. . standard sampling method MCMC sequential applicable, computation time consuming, real-time applications feasible. using kbr, expectation posterior obtained simply product.  computed. KBR \\x0capproach noneless weakness common nonparametric methods: data point appears training sample, reliability output low. thus, suﬃcient diversity training sample reliably estimate posterior. KBR computation, Gram matrix inversion necessary, cost sample size attempted directly. substantial cost reductions achieved low rank matrix approximations incomplete Cholesky decomposition], approximates Gram matrix form  matrix computing costs(nr2 Woodbury identity, KBR approximately computed cost(nr2 kernel choice model selection key effectiveness kbr, kernel methods. kbr involves model parameters: kernel parameters), regularization parameters strategy parameter selection depends posterior inference problem. applied supervised setting, standard cross-validation). general approach requires constructing related supervised problem. suppose prior marginal posterior density) averaged equal marginal density compare discrepancy kernel this leads application-fold approach. average estimators] namely, partition, disjoint subsets kernel] data posterior estimated data   prior (?,  Application nonparametric state-space model. consider state-space model,  observable hidden state. assume conditional probabilities explicitly, estimate simple parametric models. rar, assume sample    observable hidden variables training phase. this problem considered], give principled approach based kbr. conditional probability transition observation process) represented covariance (?,  operators computed training sample; bxy (?, (?, bxx defined similarly. note data., consistency achieved mixing property Markov model. for simplicity, focus filtering problem, smoothing prediction similarly. filtering, estimate current hidden state observations  sequential estimate     derived KBR give sketch below; Supplementary material detailed derivation). suppose estimator kernel  —?     form)     coeﬃcients time applying orem twice) (?, kernel   estimated here  )  ) ?transfer? matrix defined with notation) ) diag) ,...,?  ) kernel bayes? rule yields)    ) ) eqs.   ) describe update rule    contrast], estimates previous hidden state observation assumed combine additively, derivation based applying kbr. sequential filtering, substantial reduction computational cost achieved low rank approximations matrices training phase: rank computation costs step filtering. bayesian computation likelihood. when likelihood and prior obtained analytic form sampling possible, ABC approach] popular Bayesian computation. ABC rejection method generates sample follows) generate prior) generate, accept orwise reject). Step), distance tolerance acceptance. situation above, KBR approach method) generate prior) generate sample ), (iii) compute Gram matrices   ). distribution sample ABC approaches true posterior  empirical posterior estimate KBR converges true  computational eﬃciency abc, however, arbitrarily low small rarely accepted Step). finally, ABC generates sample, statistic posterior approximated. case kbr, statistics posterior (such confidence intervals) harder obtain, consistency guaranteed expectations RKHS functions. sec. , provide experimental comparisons addressing trade-off computational time accuracy ABC kbr.      \\x0cexperiments Nonparametric inference posterior First compare KBR standard kernel density estimation (kde). let. sample  with. ) conditional. ) estimated KhX )hhy  KhX  KhX HhY given. sample prior posterior) represented weighted sample  importance weight). compare estimates obtained KBR KDE, Gaussian kernels methods. note Gaussian kernel, function) belong consistency KBR method rigorously guaranteed. orem). gaussian kernels, however, approximate continuous function compact subset arbitrary accuracy]. expect posterior estimated effectively. ave. MSE runs) KBR kde]) experiments, dimensionality KBR) ranging form. distribution KBR (med dist) kde) kde (best, randomly generated run. prior , VXX), VXX-component sample sizes 200. bandwidth parameter KDE set chosen ways, square cross-validation] performance, set    }. for kbr, methods choose devi2 Dimension ation parameter Gaussian kernel: median pairwise distances data-fold Figure KBR. kde. sec.  fig. shows MSE estimates 1000 random points , while accuracy methods decrease larger dimensionality, KBR significantly outperforms kde. bayesian computation likelihood compare KBR ABC terms estimation accuracy computational time. compute estimation accuracy rigorously, Gaussian distributions true prior likelihood. samples model sec.  evaluated points performed runs covariance. cpu time Error dim.) . mean Square Errors KBR ABC 200 600 400 800 1000 2000 For abc, rejection method; CPU time (sec) advanced sampling schemes], implementation straightforward. various parameters Figure Estimation accuracy comfor acceptance used, accuracy computational time KBR abc. putational time shown fig toger total sizes generated samples. for KBR method, sample sizes likelihood prior varied. regularization parameters kbr, Gaussian kernels incomplete Cholesky decomposition employed. results KBR achieves accurate results ABC computational time.  Filtering problems KBR filter proposed sec. applied. alternative strategies state-space models complex dynamics involve extended Kalman filter (ekf) unscented Kalman filter (ukf]). works nonparametric state-space model HMM nonparametric estimation conditional. KDE partitions] and, recently, kernel method]. following, KBR method compared linear nonlinear Kalman filters. kbr regularization parameters kernel parameters., deviation parameter Gaussian kernel). validation approach applied selecting dividing training sample two. reduce search space, set Gaussian kernel deviation  median pairwise distances training samples]), leaving parameters tuned.  KBR EKF UKF KBF EKF UKF Mean square errors Mean square errors 200 400 600 800 Training sample size 1000 200 400 600 Training data size Data) 800 Data) Figure Comparisons KBR Filter ekf. (average MSEs SEs runs.)    KBR (gauss.210 .015.222 .009 KBR.146 .003.210 .008 Kalman dim.) .980 .083.935 .064 Kalman (quat.) .557 .023.541 .022 Table Average MSEs SEs camera angle estimates runs). syntic data sets kbr, ekf, ukf, assuming EKF UKF exact dynamics. dynamics hidden state  sin ))(cos sin (mod   independent noise. note dynamics nonlinear observation ). dynamics defined follows) (noisy rotation) ) (noisy oscillatory rotation) . results shown fig.  cases, EKF UKF show unrecognizably small difference. dynamics) weak nonlinearity, KBR shows slightly worse MSE EKF ukf. for dataset) strong nonlinearity, KBR outperforms 200 nonlinear Kalman filters, true dynamics. next, applied KBR filter camera rotation problem angle camera hidden variable movie frames room camera observed. 3600 frames RGB pixels ]1200 1800 frames training, half test. for details data]. make data noisy adding Gaussian noise,  our experiments cover settings. first, assume hidden state included), general matrix. case, Kalman filter estimating relations linear assumption, KBR filter Gaussian kernels setting, exploit fact ): Kalman filter, represented quanternion, KBR filter kernel table shows Frobenius norms estimated matrix true one. KBR filter significantly outperforms Kalman filter, \\x0ckbr advantage extracting complex nonlinear dependence observation hidden state. Conclusion proposed general, framework implementing Bayesian inference, prior, likelihood, posterior expressed kernel means reproducing kernel Hilbert spaces. model expressed terms set training samples, inference consists small number straightforward matrix operations. our approach suited cases simple parametric models analytic forms density available, samples easily obtained. addressed applications: Bayesian inference likelihood, sequential filtering nonparametric state-space model. future studies include comparisons sampling approaches advanced Monte carlo, applications inference problems nonparametric Bayesian models Bayesian reinforcement learning. acknowledgements. supported part JSPS KAKENHI) 22300098. Due difference noise model, results directly comparable].',\n",
       " 'PP4456': 'fitting parametric probabilistic models data basic task statistics machine learning. set training data)   ) parameter learning aims find member parametric distribution family, represent training data. practice, high dimensional parametric probabilistic models, Markov random fields] products experts], defined ?? (? ?? unnormalized model(?) ??  partition function. maximum (log) likelihood) estimation commonly Pnmethod parameter learning, optimal parameter obtained solving argmax?  log ) obtained estimators desirable prop \\x0certies, consistency asymptotic normality]. however, high dimensional integration/summation, partition function oftentimes makes learning computationally intractable. reason, non parameter learning methods ?tricks? obviate direct computation partition function experienced rapid developments, recent years. computationally eﬃcient non learning methods achieved impressive practical performances, exceptions, learning objectives numerical implementations suggest largely unrelated. work, based information geometric view parametric learning, elaborate general non learning principle termed minimum contraction (mkc), seek optimal parameters minimize contraction divergence distributions transformed contraction operator. contraction operator mapping probability distributions divergence distributions tend reduce equal. show objective functions wide range non learning methods, including contrastive divergence], noise-contrastive estimation], partial likelihood], nonlocal contrastive objectives], score matching], pseudo-likelihood], maximum conditional likelihood], maximum mutual information], maximum marginal likelihood], conditional marginal composite likelihood], unified MKC framework choices contraction operators MKC objective functions. related Works Similarities parameter updates non learning methods noticed recent works. instance], shown parameter update score matching] equivalent parameter update version contrastive divergence] performs Langevin approximation Gibbs sampling, approximations parameter update pseudo-likelihood]. connection furr generalized], shows parameter update anor variant contrastive divergence equivalent stochastic parameter update conditional composite likelihood]. however, similarities numerical implementations tangential fundamental relationship objective functions non learning methods. hand, energy based learning] presents general framework subsume non learning objectives, broad generality obscures specific statistical interpretations. objective function level, relations non methods known. instance, pseudo-likelihood special case conditional composite likelihood]. ], non learning methods unified framework minimizing Bregman divergence. contraction Operator base discussion continuous variables probability density functions. results readily extended discrete case replacing integrations probability density functions summations probability mass functions. denote set probability density functions probability distributions kulbackleibler) divergence (also relative entropy idivergence] defined(pkq) log. divergence non-negative equals.). define distribution operator, mapping density function anor density function   adopt shorthand notation }. distribution fix point distribution operator }. contraction operator distribution operator,   exist constant  condition hold(pkq) ? })  ) DKL  }  } subsequently, contraction factor, lhs. ) contraction  ? ? obviously. contraction, divergence, zero. addition, contraction operator strict equality. ) holds.). intuitively, divergence Figure Illustration contraction operator density functions regarded ?distance? metric probability distri1 butions increased distributions transformed contraction operator, graphical illustration shown fig. furrmore, strict contraction operator, divergence reduced distributions equal.). contraction operators analogous contraction operators ordinary metric spaces, similar role Lipschitz constant]. indeed, divergence behaves squared clidean distance]. . ) general abstract definition contraction operators. following, give examples contraction operators constructed common operations probability distributions.  Conditional Distribution form family contraction operators conditional distributions.  distribution)  conditional distribution), define distribution operator). ) result shows strict contraction operator  lemma (cover Thomas For distributions distribution operator defined. (pkq) }) )ktq)) rd0) ) ) induced conditional distributions furrmore, equality holds.).  Marginalization Marginal Grafting Two related types contraction operators constructed based marginal distributions. distribution) nonempty index subset ,   }. denote,    marginal distribution sub-vector formed components indices obtained integrating) sub-vector marginalization operation defines distribution operator  )dxa— Anor contraction operator termed marginal grafting defined based distribution) marginal grafting operator defined} understood replacing) term nonnegative integrates proper probability distribution furrmore, fixed point operator} result shows contraction operators, sense complementary. operators defined. . ), Lemma (huber]) For distributions distribution(pkq) }) furr more dxa conditional distributions induced )kqa lemma neir strict contraction. differ marginal distribution latter suﬃcient make contraction zero. cite original reference subsequent results, recast terminology introduced work. due limit space, defer formal proofs results supplementary materials.  Binary Mixture \\x0cfor distributions) introduce binary variable  , form joint distribution ) ) }. marginalizing ), obtain binary mixture ), induces distribution operator). result shows) strict contraction operator  lemma For distributions distribution operator(pkq) } ) ) induced posterior conditional distributions , ), respectively. equality holds.).  defined. ), Lumping Let   partition   lumping] distribution) yields distribution ,   }, subsequently induces distribution operator PiS    result shows contraction operator  lemma (csisz Shields]) For distributions distribution operator defined. ), PiS(?  ) (pkq)  )dx0 )dx0 distributions induced restricting respectively[?] indicator function. note general strict, distributions agree contraction. minimizing Contraction Parametric Learning work, information geometric view parameter learning assuming training data samples distribution seek optimal distribution statistical manifold parametric distribution family approximates]. context, maximum (log) likelihood learning equivalent finding Rparameter minimizes divergence ], argmin? (pkq?) argmax? ) log . data obtained approximate expectation sample average based objective) log    log contraction operators suggest alternative \\x0capproach parametric learning. particular, contraction contraction operator nonnegative reaches equal everywhere. refore, minimize contraction contraction operator encourage matching term general approach parameter learning minimum contraction (mkc). mamatically, minimum contraction realized related types objective functions. type With contraction operator find optimal directly minimizes contraction : argmin(pkq?)  ?  ) practice, desirable  linear operator bounded ) model defined functions]. this, (?) unnormalized model partition function. furrmore, assuming obtain samples      yn0 }, respectively, optimization. ) approximated) argmin(pkq?  argmax log??  log ?{?  )   due linearity terms(?)  ? cancel. refore, optimization require computation partition function, highly desirable property fitting parameters high dimensional probabilistic models intractable partition functions. type MKC objective functions contraction operators induced conditional distribution, marginalization, marginal grafting, linear transform, lumping fall category. however, nonlinear contraction operators, induced binary mixtures, avoid computing partition function., Section). furrmore, contraction operator. ) parameters, include model parameter ., Section). however, optimization complicated } optimizing last, note Type MKC objective functions non-strict contraction operator, guarantee contraction zero. type: Consider strict contraction operator denoted parameterized auxiliary parameter distribution } continuously differentiable divergence? regarded function, ?). thus, contraction. ) approximated Taylor expansion(pkq?)  (? ? ? ?  (? ? ? ,  ,?)  ,    ?  derivative contract tion easier work contraction., Section), fix equivalently maximizing derivative, Type MKC objective function, argmax?  )  Type iii: case access set contraction operators   implement minimum contraction principle finding optimal minimizes average contraction(pkq?)  ? }))  ) argmin contraction sum nonnegative. ) contraction zero. consistency corresponds constraints objective function. ), represents consistency constraints. special cases, minimizing. ) suﬃcient number types contraction operators ensure equality ., Section).  Fitting Gaussian Model Contraction Operator Gaussian Distribution describe instance MKC learning simple setting, approximate distribution) variance Gaussian model variance parameters estimated (?,  strict contraction operator constructed Gaussian conditional distribution  exp   variance form Type MKC objective function. simple case. ) reduced closed form objective function  argmin log (?     ?,? optimal solution,   obtained direct differentiation. detailed derivation result omitted due limit space. note that, optimal parameters rely parameter contraction operator case obtained minimizing divergence equivalently, maximizing log likelihood, samples) approximate expectation.  Relation Contrastive Divergence] next, general strict contraction operator? constructed conditional distribution, ), parametric model fixed point, ? ? ? ). words, equilibrium distribution markov chain transitional distribution ). type objective function minimum contraction. ),  ? argmin(pkq?)  ? ? ? argmin(pkq?)  ? ?)   ? }.  shorthand notation Note objective function contrastive divergence learning]. however, dependency  makes objective function diﬃcult optimize. ignoring dependency, practical parameter update contrastive divergence approximately gradient objective function].  \\x0crelation Partial Likelihood] non-local Contrastive Objectives] next, Type MKC objective function. ), combined contraction operator constructed lumping. Lemma PiS? argmin(pkq?)  ? argmin  argmax ) log?  argmax PiS log?   PiS)   ) samples). minimizing contraction case equivalent maximizing weighted sum log likelihood probability distributions formed restricting model subsets state space. step resembles partial likelihood objective function], recently rediscovered context discriminative learning nonlocal contrastive objectives]. ], partitions required overlap, result shows non-overlapping partitions non parameter learning.  Relation Noise Contrastive Estimation] next, Type MKC objective function. ), combined strict contraction operator constructed binary mixture operation (lemma). particular, simplify. ) definition: argmin(pkq?)  ?  argmin)) log) )) ) log   ) argmax) log) log.  ) )  ) )  when expectations objective function approximated averages samples)   )   ? type MKC objective function case reduces argmax   ) log log    set, treat)   )   ? data interest noise, respectively, objective function interpreted minimizing Bayesian classification error data noise, objective function noise-contrastive estimation].  Relation Score Matching] \\x0cnext, strict contraction operator, ?ctt constructed isotropic Gaussian conditional distribution time decaying variance., Gaussian diffusion process): xk2) exp  , continuous temporal index. note ?ct0}  ) ) functions differentiable temporal derivative contraction ?ctt closed form, formally stated result. lemma (lyu]) For distributions differentiable ?ctt) ?ctt ?   ) ?ctt) ?ctt ) gradient operator setting. ), obtain closed form Type MKC objective function. ), furr simplified) ) argmin) argmax? ) )   argmin log  24x log )  argmin log ) 24x log )   ) samples), Laplacian operator step objective function score matching learning].  Relation Conditional Composite Likelihood] pseudo-likelihood] next, Type MKC objective function. ), combined contraction operator, constructed marginalization. according Lemma}) argmax? argmin? (pkq)  ) log ) argmax?  log step, expectation) replaced averages samples)   ) corresponds objective function maximum conditional likelihood] maximum mutual information], non learning objectives discriminative learning high dimensional probabilistic data models. however, Lemma shows(pkq) }) \\x0csuﬃcient guarantee  alternatively, Type III MKC objective function. ), combine contraction operators formed marginalizations index subsets   x1x) argmin(pkq) log qai (xai —xai } argmax   This objective function conditional composite likelihood, (also rediscovered piecewise learning]). special case conditional composite likelihood}, resulting marginalth ization operator} singleton marginalization operator. singleton operators, rewrite objective function(pkq) marginalization dxi note case, average contraction) ) agree singleton conditional distribution brook Lemma], condition suﬃcient) .). furrmore, approximating expectations averages samples), 1x1x) argmin(pkq)  } } argmax log   objective function maximum pseudo-likelihood learning].  Relation Marginal Composite Likelihood combining Type III MKC objective function. ), contraction operator constructed marginal grafting operation. specifically, contraction operators constructed marginal grafting index subsets   lemma expand Type III minimum contraction objective function: argmin(pkq) } argmin(pai (xai )kqai (xai  ) argmax pai (xai log qai (xai )dxai argmax log qai (xai   step, maximizes log likelihood set marginal distributions training data, corresponds objective function marginal composite likelihood]. resulting objective maximum marginal likelihood type likelihood learning]. discussions work, based information geometric view parameter learning, minimum contraction unifying principle non \\x0cparameter learning, showing objective functions existing nonml parameter learning methods understood instantiations principle contraction operators. directions extend current work. first, proposed minimum contraction framework furr generalized general -divergence], divergence special case. general framework, hope reveal furr relations types non learning objectives]. second, current work, focused idealization parametric learning matching probability distributions. practice, learning performed finite data set unknown underlying distribution. cases, asymptotic properties estimation data volume increases, consistency, essential. non learning methods covered work shown consistent individually, unification based minimum contraction provide general condition asymptotic properties. last, understanding existing non learning objectives minimizing contraction principled approach devise non learning methods, seeking contraction operators, combinations existing contraction operators. acknowledgement author Jascha sohl-dickstein, Michael DeWeese Michael Gutmann helpful discussions early version work. work supported National Science Foundation CAREER Award Grant. 0953373. Fitting parametric probabilistic models data basic task statistics machine learning. given set training data)   ) parameter learning aims find member parametric distribution family, represent training data. practice, high dimensional parametric probabilistic models, Markov random fields] products experts], defined ?? (? ?? unnormalized model(?) ??  partition function. maximum (log) likelihood) estimation commonly Pnmethod parameter learning, optimal parameter obtained solving argmax?  log ) obtained estimators desirable prop \\x0certies, consistency asymptotic normality]. however, high dimensional integration/summation, partition function oftentimes makes learning computationally intractable. for reason, non parameter learning methods ?tricks? obviate direct computation partition function experienced rapid developments, recent years. while computationally eﬃcient non learning methods achieved impressive practical performances, exceptions, learning objectives numerical implementations suggest largely unrelated. work, based information geometric view parametric learning, elaborate general non learning principle termed minimum contraction (mkc), seek optimal parameters minimize contraction divergence distributions transformed contraction operator. contraction operator mapping probability distributions divergence distributions tend reduce equal. show objective functions wide range non learning methods, including contrastive divergence], noise-contrastive estimation], partial likelihood], nonlocal contrastive objectives], score matching], pseudo-likelihood], maximum conditional likelihood], maximum mutual information], maximum marginal likelihood], conditional marginal composite likelihood], unified MKC framework choices contraction operators MKC objective functions. Related Works Similarities parameter updates non learning methods noticed recent works. for instance], shown parameter update score matching] equivalent parameter update version contrastive divergence] performs Langevin approximation Gibbs sampling, approximations parameter update pseudo-likelihood]. this connection furr generalized], shows parameter update anor variant contrastive divergence equivalent stochastic parameter update conditional composite likelihood]. however, similarities numerical implementations tangential fundamental relationship objective functions non learning methods. hand, energy based learning] presents general framework subsume non learning objectives, broad generality obscures specific statistical interpretations. objective function level, relations non methods known. for instance, pseudo-likelihood special case conditional composite likelihood]. ], non learning methods unified framework minimizing Bregman divergence. Contraction Operator base discussion continuous variables probability density functions. most results readily extended discrete case replacing integrations probability density functions summations probability mass functions. denote set probability density functions for probability distributions KulbackLeibler) divergence (also relative entropy idivergence] defined(pkq) log. divergence non-negative equals.). define distribution operator, mapping density function anor density function   adopt shorthand notation }. distribution fix point distribution operator }. contraction operator distribution operator,   exist constant  condition hold(pkq) ? })  ) DKL  }  } subsequently, contraction factor, LHS. ) contraction  ? ? obviously. contraction, divergence, zero. addition, contraction operator strict equality. ) holds.). intuitively, divergence Figure Illustration contraction operator density functions regarded ?distance? metric probability distri1 butions increased distributions transformed contraction operator, graphical illustration shown fig. furrmore, strict contraction operator, divergence reduced distributions equal.). contraction operators analogous contraction operators ordinary metric spaces, similar role Lipschitz constant]. indeed, divergence behaves squared clidean distance]. . ) general abstract definition contraction operators. following, give examples contraction operators constructed common operations probability distributions.  Conditional Distribution form family contraction operators conditional distributions. consider distribution)  conditional distribution), define distribution operator). ) result shows strict contraction operator  lemma (cover Thomas For distributions distribution operator defined. (pkq) }) )ktq)) Rd0) ) where) induced conditional distributions furrmore, equality holds.).  Marginalization Marginal Grafting Two related types contraction operators constructed based marginal distributions. consider distribution) nonempty index subset ,   }. let denote,    marginal distribution sub-vector formed components indices obtained integrating) sub-vector this marginalization operation defines distribution operator  )dxa— Anor contraction operator termed marginal grafting defined based for distribution) marginal grafting operator defined} understood replacing) term nonnegative integrates proper probability distribution furrmore, fixed point operator} result shows contraction operators, sense complementary. operators defined. . ), Lemma (huber]) For distributions distribution(pkq) }) furr more dxa conditional distributions induced )kqa Lemma neir strict contraction. differ marginal distribution and latter suﬃcient make contraction zero. cite original reference subsequent results, recast terminology introduced work. due limit space, defer formal proofs results supplementary materials.  Binary Mixture \\x0cfor distributions) introduce binary variable  , form joint distribution ) ) }. marginalizing ), obtain binary mixture ), induces distribution operator). result shows) strict contraction operator  lemma For distributions distribution operator(pkq) } ) ) induced posterior conditional distributions , ), respectively. equality holds.).  defined. ), Lumping Let   partition   lumping] distribution) yields distribution ,   }, subsequently induces distribution operator PiS    result shows contraction operator  lemma (csisz Shields]) For distributions distribution operator defined. ), PiS(?  ) (pkq)  )dx0 )dx0 distributions induced restricting respectively[?] indicator function. note general strict, distributions agree contraction. Minimizing Contraction Parametric Learning work, information geometric view parameter learning assuming training data samples distribution seek optimal distribution statistical manifold parametric distribution family approximates]. context, maximum (log) likelihood learning equivalent finding Rparameter minimizes divergence ], argmin? (pkq?) argmax? ) log . data obtained approximate expectation sample average based objective) log    log contraction operators suggest alternative \\x0capproach parametric learning. particular, contraction contraction operator nonnegative reaches equal everywhere. refore, minimize contraction contraction operator encourage matching term general approach parameter learning minimum contraction (mkc). mamatically, minimum contraction realized related types objective functions. type With contraction operator find optimal directly minimizes contraction : argmin(pkq?)  ?  ) practice, desirable  linear operator bounded ) model defined functions]. this, (?) unnormalized model partition function. furrmore, assuming obtain samples      yn0 }, respectively, optimization. ) approximated) argmin(pkq?  argmax log??  log ?{?  )   due linearity terms(?)  ? cancel. refore, optimization require computation partition function, highly desirable property fitting parameters high dimensional probabilistic models intractable partition functions. type MKC objective functions contraction operators induced conditional distribution, marginalization, marginal grafting, linear transform, lumping fall category. however, nonlinear contraction operators, induced binary mixtures, avoid computing partition function., Section). furrmore, contraction operator. ) parameters, include model parameter ., Section). however, optimization complicated } optimizing last, note Type MKC objective functions non-strict contraction operator, guarantee contraction zero. type: Consider strict contraction operator denoted parameterized auxiliary parameter distribution } continuously differentiable divergence? regarded function, ?). thus, contraction. ) approximated Taylor expansion(pkq?)  (? ? ? ?  (? ? ? ,  ,?)  ,    ?  derivative contract tion easier work contraction., Section), fix equivalently maximizing derivative, Type MKC objective function, argmax?  )  Type iii: case access set contraction operators   implement minimum contraction principle finding optimal minimizes average contraction(pkq?)  ? }))  ) argmin contraction sum nonnegative. ) contraction zero. consistency corresponds constraints objective function. ), represents consistency constraints. under special cases, minimizing. ) suﬃcient number types contraction operators ensure equality ., Section).  Fitting Gaussian Model Contraction Operator Gaussian Distribution describe instance MKC learning simple setting, approximate distribution) variance Gaussian model variance parameters estimated (?,  using strict contraction operator constructed Gaussian conditional distribution  exp   variance form Type MKC objective function. simple case. ) reduced closed form objective function  argmin log (?     ?,? optimal solution,   obtained direct differentiation. detailed derivation result omitted due limit space. note that, optimal parameters rely parameter contraction operator case obtained minimizing divergence equivalently, maximizing log likelihood, samples) approximate expectation.  Relation Contrastive Divergence] next, general strict contraction operator? constructed conditional distribution, ), parametric model fixed point, ? ? ? ). words, equilibrium distribution markov chain transitional distribution ). Type objective function minimum contraction. ),  ? argmin(pkq?)  ? ? ? argmin(pkq?)  ? ?)   ? }.  shorthand notation Note objective function contrastive divergence learning]. however, dependency  makes objective function diﬃcult optimize. ignoring dependency, practical parameter update contrastive divergence approximately gradient objective function].  \\x0crelation Partial Likelihood] non-local Contrastive Objectives] next, Type MKC objective function. ), combined contraction operator constructed lumping. using Lemma PiS? argmin(pkq?)  ? argmin  argmax ) log?  argmax PiS log?   PiS)   ) samples). minimizing contraction case equivalent maximizing weighted sum log likelihood probability distributions formed restricting model subsets state space. step resembles partial likelihood objective function], recently rediscovered context discriminative learning nonlocal contrastive objectives]. ], partitions required overlap, result shows non-overlapping partitions non parameter learning.  Relation Noise Contrastive Estimation] next, Type MKC objective function. ), combined strict contraction operator constructed binary mixture operation (lemma). particular, simplify. ) definition: argmin(pkq?)  ?  argmin)) log) )) ) log   ) argmax) log) log.  ) )  ) )  When expectations objective function approximated averages samples)   )   ? Type MKC objective function case reduces argmax   ) log log    set, treat)   )   ? data interest noise, respectively, objective function interpreted minimizing Bayesian classification error data noise, objective function noise-contrastive estimation].  Relation Score Matching] \\x0cnext, strict contraction operator, ?ctt constructed isotropic Gaussian conditional distribution time decaying variance., Gaussian diffusion process): xk2) exp  , continuous temporal index. note ?ct0}  ) ) functions differentiable temporal derivative contraction ?ctt closed form, formally stated result. lemma (lyu]) For distributions differentiable ?ctt) ?ctt ?   ) ?ctt) ?ctt ) gradient operator setting. ), obtain closed form Type MKC objective function. ), furr simplified) ) argmin) argmax? ) )   argmin log  24x log )  argmin log ) 24x log )   ) samples), Laplacian operator step objective function score matching learning].  Relation Conditional Composite Likelihood] pseudo-likelihood] next, Type MKC objective function. ), combined contraction operator, constructed marginalization. According Lemma}) argmax? argmin? (pkq)  ) log ) argmax?  log step, expectation) replaced averages samples)   ) this corresponds objective function maximum conditional likelihood] maximum mutual information], non learning objectives discriminative learning high dimensional probabilistic data models. however, Lemma shows(pkq) }) \\x0csuﬃcient guarantee  alternatively, Type III MKC objective function. ), combine contraction operators formed marginalizations index subsets   X1X) argmin(pkq) log qai (xai —xai } argmax   This objective function conditional composite likelihood, (also rediscovered piecewise learning]). special case conditional composite likelihood}, resulting marginalth ization operator} singleton marginalization operator. with singleton operators, rewrite objective function(pkq) marginalization dxi note case, average contraction) ) agree singleton conditional distribution according brook Lemma], condition suﬃcient) .). furrmore, approximating expectations averages samples), 1x1x) argmin(pkq)  } } argmax log   objective function maximum pseudo-likelihood learning].  Relation Marginal Composite Likelihood combining Type III MKC objective function. ), contraction operator constructed marginal grafting operation. specifically, contraction operators constructed marginal grafting index subsets   Lemma expand Type III minimum contraction objective function: argmin(pkq) } argmin(pai (xai )kqai (xai  ) argmax pai (xai log qai (xai )dxai argmax log qai (xai   step, maximizes log likelihood set marginal distributions training data, corresponds objective function marginal composite likelihood]. with resulting objective maximum marginal likelihood type likelihood learning]. Discussions work, based information geometric view parameter learning, minimum contraction unifying principle non \\x0cparameter learning, showing objective functions existing nonml parameter learning methods understood instantiations principle contraction operators. directions extend current work. first, proposed minimum contraction framework furr generalized general -divergence], divergence special case. with general framework, hope reveal furr relations types non learning objectives]. second, current work, focused idealization parametric learning matching probability distributions. practice, learning performed finite data set unknown underlying distribution. cases, asymptotic properties estimation data volume increases, consistency, essential. while non learning methods covered work shown consistent individually, unification based minimum contraction provide general condition asymptotic properties. last, understanding existing non learning objectives minimizing contraction principled approach devise non learning methods, seeking contraction operators, combinations existing contraction operators. acknowledgement author Jascha sohl-dickstein, Michael DeWeese Michael Gutmann helpful discussions early version work. this work supported National Science Foundation CAREER Award Grant. 0953373.',\n",
       " 'PP4469': 'with advent crowdsourcing services cheap effective dataset labeled multiple annotators short amount time. methods proposed estimate consensus labels correcting bias annotators kinds expertise. low quality annotators spammers?annotators assign labels randomly., instance). spammers make cost acquiring labels expensive potentially degrade quality consensus labels. paper formalize notion spammer define score rank annotators?with spammers score close good annotators high score close one. spammers crowdsourced labeling tasks Annotating unlabeled dataset bottlenecks supervised learning build good predictive models. dataset labeled experts expensive time consuming. advent crowdsourcing services (amazon Mechanical Turk \\x0cbeing prime example) easy inexpensive acquire labels large number annotators short amount time (see] computer vision natural language processing case studies). drawback crowdsourcing services tight control quality annotators. annotators diverse pool including genuine experts, novices, biased annotators, malicious annotators, spammers. order good quality labels requestors typically instance labeled multiple annotators multiple annotations consolidated eir simple majority voting sophisticated methods model correct annotator biases] and task complexity]. paper interested ranking annotators based spammer annotator. context spammer low quality annotator assigns random labels (maybe annotator understand labeling criteria, instances labeling, bot pretending human annotator). spammers significantly increase cost acquiring annotations (since paid) time decrease accuracy final consensus labels. mechanism detect eliminate spammers desirable feature crowdsourcing market place. give monetary bonuses good annotators deny payments spammers. main contribution paper formalize notion spammer binary, categorical, ordinal labeling tasks. specifically define scalar metric rank annotators?with spammers score close good annotators score close (see Figure). summarize multiple parameters annotator single score indicative spammer annotator. spammer score implicit binary labels earlier works, extension categorical ordinal labels accuracy computed confusion rate matrix. attempt quantify quality workers based confusion matrix recently made] transformed observed labels posterior soft labels based estimated confusion matrix. obtain similar annotator rankings, differ work score directly defined terms annotator parameters (see details). rest paper organized follows. ease exposition start binary labels extend categorical ordinal labels ). annotator model used, formalize notion spammer, propose score terms annotator model parameters. dwell estimation annotator model parameters. parameters eir estimated directly gold standard iterative algorithms estimate annotator model parameters knowing gold standard]. experimental section obtain rankings annotators proposed spammer scores publicly data domains. spammer score crowdsourced binary labels Annotator model Let yij , label assigned ith instance annotator, , actual (unobserved) binary label. model accuracy annotator separately positive negative examples. true label one, sensitivity (true positive rate) annotator defined probability annotator labels one. [yij]. hand, true label zero, specificity?false positive rate) defined probability annotator labels zero.  [yij]. extensions basic model proposed include item level diﬃculty] model annotator performance based feature vector]. simplicity basic model proposed] formulation. based instances labeled multiple annotators maximum likelihood estimator annotator parameters consensus ground truth estimated iteratively, Expectation Maximization) algorithm. algorithm iteratively establishes gold standard (initialized majority voting), measures performance annotators gold standard-step), refines gold standard based performance measures-step). spammer? intuitively, spammer assigns labels randomly?maybe annotator understand labeling criteria, instances labeling, bot pretending human annotator. precisely annotator spammer probability observed label yij true label independent true label[yij[yij]. means annotator assigning labels randomly ﬂipping coin bias data. equivalently) written[yij[yij implies   [yij) Hence context annotator model defined earlier perfect spammer annotator   corresponds diagonal line Receiver Operating Characteristic (roc) plot (see Figure))   annotators lies diagonal line malicious annotator ﬂips labels. note malicious annotator discriminatory power detect ﬂip labels. fact methods proposed, automatically ﬂip labels malicious annotators. define spammer score annotator  ) annotator spammer close zero. good annotators perfect annotator one commonly strategy filter spammers inject items annotations labels. strategy CrowdFlower (http://crowdﬂower.com/docs/gold). also note  equal area shown plot considered non-parametric approximation area ROC curve (auc) based observed point. equal Balanced Classification Rate (bcr). spammer defined BCR AUC equal. equal accuracy contours (prevalence Good Annotators Biased Annotators Sensitivity Sensitivity Spammers         malicious Annotators?specificity            Biased Annotators   Area          Equal spammer score contours   sensitivity     ) Binary annotator model   ?specificity ) Accuracy  ?specificity   ) Spammer score Figure) For binary labels annotator modeled his/her sensitivity specificity. perfect spammer lies diagonal line ROC plot. ) Contours equal accuracy) equal spammer score). accuracy This notion spammer accuracy annotator. annotator high accuracy good annotator low accuracy necessarily spammer. accuracy computed Accuracyj[yij[yij   prevalence positive class. note accuracy depends prevalence. proposed spammer score depend prevalence essentially quantifies annotator inherent discriminatory power. figure) shows contours equal accuracy ROC plot. note annotators diagonal line (malicious annotators) low accuracy. malicious annotators good annotators ﬂip labels spammers detect correct ﬂipping. fact algorithms, correctly ﬂip labels malicious annotators treated spammers. figure) shows contours equal score proposed score malicious annotators high score annotators diagonal low score (spammers). log-odds Anor interpretation spammer log odds. bayes? rule posterior log-odds written log—yij—yij log[yij[yij log  essentially annotator annotator spammer) holds) log—yij log information updating posterior log-odds contribute estimation actual true label. spammer score categorical labels Annotator model Suppose categories. introduce multinomial parameter (?jc1 ?jck annotator, ?jck[yij  ?jck ?jck term denotes probability annotator assigns class instance true class When ?j11 ?j00 sensitivity specificity, respectively. spammer? earlier spammer assigns labels randomly[yij[yij. this equivalent[yij[yij,    means knowing true class label change probability annotator assigned label. annotator spammer ?jck ?jc0,   ) Let confusion rate matrix entries spammer rows equal, example, class categorical annotation problem. essentially rank matrix form¿ column vector satisfies column vector ones. binary case natural notion spammer annotator  close zero. natural summarize) terms distance (frobenius norm) confusion matrix closest rank approximation, kaj ) solves arg min kaj .  ) Solving) yields  rows )  ?jc0  spammer annotator close zero. perfect annotator  normalize score lie  ?jc0  When equivalent score proposed earlier binary labels. earlier notion spammer accuracy computed confusion rate matrix prevalence. accuracy computed Accuracyj[yij[yij ?jkk]. spammer score ordinal labels commonly paradigm annotate instances ordinal scales annotator asked rate instance ordinal scale,   }. example, rating restaurant scale assessing malignancy lesion BIRADS scale mammography. differs categorical labels order multiple class labels. ordinal variable expresses rank implicit ordering annotator model conceptually easier true label binary, }. mammography lesion eir malignant) benign) (which confirmed biopsy) BIRADS ordinal scale means radiologist quantify uncertainty based digital mammogram. radiologist assigns higher label/she thinks true label closer one. earlier characterize annotator sensitivity specificity, main difference define sensitivity specificity ordinal label threshold) ,   }.  sensitivity specificity annotator threshold[yij [yij]. definition. annotator Note corresponds parameterized set parameters    empirical ROC curve annotator (figure). who spammer? earlier define an1 notator spammer[yij   note??  annotation model[yij   implies annotator spam0 mer    leads.  means point  lies?specificity diagonal line ROC plot shown Figure area empirical ROC curve compk Figure Ordinal labels: annotator modputed (see Figure AUCj eled sensitivity/specificity threshold.   define spammer score (2aucj  rank annotators.     sensitivity With levels expression defaults binary case. annotator spammer close zero. good annotators perfect annotator  previous work Recently Ipeirotis. ] proposed score categorical labels based expected cost posterior label. section brieﬂy describe approach compare proposed score. instance labeled annotator compute posterior (soft) label—yij yij label assigned ith instance annotator true unknown label. posterior label computed bayes? rule—yij [yij (?jck  prevalence class score spammer based intuition posterior label vector—yij   —yij good annotator probability mass concentrated single class. class problem (with equal prevalence), posterior label vector, (certain class one) good annotator) (complete uncertainty class label) spammer. based define score annotator Score costck  costck misclassification cost instance class classified Essentially capturing sort uncertainty posterior label averaged instances. perfect workers score Scorej spammers high score. entropic version score based similar ideas recently proposed]. proposed spammer score differs approach aspects) Implicit score defined) assumption annotator spammer—yij., estimated posterior labels simply based prevalence depend observed labels. bayes? rule equivalent[yij[yij define spammer score. ) While notions spammer equivalent, approach] computes posterior labels based observed data, class prevalence annotator This follows[yij[(yij and (yij[yij [yij [(yij (yij[yij  [yij   fact[(yij (yij)] simulated 500 instances annotators simulated 500 instances annotators Spammer Score 2422?specificity 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 210 Sensitivity Annotator) Simulation setup) Annotator ranking Annotator rank (median) accuracy simulated 500 instances annotators Annotator rank (median) Ipeirotis. ] simulated 500 instances annotators Annotator rank (median) spammer score) Comparison accuracy Annotator rank (median) spammer score) Comparison Ipeirotis. . ] Figure) simulation setup consisting good annotators (annotators), spammers), malicious annotators). ) ranking annotators obtained proposed spammer score. spammer score ranges lower score, spammy annotator. spammer score% confidence intervals) shown?obtained 100 bootstrap replications. annotators ranked based lower limit. number top bar shows number instances annotated annotator. ) Comparison median rank obtained spammer score rank obtained) accuracy) method proposed Ipeirotis. . ]. parameters computes expected cost. proposed spammer score depend prevalence class. score directly defined terms annotator confusion matrix observed labels. ) For score defined) perfect annotators score clear good baseline spammer. authors suggest compute baseline assuming worker assigns label class maximum prevalence. proposed score natural scale perfect annotator score spammer score ) However advantage approach] directly incorporate varied misclassification costs. experiments Ranking annotators based confidence interval mentioned earlier annotator model parameters estimated iterative algorithms, estimated annotator parameters compute spammer score. spammer score rank annotators. commonly observed phenomenon working crowdsourced data lot annotators label instances. result annotator parameters reliably estimated annotators. order factor uncertainty estimation model parameters compute spammer score 100 bootstrap replications. based compute% confidence intervals) spammer score annotator. rank annotators based lower limit. cis wider Table Datasets number instances. number annotators.  mean/median number annotators instance.  mean/median number instances labeled annotator. dataset Type  bluebird binary 108 108/108 temp binary 462 Brief Description wsd categorical 177 sentiment categorical 1660 291/175 100 Spammer Score 108 108 108 108 108 108 108 108 108 108 108 108 108 108 108 108 108 238 171 654 100 917 104 284 374 249 229 453 346 428 Annotator Annotator 132 360 Spammer Score 175 119 442 462 452 525 437 541 1211 1099 Spammer Score 572 402 valence 100 instances annotators sentiment 1660 instances annotators 350 100 192 190 Annotator temp 462 instances annotators Annotator Annotator Spammer Score 117 100 Spammer Score 108 108 \\x0c108 108 108 108 108 108 108 108 108 108 108 108 108 108 108 wosi instances annotators 108 108 108 108 108 Spammer Score wsd 177 instances annotators 177 157 177 157 bluebird 108 instances annotators word similarity] Numeric judgements word similarity. affect recognition] Each annotator presented short headline asked rate scale [-100,100] denote positive negative valence. ordinal] ordinal[-100 100] word sense disambiguation] labeler paragraph text word ?president? asked label senses. irish economic sentiment analysis] Articles Irish online news sources annotated volunteer users positive, negative, irrelevant. wosi valence bird identification] annotator identify wher Indigo Bunting Blue Grosbeak image. event annotation] Given dialogue pair verbs annotators label wher event verb occurs second. Annotator \\x0cfigure Annotator Rankings rankings obtained datasets Table spammer score ranges lower score, spammy annotator. spammer score% confidence intervals) shown?obtained 100 bootstrap replications. annotators ranked based lower limit. number top bar shows number instances annotated annotator. note CIs wider annotator labels instances. annotator labels instances. crowdsourced labeling task annotator good label reasonable number instances order reliably identified. simulated data illustrate proposed spammer score simulated binary data (with equal prevalence classes) consisting 500 instances labeled annotators varying sensitivity specificity (see Figure) simulation setup). annotators good annotators (annotators lie diagonal Figure)), spammers (annotators lie diagonal), malicious annotators (annotators lie diagonal). figure) plots ranking annotators obtained proposed spammer score annotator model parameters estimated algorithm]. spammer score ranges lower score, spammy annotator. spammer score% confidence interval) obtained bootstrapping shown. annotators ranked based lower limit. spammers (annotators) low spammer score bottom list. malicious annotators higher score spammers correct ﬂipping. malicious annotators good annotators ﬂip labels spammers detect malicious. figure) compares (median) rank obtained spammer score (median) rank obtained accuracy score rank annotators. good annotators ranked high methods accuracy score low rank malicious annotators. accuracy capture notion spammer. figure) compares ranking method proposed Ipeirotis. . ] similar rankings proposed score. 1126 147 2436 299 Annotator rank (median) spammer score bluebird 108 instances annotators 342618 1013 299 Annotator rank (median) spammer score Sensitivity Annotator rank (median) accuracy bluebird 108 instances annotators Annotator rank (median) Ipeirotis. ] bluebird 108 instances annotators?specificity) Figure Comparison rank obtained spammer score rank obtained) accuracy) method proposed Ipeirotis. . ] bluebird binary dataset. ) annotator model parameters estimated algorithm]. 2315 Annotator rank (median) spammer score 1529 sentiment 1660 instances annotators Annotator rank (median) spammer score 2719 2223 1812 Annotator rank (median) spammer score Annotator rank (median) Ipeirotis. ] Annotator rank (median) accuracy Annotator rank (median) accuracy sentiment 1660 instances annotators wsd 177 instances annotators Annotator rank (median) Ipeirotis. ] wsd 177 instances annotators 1632 Annotator rank (median) spammer score Figure Comparison median rank obtained spammer score rank obtained accuracy method proposed Ipeirotis. . ] categorial datasets Table Mechanical Turk data report results publicly linguistic image annotation data collected amazon Mechanical Turk (amt) sources. table summarizes datasets. figure plots spammer scores rankings obtained. % obtained bootstrapping shown. number top bar shows number instances annotated annotator. rankings based lower limit% factors number instances labeled annotator ranking. annotator labels instances wide. annotators label instances high spammer score wide ranked lower. ideally annotators high score time label lot instances reliablly identify authors] sentiment dataset shared qualitative observations annotators agree rankings. authors made comments Annotator ?quirky annotator lot debate meaning annotation question.  changed labeling strategy process?. proposed score gave low rank annotator. comparison approaches Figure compares proposed ranking rank obtained accuracy method proposed Ipeirotis. . ] binary categorical datasets Table proposed ranking similar obtained Ipeirotis. . ] accuracy capture notion spammer. bluebird dataset annotator (see Figure)) accuracy ranks bottom list proposed score puts middle list. estimated model parameters annotator ﬂips labels (below diagonal Figure)) good annotator. conclusions proposed score rank annotators crowdsourced binary, categorical, ordinal labeling tasks. obtained rankings scores allocate monetary bonuses paid annotators eliminate spammers furr labeling tasks. mechanism rank annotators desirable feature crowdsourcing service. proposed score prior Bayesian approaches consolidate annotations. With advent crowdsourcing services cheap effective dataset labeled multiple annotators short amount time. various methods proposed estimate consensus labels correcting bias annotators kinds expertise. often low quality annotators spammers?annotators assign labels randomly., instance). spammers make cost acquiring labels expensive potentially degrade quality consensus labels. paper formalize notion spammer define score rank annotators?with spammers score close good annotators high score close one. Spammers crowdsourced labeling tasks Annotating unlabeled dataset bottlenecks supervised learning build good predictive models. getting dataset labeled experts expensive time consuming. with advent crowdsourcing services (amazon Mechanical Turk \\x0cbeing prime example) easy inexpensive acquire labels large number annotators short amount time (see] computer vision natural language processing case studies). one drawback crowdsourcing services tight control quality annotators. annotators diverse pool including genuine experts, novices, biased annotators, malicious annotators, spammers. hence order good quality labels requestors typically instance labeled multiple annotators multiple annotations consolidated eir simple majority voting sophisticated methods model correct annotator biases] and task complexity]. paper interested ranking annotators based spammer annotator. context spammer low quality annotator assigns random labels (maybe annotator understand labeling criteria, instances labeling, bot pretending human annotator). spammers significantly increase cost acquiring annotations (since paid) time decrease accuracy final consensus labels. mechanism detect eliminate spammers desirable feature crowdsourcing market place. for give monetary bonuses good annotators deny payments spammers. main contribution paper formalize notion spammer binary, categorical, ordinal labeling tasks. more specifically define scalar metric rank annotators?with spammers score close good annotators score close (see Figure). summarize multiple parameters annotator single score indicative spammer annotator. while spammer score implicit binary labels earlier works, extension categorical ordinal labels accuracy computed confusion rate matrix. attempt quantify quality workers based confusion matrix recently made] transformed observed labels posterior soft labels based estimated confusion matrix. while obtain similar annotator rankings, differ work score directly defined terms annotator parameters (see details). rest paper organized follows. for ease exposition start binary labels extend categorical ordinal labels ). annotator model used, formalize notion spammer, propose score terms annotator model parameters. dwell estimation annotator model parameters. parameters eir estimated directly gold standard iterative algorithms estimate annotator model parameters knowing gold standard]. experimental section obtain rankings annotators proposed spammer scores publicly data domains. Spammer score crowdsourced binary labels Annotator model Let yij , label assigned ith instance annotator, , actual (unobserved) binary label. model accuracy annotator separately positive negative examples. true label one, sensitivity (true positive rate) annotator defined probability annotator labels one. [yij]. hand, true label zero, specificity?false positive rate) defined probability annotator labels zero.  [yij]. extensions basic model proposed include item level diﬃculty] model annotator performance based feature vector]. for simplicity basic model proposed] formulation. based instances labeled multiple annotators maximum likelihood estimator annotator parameters consensus ground truth estimated iteratively, Expectation Maximization) algorithm. algorithm iteratively establishes gold standard (initialized majority voting), measures performance annotators gold standard-step), refines gold standard based performance measures-step). who spammer? intuitively, spammer assigns labels randomly?maybe annotator understand labeling criteria, instances labeling, bot pretending human annotator. more precisely annotator spammer probability observed label yij true label independent true label[yij[yij]. this means annotator assigning labels randomly ﬂipping coin bias data. equivalently) written[yij[yij implies   [yij) Hence context annotator model defined earlier perfect spammer annotator   this corresponds diagonal line Receiver Operating Characteristic (roc) plot (see Figure))   annotators lies diagonal line malicious annotator ﬂips labels. note malicious annotator discriminatory power detect ﬂip labels. fact methods proposed, automatically ﬂip labels malicious annotators. hence define spammer score annotator  ) annotator spammer close zero. good annotators perfect annotator One commonly strategy filter spammers inject items annotations labels. this strategy CrowdFlower (http://crowdﬂower.com/docs/gold). Also note  equal area shown plot considered non-parametric approximation area ROC curve (auc) based observed point. equal Balanced Classification Rate (bcr). spammer defined BCR AUC equal. equal accuracy contours (prevalence Good Annotators Biased Annotators Sensitivity Sensitivity Spammers         malicious Annotators?specificity            Biased Annotators   Area          Equal spammer score contours   sensitivity     ) Binary annotator model   ?specificity ) Accuracy  ?specificity   ) Spammer score Figure) For binary labels annotator modeled his/her sensitivity specificity. perfect spammer lies diagonal line ROC plot. ) Contours equal accuracy) equal spammer score). accuracy This notion spammer accuracy annotator. annotator high accuracy good annotator low accuracy necessarily spammer. accuracy computed Accuracyj[yij[yij   prevalence positive class. note accuracy depends prevalence. our proposed spammer score depend prevalence essentially quantifies annotator inherent discriminatory power. figure) shows contours equal accuracy ROC plot. note annotators diagonal line (malicious annotators) low accuracy. malicious annotators good annotators ﬂip labels spammers detect correct ﬂipping. fact algorithms, correctly ﬂip labels malicious annotators treated spammers. figure) shows contours equal score proposed score malicious annotators high score annotators diagonal low score (spammers). log-odds Anor interpretation spammer log odds. using bayes? rule posterior log-odds written log—yij—yij log[yij[yij log  essentially annotator annotator spammer) holds) log—yij log information updating posterior log-odds contribute estimation actual true label. Spammer score categorical labels Annotator model Suppose categories. introduce multinomial parameter (?jc1 ?jck annotator, ?jck[yij  ?jck ?jck term denotes probability annotator assigns class instance true class When ?j11 ?j00 sensitivity specificity, respectively. who spammer? earlier spammer assigns labels randomly[yij[yij. This equivalent[yij[yij,    means knowing true class label change probability annotator assigned label. this annotator spammer ?jck ?jc0,   ) Let confusion rate matrix entries spammer rows equal, example, class categorical annotation problem. essentially rank matrix form¿ column vector satisfies column vector ones. binary case natural notion spammer annotator  close zero. one natural summarize) terms distance (frobenius norm) confusion matrix closest rank approximation, kaj ) solves arg min kaj .  ) Solving) yields  rows )  ?jc0  spammer annotator close zero. perfect annotator  normalize score lie  ?jc0  When equivalent score proposed earlier binary labels. earlier notion spammer accuracy computed confusion rate matrix prevalence. accuracy computed Accuracyj[yij[yij ?jkk]. Spammer score ordinal labels commonly paradigm annotate instances ordinal scales annotator asked rate instance ordinal scale,   }. for example, rating restaurant scale assessing malignancy lesion BIRADS scale mammography. this differs categorical labels order multiple class labels. ordinal variable expresses rank implicit ordering Annotator model conceptually easier true label binary, }. for mammography lesion eir malignant) benign) (which confirmed biopsy) BIRADS ordinal scale means radiologist quantify uncertainty based digital mammogram. radiologist assigns higher label/she thinks true label closer one. earlier characterize each annotator sensitivity specificity, main difference define sensitivity specificity ordinal label threshold) ,   }. let sensitivity specificity annotator threshold[yij [yij]. definition. hence annotator Note this corresponds parameterized set parameters    empirical ROC curve annotator (figure). Who spammer? earlier define an1 notator spammer[yij   Note??  annotation model[yij   this implies annotator spam0 mer    leads. this means point  lies?specificity diagonal line ROC plot shown Figure area empirical ROC curve compk Figure Ordinal labels: annotator modputed (see Figure AUCj eled sensitivity/specificity threshold.   define spammer score (2aucj  rank annotators.     sensitivity With levels expression defaults binary case. annotator spammer close zero. good annotators perfect annotator  Previous work Recently Ipeirotis. ] proposed score categorical labels based expected cost posterior label. section brieﬂy describe approach compare proposed score. for instance labeled annotator compute posterior (soft) label—yij yij label assigned ith instance annotator true unknown label. posterior label computed bayes? rule—yij [yij (?jck  prevalence class score spammer based intuition posterior label vector—yij   —yij good annotator probability mass concentrated single class. for class problem (with equal prevalence), posterior label vector, (certain class one) good annotator) (complete uncertainty class label) spammer. based define score annotator Score costck  costck misclassification cost instance class classified Essentially capturing sort uncertainty posterior label averaged instances. perfect workers score Scorej spammers high score. entropic version score based similar ideas recently proposed]. our proposed spammer score differs approach aspects) Implicit score defined) assumption annotator spammer—yij., estimated posterior labels simply based prevalence depend observed labels. bayes? rule equivalent[yij[yij define spammer score. ) While notions spammer equivalent, approach] computes posterior labels based observed data, class prevalence annotator This follows[yij[(yij AND (yij[yij [yij [(yij (yij[yij  [yij   here fact[(yij (yij)] simulated 500 instances annotators simulated 500 instances annotators Spammer Score 2422?specificity 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 210 Sensitivity Annotator) Simulation setup) Annotator ranking Annotator rank (median) accuracy simulated 500 instances annotators Annotator rank (median) Ipeirotis. ] simulated 500 instances annotators Annotator rank (median) spammer score) Comparison accuracy Annotator rank (median) spammer score) Comparison Ipeirotis. . ] Figure) simulation setup consisting good annotators (annotators), spammers), malicious annotators). ) ranking annotators obtained proposed spammer score. spammer score ranges lower score, spammy annotator. spammer score% confidence intervals) shown?obtained 100 bootstrap replications. annotators ranked based lower limit. number top bar shows number instances annotated annotator. ) Comparison median rank obtained spammer score rank obtained) accuracy) method proposed Ipeirotis. . ]. parameters computes expected cost. our proposed spammer score depend prevalence class. our score directly defined terms annotator confusion matrix observed labels. ) For score defined) perfect annotators score clear good baseline spammer. authors suggest compute baseline assuming worker assigns label class maximum prevalence. our proposed score natural scale perfect annotator score spammer score ) However advantage approach] directly incorporate varied misclassification costs. Experiments Ranking annotators based confidence interval mentioned earlier annotator model parameters estimated iterative algorithms, estimated annotator parameters compute spammer score. spammer score rank annotators. however commonly observed phenomenon working crowdsourced data lot annotators label instances. result annotator parameters reliably estimated annotators. order factor uncertainty estimation model parameters compute spammer score 100 bootstrap replications. based compute% confidence intervals) spammer score annotator. rank annotators based lower limit. CIs wider Table Datasets number instances. number annotators.  mean/median number annotators instance.  mean/median number instances labeled annotator. dataset Type  bluebird binary 108 108/108 temp binary 462 Brief Description wsd categorical 177 sentiment categorical 1660 291/175 100 Spammer Score 108 108 108 108 108 108 108 108 108 108 108 108 108 108 108 108 108 238 171 654 100 917 104 284 374 249 229 453 346 428 Annotator Annotator 132 360 Spammer Score 175 119 442 462 452 525 437 541 1211 1099 Spammer Score 572 402 valence 100 instances annotators sentiment 1660 instances annotators 350 100 192 190 Annotator temp 462 instances annotators Annotator Annotator Spammer Score 117 100 Spammer Score 108 108 \\x0c108 108 108 108 108 108 108 108 108 108 108 108 108 108 108 wosi instances annotators 108 108 108 108 108 Spammer Score wsd 177 instances annotators 177 157 177 157 bluebird 108 instances annotators word similarity] Numeric judgements word similarity. affect recognition] Each annotator presented short headline asked rate scale [-100,100] denote positive negative valence. ordinal] ordinal[-100 100] word sense disambiguation] labeler paragraph text word ?president? asked label senses. irish economic sentiment analysis] Articles Irish online news sources annotated volunteer users positive, negative, irrelevant. wosi valence bird identification] annotator identify wher Indigo Bunting Blue Grosbeak image. event annotation] Given dialogue pair verbs annotators label wher event verb occurs second. Annotator \\x0cfigure Annotator Rankings rankings obtained datasets Table spammer score ranges lower score, spammy annotator. spammer score% confidence intervals) shown?obtained 100 bootstrap replications. annotators ranked based lower limit. number top bar shows number instances annotated annotator. note CIs wider annotator labels instances. annotator labels instances. for crowdsourced labeling task annotator good label reasonable number instances order reliably identified. simulated data illustrate proposed spammer score simulated binary data (with equal prevalence classes) consisting 500 instances labeled annotators varying sensitivity specificity (see Figure) simulation setup). annotators good annotators (annotators lie diagonal Figure)), spammers (annotators lie diagonal), malicious annotators (annotators lie diagonal). figure) plots ranking annotators obtained proposed spammer score annotator model parameters estimated algorithm]. spammer score ranges lower score, spammy annotator. spammer score% confidence interval) obtained bootstrapping shown. annotators ranked based lower limit. spammers (annotators) low spammer score bottom list. malicious annotators higher score spammers correct ﬂipping. malicious annotators good annotators ﬂip labels spammers detect malicious. figure) compares (median) rank obtained spammer score (median) rank obtained accuracy score rank annotators. while good annotators ranked high methods accuracy score low rank malicious annotators. accuracy capture notion spammer. figure) compares ranking method proposed Ipeirotis. . ] similar rankings proposed score. 1126 147 2436 299 Annotator rank (median) spammer score bluebird 108 instances annotators 342618 1013 299 Annotator rank (median) spammer score Sensitivity Annotator rank (median) accuracy bluebird 108 instances annotators Annotator rank (median) Ipeirotis. ] bluebird 108 instances annotators?specificity) Figure Comparison rank obtained spammer score rank obtained) accuracy) method proposed Ipeirotis. . ] bluebird binary dataset. ) annotator model parameters estimated algorithm].  2315 Annotator rank (median) spammer score 1529 sentiment 1660 instances annotators Annotator rank (median) spammer score 2719 2223 1812 Annotator rank (median) spammer score Annotator rank (median) Ipeirotis. ] Annotator rank (median) accuracy Annotator rank (median) accuracy sentiment 1660 instances annotators wsd 177 instances annotators Annotator rank (median) Ipeirotis. ] wsd 177 instances annotators 1632 Annotator rank (median) spammer score Figure Comparison median rank obtained spammer score rank obtained accuracy method proposed Ipeirotis. . ] categorial datasets Table Mechanical Turk data report results publicly linguistic image annotation data collected amazon Mechanical Turk (amt) sources. table summarizes datasets. figure plots spammer scores rankings obtained. % obtained bootstrapping shown. number top bar shows number instances annotated annotator. rankings based lower limit% factors number instances labeled annotator ranking. annotator labels instances wide. some annotators label instances high spammer score wide ranked lower. ideally annotators high score time label lot instances reliablly identify authors] sentiment dataset shared qualitative observations annotators agree rankings. for authors made comments Annotator ?quirky annotator lot debate meaning annotation question.  changed labeling strategy process?. our proposed score gave low rank annotator. comparison approaches Figure compares proposed ranking rank obtained accuracy method proposed Ipeirotis. . ] binary categorical datasets Table our proposed ranking similar obtained Ipeirotis. . ] accuracy capture notion spammer. for bluebird dataset annotator (see Figure)) accuracy ranks bottom list proposed score puts middle list. from estimated model parameters annotator ﬂips labels (below diagonal Figure)) good annotator. Conclusions proposed score rank annotators crowdsourced binary, categorical, ordinal labeling tasks. obtained rankings scores allocate monetary bonuses paid annotators eliminate spammers furr labeling tasks. mechanism rank annotators desirable feature crowdsourcing service. proposed score prior Bayesian approaches consolidate annotations.',\n",
       " 'PP4532': 'online social networks users follow streams posts generated hundreds friends acquaintances. users? friends generate overwhelming volumes information cope ?information overload? organize personal social networks. main mechanisms users social networking sites organize networks content generated categorize friends refer social circles. practically major social networks provide functionality, example, ?circles? google+, ?lists? facebook twitter. user creates circles, content filtering. filter status updates posted distant acquaintances), privacy. hide personal information coworkers), sharing groups users ors follow. currently, users facebook, google+ Twitter identify circles eir manually?  fashion identifying friends sharing \\x0ccommon attribute. neir approach satisfactory: time consuming update automatically user adds friends, fails capture individual aspects users? communities, function poorly profile information missing withheld. paper study problem automatically discovering users? social circles. particular, single user personal social network, goal identify circles, subset friends. circles userspecific user organizes personal network friends independently users connected. means formulate problem circle detection clustering problem ego-network, network friendships friends. figure single user form network friends refer user ego nodes alters. task identify circles alter belongs, Figure words, goal find nested overlapping communities/clusters ego-network. generally, sources data task. set edges ego-network. expect circles formed densely-connected sets alters]. figure ego-network labeled circles. network shows typical behavior observe data: Approximately% ground-truth circles (from facebook) contained completely anor circle% overlap anor circle% circles members common circle. goal discover circles network ego friends. aim discover circle memberships find common properties circles form. however, circles overlap heavily., alters belong multiple circles simultaneously], circles hierarchically nested larger (figure). important model alter memberships multiple circles. secondly, expect circle densely connected members share common properties traits]. explicitly model dimensions user profiles circle emerges. model circle aﬃliations latent variables, similarity alters function common profile information. propose unsupervised method learn dimensions profile similarity lead densely linked circles. model innovations: first, contrast mixedmembership models] predict hard assignment node multiple circles, proves critical good performance. second, proposing parameterized definition profile similarity, learn dimensions similarity links emerge. extends notion homophily] allowing circles form social dimensions, idea related concept Blau spaces]. achieve allowing circle definition profile similarity, circle form friends school, anor friends location. learn model simultaneously choosing node circle memberships profile similarity functions explain observed data. introduce dataset,143 ego-networks facebook, google+, twitter, obtain hand \\x0clabeled ground-truth,636 circles Experimental results show simultaneously social network structure user profile information method performs significantly natural alternatives current state--art. accurate method generate automatic explanations nodes belong common communities. method completely unsupervised, automatically determine number circles circles mselves. furr Related work. topic-modeling techniques uncover ?mixedmemberships? nodes multiple groups], extensions entities attributed text information]. classical algorithms tend identify communities based node features] graph structure], rarely concert. work related] sense performs clustering social-network data], models memberships multiple communities. finally, works model network data similar], underlying models form communities. see, problem unique characteristics require model. extended version article appears]. generative Model Friendships Social Circles desire model circle formation properties) Nodes circles common properties, ?aspects?. ) Different circles formed aspects. circle formed family members, anor students attended university. ) Circles allowed overlap, ?stronger? circles allowed form ?weaker? ones. circle friends degree program form circle http://snap.stanford.edu/data/ university, Figure ) leverage profile information network structure order identify circles. ideally pinpoint aspects profile caused circle form, model interpretable user. input model ego-network), ?profiles? user  ?center? node ego-network ?ego?) included rar consists friends ?alters?). define ego-network precisely creators circles mselves circles. ego-network, goal predict set circles    parameter vectors encode circle emerged. encode ?user profiles? pairwise features , capture properties users common. describe model, applied arbitrary feature vectors ), Section describe ways construct feature vectors , suited application. describe model social circles treats circle memberships latent variables. nodes common circle opportunity form edge, naturally leads hierarchical overlapping circles. devise unsupervised algorithm jointly \\x0coptimize latent variables profile similarity parameters explain observed network data. model social circles defined follows. ego-network set circles   model probability pair nodes,  form edge,  exp ),  ), ) } circles nodes circles For circle profile similarity parameter learn. idea ), high nodes belong low eir trades-off effects). feature vector , encodes similarity profiles users parameter vector encodes dimensions profile similarity caused circle form, nodes circle ?look similar?  edges, generated independently, write probability      set model parameters. defining shorthand notation)      ) ), write log-likelihood ; ) log )  next, describe optimize node circle memberships parameters user profile similarity functions    graph user profiles. unsupervised Learning Model Parameters  {?, Treating circles latent variables, aim find  maximize regularized log-likelihood. .,  argmax ; ??(?).  solve problem coordinate ascent  argmax) argmax ; ??(?). ) noting. concave optimize. gradient ascent, partial derivatives   )        )    ),   For fixed note solving argmaxci ; expressed pseudo-boolean optimization pairwise graphical model., written argmax)  ),  )).  words, edges high weight(under edges low weight defining) ?cci) ), energy. , )  ), log eok)?   log eok)?   ) ), log eok?  log eok?    expressing problem form draw existing work pseudo-boolean optimization. publicly-available ?qpbo? software], accurately approximate problems form shown. ). solve. random order. optimization steps. . repeated convergence  regularize. norm., ?(?)  leads sparse (and readily interpretable) parameters. ego-networks naturally small, algorithm readily handle problems scale required. case facebook, average ego-network 190 nodes], largest network encountered,964 nodes. note method unsupervised, \\x0cinference performed independently ego-network. means method run full Facebook graph (for example), circles independently detected user, ego-networks typically hundreds nodes. hyperparameter estimation. choose optimal number circles, choose minimize approximation Bayesian Information Criterion (bic], argmin BIC) set parameters predicted number munities BIC log—. ) regularization parameter  , 100} determined leave-one-out cross validation, experience significantly impact performance. dataset Description Our goal evaluate unsupervised method ground-truth data. expended significant time, effort, resources obtain high quality handlabeled data obtain ego-networks ground-truth major social networking sites: facebook, google+, twitter. Facebook obtained profile network data ego-networks, consisting 193 circles,039 users. developed Facebook application conducted survey ten users, asked manually identify circles friends belonged. average, users identified circles ego-networks, average circle size friends. examples circles include students common universities, sports teams, relatives, etc. http://snap.stanford.edu/data/ ?rst Alan Turing position company work type education type ?rst Dilly Knox position company position work education company type Cryptanalyst Cambridge College  \\x0cprinceton Graduate School Cryptanalyst Cryptanalyst Royal Navy  Cambridge College 203first Dilly 607last Knox 607first Alan 607last Turing 617work position Cryptanalyst 617work location 607work location Royal Navy 617education Cambridge 617education type College education Princeton education type Graduate School 607last 617work position 617work location 415education education type Figure Feature construction. profiles tree-structured, construct features comparing paths trees. examples trees users (blue) (pink) shown left. schemes constructing feature vectors profiles shown right) (top right) construct binary indicators measuring difference leaves trees. ?work?position?cryptanalyst? appears trees. ) (bottom right) sum leaf nodes scheme, maintaining fact users worked institution, discarding identity institution. datasets obtained publicly accessible data. google+ obtained data 133 ego-networks, consisting 479 circles 106,674 users. 133 ego-networks represent 133 google+ users shared circles, network information publicly accessible time crawl. google+ circles facebook, sense creators chosen release publicly, google+ directed network (note model naturally applied directed undirected networks). example, circle candidates 2012 republican primary, follow followers. finally, Twitter obtained data,000 ego-networks, consisting,869 circles ?lists? ,362 users. ego-networks obtained range size,964 nodes. toger data,143 egonetworks,541 circles, 192,075 users. size differences datasets simply reﬂects availability data sources. Facebook data fully labeled, sense obtain circle user considers cohesive community, google+ Twitter data partially labeled, sense access public circles. design evaluation procedure Section partial labels issues. constructing Features User Profiles Profile information datasets represented tree level encodes increasingly specific information (figure left). google+ collect data categories (gender, name, job titles, institutions, universities, places lived). Facebook collect data categories, including hometowns, birthdays, colleagues, political aﬃliations, etc. twitter, choices exist proxies user profiles; simply collect \\x0cdata categories, set hashtags mentions user two-weeks? worth tweets. ?categories? correspond parents leaf nodes profile tree, shown Figure describe difference vector encode relationship profiles. non-technical description Figure suppose users profile tree leaf tree. define difference vector users binary indicator encoding profile aspects users differ (figure top right]   )). ) Note feature descriptors defined ego-network: thousands high schools (for example) exist Facebook users, small number user friends. difference vector advantage encodes profile information fine granularity, disadvantage high-dimensional,122 dimensions data considered). address form difference vectors based parents leaf nodes: way, encode profile categories users common, disregard specific values (figure bottom right). example, encode hashtags users tweeted common, discard hashtags tweeted?children]. ) This scheme advantage requires constant number dimensions, size ego-network facebook, google+, twitter, above). based difference vectors (and describe construct edge features ). property model members circles common relationships;  ) property model members circles common relationships ego ego-network. case, profile tree ego user define features terms user;    elementwise). parameterizations assess mechanism captures users? subjective definition circle. cases, include constant feature? controls probability edges form circles, equivalently measures extent circles made friends. importantly, predict memberships users profile information, simply due patterns connectivity. define similarly, ?compressed? difference vector   ;  ; ) summarize, identified ways representing compatibility aspects profiles users. considered ways constructing difference vector ways capturing compatibility pair profiles .  )).  Experiments Although method unsupervised, evaluate ground-truth \\x0cdata examining maximum-likelihood assignments latent circles   convergence. goal properly regularized model, latent variables align closely human labeled groundtruth circles   ?  evaluation metrics. measure alignment predicted circle ground-truth compute Balanced Error Rate (ber) circles], ber, circle   — measure assigns equal importance false positives false negatives, — trivial random predictions incur error average. measure preferable loss (for example), assigns extremely low error trivial predictions. report score, find produces qualitatively similar results. aligning predicted ground-truth circles. correspondence compute optimal match linear assignment maximizing: circles max ber) ?dom , number predicted circles— (partial) correspondence circle match   number ground-truth circles—, —, incur penalty additional predictions circles included ground-truth. established techniques estimate number —, circles, baselines suffers disadvantage mispredicting method predict ?trivial? solution returning powerset users. note removing bijectivity requirement., forcing circles aligned allowing multiple predicted circles match single groundtruth circle vice versa) lead qualitatively similar results. Accuracy ber) Accuracy score) Accuracy detected communities Balanced Error rate, higher bet ter multi-assignment clustering (streich, frank.) (yoshida) block-lda (balasubramanyan cohen) low-rank embedding model (friend-friend features. ) model (friend-user features. ) model (compressed features .  model (compressed features . ) Facebook Twitter google+ Accuracy detected communities score, higher better \\x0cmulti-assignment clustering (streich, frank.) low-rank embedding (yoshida) block-lda (balasubramanyan cohen model (friend-friend features. ) model (friend-user features. ) model (compressed features .  model (compressed features . ) Facebook google+ Twitter Figure Performance facebook, google+, twitter, terms Balanced Error Rate (top), score (bottom). higher better. error bars show standard error. improvement features compared nearest competitor significant level better. baselines. considered wide number baseline methods, including network structure, profile information, both. experimented Mixed Membership Stochastic Block Models], network information, variants text attributes]. node, mixedmembership models predict stochastic vector encoding partial circle memberships, threshold generate ?hard? assignments. considered block-lda], generate ?documents? treating aspects user profiles words bag-words model. secondly, experimented classical clustering algorithms-means Hierarchical Clustering], form clusters based node profiles, ignore network. conversely considered Link Clustering] Clique Percolation], network information, ignore profiles. considered low-rank Embedding approach], node attributes edge information projected feature space classical clustering techniques applied. finally considered multi-assignment Clustering], promising predicts hard assignments multiple clusters, network. baselines highlighted report performance best, block-lda] (which slightly outperformed mixed membership stochastic block models]), low-rank Embedding], multi-assignment Clustering]. performance facebook, google+, Twitter data. figure shows results facebook, google+, Twitter data. circles aligned. ), number circles determined Section non-probabilistic baselines, chose maximize modularity]. terms absolute performance model achieves BER scores facebook google Twitter scores, respectively). lower scores \\x0cgoogle+ Twitter explained fact circles maintained initially created: achieve high recall recover friends circle), low precision recover additional friends appeared circle created). comparing method baselines notice outperform baselines datasets statistically significant margin. compared nearest competitors, performing features improve BER% facebook% google% Twitter (improvements terms score similar). performance baseline methods, note good performance depend critically predicting hard memberships multiple circles, combination node edge information; baselines exhibit precisely combination, shortcoming model addresses. features propose (friend-friend features friend-user features perform similarly, revealing schemes ultimately encode similar information, surprising, studied degree speak languages feature index Americans weight feature index weight weight feature index Germans school 1997 studied degree feature index level education feature index college educated people working institute feature index feature index weight living. stanford weight people PhDs weight weight weight Figure Three detected circles small ego-network facebook, compared groundtruth circles (ber). blue nodes: true positives. grey: true negatives. red: false positives. yellow: false negatives. method correctly identifies largest circle (left), sub-circle contained (center), circle significantly overlaps (right). worked employer time feature index Figure Parameter vectors communities Facebook user. top plots show ?complete? features bottom plots show ?compressed? features  cases, BER). features encode fact members community tend speak german, features encode fact speak language. (personally identifiable annotations suppressed.) users friends similar profiles. ?compressed? features  significantly impact performance, promising lower dimension full features; reveals suﬃcient model categories attributes users common. school, town), rar attribute values mselves. found algorithms perform significantly Facebook google+ twitter. explanations: firstly, Facebook data complete, sense survey participants manually labeled circle ego-networks, datasets observe publicly-visible circles-date. secondly, profile categories Facebook informative categories google+, tweet-based profiles build twitter. basic difference lies nature networks mselves: edges Facebook encode mutual ties, edges google+ Twitter encode follower relationships, role circles serve]. points explain algorithms eir edge profile information isolation perform data. qualitative analysis. finally examine output model greater detail. figure shows results method ego-network facebook. colors true-, falsepositives negatives. method correctly identify overlapping circles sub-circles (circles circles). figure shows parameter vectors learned circles Facebook user. positive weights properties users circle common. notice model naturally learns social dimensions lead social circle. moreover, parameter corresponds constant feature? highest weight; reveals membership community strongest signal edges form, profile data weaker (but relevant) signal. acknowledgements. research supported part NSF iis-1016909, cns-1010921, iis-1159679, DARPA xdata, DARPA graphs, Albert Mary Bechmann foundation, boeing, allyes, samsung, intel, Alfred Sloan Fellowship Microsoft Faculty fellowship. Online social networks users follow streams posts generated hundreds friends acquaintances. users? friends generate overwhelming volumes information cope ?information overload? organize personal social networks. one main mechanisms users social networking sites organize networks content generated categorize friends refer social circles. practically major social networks provide functionality, example, ?circles? google+, ?lists? Facebook twitter. once user creates circles, content filtering. filter status updates posted distant acquaintances), privacy. hide personal information coworkers), sharing groups users ors follow. currently, users facebook, google+ Twitter identify circles eir manually?  fashion identifying friends sharing \\x0ccommon attribute. neir approach satisfactory: time consuming update automatically user adds friends, fails capture individual aspects users? communities, function poorly profile information missing withheld. paper study problem automatically discovering users? social circles. particular, single user personal social network, goal identify circles, subset friends. circles userspecific user organizes personal network friends independently users connected. this means formulate problem circle detection clustering problem ego-network, network friendships friends. Figure single user form network friends refer user ego nodes alters. task identify circles alter belongs, Figure words, goal find nested overlapping communities/clusters ego-network. generally, sources data task. set edges ego-network. expect circles formed densely-connected sets alters]. Figure ego-network labeled circles. this network shows typical behavior observe data: Approximately% ground-truth circles (from facebook) contained completely anor circle% overlap anor circle% circles members common circle. goal discover circles network ego friends. aim discover circle memberships find common properties circles form. however, circles overlap heavily., alters belong multiple circles simultaneously], circles hierarchically nested larger (figure). thus important model alter memberships multiple circles. secondly, expect circle densely connected members share common properties traits]. thus explicitly model dimensions user profiles circle emerges. model circle aﬃliations latent variables, similarity alters function common profile information. propose unsupervised method learn dimensions profile similarity lead densely linked circles. our model innovations: first, contrast mixedmembership models] predict hard assignment node multiple circles, proves critical good performance. second, proposing parameterized definition profile similarity, learn dimensions similarity links emerge. this extends notion homophily] allowing circles form social dimensions, idea related concept Blau spaces]. achieve allowing circle definition profile similarity, circle form friends school, anor friends location. learn model simultaneously choosing node circle memberships profile similarity functions explain observed data. introduce dataset,143 ego-networks facebook, google+, twitter, obtain hand \\x0clabeled ground-truth,636 circles Experimental results show simultaneously social network structure user profile information method performs significantly natural alternatives current state--art. besides accurate method generate automatic explanations nodes belong common communities. our method completely unsupervised, automatically determine number circles circles mselves. furr Related work. topic-modeling techniques uncover ?mixedmemberships? nodes multiple groups], extensions entities attributed text information]. classical algorithms tend identify communities based node features] graph structure], rarely concert. our work related] sense performs clustering social-network data], models memberships multiple communities. finally, works model network data similar], underlying models form communities. see, problem unique characteristics require model. extended version article appears]. Generative Model Friendships Social Circles desire model circle formation properties) Nodes circles common properties, ?aspects?. ) Different circles formed aspects. circle formed family members, anor students attended university. ) Circles allowed overlap, ?stronger? circles allowed form ?weaker? ones. circle friends degree program form circle http://snap.stanford.edu/data/ university, Figure ) leverage profile information network structure order identify circles. ideally pinpoint aspects profile caused circle form, model interpretable user. input model ego-network), ?profiles? user  ?center? node ego-network ?ego?) included rar consists friends ?alters?). define ego-network precisely creators circles mselves circles. for ego-network, goal predict set circles    parameter vectors encode circle emerged. encode ?user profiles? pairwise features , capture properties users common. describe model, applied arbitrary feature vectors ), Section describe ways construct feature vectors , suited application. describe model social circles treats circle memberships latent variables. nodes common circle opportunity form edge, naturally leads hierarchical overlapping circles. devise unsupervised algorithm jointly \\x0coptimize latent variables profile similarity parameters explain observed network data. our model social circles defined follows. given ego-network set circles   model probability pair nodes,  form edge,  exp ),  ), ) } circles nodes circles For circle profile similarity parameter learn. idea ), high nodes belong low eir trades-off effects). since feature vector , encodes similarity profiles users parameter vector encodes dimensions profile similarity caused circle form, nodes circle ?look similar?  considering edges, generated independently, write probability      set model parameters. defining shorthand notation)      ) ), write log-likelihood ; ) log )  next, describe optimize node circle memberships parameters user profile similarity functions    graph user profiles. Unsupervised Learning Model Parameters  {?, Treating circles latent variables, aim find  maximize regularized log-likelihood. .,  argmax ; ??(?).  solve problem coordinate ascent  argmax) argmax ; ??(?). ) Noting. concave optimize. gradient ascent, partial derivatives   )        )    ),   For fixed note solving argmaxci ; expressed pseudo-boolean optimization pairwise graphical model., written argmax)  ),  )).  words, edges high weight(under edges low weight defining) ?cci) ), energy. , )  ), log eok)?   log eok)?   ) ), log eok?  log eok?    expressing problem form draw existing work pseudo-boolean optimization. publicly-available ?qpbo? software], accurately approximate problems form shown. ). solve. random order. optimization steps. . repeated convergence  regularize. norm., ?(?)  leads sparse (and readily interpretable) parameters. since ego-networks naturally small, algorithm readily handle problems scale required. case facebook, average ego-network 190 nodes], largest network encountered,964 nodes. note method unsupervised, \\x0cinference performed independently ego-network. this means method run full Facebook graph (for example), circles independently detected user, ego-networks typically hundreds nodes. hyperparameter estimation. choose optimal number circles, choose minimize approximation Bayesian Information Criterion (bic], argmin BIC) set parameters predicted number munities BIC log—. ) regularization parameter  , 100} determined leave-one-out cross validation, experience significantly impact performance. Dataset Description Our goal evaluate unsupervised method ground-truth data. expended significant time, effort, resources obtain high quality handlabeled data obtain ego-networks ground-truth major social networking sites: facebook, google+, twitter. from Facebook obtained profile network data ego-networks, consisting 193 circles,039 users. developed Facebook application conducted survey ten users, asked manually identify circles friends belonged. average, users identified circles ego-networks, average circle size friends. examples circles include students common universities, sports teams, relatives, etc. http://snap.stanford.edu/data/ ?rst Alan Turing position company work type education type ?rst Dilly Knox position company position work education company type Cryptanalyst Cambridge College  \\x0cprinceton Graduate School Cryptanalyst Cryptanalyst Royal Navy  Cambridge College 203first Dilly 607last Knox 607first Alan 607last Turing 617work position Cryptanalyst 617work location 607work location Royal Navy 617education Cambridge 617education type College education Princeton education type Graduate School 607last 617work position 617work location 415education education type Figure Feature construction. profiles tree-structured, construct features comparing paths trees. examples trees users (blue) (pink) shown left. two schemes constructing feature vectors profiles shown right) (top right) construct binary indicators measuring difference leaves trees. ?work?position?cryptanalyst? appears trees. ) (bottom right) sum leaf nodes scheme, maintaining fact users worked institution, discarding identity institution. for datasets obtained publicly accessible data. from google+ obtained data 133 ego-networks, consisting 479 circles 106,674 users. 133 ego-networks represent 133 google+ users shared circles, network information publicly accessible time crawl. google+ circles facebook, sense creators chosen release publicly, google+ directed network (note model naturally applied directed undirected networks). for example, circle candidates 2012 republican primary, follow followers. finally, Twitter obtained data,000 ego-networks, consisting,869 circles ?lists? ,362 users. ego-networks obtained range size,964 nodes. taken toger data,143 egonetworks,541 circles, 192,075 users. size differences datasets simply reﬂects availability data sources. our Facebook data fully labeled, sense obtain circle user considers cohesive community, google+ Twitter data partially labeled, sense access public circles. design evaluation procedure Section partial labels issues. Constructing Features User Profiles Profile information datasets represented tree level encodes increasingly specific information (figure left). from google+ collect data categories (gender, name, job titles, institutions, universities, places lived). from Facebook collect data categories, including hometowns, birthdays, colleagues, political aﬃliations, etc. for twitter, choices exist proxies user profiles; simply collect \\x0cdata categories, set hashtags mentions user two-weeks? worth tweets. ?categories? correspond parents leaf nodes profile tree, shown Figure describe difference vector encode relationship profiles. non-technical description Figure suppose users profile tree leaf tree. define difference vector users binary indicator encoding profile aspects users differ (figure top right]   )). ) Note feature descriptors defined ego-network: thousands high schools (for example) exist Facebook users, small number user friends. although difference vector advantage encodes profile information fine granularity, disadvantage high-dimensional,122 dimensions data considered). one address form difference vectors based parents leaf nodes: way, encode profile categories users common, disregard specific values (figure bottom right). for example, encode hashtags users tweeted common, discard hashtags tweeted?children]. ) This scheme advantage requires constant number dimensions, size ego-network facebook, google+, twitter, above). Based difference vectors (and describe construct edge features ). property model members circles common relationships;  ) property model members circles common relationships ego ego-network. case, profile tree ego user define features terms user;    elementwise). parameterizations assess mechanism captures users? subjective definition circle. cases, include constant feature? controls probability edges form circles, equivalently measures extent circles made friends. importantly, predict memberships users profile information, simply due patterns connectivity. define similarly, ?compressed? difference vector   ;  ; ) summarize, identified ways representing compatibility aspects profiles users. considered ways constructing difference vector ways capturing compatibility pair profiles .  )).  Experiments Although method unsupervised, evaluate ground-truth \\x0cdata examining maximum-likelihood assignments latent circles   convergence. our goal properly regularized model, latent variables align closely human labeled groundtruth circles   ?  evaluation metrics. measure alignment predicted circle ground-truth compute Balanced Error Rate (ber) circles], ber, circle   — this measure assigns equal importance false positives false negatives, — trivial random predictions incur error average. such measure preferable loss (for example), assigns extremely low error trivial predictions. report score, find produces qualitatively similar results. aligning predicted ground-truth circles. since correspondence compute optimal match linear assignment maximizing: circles max ber) ?dom that, number predicted circles— (partial) correspondence circle match   number ground-truth circles—, —, incur penalty additional predictions circles included ground-truth. established techniques estimate number —, circles, baselines suffers disadvantage mispredicting method predict ?trivial? solution returning powerset users. note removing bijectivity requirement., forcing circles aligned allowing multiple predicted circles match single groundtruth circle vice versa) lead qualitatively similar results. Accuracy ber) Accuracy score) Accuracy detected communities Balanced Error rate, higher bet ter multi-assignment clustering (streich, frank.) (yoshida) block-lda (balasubramanyan cohen) low-rank embedding model (friend-friend features. ) model (friend-user features. ) model (compressed features .  model (compressed features . ) Facebook Twitter google+ Accuracy detected communities score, higher better \\x0cmulti-assignment clustering (streich, frank.) low-rank embedding (yoshida) block-lda (balasubramanyan cohen model (friend-friend features. ) model (friend-user features. ) model (compressed features .  model (compressed features . ) Facebook google+ Twitter Figure Performance facebook, google+, twitter, terms Balanced Error Rate (top), score (bottom). higher better. error bars show standard error. improvement features compared nearest competitor significant level better. baselines. considered wide number baseline methods, including network structure, profile information, both. first experimented Mixed Membership Stochastic Block Models], network information, variants text attributes]. for node, mixedmembership models predict stochastic vector encoding partial circle memberships, threshold generate ?hard? assignments. considered block-lda], generate ?documents? treating aspects user profiles words bag-words model. secondly, experimented classical clustering algorithms-means Hierarchical Clustering], form clusters based node profiles, ignore network. conversely considered Link Clustering] Clique Percolation], network information, ignore profiles. considered low-rank Embedding approach], node attributes edge information projected feature space classical clustering techniques applied. finally considered multi-assignment Clustering], promising predicts hard assignments multiple clusters, network. baselines highlighted report performance best, block-lda] (which slightly outperformed mixed membership stochastic block models]), low-rank Embedding], multi-assignment Clustering]. performance facebook, google+, Twitter data. figure shows results facebook, google+, Twitter data. circles aligned. ), number circles determined Section for non-probabilistic baselines, chose maximize modularity]. terms absolute performance model achieves BER scores facebook google Twitter scores, respectively). lower scores \\x0cgoogle+ Twitter explained fact circles maintained initially created: achieve high recall recover friends circle), low precision recover additional friends appeared circle created). comparing method baselines notice outperform baselines datasets statistically significant margin. compared nearest competitors, performing features improve BER% facebook% google% Twitter (improvements terms score similar). regarding performance baseline methods, note good performance depend critically predicting hard memberships multiple circles, combination node edge information; baselines exhibit precisely combination, shortcoming model addresses. both features propose (friend-friend features friend-user features perform similarly, revealing schemes ultimately encode similar information, surprising, studied degree speak languages feature index Americans weight feature index weight weight feature index Germans school 1997 studied degree feature index level education feature index college educated people working institute feature index feature index weight living. Stanford weight people PhDs weight weight weight Figure Three detected circles small ego-network facebook, compared groundtruth circles (ber). blue nodes: true positives. grey: true negatives. red: false positives. yellow: false negatives. our method correctly identifies largest circle (left), sub-circle contained (center), circle significantly overlaps (right). worked employer time feature index Figure Parameter vectors communities Facebook user. top plots show ?complete? features bottom plots show ?compressed? features  cases, BER). for features encode fact members community tend speak german, features encode fact speak language. (personally identifiable annotations suppressed.) users friends similar profiles. using ?compressed? features  significantly impact performance, promising lower dimension full features; reveals suﬃcient model categories attributes users common. school, town), rar attribute values mselves. found algorithms perform significantly Facebook google+ twitter. explanations: firstly, Facebook data complete, sense survey participants manually labeled circle ego-networks, datasets observe publicly-visible circles-date. secondly, profile categories Facebook informative categories google+, tweet-based profiles build twitter. basic difference lies nature networks mselves: edges Facebook encode mutual ties, edges google+ Twitter encode follower relationships, role circles serve]. points explain algorithms eir edge profile information isolation perform data. qualitative analysis. finally examine output model greater detail. figure shows results method ego-network facebook. different colors true-, falsepositives negatives. our method correctly identify overlapping circles sub-circles (circles circles). figure shows parameter vectors learned circles Facebook user. positive weights properties users circle common. notice model naturally learns social dimensions lead social circle. moreover, parameter corresponds constant feature? highest weight; reveals membership community strongest signal edges form, profile data weaker (but relevant) signal. acknowledgements. this research supported part NSF iis-1016909, cns-1010921, iis-1159679, DARPA xdata, DARPA graphs, Albert Mary Bechmann foundation, boeing, allyes, samsung, intel, Alfred Sloan Fellowship Microsoft Faculty fellowship.',\n",
       " 'PP4556': 'nominal goal predictive inference achieve high accuracy. unfortunately, high accuracy price slow computation. practice ?reasonable? tradeoff accuracy speed. definition ?reasonable? varies application. goal optimize system respect user-specified speed/accuracy tradeoff, user-specified data distribution. formalize problem terms learning priority functions generic inference algorithms (section). research natural language processing (nlp) dedicated finding speedups exact approximate computation wide range inference problems including sequence tagging, constituent parsing, dependency parsing, machine translation. speedup strategies literature expressed pruning prioritization heuristics. prioritization heuristics govern order search actions pruning heuristics explicitly dictate wher actions all. examples prioritization include ] Hierarchical ] heuristics, which, case agendabased parsing, prioritize parse actions reduce work maintaining guarantee parse found. alternatively, coarse-fine pruning], classifier-based pruning] beam-width prediction], result faster inference small amount search error tolerated. unfortunately, deciding techniques specific setting diﬃcult: impractical ?try everything.? statistical learning dramatically improved accuracy NLP applications, seek develop statistical learning technology dramatically improve speed maintaining tolerable accuracy. combining reinforcement learning imitation learning methods, develop algorithm successfully learn tradeoff context constituency parsing. paper focuses parsing, expect approach transfer prioritization agenda-based algorithms, machine translation residual belief propagation. give broader discussion setting].  material based work supported National Science Foun dation Grant. 0964681. priority-based Inference Inference algorithms NLP. parsers, taggers, translation systems) broadly artificial intelligence., planners) rely prioritized exploration. concreteness, describe inference context parsing, setting captures essential structure larger family ?deductive inference? problems].  Prioritized Parsing Given probabilistic context-free grammar, approach inferring parse tree sentence build tree bottom dynamic programming, CKY]. prospective constituent built, Viterbi inside score log-probability subparse matches description standard extension CKY algorithm] agenda priority queue constituents built far decide constituent promising extend next, detailed section below. success inference algorithm terms speed accuracy hinge ability prioritize ?good? actions ?bad? actions. context, constituent ?good? leads high accuracy solution, quickly. running Example eir CKY agendabased parser prioritizes Viterbi inside score find highest-scoring parse. achieves percentage accuracy, large grammar experimental conditions Section however, agenda-based parser order magnitude faster CKY (wall clock time) stops finds parse, building furr constituents. mild pruning Viterbi inside score, accuracy remains speed triples. aggressive pruning, accuracy drops speed triples again. goal learn prioritization function satisfies condition. order operationalize approach, define test-time objective function optimize; choose simple linear interpolation accuracy speed: quality accuracy   time) choose reﬂects true preferences. goal encode ?how time spend achieve additional unit accuracy?? paper, simple notion time: number constituents popped from/pushed agenda inference, halting inference parser pops complete parse. optimize expectation) test data, challenges present mselves. first, sequential decision process: parsing decisions made time affect availability goodness future decisions. second, parser total runtime accuracy sentence unknown parsing complete, making instance delayed reward. considerations lead formulate problem Markov Decision Process (mdp), well-studied model decision processes.  Inference Markov Decision Process Markov Decision Process (mdp) formalization memoryless search process. mdp consists state space action space transition function agent MDP observes current state chooses action  environment responds transitioning state sampled transition distribution). agent observes state chooses action. agent policy describes (memoryless) agent chooses action based current state, eir deterministic function state., ) stochastic distribution actions., )). parsing, state full current chart agenda (and astronomically large: roughly 1017 states average sentences). agent controls item (constituent) ?pop? agenda. initial state agenda consisting single-word constituents, empty chart previously popped constituents. actions correspond items agenda. agent chooses pop item environment deterministically adds chart, combines licensed grammar adjacent items chart, places resulting item., maximum log-probability generating tree fringe substring spanning words], (noun phrase) root nonterminal. total log-probability rules tree. agenda. (duplicates chart agenda merged: highest Viterbi inside score kept.) stochasticity initial draw sentence parsed. interested learning deterministic policy pops highest-priority action. thus, learning policy corresponds learning priority function. define priority action state dot product feature vector , weight vector features Section. formally, policy ) arg max   ) admissible policy sense search] guarantee return parse highest Viterbi inside score?but require this, aiming optimize).  Features Prioritized Parsing simple features prioritize constituent. ) Viterbi inside score) constituent touches start sentence) constituent touches end sentence) constituent length; length) constituent sentence length) log(constituent label prev. word POS tag) log(constituent label word POS tag), part-speech (pos) tag arg maxt grammar) features indicating wher constituent {preceding, following, initial} word starts {uppercase, lowercase, number, symbol} character) positive negative punctuation features], placement punctuation marks constituent. log-probability features) inspired work figures merit agenda-based parsing], case punctuation patterns) inspired structure-free parsing]. Reinforcement Learning Reinforcement learning) generic solution solving learning problems delayed reward]. reward function takes state world agent chosen action returns real ?immediate reward? agent receives taking action. general reward function stochastic, case, deterministic, reward function, acc)   time) full parse tree orwise) here, acc) measures accuracy full parse tree popped action (against gold standard) time) user-defined measure time. words, parser completes parsing, receives reward); times, receives reward.  Boltzmann Exploration test time, transition states deterministic: policy chooses action highest priority current state however, training, promote exploration policy space running stochastic policies ). thus, chance popping lower-priority action, find higher-priority. particular, Boltzmann exploration construct stochastic policy Gibbs distribution. policy: exp   ) normalizing constant) ) temp That, log-likelihood action state aﬃne function priority. temperature temp controls amount exploration. temp  approaches deterministic policy); temp  approaches uniform distribution actions. training, temp decreased shift exploration exploitation. trajectory complete sequence state/action/reward triples parsing single sentence. common, denote hs0   where: starting state; chosen agent  drawn environment deterministically case. temperature, weight vector rise distribution trajectories expected total reward: ??? (?  ???   random trajectory chosen policy reward step   Policy Gradient Given features, find parameters yield highest expected reward. carry optimization stochastic gradient ascent algorithm policy gradient]. operates taking steps direction   (?     (?  (? )?? log   (?  log   ) expectation approximated sampling trajectories. requires computing gradient policy decision, which:     ) log  temp Combining) form gradient respect single trajectory. policy gradient algorithm samples trajectory several) current takes gradient step). increases probability actions high-reward trajectories actions low-reward trajectories. running Example baseline system Running Example returns target parse complete parse maximum Viterbi inside score). achieves accuracy (percent recall) speed mpops (million pops) training data. unfortunately, running policy gradient starting point degrades speed accuracy. training practically feasible: pass 100 training sentences (sampling trajectories sentence) takes day.  Analysis One policy gradient performed poorly problem. hyposis fault stochastic gradient descent: optimization problem hard step sizes chosen poorly. address this, attempted experiment added ?cheating? feature model, constituents \\x0cfinal parse, orwise. condition, policy gradient learn near-optimal policy placing high weight cheating feature. alternative hyposis overfitting training data. however, unable achieve significantly higher accuracy evaluating training data?indeed, single train/test sentence. main diﬃculty policy gradient credit assignment: determine actions ?responsible? trajectory reward. causal reasoning, sample trajectories order distinguish actions reliably higher-reward. significant problem, average trajectory length parser word sentence,000 steps, (less%) needed successfully complete parse optimally.  Reward Shaping classic approach attenuating credit assignment problem knowledge domain reward shaping]. goal reward shaping heuristically associate portions total reward specific time steps, favor actions observed reward, assumption caused reward. speed measured number popped items accuracy measured labeled constituent recall first-popped complete parse (compared gold-standard parse), natural shape rewards give penalty time incurred performing action giving positive reward actions build constituents gold parse. correct constituents built make returned tree, correct ?incorrectly? rewarded ors penalizing final action. thus, shaped reward:    ,  ??  ,  pops complete parse (causing parser halt return pops labeled constituent appears gold parse orwise) ), penalizing runtime step. rewards correct constituent. correction , number correct constituents popped chart first-popped parse easy trajectory ending complete parse, total shaped unshaped rewards trajectory equal. (? ?(? )). modify total reward temporal discounting.    discount factor. rewards discounted time, policy gradient      ???     log  (st0 at0  gradient turns equivalent, section], refore gradient equivalent policy gradient.  parser reward?and general, small assigns credit local reward actions closely preceding times. gradient step achieve credit assignment. action good trajectory occurs actions (pops correct constituents), receive credit previously occurring actions. however, occurs actions, receive credit (without additional simulation) wher step actions. running Example reward shaping helps significantly, competitive. parser speeds, training times faster before. setting   achieves accuracy mid mpops. settings achieve higher accuracy. apprenticeship Learning reinforcement learning, agent interacts environment attempts learn maximize reward repeating actions led high reward past. apprenticeship learning, assume access collection trajectories optimal policy attempt learn mimic trajectories. learner goal behave teacher step: notion reward. contrast, related task inverse reinforcement learning/optimal control] attempts infer reward function teacher optimal behavior. algorithms exist apprenticeship learning. work executing inverse reinforcement learning] induce reward function feeding reward function off--shelf reinforcement learning algorithm policy gradient learn approximately optimal agent]. alternatively, directly learn mimic optimal demonstrator, side task induce reward function].  Oracle Actions With teacher guide learning process, explore intelligently Boltzmann exploration, particular, focusing highreward regions policy space. introduce oracle actions guidance areas explore. ideally, oracle actions lead maximum-reward tree. training, identify oracle actions build items maximum likelihood parse consistent gold parse. multiple oracle actions agenda, break ties priority assigned current policy., choose oracle action likes best).  Apprenticeship Learning Classification \\x0cgiven notion oracle actions, straightforward approach policy learning simply train classifier follow oracle popular approach incremental parsing]. indeed, serves initial iteration state--art apprenticeship learning algorithm, DAGGER]. train classifier follows. trajectories generated oracle actions, breaking ties initial policy (viterbi inside score) multiple oracle actions available. trajectories incredibly short (roughly double number words sentence). step trajectory classification generated, action oracle considered correct class actions considered incorrect. classifier train examples maximum entropy classifier, form Boltzmann exploration model)) temperature control. fact, gradient classifier)) identical policy gradient)) distributed differently total reward(? appear: mimicking high-reward trajectories mimic oracle trajectories.  ???       denotes oracle policy oracle action. potential benefit classifier-based approach policy gradient shaped rewards increased credit assignment. policy gradient reward shaping, action credit future reward (though past reward). classifier-based approach, credit wher builds item true parse. running Example classifier-based approach performs marginally policy gradient shaped rewards. accuracy obtain mpops. execute DAGGER algorithm, continue iteration trajectories learned classifier generating classification examples states. unfortunately, computationally feasible due poor quality policy learned iteration. attempting follow learned policy essentially build constituents licensed grammar, prohibitively expensive. remedy section  what Wrong With Apprenticeship Learning obvious practical issue classifier-based approach trains classifier states visited oracle. leads well-known problem unable learn recover past errors]. current feature set depends action state, making action scores independent current state, issue set actions choose depend state. , classifier trained discriminate small set agenda items oracle trajectory (which combinations correct constituents). action sets parser faces test time larger diverse. additional objection classifiers errors \\x0care created equal. incorrect actions expensive ors, create constituents combined locally-attractive ways slow parser result errors. classification problem distinguish incorrect actions. earn algorithm] distinguish explicitly evaluating future reward action (instead teacher) incorporating classification problem. explicit evaluation computationally infeasible setting time step, roll full future trajectory action agenda). policy gradient anor approach observing actions good bad random trajectories, recall found impractical well. furr address problem paper] suggested explicit causality analysis. final issue nature oracle. recall oracle ?supposed? choose optimal actions reward. recall oracle picks correct constituents. contradiction here: oracle action selector ignores tradeoff accuracy speed, focuses accuracy. reasonable setting optimal thing build correct tree building extra constituents. large values optimal else, values learned model hugely negative reward. means apprenticeship learning setting, learn trade accuracy speed: oracle concerned, both! tradeoff appears model remotely close mimicking oracle. oracle-infused Policy Gradient failure standard reinforcement learning algorithms standard apprenticeship learning algorithms problem leads develop approach. start policy gradient algorithm (section) ideas apprenticeship learning improve. formulation preserves reinforcement learning ﬂavor setting, involves delayed reward reward function. approach specifically designed non-deterministic nature agenda-based parsing setting]: action (appears agenda), taken. makes notion ?interleaving? oracle actions policy actions feasible sensible. policy gradient, draw trajectories policy gradient steps favor actions high reward reward shaping. EARN dagger, begin exploring space optimal policy slowly explore. achieve this, define notion oracle-infused policy.  arbitrary policy  ]. define oracle-infused policy follows:    ?)? ) words, choosing action, explores policy space probability  (according current model), probability \\x0cforce oracle action. algorithm takes policy gradient steps reward shaping (eqs)), respect trajectories drawn rar  reduces policy gradient, reward shaping reward    case reduces classifier-based approach  (which turn breaks ties choosing action ?). similar DAGGER earn, stay wean learner oracle supervision starts find good policy imitates classifier well.  .8epoch epoch total number passes made training set point  initial pass). time,  eventually training policy distribution states pass test time policy gradient). intermediate values (and  ), iteration behaves similarly iteration earn, ?rolls out? consequences action chosen randomly) evaluating actions parallel. running Example oracle-infusion competitive speed accuracy tradeoff. typical result mpops. experiments All experiments (including discussed earlier) based Wall Street Journal portion Penn Treebank]. probabilistic context-free grammar 370,396 rules?enough make baseline system accurate slow. obtained latent-variable grammar] split-merge iterations] sections treebank, reserving section learning parameters policy. approaches trading speed accuracy trained section; particular, running Section, 100 sentences words section training test. measure accuracy terms labeled recall (including preterminals) measure speed terms number pops agenda. limitation short sentences purely improved eﬃciency training time.  Baseline Approaches Our baseline approaches trade speed accuracy learning prioritize, varying pruning level constituent pruned Viterbi inside score worse constituent covers substring. baselines are? hierarchical parser] pruning threshold hierarchy level parser heuristic function pruning; (ida iterative deepening algorithm, failure find parse increase aggressive pruning (note traditional meaning ida*); (ctf) default coarse-fine parser Berkeley parser]. algorithms make multiple passes, case runtime (number pops) assessed cumulatively.  Learned Prioritization Approaches \\x0cmodel pops Recall explored variants oracle-infused pola pruning) 1496080 icy gradient  figure shows D686641 result 100 training sentences. ?-? tests I187403 degenerate case apprenticeship 1275292 learning (section), ?+? tests 682540.8epoch recommended section temperature matters ?+? tests temp figure Performance 100 sentences. performed stochastic gradient descent passes data, sampling trajectories row sentence (when trajectories random). classifier-based approaches ?-? perform poorly: training trajectories consist oracle actions, learning severely biased. section oracle actions, suffer large variance training trajectories performance degrades rapidly learning converge days training. ?oracle-infused? compromise ?+? oracle actions: passes data, parser learns make good decisions oracle. pops Change recall pops ida* CTF Recall Figure Pareto frontiers: Our parser values baselines pruning levels. axis variation? tests (delayed reward) ? tests (immediate reward)  note attempts form credit assignment works results intermediate values crudely assigns credit reward (correct constituent) actions closely preceded, agenda-based parser, reward (correct subconstituents) related actions happened earlier].  \\x0cpareto Frontier Our final evaluation held-out test set (length-limited sentences Section). -split grammar trained section used. previous results Table model: reward oracle infusion. investigate trading speed accuracy, learn evaluate policy settings tradeoff parameter: train policy sentences words Section evaluate learned policy held data (from Section). measure accuracy labeled constituent recall evaluate speed terms number pops pushes) performed agenda. figure shows baselines pruning thresholds performance policies trained      agenda pops measure time. times fast unpruned cost drop accuracy-score). thus, achieves accuracy pruned version fast. improves? ida respect speed% pops. coarse-fine parser (ctf) terms speed accuracy, number agenda pops measure speed puts hierarchical baselines disadvantage. ran experiments number agenda pushes accurate measure time, sweeping settings reward shaping crafted agenda pops mind, surprising learning performs poorly setting. still, manage learn trade speed accuracy. drop recall-score), speed factor (from billion pushes billion). note pruning methods employed conjunction learned prioritization. conclusions Future Work paper, considered application reinforcement learning apprenticeship learning prioritize search sensitive user-defined tradeoff speed accuracy. found oracleinfused variant policy gradient algorithm reinforcement learning effective learning fast accurate parser simple set features. addition, uncovered properties problem separate standard learning scenarios, designed experiments determine reasons off--shelf learning algorithms fail. important avenue future work credit assignment. interested designing richer feature sets, including ?dynamic? features depend action state chart agenda. role dynamic features decide halt. parser decide continue working past complete parse, give (returning partial default parse) complete parse found. dand Iapproaches similar. train oracle trajectories actions receive reward simply make oracle actions probable. however, Dtrains aggressively long trajectories) implies weights training action number future actions trajectory. difference interesting trajectory includes non-oracle actions well. nominal goal predictive inference achieve high accuracy. unfortunately, high accuracy price slow computation. practice ?reasonable? tradeoff accuracy speed. but definition ?reasonable? varies application. our goal optimize system respect user-specified speed/accuracy tradeoff, user-specified data distribution. formalize problem terms learning priority functions generic inference algorithms (section). much research natural language processing (nlp) dedicated finding speedups exact approximate computation wide range inference problems including sequence tagging, constituent parsing, dependency parsing, machine translation. many speedup strategies literature expressed pruning prioritization heuristics. prioritization heuristics govern order search actions pruning heuristics explicitly dictate wher actions all. examples prioritization include ] Hierarchical ] heuristics, which, case agendabased parsing, prioritize parse actions reduce work maintaining guarantee parse found. alternatively, coarse-fine pruning], classifier-based pruning] beam-width prediction], result faster inference small amount search error tolerated. unfortunately, deciding techniques specific setting diﬃcult: impractical ?try everything.? statistical learning dramatically improved accuracy NLP applications, seek develop statistical learning technology dramatically improve speed maintaining tolerable accuracy. combining reinforcement learning imitation learning methods, develop algorithm successfully learn tradeoff context constituency parsing. although paper focuses parsing, expect approach transfer prioritization agenda-based algorithms, machine translation residual belief propagation. give broader discussion setting].  this material based work supported National Science Foun dation Grant. 0964681. priority-based Inference Inference algorithms NLP. parsers, taggers, translation systems) broadly artificial intelligence., planners) rely prioritized exploration. for concreteness, describe inference context parsing, setting captures essential structure larger family ?deductive inference? problems].  Prioritized Parsing Given probabilistic context-free grammar, approach inferring parse tree sentence build tree bottom dynamic programming, CKY]. when prospective constituent built, Viterbi inside score log-probability subparse matches description standard extension CKY algorithm] agenda priority queue constituents built far decide constituent promising extend next, detailed section below. success inference algorithm terms speed accuracy hinge ability prioritize ?good? actions ?bad? actions. context, constituent ?good? leads high accuracy solution, quickly. running Example eir CKY agendabased parser prioritizes Viterbi inside score find highest-scoring parse. this achieves percentage accuracy, large grammar experimental conditions Section however, agenda-based parser order magnitude faster CKY (wall clock time) stops finds parse, building furr constituents. with mild pruning Viterbi inside score, accuracy remains speed triples. with aggressive pruning, accuracy drops speed triples again. our goal learn prioritization function satisfies condition. order operationalize approach, define test-time objective function optimize; choose simple linear interpolation accuracy speed: quality accuracy   time) choose reﬂects true preferences. goal encode ?how time spend achieve additional unit accuracy?? paper, simple notion time: number constituents popped from/pushed agenda inference, halting inference parser pops complete parse. when optimize expectation) test data, challenges present mselves. first, sequential decision process: parsing decisions made time affect availability goodness future decisions. second, parser total runtime accuracy sentence unknown parsing complete, making instance delayed reward. considerations lead formulate problem Markov Decision Process (mdp), well-studied model decision processes.  Inference Markov Decision Process Markov Decision Process (mdp) formalization memoryless search process. MDP consists state space action space transition function agent MDP observes current state chooses action  environment responds transitioning state sampled transition distribution). agent observes state chooses action. agent policy describes (memoryless) agent chooses action based current state, eir deterministic function state., ) stochastic distribution actions., )). for parsing, state full current chart agenda (and astronomically large: roughly 1017 states average sentences). agent controls item (constituent) ?pop? agenda. initial state agenda consisting single-word constituents, empty chart previously popped constituents. possible actions correspond items agenda. when agent chooses pop item environment deterministically adds chart, combines licensed grammar adjacent items chart, places resulting item., maximum log-probability generating tree fringe substring spanning words], (noun phrase) root nonterminal. this total log-probability rules tree. agenda. (duplicates chart agenda merged: highest Viterbi inside score kept.) stochasticity initial draw sentence parsed. interested learning deterministic policy pops highest-priority action. thus, learning policy corresponds learning priority function. define priority action state dot product feature vector , weight vector features Section. formally, policy ) arg max   ) admissible policy sense search] guarantee return parse highest Viterbi inside score?but require this, aiming optimize).  Features Prioritized Parsing simple features prioritize constituent. ) Viterbi inside score) constituent touches start sentence) constituent touches end sentence) constituent length; length) constituent sentence length) log(constituent label prev. word POS tag) log(constituent label word POS tag), part-speech (pos) tag arg maxt grammar) features indicating wher constituent {preceding, following, initial} word starts {uppercase, lowercase, number, symbol} character) positive negative punctuation features], placement punctuation marks constituent. log-probability features) inspired work figures merit agenda-based parsing], case punctuation patterns) inspired structure-free parsing]. Reinforcement Learning Reinforcement learning) generic solution solving learning problems delayed reward]. reward function takes state world agent chosen action returns real ?immediate reward? agent receives taking action. general reward function stochastic, case, deterministic, reward function, acc)   time) full parse tree orwise) here, acc) measures accuracy full parse tree popped action (against gold standard) time) user-defined measure time. words, parser completes parsing, receives reward); times, receives reward.  Boltzmann Exploration test time, transition states deterministic: policy chooses action highest priority current state however, training, promote exploration policy space running stochastic policies ). thus, chance popping lower-priority action, find higher-priority. particular, Boltzmann exploration construct stochastic policy Gibbs distribution. our policy: exp   ) normalizing constant) ) temp That, log-likelihood action state aﬃne function priority. temperature temp controls amount exploration. temp  approaches deterministic policy); temp  approaches uniform distribution actions. during training, temp decreased shift exploration exploitation. trajectory complete sequence state/action/reward triples parsing single sentence. common, denote hs0   where: starting state; chosen agent  drawn environment deterministically case. temperature, weight vector rise distribution trajectories expected total reward: ??? (?  ???   random trajectory chosen policy reward step   Policy Gradient Given features, find parameters yield highest expected reward. carry optimization stochastic gradient ascent algorithm policy gradient]. this operates taking steps direction   (?     (?  (? )?? log   (?  log   ) expectation approximated sampling trajectories. requires computing gradient policy decision, which:     ) log  temp Combining) form gradient respect single trajectory. policy gradient algorithm samples trajectory several) current takes gradient step). this increases probability actions high-reward trajectories actions low-reward trajectories. running Example baseline system Running Example returns target parse complete parse maximum Viterbi inside score). this achieves accuracy (percent recall) speed mpops (million pops) training data. unfortunately, running policy gradient starting point degrades speed accuracy. training practically feasible: pass 100 training sentences (sampling trajectories sentence) takes day.  Analysis One policy gradient performed poorly problem. one hyposis fault stochastic gradient descent: optimization problem hard step sizes chosen poorly. address this, attempted experiment added ?cheating? feature model, constituents \\x0cfinal parse, orwise. under condition, policy gradient learn near-optimal policy placing high weight cheating feature. alternative hyposis overfitting training data. however, unable achieve significantly higher accuracy evaluating training data?indeed, single train/test sentence. main diﬃculty policy gradient credit assignment: determine actions ?responsible? trajectory reward. without causal reasoning, sample trajectories order distinguish actions reliably higher-reward. this significant problem, average trajectory length parser word sentence,000 steps, (less%) needed successfully complete parse optimally.  Reward Shaping classic approach attenuating credit assignment problem knowledge domain reward shaping]. goal reward shaping heuristically associate portions total reward specific time steps, favor actions observed reward, assumption caused reward. speed measured number popped items accuracy measured labeled constituent recall first-popped complete parse (compared gold-standard parse), natural shape rewards give penalty time incurred performing action giving positive reward actions build constituents gold parse. since correct constituents built make returned tree, correct ?incorrectly? rewarded ors penalizing final action. thus, shaped reward:    ,  ??  ,  pops complete parse (causing parser halt return pops labeled constituent appears gold parse orwise) ), penalizing runtime step. rewards correct constituent. correction , number correct constituents popped chart first-popped parse easy trajectory ending complete parse, total shaped unshaped rewards trajectory equal. (? ?(? )). modify total reward temporal discounting. let   discount factor. when rewards discounted time, policy gradient      ???     log  (st0 at0 when gradient turns equivalent, section], refore gradient equivalent policy gradient. when parser reward?and general, small assigns credit local reward actions closely preceding times. this gradient step achieve credit assignment. action good trajectory occurs actions (pops correct constituents), receive credit previously occurring actions. however, occurs actions, receive credit (without additional simulation) wher step actions. running Example reward shaping helps significantly, competitive. parser speeds, training times faster before. setting   achieves accuracy mid mpops. settings achieve higher accuracy. Apprenticeship Learning reinforcement learning, agent interacts environment attempts learn maximize reward repeating actions led high reward past. apprenticeship learning, assume access collection trajectories optimal policy attempt learn mimic trajectories. learner goal behave teacher step: notion reward. contrast, related task inverse reinforcement learning/optimal control] attempts infer reward function teacher optimal behavior. many algorithms exist apprenticeship learning. some work executing inverse reinforcement learning] induce reward function feeding reward function off--shelf reinforcement learning algorithm policy gradient learn approximately optimal agent]. alternatively, directly learn mimic optimal demonstrator, side task induce reward function].  Oracle Actions With teacher guide learning process, explore intelligently Boltzmann exploration, particular, focusing highreward regions policy space. introduce oracle actions guidance areas explore. ideally, oracle actions lead maximum-reward tree. training, identify oracle actions build items maximum likelihood parse consistent gold parse. when multiple oracle actions agenda, break ties priority assigned current policy., choose oracle action likes best).  Apprenticeship Learning Classification \\x0cgiven notion oracle actions, straightforward approach policy learning simply train classifier follow oracle popular approach incremental parsing]. indeed, serves initial iteration state--art apprenticeship learning algorithm, DAGGER]. train classifier follows. trajectories generated oracle actions, breaking ties initial policy (viterbi inside score) multiple oracle actions available. trajectories incredibly short (roughly double number words sentence). step trajectory classification generated, action oracle considered correct class actions considered incorrect. classifier train examples maximum entropy classifier, form Boltzmann exploration model)) temperature control. fact, gradient classifier)) identical policy gradient)) distributed differently total reward(? appear: mimicking high-reward trajectories mimic oracle trajectories.  ???       denotes oracle policy oracle action. potential benefit classifier-based approach policy gradient shaped rewards increased credit assignment. policy gradient reward shaping, action credit future reward (though past reward). classifier-based approach, credit wher builds item true parse. running Example classifier-based approach performs marginally policy gradient shaped rewards. accuracy obtain mpops. execute DAGGER algorithm, continue iteration trajectories learned classifier generating classification examples states. unfortunately, computationally feasible due poor quality policy learned iteration. attempting follow learned policy essentially build constituents licensed grammar, prohibitively expensive. remedy section  what Wrong With Apprenticeship Learning obvious practical issue classifier-based approach trains classifier states visited oracle. this leads well-known problem unable learn recover past errors]. even current feature set depends action state, making action scores independent current state, issue set actions choose depend state. that, classifier trained discriminate small set agenda items oracle trajectory (which combinations correct constituents). but action sets parser faces test time larger diverse. additional objection classifiers errors \\x0care created equal. some incorrect actions expensive ors, create constituents combined locally-attractive ways slow parser result errors. our classification problem distinguish incorrect actions. EARN algorithm] distinguish explicitly evaluating future reward action (instead teacher) incorporating classification problem. but explicit evaluation computationally infeasible setting time step, roll full future trajectory action agenda). policy gradient anor approach observing actions good bad random trajectories, recall found impractical well. furr address problem paper] suggested explicit causality analysis. final issue nature oracle. recall oracle ?supposed? choose optimal actions reward. also recall oracle picks correct constituents. contradiction here: oracle action selector ignores tradeoff accuracy speed, focuses accuracy. this reasonable setting optimal thing build correct tree building extra constituents. only large values optimal else, values learned model hugely negative reward. this means apprenticeship learning setting, learn trade accuracy speed: oracle concerned, both! tradeoff appears model remotely close mimicking oracle. oracle-infused Policy Gradient failure standard reinforcement learning algorithms standard apprenticeship learning algorithms problem leads develop approach. start policy gradient algorithm (section) ideas apprenticeship learning improve. our formulation preserves reinforcement learning ﬂavor setting, involves delayed reward reward function. our approach specifically designed non-deterministic nature agenda-based parsing setting]: action (appears agenda), taken. this makes notion ?interleaving? oracle actions policy actions feasible sensible. like policy gradient, draw trajectories policy gradient steps favor actions high reward reward shaping. like EARN dagger, begin exploring space optimal policy slowly explore. achieve this, define notion oracle-infused policy. let arbitrary policy  ]. define oracle-infused policy follows:    ?)? ) words, choosing action, explores policy space probability  (according current model), probability \\x0cforce oracle action. our algorithm takes policy gradient steps reward shaping (eqs)), respect trajectories drawn rar  reduces policy gradient, reward shaping reward  for  case reduces classifier-based approach  (which turn breaks ties choosing action ?). similar DAGGER earn, stay wean learner oracle supervision starts find good policy imitates classifier well.  .8epoch epoch total number passes made training set point  initial pass). over time,  eventually training policy distribution states pass test time policy gradient). with intermediate values (and  ), iteration behaves similarly iteration earn, ?rolls out? consequences action chosen randomly) evaluating actions parallel. running Example oracle-infusion competitive speed accuracy tradeoff. typical result mpops. Experiments All experiments (including discussed earlier) based Wall Street Journal portion Penn Treebank]. probabilistic context-free grammar 370,396 rules?enough make baseline system accurate slow. obtained latent-variable grammar] split-merge iterations] sections treebank, reserving section learning parameters policy. all approaches trading speed accuracy trained section; particular, running Section, 100 sentences words section training test. measure accuracy terms labeled recall (including preterminals) measure speed terms number pops agenda. limitation short sentences purely improved eﬃciency training time.  Baseline Approaches Our baseline approaches trade speed accuracy learning prioritize, varying pruning level constituent pruned Viterbi inside score worse constituent covers substring. our baselines are? Hierarchical parser] pruning threshold hierarchy level parser heuristic function pruning; (ida iterative deepening algorithm, failure find parse increase aggressive pruning (note traditional meaning ida*); (ctf) default coarse-fine parser Berkeley parser]. several algorithms make multiple passes, case runtime (number pops) assessed cumulatively.  Learned Prioritization Approaches \\x0cmodel pops Recall explored variants oracle-infused pola pruning) 1496080 icy gradient  figure shows D686641 result 100 training sentences. ?-? tests I187403 degenerate case apprenticeship 1275292 learning (section), ?+? tests 682540.8epoch recommended section temperature matters ?+? tests temp Figure Performance 100 sentences. performed stochastic gradient descent passes data, sampling trajectories row sentence (when trajectories random). classifier-based approaches ?-? perform poorly: training trajectories consist oracle actions, learning severely biased. yet section oracle actions, suffer large variance training trajectories performance degrades rapidly learning converge days training. our ?oracle-infused? compromise ?+? oracle actions: passes data, parser learns make good decisions oracle. pops Change recall pops ida* CTF Recall Figure Pareto frontiers: Our parser values baselines pruning levels. axis variation? tests (delayed reward) ? tests (immediate reward)  note attempts form credit assignment works results intermediate values crudely assigns credit reward (correct constituent) actions closely preceded, agenda-based parser, reward (correct subconstituents) related actions happened earlier].  \\x0cpareto Frontier Our final evaluation held-out test set (length-limited sentences Section). -split grammar trained section used. given previous results Table model: reward oracle infusion. investigate trading speed accuracy, learn evaluate policy settings tradeoff parameter: train policy sentences words Section evaluate learned policy held data (from Section). measure accuracy labeled constituent recall evaluate speed terms number pops pushes) performed agenda. figure shows baselines pruning thresholds performance policies trained      agenda pops measure time. times fast unpruned cost drop accuracy-score). thus, achieves accuracy pruned version fast. improves? ida respect speed% pops. coarse-fine parser (ctf) terms speed accuracy, number agenda pops measure speed puts hierarchical baselines disadvantage. ran experiments number agenda pushes accurate measure time, sweeping settings since reward shaping crafted agenda pops mind, surprising learning performs poorly setting. still, manage learn trade speed accuracy. with drop recall-score), speed factor (from billion pushes billion). note pruning methods employed conjunction learned prioritization. Conclusions Future Work paper, considered application reinforcement learning apprenticeship learning prioritize search sensitive user-defined tradeoff speed accuracy. found oracleinfused variant policy gradient algorithm reinforcement learning effective learning fast accurate parser simple set features. addition, uncovered properties problem separate standard learning scenarios, designed experiments determine reasons off--shelf learning algorithms fail. important avenue future work credit assignment. interested designing richer feature sets, including ?dynamic? features depend action state chart agenda. one role dynamic features decide halt. parser decide continue working past complete parse, give (returning partial default parse) complete parse found. Dand Iapproaches similar. both train oracle trajectories actions receive reward simply make oracle actions probable. however, Dtrains aggressively long trajectories) implies weights training action number future actions trajectory. difference interesting trajectory includes non-oracle actions well.',\n",
       " 'PP4595': 'neuromorphic systems replicate cognitive processing functions integrated circuits. complexity/size largely determined synapse implementation, synapses significantly numerous neurons]. recent push larger neuromorphic systems higher integration density systems, resulted approaches synapse realization. proposed solutions hand employ nanoscale devices conjuction conventional circuits] hand integrate synaptic functionality (shortand long term plasticity, pulse shaping, etc) small number devices possible. context, memristive devices introduced Chua] recently proposed eﬃcient implementations plastic synapses neuromorphic systems. memristive devices offer possibility actual learning mechanism, synaptic weight storage synaptic weight effect. amplification presynaptic current) device, compared distributed mechanisms conventional circuit implementations]. moreover, high-density passive array top conventional semiconductor chip]. plasticity memristors. resistance change, defined applied waveforms], fed rows columns memristive array CMOS preand postsynaptic neurons]. resembles biological synapses, plasticity triggered mechanisms determined local waveforms]. however, learning memristors approached pragmatic technological level. goal find waveform achieves spiketiming-dependent plasticity (stdp], regard biological veracity waveforms furr important forms plasticity]. bridging gap, make plasticity rule introduced Mayr Partzsch] driven biologically realistic neuron waveforms explains large number experimental observations. adapt model recently introduced BiFeO3 memristive material]. measurement results modified plasticity rule implemented sample device given, exhbiting configurable STDP behaviour pulse triplet] reproduction. materials Methods Local Correlation Plasticity (lcp) LCP rule introduced Mayr Partzsch] combines local waveforms, synaptic conductance) membrane potential). presynaptic activity encoded), determines conductance change due presynaptic spiking. postsynaptic activity turn signaled synapse). lcp rule combines formulation change synaptic weight similar well-known bienenstock-cooper-munroe rule]: ) ) ) equation, denotes voltage threshold weight potentiation depression, set resting potential. note coincident preand postsynaptic activities detected rule multiplication: weight change occurs presynaptic conductance elevated postsynaptic membrane potential rest. waveforms) determined] spike response model], employed neuron model. mayr. waveforms triggered times preand postsynaptic spikes:   ?tpre ?pre   tpost Urefr tpre?tpost  post pre tpre ) tpost tpost) tpost denote preand postsynaptic spike, respectively. presynaptic con? decay time constant ?pre postsynaptic ductance waveform exponential height potential spike defined Dirac pulse integral exponential decay height Urefr membrane time constant ?post ], postsynaptic adaptation realised this decreased nominal postsynaptic pulse occurs shortly anor postsynaptic pulse:   post tpost ?post) time constant exponential decay equation membrane time constant. 1971 Leon Chua postulated existence device current voltage directly controlled voltage ﬂux charge respectively, called memristor. general state space description Chua Kang extended ory cover broad class memristive devices]. terms interchangeably studies, devices study fit strict definition memristor, refer memristive devices following.  100 120 Figure Progression conductance membrane potential synapse weight sample spike pattern. figure shows preand postsynaptic waveforms, synaptic weight sample spike train. simple waveforms, principal weight change mechanisms present: presynaptic side active postsynaptic spike, weight instantaneously increased large elevation membrane potential. contrast, presynaptic activity falling refractoriness period neuron (exponential decay spike) integrates weight decrease. shown], simple model replicate multitude experimental evidence, par advanced (and complex) phenomenological plasticity models available. addition, LCP rule directly links synaptic plasticity preand postsynaptic adaptation processes inﬂuence local waveforms. explain furr experimental results]. sec. , adapt rule equations characteristics memristive device, introduced section.  Memristive Device non-volatile passive analog memory discussed applications neuromorphic systems space limitations analog circuitry. however, recently groups access suﬃcient materials devices. developments field nano material science, decade, opened possibilities creating compact circuit elements unique properties. notably released information-called Memristor] effort put analysis thin film semiconductor-metal-metaloxide compounds. commonly materials class BiFeO3 (bfo). complete conducting mechanisms BFO fully understood yet, partly contradictory results reported literature, confirmed physical effects overlayed dominate states. resistive switching effect promising neuromorphic devices discussed detail. shown, effect unior bipolar highly dependent processing substrate, growth method, doping, etc. ]. bfo grown pulsed laser deposition/sio2 substrate top contact, fig.  memristors fabricated circular top plates, contacted needle probes, continuous bottom plate contacted edge die. bfo films thickness 100nm. created devices show unipolar resistive switching rectifying behavior. positive bias device low resistive state (lrs) stays negative bias applied resets back high resistive state (hrs). state measured inﬂuencing applying low voltage. figure shows voltage-current-diagram characteristics device. measurement consists parts: rising negative voltage applied resets device intermediate level hrs. rising voltage lowers resistance exponentially. figure Photograph fabricated memristive material measurements. falling positive voltage affect resistance anymore relation ohmic. rectifying characteristic current LRS HRS negative voltages exhibit large dynamic range positive voltages. 800 700 600 abs 500 400 300 200 100 ?100 Figure voltage-current diagram device linear log-scale plot Phenomenological Device Model apply LCP model BFO device enable circuit design, simplified device model required. based model framework Chua Kang, output function., current dependent time, state input., voltage recently, widely modeling memristive devices]. contrast memristive device models based sinh function output relationship (following Yang. ]), model BFO device semiconductor junctions. junctions abstractly diode equation: (exp ]. attempt catch basic characteristics, device modeled employing diode equations letting state variable, inﬂuence output roughly represent conductance, I01 (ed1)  i02 )  ) voltage device2 diode equations guarantee crossing hysteresis. parameters I0i individual control current characteristics negative positive voltages, shown previous section rar asymmetric BFO devices. purpose modeling plasticity, focus dynamic behavior conductance change; investigated detail Querlioz. ] served basis model state variable, )  ) With sinh   approach fundamentally sinh function. functions )  relate current state affects state development effect applied voltage, respectively.  ) exponential function.  ?gmin   Gmax ?gmin) Gmax  ) Gmax ?gmin) gmin    favor separate exponential sinh functions increased controllability voltage domains (positive negative). parameters govern voltage dependence state modification, scaling result. speed state saturation set:  ) )    ) For implementation, prominent commercially simulators custom analog mixed-signal integrated circuit design, cadence? spectre?  behavioral current sources, equations, implemented simulated feasibility circuit design. depicted fig. conductance change time, voltages, model (fig. ) measurements (fig. ). exponential dependency device voltage rise levels operation (equations)). saturation conductance change voltage visible (equation)). sharp current model result simplistic approach, real devices show slower transitions. addition, noted real device appears experience significantly steeper rise current. however, target reasonable characteristics region operation relevant plasticity rule experiments.  Volt Volt) Figure Device current applied voltages model) measurement).  Results Modified LCP nonlinearity learning threshold required order carry correlation operation preand postsynaptic waveforms characterizes forms long term learning]. original LCP rule, multiplication preand postsynaptic waveforms. coincident activity results learning. memristive devices operated additive manner. preand postsynaptic waveforms applied terminals device, adding/subtracting voltage curves. order state memristive device affected overlap waveforms, positive negative modification threshold required]. equation, internal voltage driven state change  affected parameters govern thresholds negative positive voltages. devices, work effective modification thresholds Vpre Vpost vpre-vpost 100 120 Figure Modification original LCP rule BFO memristive device, top bottom: preand postsynaptic voltages/waveforms, exponential decay ?pre resp. ?post (postsynaptic waveform plotted inverse illustrate waveform function); resultant voltage difference memristive device memristance modification thresholds (horizontal grey lines); memristance change computed model sec. . thus, waveforms coincident activity voltage rise positive threshold resp. voltage drop negative threshold. addition, dependence voltage level weight change, simplest method differentiate weights voltage saturation characteristic fig.  , single stimulus. pulse pairing stdp) result distinctive memristive programming voltage, driving memristive device voltage saturation level (for typical experiments) stimulus repetitions. quantitative adjustments original LCP rule, requires qualitative adjustment. presynaptic conductance waveform voltage trace short rectangular pulse added immediately exponential downward trace, arriving waveform similar spike response model postsynaptic trace, uppermost curve fig.  call modified LCP rule. overlapping preand postsynaptic waveforms, rectangular pulses waveforms ?ride? exponential slopes counterparts voltage difference Vpre vpost memristive device preand postsynaptic waveforms applied terminals device (see curve top fig. ). rectangular pulses short compared exponential waveforms, represent constant voltage amplitude depends time difference waveforms expressed exponential slopes) required above. thus, original LCP rule, exponential slopes preand postsynaptic neuron govern STDP time windows. repeated application pre-post pairing drives memristive device voltage-dependent saturation level. similar original LCP rule, short term plasticity postsynaptic action potentials added make model biologically realistic. respect triplet learning protocol]). employ attenuation function equation adjusting duration postsynaptic action potential, curve top fig.  note: One furr important advantage modified LCP rule preand postsynaptic waveform causal. start prerespectively postsynaptic pulse. contrast proposed waveforms memristive learning. waveforms start advance actual pulse], requires preknowledge pulse occurrence. unsupervised learning context self-driven neuron spiking, preknowledge \\x0csimply existent. 120 120 ?pre=15ms, ?post=35ms ?pre=30ms, ?post=50ms ?200 ?150 ?100 ?pre=15ms, ?post=35ms ?pre=30ms, ?post=50ms 100 100 100 150 ?200 200 ?150 ?100) 100 150 200) Figure Results STDP protocol) model simulation) measurement BFO memristive device.  \\x0cmeasurement results waveforms developed previous section tested actual protocols synaptic plasticity. step, investigate behaviour BFO memristive device standard pair-based STDP experiment. this, apply spike pairings relative timings low repetition frequency (4hz), comparable biological measurement protocols]. measurements performed BFO memristive device shown fig.  shown model simulations fig. , developed waveforms transformed memristive device approx. exponentially decaying conductance changes. good agreement biological measurements] common STDP models]. model results confirmed measurements BFO memristive device, shown fig. . notably, measurements result smooth, continuous curves. expression continuous resistance change BFO material, results large number stable resistance levels. contrast. memristive materials rely ferroelectric switching, exhibit limited number discrete resistance levels]. moreover, nonlinear behaviour BFO memristive device limited effect resulting STDP learning window. resistance change directly linked applied waveforms. example, shown fig. increase time constants results correspondingly longer STDP time windows. modeling approach, time constants directly linked time constants underlying neuron synapse model.  ) Figure Measurement results triplet protocol Froemke Dan]. ) biological measurement data, adapted) measurement BFO memristive device. experiments shown weight single spike pairings, expressed stdp, nonlinearly integrated occuring shortly anor. commonly, triplets spikes investigate effect, \\x0ccarried]. main deviation experimental results compared pure STDP rule occur post-pre-post triplet], attributed postsynaptic adaptation]. adaptation included waveforms (equation action potential duration curve top fig. ), BFO memristive device measurements resemble post-pre-post results]. measurement results fig. show depression biological data pre-post-pre triplet (upper left quadrant). resistance time build stimulating pulse. pre-post-pre case, weight increase fully developed overwritten presynaptic pulse, results weight decrease. effect dependent measured device parameters stimulation waveforms. supplementary material). keeping stimulation waveforms simple possible, postsynaptic adaptation included. however, shown presynaptic short-term plasticity strong inﬂuence long-term learning]. modeling approach, model short-term plasticity easily connected stimulation waveforms modulating length presynaptic pulse. lines, postsynaptic waveform shifted slowly changing voltage analogous original LCP rule. . introduce metaplastic regulation weight potentiation depression]. toger, extensions open avenue seamless integration forms plasticity learning memristive devices.  Conclusion Starting waveform-based general plasticity rule model memristive device, shown direct premises biologically realistic learning BiFeO3 memristive device. employing LCP rule memristive learning advantages. memristor two-terminal device, separation learning waveforms LCP rule lends naturally employing passive array memristors]. addition, waveform-defined plasticity behaviour enables easy control STDP time windows, furr aided excellent multi-level memristive programming capability BiFeO3 memristive devices. small number memristors plasticity shown actual devices]. those, highly-configurable, finely grained learning curves unique, implementations exhibit statistical variations], assume discrete levels] learning windows device-inherent. adjusted]. price contrast. phase-change materials, BiFeO3 easily integrated top CMOS]. waveform-defined plasticity LCP rule enables explicit inclusion short term plasticity long term memristive learning, shown triplet protocol. preand postsynaptic waveforms generated CMOS neuron circuits memristive array], short term plasticity added extra circuit cost modification memristive array itself. contrast easily controlled short term plasticity, previous work targeting memristive short term \\x0cplasticity employed intrinsic. non-controllable) device properties]. knowledge, time triplets higher-order forms plasticity shown physical memristive device. wider neuroscience context, waveform defined plasticity shown general computational principle. synapses measure time differences naive forms STDP rules, react local static] dynamic] state variables. interesting predictions derived that. stdp time constants linked synaptic conductance membrane time constant]. predictions easily verified experimentally. acknowledgments research leading results received funding European Union Seventh Framework Programme (fp7/20072013) grant agreement. 269459 (coronet). Neuromorphic systems replicate cognitive processing functions integrated circuits. complexity/size largely determined synapse implementation, synapses significantly numerous neurons]. with recent push larger neuromorphic systems higher integration density systems, resulted approaches synapse realization. proposed solutions hand employ nanoscale devices conjuction conventional circuits] hand integrate synaptic functionality (shortand long term plasticity, pulse shaping, etc) small number devices possible. context, memristive devices introduced Chua] recently proposed eﬃcient implementations plastic synapses neuromorphic systems. memristive devices offer possibility actual learning mechanism, synaptic weight storage synaptic weight effect. amplification presynaptic current) device, compared distributed mechanisms conventional circuit implementations]. moreover, high-density passive array top conventional semiconductor chip]. plasticity memristors. resistance change, defined applied waveforms], fed rows columns memristive array CMOS preand postsynaptic neurons]. this resembles biological synapses, plasticity triggered mechanisms determined local waveforms]. however, learning memristors approached pragmatic technological level. goal find waveform achieves spiketiming-dependent plasticity (stdp], regard biological veracity waveforms furr important forms plasticity]. bridging gap, make plasticity rule introduced Mayr Partzsch] driven biologically realistic neuron waveforms explains large number experimental observations. adapt model recently introduced BiFeO3 memristive material]. measurement results modified plasticity rule implemented sample device given, exhbiting configurable STDP behaviour pulse triplet] reproduction. Materials Methods Local Correlation Plasticity (lcp) LCP rule introduced Mayr Partzsch] combines local waveforms, synaptic conductance) membrane potential). presynaptic activity encoded), determines conductance change due presynaptic spiking. postsynaptic activity turn signaled synapse). LCP rule combines formulation change synaptic weight similar well-known bienenstock-cooper-munroe rule]: ) ) ) equation, denotes voltage threshold weight potentiation depression, set resting potential. please note coincident preand postsynaptic activities detected rule multiplication: weight change occurs presynaptic conductance elevated postsynaptic membrane potential rest. waveforms) determined] spike response model], employed neuron model. mayr. waveforms triggered times preand postsynaptic spikes:   ?tpre ?pre   tpost Urefr tpre?tpost  post pre tpre ) tpost tpost) tpost denote preand postsynaptic spike, respectively. presynaptic con? decay time constant ?pre postsynaptic ductance waveform exponential height potential spike defined Dirac pulse integral exponential decay height Urefr membrane time constant ?post following], postsynaptic adaptation realised for this decreased nominal postsynaptic pulse occurs shortly anor postsynaptic pulse:   post tpost ?post) time constant exponential decay equation membrane time constant. 1971 Leon Chua postulated existence device current voltage directly controlled voltage ﬂux charge respectively, called memristor. using general state space description Chua Kang extended ory cover broad class memristive devices]. even terms interchangeably studies, devices study fit strict definition memristor, refer memristive devices following.  100 120 Figure Progression conductance membrane potential synapse weight sample spike pattern. figure shows preand postsynaptic waveforms, synaptic weight sample spike train. for simple waveforms, principal weight change mechanisms present: presynaptic side active postsynaptic spike, weight instantaneously increased large elevation membrane potential. contrast, presynaptic activity falling refractoriness period neuron (exponential decay spike) integrates weight decrease. shown], simple model replicate multitude experimental evidence, par advanced (and complex) phenomenological plasticity models available. addition, LCP rule directly links synaptic plasticity preand postsynaptic adaptation processes inﬂuence local waveforms. this explain furr experimental results]. sec. , adapt rule equations characteristics memristive device, introduced section.  Memristive Device non-volatile passive analog memory discussed applications neuromorphic systems space limitations analog circuitry. however, recently groups access suﬃcient materials devices. developments field nano material science, decade, opened possibilities creating compact circuit elements unique properties. most notably released information-called Memristor] effort put analysis thin film semiconductor-metal-metaloxide compounds. one commonly materials class BiFeO3 (bfo). complete conducting mechanisms BFO fully understood yet, partly contradictory results reported literature, confirmed physical effects overlayed dominate states. particularly resistive switching effect promising neuromorphic devices discussed detail. shown, effect unior bipolar highly dependent processing substrate, growth method, doping, etc. ]. BFO grown pulsed laser deposition/sio2 substrate top contact, fig.  memristors fabricated circular top plates, contacted needle probes, continuous bottom plate contacted edge die. BFO films thickness 100nm. created devices show unipolar resistive switching rectifying behavior. for positive bias device low resistive state (lrs) stays negative bias applied resets back high resistive state (hrs). state measured inﬂuencing applying low voltage. figure shows voltage-current-diagram characteristics device. measurement consists parts: rising negative voltage applied resets device intermediate level hrs. rising voltage lowers resistance exponentially. Figure Photograph fabricated memristive material measurements. falling positive voltage affect resistance anymore relation ohmic. because rectifying characteristic current LRS HRS negative voltages exhibit large dynamic range positive voltages. 800 700 600 abs 500 400 300 200 100 ?100 Figure voltage-current diagram device linear log-scale plot Phenomenological Device Model apply LCP model BFO device enable circuit design, simplified device model required. based model framework Chua Kang, output function., current dependent time, state input., voltage recently, widely modeling memristive devices]. contrast memristive device models based sinh function output relationship (following Yang. ]), model BFO device semiconductor junctions. junctions abstractly diode equation: (exp ]. attempt catch basic characteristics, device modeled employing diode equations letting state variable, inﬂuence output roughly represent conductance, I01 (ed1)  i02 )  ) voltage device2 diode equations guarantee crossing hysteresis. parameters I0i individual control current characteristics negative positive voltages, shown previous section rar asymmetric BFO devices. for purpose modeling plasticity, focus dynamic behavior conductance change; investigated detail Querlioz. ] served basis model state variable, )  ) With sinh   approach fundamentally sinh function. functions )  relate current state affects state development effect applied voltage, respectively.  ) exponential function.  ?gmin   Gmax ?gmin) Gmax  ) Gmax ?gmin) Gmin    favor separate exponential sinh functions increased controllability voltage domains (positive negative). here parameters govern voltage dependence state modification, scaling result. with speed state saturation set:  ) )    ) For implementation, prominent commercially simulators custom analog mixed-signal integrated circuit design, cadence? spectre?  using behavioral current sources, equations, implemented simulated feasibility circuit design. depicted fig. conductance change time, voltages, model (fig. ) measurements (fig. ). exponential dependency device voltage rise levels operation (equations)). also saturation conductance change voltage visible (equation)). sharp current model result simplistic approach, real devices show slower transitions. addition, noted real device appears experience significantly steeper rise current. however, target reasonable characteristics region operation relevant plasticity rule experiments.  Volt Volt) Figure Device current applied voltages model) measurement).  Results Modified LCP nonlinearity learning threshold required order carry correlation operation preand postsynaptic waveforms characterizes forms long term learning]. original LCP rule, multiplication preand postsynaptic waveforms. coincident activity results learning. memristive devices operated additive manner. preand postsynaptic waveforms applied terminals device, adding/subtracting voltage curves. order state memristive device affected overlap waveforms, positive negative modification threshold required]. equation, internal voltage driven state change  affected parameters govern thresholds negative positive voltages. for devices, work effective modification thresholds Vpre Vpost vpre-vpost 100 120 Figure Modification original LCP rule BFO memristive device, top bottom: preand postsynaptic voltages/waveforms, exponential decay ?pre resp. ?post (postsynaptic waveform plotted inverse illustrate waveform function); resultant voltage difference memristive device memristance modification thresholds (horizontal grey lines); memristance change computed model sec. . thus, waveforms coincident activity voltage rise positive threshold resp. voltage drop negative threshold. addition, dependence voltage level weight change, simplest method differentiate weights voltage saturation characteristic fig.  that, single stimulus. pulse pairing stdp) result distinctive memristive programming voltage, driving memristive device voltage saturation level (for typical experiments) stimulus repetitions. apart quantitative adjustments original LCP rule, requires qualitative adjustment. presynaptic conductance waveform voltage trace short rectangular pulse added immediately exponential downward trace, arriving waveform similar spike response model postsynaptic trace, uppermost curve fig.  call modified LCP rule. for overlapping preand postsynaptic waveforms, rectangular pulses waveforms ?ride? exponential slopes counterparts voltage difference Vpre vpost memristive device preand postsynaptic waveforms applied terminals device (see curve top fig. ). since rectangular pulses short compared exponential waveforms, represent constant voltage amplitude depends time difference waveforms expressed exponential slopes) required above. thus, original LCP rule, exponential slopes preand postsynaptic neuron govern STDP time windows. repeated application pre-post pairing drives memristive device voltage-dependent saturation level. similar original LCP rule, short term plasticity postsynaptic action potentials added make model biologically realistic. respect triplet learning protocol]). employ attenuation function equation adjusting duration postsynaptic action potential, curve top fig.  please note: One furr important advantage modified LCP rule preand postsynaptic waveform causal. start prerespectively postsynaptic pulse. this contrast proposed waveforms memristive learning. waveforms start advance actual pulse], requires preknowledge pulse occurrence. especially unsupervised learning context self-driven neuron spiking, preknowledge \\x0csimply existent. 120 120 ?pre=15ms, ?post=35ms ?pre=30ms, ?post=50ms ?200 ?150 ?100 ?pre=15ms, ?post=35ms ?pre=30ms, ?post=50ms 100 100 100 150 ?200 200 ?150 ?100) 100 150 200) Figure Results STDP protocol) model simulation) measurement BFO memristive device.  \\x0cmeasurement results waveforms developed previous section tested actual protocols synaptic plasticity. step, investigate behaviour BFO memristive device standard pair-based STDP experiment. for this, apply spike pairings relative timings low repetition frequency (4hz), comparable biological measurement protocols]. measurements performed BFO memristive device shown fig.  shown model simulations fig. , developed waveforms transformed memristive device approx. exponentially decaying conductance changes. this good agreement biological measurements] common STDP models]. model results confirmed measurements BFO memristive device, shown fig. . notably, measurements result smooth, continuous curves. this expression continuous resistance change BFO material, results large number stable resistance levels. this contrast. memristive materials rely ferroelectric switching, exhibit limited number discrete resistance levels]. moreover, nonlinear behaviour BFO memristive device limited effect resulting STDP learning window. resistance change directly linked applied waveforms. for example, shown fig. increase time constants results correspondingly longer STDP time windows. following modeling approach, time constants directly linked time constants underlying neuron synapse model.  ) Figure Measurement results triplet protocol Froemke Dan]. ) biological measurement data, adapted) measurement BFO memristive device. Experiments shown weight single spike pairings, expressed stdp, nonlinearly integrated occuring shortly anor. commonly, triplets spikes investigate effect, \\x0ccarried]. main deviation experimental results compared pure STDP rule occur post-pre-post triplet], attributed postsynaptic adaptation]. with adaptation included waveforms (equation action potential duration curve top fig. ), BFO memristive device measurements resemble post-pre-post results]. measurement results fig. show depression biological data pre-post-pre triplet (upper left quadrant). this resistance time build stimulating pulse. pre-post-pre case, weight increase fully developed overwritten presynaptic pulse, results weight decrease. this effect dependent measured device parameters stimulation waveforms. supplementary material). for keeping stimulation waveforms simple possible, postsynaptic adaptation included. however, shown presynaptic short-term plasticity strong inﬂuence long-term learning]. with modeling approach, model short-term plasticity easily connected stimulation waveforms modulating length presynaptic pulse. along lines, postsynaptic waveform shifted slowly changing voltage analogous original LCP rule. . introduce metaplastic regulation weight potentiation depression]. toger, extensions open avenue seamless integration forms plasticity learning memristive devices.  Conclusion Starting waveform-based general plasticity rule model memristive device, shown direct premises biologically realistic learning BiFeO3 memristive device. employing LCP rule memristive learning advantages. memristor two-terminal device, separation learning waveforms LCP rule lends naturally employing passive array memristors]. addition, waveform-defined plasticity behaviour enables easy control STDP time windows, furr aided excellent multi-level memristive programming capability BiFeO3 memristive devices. small number memristors plasticity shown actual devices]. among those, highly-configurable, finely grained learning curves unique, implementations exhibit statistical variations], assume discrete levels] learning windows device-inherent. adjusted]. this price contrast. phase-change materials, BiFeO3 easily integrated top CMOS]. waveform-defined plasticity LCP rule enables explicit inclusion short term plasticity long term memristive learning, shown triplet protocol. preand postsynaptic waveforms generated CMOS neuron circuits memristive array], short term plasticity added extra circuit cost modification memristive array itself. contrast easily controlled short term plasticity, previous work targeting memristive short term \\x0cplasticity employed intrinsic. non-controllable) device properties]. knowledge, time triplets higher-order forms plasticity shown physical memristive device. wider neuroscience context, waveform defined plasticity shown general computational principle. synapses measure time differences naive forms STDP rules, react local static] dynamic] state variables. some interesting predictions derived that. stdp time constants linked synaptic conductance membrane time constant]. predictions easily verified experimentally. acknowledgments research leading results received funding European Union Seventh Framework Programme (fp7/20072013) grant agreement. 269459 (coronet).',\n",
       " 'PP4651': 'motivation paper study problem learning random samples probability distribution supported manifold, learning error measured transportation metrics. problem learning probability distribution classic statistics, typically analyzed distributions density respect Lebesgue measure, total variation, common distances measure closeness densities (see instance] references rein.) setting data distribution supported low dimensional manifold embedded high dimensional space considered recently. particular, kernel density estimators manifolds], pointwise consistency, convergence rates, studied]. discussion topics related statistics Riemannian manifold found]. interestingly, problem approximating measures respect transportation distances deep connections fields optimal quantization], optimal transport] and, point work, unsupervised learning (see sec. .) fact, sequel, widely-used algorithms unsupervised learning-means (but ors PCA-ﬂats), shown performing task estimating data-generating measure sense-wasserstein distance. close relation learning ory, optimal transport quantization interest right. indeed, work, techniques fields derive probabilistic bounds below. technical contribution summarized follows) prove uniform lower bounds distance measure estimates based discrete sets (such empirical measure measures derived algorithms kmeans) provide probabilistic bounds rate convergence empirical population measures which, unlike existing probabilistic bounds, hold large class measures) provide probabilistic bounds rate convergence measures derived-means data measure. structure paper end Section discuss exact formulation problem related previous works. setup Previous work Consider problem learning probability measure supported space. sample    size assume compact, smooth-dimensional manifold bounded curvature, metric volume measure embedded unit ball separable Hilbert space product, induced norm distance (for instance B2d) unit ball ) denote Wasserstein space order  )  ) kxkp ) probability measures) supported finite moment. -wasserstein distance (?, inf [ekx law) law  random variables distributed  respectively, optimal expected cost transporting points generated generated guaranteed finite]. space) metric complete separable metric space]. problem learning probability measures  ), performance measured distance choices distances probability measures]. metrizes weak convergence (see] orem), sequence measures converges weakly iff   order moments converge distances?evy-prokhorov, weak-* distance, metrize weak convergence. however, pointed Villani excellent monograph], ?wasserstein distances rar strong definite advantage weak-* distance?.   diﬃcult combine information convergence Wasserstein distance smoothness bound, order convergence stronger distances.? wasserstein distances \\x0cstudy mixing convergence Markov chains], concentration measure phenomena]. list add important fact existing widely-used algorithms unsupervised learning easily extended (see sec. compute measure minimizes distance empirical measure  fact prove, sec. bounds convergence measure induced-means population measure versions Wasserstein distance weaker?older inequality,    particular, ?results distance stronger, diﬃcult establish results distance? ]. discussion topic, behavior markedly different.  Closeness Empirical Population Measures strong law large numbers, empirical measure converges surely population measure:   sense weak topology]. weak convergence convergence convergence pth moments equivalent), means that, sense, empirical measure converges  fundamental question refore fast rate convergence   .  Convergence expectation rate convergence   expectation widely studied past, resulting upper bounds order EW2 (?, ], lower bounds order EW2 (?,  ] (both assuming absolutely continuous part possibly rates orwise). recently, upper bound order EWp (?,  proposed] proving bound Optimal Bipartite Matching (obm) problem], relating problem expected distance EWp (?,  particular, independent samplespxn OBM problem finding permutation minimizes matching cost kxi? ]. hard show optimal matching cost     empirical measures jensen inequality, triangle inequality  holds EWp (?,  ewp    EWp (?, refore bound order OBM problem] implies bound EWp (?,  matching lower bound special case: constant bounded set non-null measure. uniform.) similar results, matching lower bounds found].  Convergence probability Results convergence probability, main results work, considerably harder obtain. fruitful avenue analysis-called transportation, Talagrand inequalities prove concentration inequalities]. particular, \\x0csay satisfies) inequality iff (?,  (?—?   (?—?) relative entropy]. shown], obtain probabilistic upper bounds (?,  satisfy inequality order, reby reducing problem bounding (?, obtaining inequality. note that, jensen inequality, expected behavior inequality stronger]. shown satisfies inequality iff finite square-exponential moment?kxk finite ], general conditions found example, that, compact diameter orem], celebrated csisz-kullback-pinsker inequality],  ), (?,    ?k2tv 22p D2p(?—?  ktv total variation norm. clearly, implies inequality, not. inequality shown Talagrand satisfied Gaussian distribution], slightly generally strictly log-concave measures (see, 123].) however, noted], ?contrary case, hope obtain inequalities integrability decay estimates.? structure paper. work obtain bounds probability (learning rates) problem learning probability measure sense begin establishing (lower) bounds convergence empirical population measures, serve set problem introduce connection quantization measure learning (sec. .) describe existing unsupervised learning algorithms compute set-means-ﬂats, pca,.  easily extended produce measure (sec. .) due simplicity widespread use, focus-means. measure estimates empirical measure, measure induced-means, set prove upper bounds convergence data-generating measure (sec. .) arrive bounds means intermediate measures, related problem optimal quantization. bounds apply broad setting (unlike existing bounds based transportation inequalities, restricted log-concave measures].)  learning probability measures, optimal transport quantization address problem learning probability measure observations disposal. samples  begin establishing notation intermediate results. closed set  borel Voronoi partition composed sets closest ,   minr } measurable (see instance].) projection function mapping  virtue Borel Voronoi partition, map measurable)) minq     ), pushforward, image measure mapping defined ) )) Borel measurable sets definition, clear supported establish connection  expected distance set distance set induced pushforward measure. notice that, discrete sets expected distance expected quantization error,? ?? ??   incurred encoding points drawn closest point]. close connection optimal quantization Wasserstein distance pointed past statistics], optimal quantization], approximation ory] literatures. lemmas key tools reminder paper. highlights close link quantization optimal transport. lemma. closed   ), holds??  (?,  note key element lemma measures expression (?, match. mismatch, distance increase. , (?,  (?,   ). fact, lemma shows that, measures support closest lemma. closed   ) supp(?)   holds (?,  (?, ?). combined, lemmas behavior measure learning problem limited performance optimal quantization problem. instance, (?, , best-case, low optimal quantization cost codebook size section makes claim precise.  Lower bounds Consider situation depicted fig. sample drawn distribution assume absolutely continuous support. shown, projection map sends points closest point resulting Voronoi decomposition supp(?) drawn shades blue. lemma], pairwise intersections Voronoi regions null ambient measure, absolutely continuous, pushforward measure written case  (vxj Vxj Voronoi region note that, finite sets decomposition form Borel Voronoi tiling, Borel Voronoi partition. , instance, atom falling Voronoi regionspin tiling, regions count atom irs, double-counting imply  technicalities required correctly define Borel Voronoi partition that, general, simpler write discrete) measure written sum deltas masses. lemma, distance (expected) quantization cost codebook. clearly, cost lower optimal quantization cost size reasoning leads lower bound empirical population measures. orem.   ) absolutely continuous part holds (?,   uniformly constants depend only. proof: Let (?) inf??  optimal quantization cost order centers.  finite  order moment, (since supported unit ball (?)   constants depending (see].) supp(? (?,  lemma lemma?? ,  (?)   Note bound orem holds derived sample refore stronger existing lower bounds convergence rates EWp (?,   particular, trivially induces lower bound  rate convergence expectation. unsupervised learning algorithms learning probability measure], widely unsupervised learning algorithms interpreted input sample output set typically free parameter algorithm, number means-means1 dimension aﬃne spaces pca, etc. performance measured empirical quantity minimized sets class. sets size aﬃne spaces dimension,.  formulation general encompass-means pca-ﬂats, non-negative matrix factorization, sparse coding (see] references rein.) discussion sec. establish clear connection unsupervised learning problem learning probability measures respect running-means problem, argument general. input-means problem find set minimizing average distance points associating pushforward measure find ??  lemma   ) Since-means minimizes equation finds measure closest support size This connection-means measure approximation was, authors? knowledge, suggested Pollard] though, mentioned earlier, argument carries unsupervised learning algorithms. unsupervised measure learning algorithms. brieﬂy clarify steps involved existing unsupervised learning algorithm probability measure learning. parametrized algorithm. -means) takes sample outputs set measure learning algorithm ) defined follows: takes sample outputs measure supported discrete ?  practice simply store-vector    reconstructed placing atoms mass point. case-means algorithm, points masses stored. note algorithm attempts output measure close cast framework. indeed, support lemma, measure closest support effectively reduces problem learning measure slight abuse notation, refer-means algorithm ideal algorithm solves-means problem, practice approximation algorithm used. finding set, akin fact optimal quantizer nearest-neighbor quantizer (see, 350]) reduces problem finding optimal quantizer finding optimal quantizing set. clearly, minimum equation sets size output kmeans) monotonically non-increasing particular  ??    , make learned measure arbitrarily close increasing however, pointed sec. problem measure learning concerned minimizing-wasserstein distance data-generating measure. actual performance-means necessarily guaranteed behave empirical one, question characterizing behavior function naturally arises. finally, note that??   empirical performances optimal quantization, measure learning problem formulations), actual performances satisfy??  (?,    (?,     lemma lemma consequently, identification sets measures measure learning problem, general, harder set-approximation problem (for example, absolutely continuous set non-null volume, hard show inequality surely strict??  .) remainder, characterize performance-means measure learning problem, varying Although unsupervised learning algorithms chosen basis analysis-means oldest widely used, deep connection optimal quantization measure approximation manifested. note that, setting analysis includes problem characterizing behavior distance (?, empirical population measures which, sec. , fundamental question statistics. speed convergence empirical population measures.) learning rates order analyze performance-means measure learning algorithm, convergence empirical population measures, propose decomposition shown fig.  diagram includes measures considered paper, shows decompositions prove upper bounds. upper arrow (green), illustrates decomposition bound distance (?,  decomposition measures  intermediates arrive -point optimal quantizer, set minimizing??  sets size— lower arrow (blue) corresponds decomposition performance-means), labelled black arrows correspond individual terms bounds. begin (slightly) simpler results.  Convergence rates empirical population measures Let optimal-point quantizer order]. triangle inequality identity  (?,       ) This decomposition depicted upper arrow fig.  lemma, term sum equation optimal-point quantization error -manifold which, recent techniques] (see, 491]), shown proof orem (part order  remaining terms), slightly technical bounded proof orem. equation holds  bound (?, obtained optimizing righthand side values resulting probabilistic bound rate convergence empirical population measures.   (?, supp       figure sample drawn distribution support supp projection map  sends points closest sample. induced Voronoi tiling shown shades blue. figure measures considered paper linked arrows upper bounds distance derived. bounds quantities interest (?,  decomposed top bottom colored arrows. orem.   ) absolutely continuous part suﬃciently large holds (?,    ) probability ??  ), depends Learning rates-means key element proof orem distance population empirical measures bounded choosing intermediate optimal quantizing measure size analysis, bounds obtained smaller output-means close optimal quantizer (for instance suﬃcient data available), similarly expect bounds-means correspond choice decomposition bottom (blue) arrow figure leads bound probability. orem.   ) absolutely continuous part suﬃciently large letting  ) holds    ) probability ??  ), depends Note upper bounds orem same. surprising, stems fact.  minimizer  bound figure satisfies     refore definition), term order). adding term bound affects constants, orwise leaves unchanged. term takes output measure-means empirical measure, implies rate convergence-means (for suitably chosen worse   conversely, bounds   obtained rates convergence optimal quantizers, convergence slower-means (since quantizers-means produces suboptimal.) since bounds obtained convergence   -means order ) suggests estimates accurate derived point-mass measure derived point-mass measures finally, note introduced bounds limited statistical bound    ??  ??  lemma) \\x0c(see instance]), non-matching lower bounds known. means that, upper bounds obtained equation bounds orems automatically improve (would closer lower bound.) Motivation paper study problem learning random samples probability distribution supported manifold, learning error measured transportation metrics. problem learning probability distribution classic statistics, typically analyzed distributions density respect Lebesgue measure, total variation, common distances measure closeness densities (see instance] references rein.) setting data distribution supported low dimensional manifold embedded high dimensional space considered recently. particular, kernel density estimators manifolds], pointwise consistency, convergence rates, studied]. discussion topics related statistics Riemannian manifold found]. interestingly, problem approximating measures respect transportation distances deep connections fields optimal quantization], optimal transport] and, point work, unsupervised learning (see sec. .) fact, sequel, widely-used algorithms unsupervised learning-means (but ors PCA-ﬂats), shown performing task estimating data-generating measure sense-wasserstein distance. this close relation learning ory, optimal transport quantization interest right. indeed, work, techniques fields derive probabilistic bounds below. our technical contribution summarized follows) prove uniform lower bounds distance measure estimates based discrete sets (such empirical measure measures derived algorithms kmeans) provide probabilistic bounds rate convergence empirical population measures which, unlike existing probabilistic bounds, hold large class measures) provide probabilistic bounds rate convergence measures derived-means data measure. structure paper end Section discuss exact formulation problem related previous works. Setup Previous work Consider problem learning probability measure supported space. sample    size assume compact, smooth-dimensional manifold bounded curvature, metric volume measure embedded unit ball separable Hilbert space product, induced norm distance (for instance B2d) unit ball following) denote Wasserstein space order  )  ) kxkp ) probability measures) supported finite moment. -wasserstein distance (?, inf [ekx law) law  random variables distributed  respectively, optimal expected cost transporting points generated generated guaranteed finite]. space) metric complete separable metric space]. problem learning probability measures  ), performance measured distance choices distances probability measures]. among metrizes weak convergence (see] orem), sequence measures converges weakly iff   order moments converge distances?evy-prokhorov, weak-* distance, metrize weak convergence. however, pointed Villani excellent monograph], ?wasserstein distances rar strong definite advantage weak-* distance?.   diﬃcult combine information convergence Wasserstein distance smoothness bound, order convergence stronger distances.? wasserstein distances \\x0cstudy mixing convergence Markov chains], concentration measure phenomena]. list add important fact existing widely-used algorithms unsupervised learning easily extended (see sec. compute measure minimizes distance empirical measure  fact prove, sec. bounds convergence measure induced-means population measure versions Wasserstein distance weaker?older inequality,    particular, ?results distance stronger, diﬃcult establish results distance? ]. discussion topic, behavior markedly different.  Closeness Empirical Population Measures strong law large numbers, empirical measure converges surely population measure:   sense weak topology]. since weak convergence convergence convergence pth moments equivalent), means that, sense, empirical measure converges  fundamental question refore fast rate convergence   .  Convergence expectation rate convergence   expectation widely studied past, resulting upper bounds order EW2 (?, ], lower bounds order EW2 (?,  ] (both assuming absolutely continuous part possibly rates orwise). more recently, upper bound order EWp (?,  proposed] proving bound Optimal Bipartite Matching (obm) problem], relating problem expected distance EWp (?,  particular, independent samplespxn OBM problem finding permutation minimizes matching cost kxi? ]. hard show optimal matching cost     empirical measures jensen inequality, triangle inequality  holds EWp (?,  ewp    EWp (?, refore bound order OBM problem] implies bound EWp (?,  matching lower bound special case: constant bounded set non-null measure. uniform.) similar results, matching lower bounds found].  Convergence probability Results convergence probability, main results work, considerably harder obtain. one fruitful avenue analysis-called transportation, Talagrand inequalities prove concentration inequalities]. particular, \\x0csay satisfies) inequality iff (?,  (?—?   (?—?) relative entropy]. shown], obtain probabilistic upper bounds (?,  satisfy inequality order, reby reducing problem bounding (?, obtaining inequality. note that, jensen inequality, expected behavior inequality stronger]. while shown satisfies inequality iff finite square-exponential moment?kxk finite ], general conditions found example, that, compact diameter orem], celebrated csisz-kullback-pinsker inequality],  ), (?,    ?k2tv 22p D2p(?—?  ktv total variation norm. clearly, implies inequality, not. inequality shown Talagrand satisfied Gaussian distribution], slightly generally strictly log-concave measures (see, 123].) however, noted], ?contrary case, hope obtain inequalities integrability decay estimates.? structure paper. work obtain bounds probability (learning rates) problem learning probability measure sense begin establishing (lower) bounds convergence empirical population measures, serve set problem introduce connection quantization measure learning (sec. .) describe existing unsupervised learning algorithms compute set-means-ﬂats, pca,.  easily extended produce measure (sec. .) due simplicity widespread use, focus-means. since measure estimates empirical measure, measure induced-means, set prove upper bounds convergence data-generating measure (sec. .) arrive bounds means intermediate measures, related problem optimal quantization. bounds apply broad setting (unlike existing bounds based transportation inequalities, restricted log-concave measures].)  Learning probability measures, optimal transport quantization address problem learning probability measure observations disposal. samples  begin establishing notation intermediate results. given closed set  Borel Voronoi partition composed sets closest ,   minr } measurable (see instance].) consider projection function mapping  virtue Borel Voronoi partition, map measurable)) minq   for  ), pushforward, image measure mapping defined ) )) Borel measurable sets from definition, clear supported establish connection  expected distance set distance set induced pushforward measure. notice that, discrete sets expected distance expected quantization error,? ?? ??   incurred encoding points drawn closest point]. this close connection optimal quantization Wasserstein distance pointed past statistics], optimal quantization], approximation ory] literatures. lemmas key tools reminder paper. highlights close link quantization optimal transport. lemma. for closed   ), holds??  (?,  note key element lemma measures expression (?, match. when mismatch, distance increase. that, (?,  (?,   ). fact, lemma shows that, measures support closest lemma. for closed   ) supp(?)   holds (?,  (?, ?). when combined, lemmas behavior measure learning problem limited performance optimal quantization problem. for instance, (?, , best-case, low optimal quantization cost codebook size section makes claim precise.  Lower bounds Consider situation depicted fig. sample drawn distribution assume absolutely continuous support. shown, projection map sends points closest point resulting Voronoi decomposition supp(?) drawn shades blue. lemma], pairwise intersections Voronoi regions null ambient measure, absolutely continuous, pushforward measure written case  (vxj Vxj Voronoi region note that, finite sets decomposition form Borel Voronoi tiling, Borel Voronoi partition. , instance, atom falling Voronoi regionspin tiling, regions count atom irs, double-counting imply  technicalities required correctly define Borel Voronoi partition that, general, simpler write discrete) measure written sum deltas masses. lemma, distance (expected) quantization cost codebook. clearly, cost lower optimal quantization cost size this reasoning leads lower bound empirical population measures. orem. for  ) absolutely continuous part holds (?,   uniformly constants depend only. proof: Let (?) inf??  optimal quantization cost order centers. since finite  order moment, (since supported unit ball (?)   constants depending (see].) since supp(? (?,  lemma lemma?? ,  (?)   Note bound orem holds derived sample refore stronger existing lower bounds convergence rates EWp (?,   particular, trivially induces lower bound  rate convergence expectation. Unsupervised learning algorithms learning probability measure], widely unsupervised learning algorithms interpreted input sample output set typically free parameter algorithm, number means-means1 dimension aﬃne spaces pca, etc. performance measured empirical quantity minimized sets class. sets size aﬃne spaces dimension,.  this formulation general encompass-means pca-ﬂats, non-negative matrix factorization, sparse coding (see] references rein.) using discussion sec. establish clear connection unsupervised learning problem learning probability measures respect consider running-means problem, argument general. given input-means problem find set minimizing average distance points associating pushforward measure find ??  lemma   ) Since-means minimizes equation finds measure closest support size This connection-means measure approximation was, authors? knowledge, suggested Pollard] though, mentioned earlier, argument carries unsupervised learning algorithms. unsupervised measure learning algorithms. brieﬂy clarify steps involved existing unsupervised learning algorithm probability measure learning. let parametrized algorithm. -means) takes sample outputs set measure learning algorithm ) defined follows: takes sample outputs measure supported discrete ?  practice simply store-vector    reconstructed placing atoms mass point. case-means algorithm, points masses stored. note algorithm attempts output measure close cast framework. indeed, support lemma, measure closest support this effectively reduces problem learning measure slight abuse notation, refer-means algorithm ideal algorithm solves-means problem, practice approximation algorithm used. finding set, akin fact optimal quantizer nearest-neighbor quantizer (see, 350]) reduces problem finding optimal quantizer finding optimal quantizing set. clearly, minimum equation sets size output kmeans) monotonically non-increasing particular  ??    that, make learned measure arbitrarily close increasing however, pointed sec. problem measure learning concerned minimizing-wasserstein distance data-generating measure. actual performance-means necessarily guaranteed behave empirical one, question characterizing behavior function naturally arises. finally, note that??   empirical performances optimal quantization, measure learning problem formulations), actual performances satisfy??  (?,    (?,     lemma lemma consequently, identification sets measures measure learning problem, general, harder set-approximation problem (for example, absolutely continuous set non-null volume, hard show inequality surely strict??  .) remainder, characterize performance-means measure learning problem, varying Although unsupervised learning algorithms chosen basis analysis-means oldest widely used, deep connection optimal quantization measure approximation manifested. note that, setting analysis includes problem characterizing behavior distance (?, empirical population measures which, sec. , fundamental question statistics. speed convergence empirical population measures.) Learning rates order analyze performance-means measure learning algorithm, convergence empirical population measures, propose decomposition shown fig.  diagram includes measures considered paper, shows decompositions prove upper bounds. upper arrow (green), illustrates decomposition bound distance (?,  this decomposition measures  intermediates arrive -point optimal quantizer, set minimizing??  sets size— lower arrow (blue) corresponds decomposition performance-means), labelled black arrows correspond individual terms bounds. begin (slightly) simpler results.  Convergence rates empirical population measures Let optimal-point quantizer order]. triangle inequality identity  (?,       ) This decomposition depicted upper arrow fig.  lemma, term sum equation optimal-point quantization error -manifold which, recent techniques] (see, 491]), shown proof orem (part order  remaining terms), slightly technical bounded proof orem. since equation holds  bound (?, obtained optimizing righthand side values resulting probabilistic bound rate convergence empirical population measures.   (?, supp       Figure sample drawn distribution support supp projection map  sends points closest sample. induced Voronoi tiling shown shades blue. figure measures considered paper linked arrows upper bounds distance derived. bounds quantities interest (?,  decomposed top bottom colored arrows. orem. given  ) absolutely continuous part suﬃciently large holds (?,    ) probability ??  ), depends Learning rates-means key element proof orem distance population empirical measures bounded choosing intermediate optimal quantizing measure size analysis, bounds obtained smaller output-means close optimal quantizer (for instance suﬃcient data available), similarly expect bounds-means correspond choice decomposition bottom (blue) arrow figure leads bound probability. orem. given  ) absolutely continuous part suﬃciently large letting  ) holds    ) probability ??  ), depends Note upper bounds orem same. although surprising, stems fact. since minimizer  bound figure satisfies     refore definition), term order). adding term bound affects constants, orwise leaves unchanged. since term takes output measure-means empirical measure, implies rate convergence-means (for suitably chosen worse   conversely, bounds   obtained rates convergence optimal quantizers, convergence slower-means (since quantizers-means produces suboptimal.) Since bounds obtained convergence   -means order ) suggests estimates accurate derived point-mass measure derived point-mass measures finally, note introduced bounds limited statistical bound    ??  ??  lemma) \\x0c(see instance]), non-matching lower bounds known. this means that, upper bounds obtained equation bounds orems automatically improve (would closer lower bound.)',\n",
       " 'PP4708': 'this paper proposes image representation called Graphical Gaussian Vector (ggv), counterpart codebook local feature matching approaches. model distribution local features Gaussian Markov Random Field (gmrf) eﬃciently represent spatial relationship local features. concepts information geometry, proper parameters metric GMRF obtained. define image feature embedding proper metric parameters, directly applied scalable linear classifiers. show GGV obtains performance state--art methods standard object recognition datasets comparable performance scene dataset. introduction Bag Words (bow] facto standard image feature image categorization. bow, local feature assigned nearest \\x0ccodeword image represented histogram quantized features. approaches inspired BoW proposed recent years]. established large number codewords improves classification performance, drawback assigning local features nearest codeword computationally expensive. overcome problem, studies proposed building eﬃcient image representation smaller number codewords]. finding explicit correspondence local features anor categorizing images BoW], approach improved representing spatial layout local features graph]. explicit correspondences features advantage BoW information loss vector quantization avoided. however, drawback approach identification points minimum distortion computationally expensive. refore, aim research build eﬃcient image representation codewords explicit correspondences local features, achieving high classification accuracy. spatial layout local features important image semantic meaning, natural embedding spatial information image feature improves classification performance]. approaches advantage fact, ranging local., sift) global., Spatial pyramid). meanwhile, focus spatial layout local features, midlevel spatial information. paper, model image graph representing spatial layout local features define image feature based graph, proper metric embedded feature. show feature high classification accuracy, linear classifier. specifically, model image Gaussian Markov Random Field (gmrf) nodes correspond local features GMRF parameters image feature. GMRF commonly image segmentation, rarely modern image categorization pipelines effective modeling spatial layout. order extract repre1 sentative feature vector gmrf, choice coordinates parameters metric carefully made. define proper coordinates metric information geometry standpoint] derive optimal feature vector. resultant feature vector called Graphical Gaussian vector. contributions study summarized follows: eﬃcient image feature developed utilizing GMRF tool object categorization. this approach implemented developing Graphical Gaussian Vector feature, based GMRF information geometry. using standard image categorization benchmarks, demonstrate proposed feature performance state--art methods, based mainstream modules (such codebooks correspondence local features). knowledge, image feature object categorization utilizes expectation parameters GMRF Fisher information metric, achieves level accuracy comparable \\x0ccodebook local feature matching approaches. graphical Gaussian Vector Overview Proposed Method) Densely sampled local features) Multivariate Gaussian Markov Random Field (mgmrf) Local features x1t; ; parameter MGMRF MGMRF MGMRF; MGMRF) Parameter space) PDF) Feature Vector MGMRF; ; mgmrf; mgmrf; manifold Geodesic distance figure Overview image feature extraction based multivariate gmrf. section, present overview method. initially, local \\x0cfeatures  extracted dense sampling strategy (fig. )). multivariate GMRF model spatial relationships local features (fig. )). gmrf represented graph), vertices edges correspond local features dependent relationships features, respectively. vector concatenation local features parameter GMRF image image represented probability distribution; gmrf (fig. )). parameter GMRF feature vector image (fig. )). assuming coordinate system, probability distribution model considered manifold, probability distribution represented point space (fig. )). however, space spanned parameters probability distribution Euclidean space, careful choosing parameters probability distribution metric make concepts information geometry] extract proper parameters metric gmrf. finally, define image feature embedding metric extracted parameters build image categorization system scalable linear classifier. sections, describe process detail.  Image Model Parameters Given local features  aim model probability distribution local features representing spatial layout image multivariate GMRF). first, vector built concatenating local features vertices gmrf.  local features focusing, obtain concatenated vector   ., fig. ). note dimensionality depend number local features image size, aspect ratio. however, results valid scalar local feature valid multivariate local feature, section dimensionality local features simplicity. dim) Let      random vector called Gaussian Markov Random Field (gmrf) respect), density form?  exp(?    ?)) jij, because Gaussian distribution represented exponential family, here, exponential family follows) exp  ) ?(?) ) natural parameters, ) suﬃcient statistic, ?(?) log exp(?   log-normalizer.   ) GMRF obtained jii ?jjk , ) , ?. expectation parameter [? )] implicit parameterization belonging exponential family. expectation parameters obtained Pii Pjk , ). ) natural expectation parameters \\x0ctransformed]. called mutually dual dual coordinate system. coordinate systems closely related Fisher information matrices (fims) Gij (?)  gij (?)    (?)  (?) (?). natural parameters expectation parameters coordinate system exponential family, ﬂat structure realized]. particular, called-aﬃne coordinate system, space spanned called-ﬂat. similarly, called)-aﬃne coordinate system, space spanned called)-ﬂat. spaces similar Euclidean space, careful spaces spanned natural expectation parameters Euclidean space, metrics vary parameters. discuss determine metrics spaces sections. . summarize section, natural expectation parameters similar interchangeable fims. parameters, obtain ﬂatness similar Euclidean space. matter wher choose natural expectation parameters, expectation parameters. )) feature vectors calculated directly covariance local features. multivariate extension GMRF calculation section.  Calculation Expectation Parameters section, describe calculations expectation parameters multivariate gmrf. first, define graph structure gmrf. star graphs shown fig. neighbors (fig. )) neighbors (fig. )) used. graph neighbors represent richer spatial information, compact structure preferable eﬃciency. refore, employed approximated graph structures shown fig. ), represents vertical horizontal relationships local features, fig. ), represents vertical, horizontal and, diagonal relationships. ) Image region) Figure Structures gmrf. next, show method estimating expectation parameters image. practice. ) multivariate case determined calculating local auto-correlations local features. present detailed calculations. ) fig. ) example.  local feature reference point displacement vectors, defined structure gmrf. local auto-correlation matrices obtained N1J number local features image region define N1J vector concatenating local features vertices reference point calculated   expectation parameters GMRF depicted fig. ) obtained:   (?) returns column vector consisting elements upper triangular portion input matrix(?) returns column vector elements input matrix N1J note omitted, edge vertices general, expectation parameters. )) star graph calculated:  )         — number vertices. dimensionality   dimensionality local feature. note omitted. scanning image region (fig. )), local features, means covariance matrices local features region vector matrix respectively. expectation parameters. )) approximated:              Equation) calcuated eﬃciently. ) vector. ). however, preliminary experiment. . ) terms classification accuracies. sections. ) expectation parameters.  Metric Section, mentioned metric varies depending parameters. derive metric expectation parameters]. represent length small lineelement connecting  ?.  represented basis vectors  squared distance calculated: ds2?,  product vectors. applying Taylor expansion divergence; ; ds2 represented follows: ds2; ; ?)]   fim. comparing equations, clear  metric matrix consisting products basis vectors corresponds fim ) \\x0cthus, FIM proper metric feature vectors expectation parameters) obtained gmrf.  cram-rao inequality understanding fim. assuming  satisfies: var[? biased estimator, variance-covariance matrix   ?  consequently, FIM considered inverse variance estimator, making natural matrix distance metric parameters.  Implementation Graphical Gaussian Vector first, build concatenated vector   corresponds local feature vertex training data, ] precision FGG FFGG ffg FGG) Figure Here}}. dimensionality local features vector concatenating local features training data, calculate precision matrix Using fisher information matrix full Gaussian family calculated), rows columns correspond),     (?) elements expectation parameters. partitioned submatrices   (?)  (?). fisher information matrix GMRF obtained shown) submatrices (?). matrix    obtained. FIM (?) gmrf derived FIM full Gaussian family  calculate (?)    denote basis vectors Pij. ) respec tively. elements (?) obtained Jij ?) (?)  (?) ?jpi Jkq jqi Jkp (?) jki Jkj ?jpi Jkp (?)  Jps Jqr Jqs Jpr (?)   Jpr Jrq (?)  Jpr Next derive (?)  (?).  (?) partitioned graphs   (?)  (?)  (?)  )   (?)  (?) fim GMRF (?) obtained Schur complement (?) respect submatrix (?) ]:    (?)   (?)  (?)  (?). ) (?)  calculations complicated, present simple GMRF vertices, shown fig.  however, (?) diﬃcult deal depends expectation parameters. thus, approximate model space tangent space center point training data]: (?)      number training images. order embed proper metric expectation parameters, multiply              ) call graphical Gaussian Vector (ggv). vector directly build sophisticated linear classifiers. derivation ggv, algorithm simple, consisting steps: calculation local auto-correlations local features; estimation expectation parameters gmrf; embedding distance metric Fisher information metric) expectation parameters. calculation GGV Algorithm calculation ggv, estimate FIM GMRF decomposing FIM full gaussian. consequence, obtain common FIM expectation parameters. practice, training data infeasible estimate fim, subset local features randomly sampled training data. note calculation FIM preprocessing stage, calculate FIM extracting ggvs. algorithm Calculation ggv. input: image region Fisher information matrix GMRF  output: GGV  calculate local auto-correlations local features: N1J N1J estimate expectation parameters:            \\x0c)) embed Fisher information metric expectation parameters ?   experiment tested method standard object scene datasets (caltech101, caltech256-scenes). experiment, evaluated effects graph structure. spatial information) fim. baseline methods, Generalized Local Correlation (glc]: glc  fim, Local auto-correlation features (lac]: lac    fim, Global Gaussian center linear kernel]: glc   comparison methods shown Table types graph structures utilized ggvs. shown fig. ) (ggv), models horizontal vertical spatial layout local features. shown fig. ) (ggv), adds diagonal spatial layouts features fig. ). compared normalized GGVs., ?/——?——). embed global spatial information, spatial pyramid representation   pyramid structure. table relationships glc, lac, GGV terms spatial information Fisher information metrics. method GLC LAC GGV (proposed) Spatial information  fisher information metric  experiment, compared GGVs Improved Fisher kernel (ifk], image representation time writing. experiment, spatial pyramid representation   structure. number components GMMs important parameter ifk. tested GMMs, 128, 256 Gaussians compute IFKs compared ggvs. datasets, SIFT features densely sampled patches. downsized images longest side 300 pixels. aforementioned features depend dimensionality local feature, reduced dimensionality PCA compared performance function dimensionality. linear classifier, multi-class PassiveAggressive Algorithm].  Caltech101 Caltech101 facto standard object-recognition dataset]. evaluate classification performance, commonly methodology. fifteen images randomly selected 102 categories training purposes remaining images testing. classification score averaged trials. comparison GGVs baselines, evaluate sensitivities sampling step local features. sampling step important parameters ggv, GGV calculates auto-correlations neighboring local features. preliminary experiment, fix number vertices dimensionality local feature. Caltech101 Generalized local correlation Local auto?correlation Global gaussian GGV (proposed GGV (proposed GGV norm (proposed GGV norm (proposed dimensionality local feature classification rate [%] classification rate [%] classification rate [%] Caltech101 Caltech101 IFK IFK IFK=128 IFK=256 GGV norm (proposed GGV norm (proposed dimensionality local feature IFK IFK IFK=128 IFK=256 GGV norm (proposed GGV norm (proposed dimensionality image feature \\x0cfigure comparison classification accuracies: (left) ggv, glc, LAC; (center) GGV IFK respect dimensionality ?local features? (right) GGV IFK respect dimensionality ?image features? caltech101 dataset. spatial pyramid. results follows (step pixels (step pixels (step pixels (step pixels (step pixels). clear difference step sizes pixels. refore experiments, pixels sampling step local feature extraction. figure (left) shows classification accuracies function dimensionality local features. large dimensionality yielded performance, proposed method (ggv) outperformed methods (glc, lac). comparing GGV lac, glc, clear embedding Fisher information metric improved classification accuracy significantly. comparing GGV, LAC glc, embedding spatial layout local features improved accuracy. comparison graph structures, four-neighbor structure (fig. )) performed slightly twoneighbor structure (fig. )). compare regular GGVs normalized ggvs, find normalization improved accuracy experiment, compared normalized GGVs ifks. results shown fig. (center). dimensionalities numbers components, GGVs performed ifks. fig. (right) shows classification accuracy function dimensionality image features converted results shown fig. (center). ggvs achieved higher accuracy lower dimensionality image features. results compared leading methods linear classifier. performance scores referenced original papers. llc] scored ScSPM] scored method achieved dimensionality local feature number vertices refore, method methods dataset, linear classifier requiring codebook descriptor matching.  Caltech256 Caltech256 consists images 256 object categories]. database significant large inter-class variability, intra-class variability greater found caltech101. evaluate performance, commonly methodology. fifteen images randomly selected categories training purposes remaining images testing. classification score averaged trials. figure (left) shows comparison classification accuracies ggv, glc, LAC. fig. (center) (right) show comparisons normalized GGVs IFKs Caltech256 dataset respect dimensionality local features image features, respectively. results show trends caltech101. method baseline methods ifks. ] reported IFK achieved] reported LLC scored%, GGV obtained%. however, fair comparison diﬃcult \\x0cmethod single-scale SIFT-scale SIFT-scale hog, respectively. multi-scale local features improves classification accuracies. ]). fair comparison-scale SIFT (patch size , , ) GGV normalization. ggv-scale SIFT achieved% leading methods. Caltech256 Caltech256 Caltech256 Generalized local correlation Local auto?correlation Global gaussian GGV (proposed GGV (proposed GGV norm (proposed GGV norm (proposed dimensionality local feature classification rate [%] classification rate [%] classification rate [%] IFK IFK IFK=128 IFK=256 GGV norm (pro posed GGV norm (proposed dimensionality local feature IFK IFK IFK=128 IFK=256 GGV norm (pro posed GGV norm (proposed dimensionality image feature Figure comparison classification accuracies: (left) ggv, glc, LAC; (center) GGV IFK respect dimensionality \\x0c?local features? (right) GGV IFK respect dimensionality ?image features? caltech256 dataset. -scenes experimented-scenes, commonly scene classification dataset]. randomly selected 100 training images class remaining samples test data. calculated classification rate class. score averaged trials, training test sets randomly-selected trial. methodology previous studies. 15scenes 15scenes 15scenes classification rate [%] classification rate [%] Generalized local correlation Local auto?correlation Global gaussian GGV (proposed GGV (proposed GGV norm (proposed GGV norm (proposed dimensionality local feature classification rate [%] IFK IFK IFK=128 IFK=256 GGV norm (proposed GGV norm (proposed dimensionality local feature IFK IFK IFK=128 IFK=256 GGV norm (proposed GGV norm (proposed dimensionality image feature Figure comparison classification accuracies: (left) ggv, glc, LAC; (center) GGV IFK respect dimensionality ?local features? (right) GGV IFK respect dimensionality ?image features? -scenes dataset. figure (left) shows comparison classification accuracies ggv, glc, LAC-scenes dataset. results show similar trends Caltech101 caltech256, difference scores graph structures. experiment, results respect dimensionality local features image features shown figs. (center) (right), respectively. contrast results Caltech101 256, IFKs scored slightly higher GGVs (ifk 256%, GGV normalized%). leading method, spatial Fisher kernel] reported highest score%). however-scale SIFT descriptors, provide richer information single-scale SIFT descriptors used, diﬃcult make direct comparison. conclusion paper, proposed eﬃcient image feature called Graphical Gaussian vector, neir codebook local feature matching. proposed method, spatial information local features Fisher information metric embedded feature modeling image Gaussian Markov Random Field (gmrf). experimental results standard datasets demonstrated proposed method offers performance superior comparable state--art methods. proposed image feature calculates expectation parameters GMRF simply effectively maintaining high classification rate. This paper proposes image representation called Graphical Gaussian Vector (ggv), counterpart codebook local feature matching approaches. model distribution local features Gaussian Markov Random Field (gmrf) eﬃciently represent spatial relationship local features. using concepts information geometry, proper parameters metric GMRF obtained. define image feature embedding proper metric parameters, directly applied scalable linear classifiers. show GGV obtains performance state--art methods standard object recognition datasets comparable performance scene dataset. Introduction Bag Words (bow] facto standard image feature image categorization. bow, local feature assigned nearest \\x0ccodeword image represented histogram quantized features. several approaches inspired BoW proposed recent years]. while established large number codewords improves classification performance, drawback assigning local features nearest codeword computationally expensive. overcome problem, studies proposed building eﬃcient image representation smaller number codewords]. finding explicit correspondence local features anor categorizing images BoW], approach improved representing spatial layout local features graph]. explicit correspondences features advantage BoW information loss vector quantization avoided. however, drawback approach identification points minimum distortion computationally expensive. refore, aim research build eﬃcient image representation codewords explicit correspondences local features, achieving high classification accuracy. since spatial layout local features important image semantic meaning, natural embedding spatial information image feature improves classification performance]. several approaches advantage fact, ranging local., sift) global., Spatial pyramid). meanwhile, focus spatial layout local features, midlevel spatial information. paper, model image graph representing spatial layout local features define image feature based graph, proper metric embedded feature. show feature high classification accuracy, linear classifier. specifically, model image Gaussian Markov Random Field (gmrf) nodes correspond local features GMRF parameters image feature. although GMRF commonly image segmentation, rarely modern image categorization pipelines effective modeling spatial layout. order extract repre1 sentative feature vector gmrf, choice coordinates parameters metric carefully made. define proper coordinates metric information geometry standpoint] derive optimal feature vector. resultant feature vector called Graphical Gaussian vector. contributions study summarized follows: eﬃcient image feature developed utilizing GMRF tool object categorization. This approach implemented developing Graphical Gaussian Vector feature, based GMRF information geometry. Using standard image categorization benchmarks, demonstrate proposed feature performance state--art methods, based mainstream modules (such codebooks correspondence local features). knowledge, image feature object categorization utilizes expectation parameters GMRF Fisher information metric, achieves level accuracy comparable \\x0ccodebook local feature matching approaches. Graphical Gaussian Vector Overview Proposed Method) Densely sampled local features) Multivariate Gaussian Markov Random Field (mgmrf) Local features x1t; ; parameter MGMRF MGMRF MGMRF; MGMRF) Parameter space) PDF) Feature Vector MGMRF; ; MGMRF; MGMRF; Manifold Geodesic distance Figure Overview image feature extraction based multivariate gmrf. section, present overview method. initially, local \\x0cfeatures  extracted dense sampling strategy (fig. )). multivariate GMRF model spatial relationships local features (fig. )). GMRF represented graph), vertices edges correspond local features dependent relationships features, respectively. let vector concatenation local features parameter GMRF image image represented probability distribution; GMRF (fig. )). parameter GMRF feature vector image (fig. )). assuming coordinate system, probability distribution model considered manifold, probability distribution represented point space (fig. )). however, space spanned parameters probability distribution Euclidean space, careful choosing parameters probability distribution metric make concepts information geometry] extract proper parameters metric gmrf. finally, define image feature embedding metric extracted parameters build image categorization system scalable linear classifier. sections, describe process detail.  Image Model Parameters Given local features  aim model probability distribution local features representing spatial layout image multivariate GMRF). first, vector built concatenating local features vertices gmrf. let local features focusing, obtain concatenated vector   ., fig. ). note dimensionality depend number local features image size, aspect ratio. however, results valid scalar local feature valid multivariate local feature, section dimensionality local features simplicity. that dim) Let      random vector called Gaussian Markov Random Field (gmrf) respect), density form?  exp(?    ?)) Jij, Because Gaussian distribution represented exponential family, here, exponential family follows) exp  ) ?(?) ) natural parameters, ) suﬃcient statistic, ?(?) log exp(?   log-normalizer.   ) GMRF obtained jii ?jjk , ) , ?. expectation parameter [? )] implicit parameterization belonging exponential family. expectation parameters obtained Pii Pjk , ). ) natural expectation parameters \\x0ctransformed]. called mutually dual dual coordinate system. coordinate systems closely related Fisher information matrices (fims) Gij (?)  Gij (?)    (?)  (?) (?). natural parameters expectation parameters coordinate system exponential family, ﬂat structure realized]. particular, called-aﬃne coordinate system, space spanned called-ﬂat. similarly, called)-aﬃne coordinate system, space spanned called)-ﬂat. those spaces similar Euclidean space, careful spaces spanned natural expectation parameters Euclidean space, metrics vary parameters. discuss determine metrics spaces sections. . summarize section, natural expectation parameters similar interchangeable fims. parameters, obtain ﬂatness similar Euclidean space. although matter wher choose natural expectation parameters, expectation parameters. )) feature vectors calculated directly covariance local features. multivariate extension GMRF calculation section.  Calculation Expectation Parameters section, describe calculations expectation parameters multivariate gmrf. first, define graph structure gmrf. star graphs shown fig. neighbors (fig. )) neighbors (fig. )) used. while graph neighbors represent richer spatial information, compact structure preferable eﬃciency. refore, employed approximated graph structures shown fig. ), represents vertical horizontal relationships local features, fig. ), represents vertical, horizontal and, diagonal relationships. ) Image region) Figure Structures gmrf. next, show method estimating expectation parameters image. practice. ) multivariate case determined calculating local auto-correlations local features. here present detailed calculations. ) fig. ) example. let local feature reference point displacement vectors, defined structure gmrf. local auto-correlation matrices obtained N1J number local features image region especially define N1J let vector concatenating local features vertices reference point calculated   expectation parameters GMRF depicted fig. ) obtained:   (?) returns column vector consisting elements upper triangular portion input matrix(?) returns column vector elements input matrix N1J note omitted, edge vertices general, expectation parameters. )) star graph calculated:  )         — number vertices. dimensionality   dimensionality local feature. also note omitted. scanning image region (fig. )), local features, means covariance matrices local features region vector matrix respectively. expectation parameters. )) approximated:              Equation) calcuated eﬃciently. ) vector. ). however, preliminary experiment. . ) terms classification accuracies. sections. ) expectation parameters.  Metric Section, mentioned metric varies depending parameters. derive metric expectation parameters]. let represent length small lineelement connecting  ?.  represented basis vectors  squared distance calculated: ds2?,  product vectors. applying Taylor expansion divergence; ; ds2 represented follows: ds2; ; ?)]   fim. comparing equations, clear  metric matrix consisting products basis vectors corresponds fim ) \\x0cthus, FIM proper metric feature vectors expectation parameters) obtained gmrf.  cram-rao inequality understanding fim. assuming  satisfies: var[? biased estimator, variance-covariance matrix   ?  consequently, FIM considered inverse variance estimator, making natural matrix distance metric parameters.  Implementation Graphical Gaussian Vector first, build concatenated vector   corresponds local feature vertex training data, ] precision FGG FFGG ffg FGG) Figure Here}}. dimensionality local features vector concatenating local features using training data, calculate precision matrix Using Fisher information matrix full Gaussian family calculated), rows columns correspond),     (?) elements expectation parameters. partitioned submatrices   (?)  (?). Fisher information matrix GMRF obtained shown) submatrices (?). matrix    obtained. since FIM (?) GMRF derived FIM full Gaussian family  calculate (?)   let denote basis vectors Pij. ) respec tively. elements (?) obtained Jij ?) (?)  (?) ?jpi Jkq jqi Jkp (?) Jki Jkj ?jpi Jkp (?)  Jps Jqr Jqs Jpr (?)   Jpr Jrq (?)  Jpr Next derive (?)  (?).  (?) partitioned graphs   (?)  (?)  (?)  )   (?)  (?) FIM GMRF (?) obtained Schur complement (?) respect submatrix (?) ]:    (?)   (?)  (?)  (?). ) (?)  calculations complicated, present simple GMRF vertices, shown fig.  however, (?) diﬃcult deal depends expectation parameters. thus, approximate model space tangent space center point training data]: (?)      number training images. order embed proper metric expectation parameters, multiply              ) call graphical Gaussian Vector (ggv). this vector directly build sophisticated linear classifiers. derivation ggv, algorithm simple, consisting steps: calculation local auto-correlations local features; estimation expectation parameters gmrf; embedding distance metric Fisher information metric) expectation parameters. calculation GGV Algorithm before calculation ggv, estimate FIM GMRF decomposing FIM full gaussian. consequence, obtain common FIM expectation parameters. practice, training data infeasible estimate fim, subset local features randomly sampled training data. note calculation FIM preprocessing stage, calculate FIM extracting ggvs. Algorithm Calculation ggv. input: image region Fisher information matrix GMRF  output: GGV  calculate local auto-correlations local features: N1J N1J estimate expectation parameters:            \\x0c)) embed Fisher information metric expectation parameters ?   Experiment tested method standard object scene datasets (caltech101, caltech256-scenes). for experiment, evaluated effects graph structure. spatial information) fim. baseline methods, Generalized Local Correlation (glc]: glc  fim, Local auto-correlation features (lac]: lac    fim, Global Gaussian center linear kernel]: glc   comparison methods shown Table two types graph structures utilized ggvs. shown fig. ) (ggv), models horizontal vertical spatial layout local features. shown fig. ) (ggv), adds diagonal spatial layouts features fig. ). compared normalized GGVs., ?/——?——). embed global spatial information, spatial pyramid representation   pyramid structure. table relationships glc, lac, GGV terms spatial information Fisher information metrics. method GLC LAC GGV (proposed) Spatial information  fisher information metric  experiment, compared GGVs Improved Fisher kernel (ifk], image representation time writing. experiment, spatial pyramid representation   structure. number components GMMs important parameter ifk. tested GMMs, 128, 256 Gaussians compute IFKs compared ggvs. for datasets, SIFT features densely sampled patches. downsized images longest side 300 pixels. aforementioned features depend dimensionality local feature, reduced dimensionality PCA compared performance function dimensionality. linear classifier, multi-class PassiveAggressive Algorithm].  Caltech101 Caltech101 facto standard object-recognition dataset]. evaluate classification performance, commonly methodology. fifteen images randomly selected 102 categories training purposes remaining images testing. classification score averaged trials. before comparison GGVs baselines, evaluate sensitivities sampling step local features. sampling step important parameters ggv, GGV calculates auto-correlations neighboring local features. preliminary experiment, fix number vertices dimensionality local feature. Caltech101 Generalized local correlation Local auto?correlation Global gaussian GGV (proposed GGV (proposed GGV norm (proposed GGV norm (proposed dimensionality local feature classification rate [%] classification rate [%] classification rate [%] Caltech101 Caltech101 IFK IFK IFK=128 IFK=256 GGV norm (proposed GGV norm (proposed dimensionality local feature IFK IFK IFK=128 IFK=256 GGV norm (proposed GGV norm (proposed dimensionality image feature \\x0cfigure comparison classification accuracies: (left) ggv, glc, LAC; (center) GGV IFK respect dimensionality ?local features? (right) GGV IFK respect dimensionality ?image features? Caltech101 dataset. spatial pyramid. results follows (step pixels (step pixels (step pixels (step pixels (step pixels). clear difference step sizes pixels. refore experiments, pixels sampling step local feature extraction. figure (left) shows classification accuracies function dimensionality local features. large dimensionality yielded performance, proposed method (ggv) outperformed methods (glc, lac). comparing GGV lac, glc, clear embedding Fisher information metric improved classification accuracy significantly. comparing GGV, LAC glc, embedding spatial layout local features improved accuracy. comparison graph structures, four-neighbor structure (fig. )) performed slightly twoneighbor structure (fig. )). compare regular GGVs normalized ggvs, find normalization improved accuracy experiment, compared normalized GGVs ifks. results shown fig. (center). for dimensionalities numbers components, GGVs performed ifks. fig. (right) shows classification accuracy function dimensionality image features converted results shown fig. (center). GGVs achieved higher accuracy lower dimensionality image features. results compared leading methods linear classifier. performance scores referenced original papers. llc] scored ScSPM] scored method achieved dimensionality local feature number vertices refore, method methods dataset, linear classifier requiring codebook descriptor matching.  Caltech256 Caltech256 consists images 256 object categories]. this database significant large inter-class variability, intra-class variability greater found caltech101. evaluate performance, commonly methodology. fifteen images randomly selected categories training purposes remaining images testing. classification score averaged trials. figure (left) shows comparison classification accuracies ggv, glc, LAC. fig. (center) (right) show comparisons normalized GGVs IFKs Caltech256 dataset respect dimensionality local features image features, respectively. results show trends caltech101. our method baseline methods ifks. ] reported IFK achieved] reported LLC scored%, GGV obtained%. however, fair comparison diﬃcult \\x0cmethod single-scale SIFT-scale SIFT-scale hog, respectively. multi-scale local features improves classification accuracies. ]). fair comparison-scale SIFT (patch size , , ) GGV normalization. ggv-scale SIFT achieved% leading methods. Caltech256 Caltech256 Caltech256 Generalized local correlation Local auto?correlation Global gaussian GGV (proposed GGV (proposed GGV norm (proposed GGV norm (proposed dimensionality local feature classification rate [%] classification rate [%] classification rate [%] IFK IFK IFK=128 IFK=256 GGV norm (pro posed GGV norm (proposed dimensionality local feature IFK IFK IFK=128 IFK=256 GGV norm (pro posed GGV norm (proposed dimensionality image feature Figure comparison classification accuracies: (left) ggv, glc, LAC; (center) GGV IFK respect dimensionality \\x0c?local features? (right) GGV IFK respect dimensionality ?image features? Caltech256 dataset. -scenes experimented-scenes, commonly scene classification dataset]. randomly selected 100 training images class remaining samples test data. calculated classification rate class. this score averaged trials, training test sets randomly-selected trial. this methodology previous studies. 15scenes 15scenes 15scenes classification rate [%] classification rate [%] Generalized local correlation Local auto?correlation Global gaussian GGV (proposed GGV (proposed GGV norm (proposed GGV norm (proposed dimensionality local feature classification rate [%] IFK IFK IFK=128 IFK=256 GGV norm (proposed GGV norm (proposed dimensionality local feature IFK IFK IFK=128 IFK=256 GGV norm (proposed GGV norm (proposed dimensionality image feature Figure comparison classification accuracies: (left) ggv, glc, LAC; (center) GGV IFK respect dimensionality ?local features? (right) GGV IFK respect dimensionality ?image features? -scenes dataset. figure (left) shows comparison classification accuracies ggv, glc, LAC-scenes dataset. results show similar trends Caltech101 caltech256, difference scores graph structures. experiment, results respect dimensionality local features image features shown figs. (center) (right), respectively. contrast results Caltech101 256, IFKs scored slightly higher GGVs (ifk 256%, GGV normalized%). leading method, spatial Fisher kernel] reported highest score%). however-scale SIFT descriptors, provide richer information single-scale SIFT descriptors used, diﬃcult make direct comparison. Conclusion paper, proposed eﬃcient image feature called Graphical Gaussian vector, neir codebook local feature matching. proposed method, spatial information local features Fisher information metric embedded feature modeling image Gaussian Markov Random Field (gmrf). experimental results standard datasets demonstrated proposed method offers performance superior comparable state--art methods. proposed image feature calculates expectation parameters GMRF simply effectively maintaining high classification rate.',\n",
       " 'PP4711': 'probabilistic methods based Gaussian densities celebrated successes machine learning. crucial ingredient Gaussian mixture models (gmm], Gaussian processes] Gaussian mixture regression (gmr] found applications fields robotics, speech recognition computer vision, few. Gaussian distributions convenient work oretical practical reasons central limit orem, easy computation means marginals, etc.) fall class densities supp. assign non-zero probability subset non-zero volume property Gaussians problematic application dictates \\x0csubsets space constitute ?forbidden? region probability mass. simple probabilistic model admissible positions robot indoor environment, assign rar ?low?  probability positions collisions environment. encoding constraints. gaussian mixture model natural assigns potentially low, non-zero probability mass portion space. contrast Gaussian models, non-parametric density estimators based spherical kernels bounded support. explain, enables study topological properties support region estimators. kernel-based density estimators wellestablished statistical literature] basic idea put rescaled version model density observed datapoint obtain estimate probability density data sampled. choice rescaling ?bandwidth?   studied respect standard error active area research]. focus spherical truncated Gaussian kernels some? work supported projects FLEXBOT (fp7erc-279933) TOMSY (ist-fp7270436) Swedish Foundation Strategic Research overlooked tool probabilistic modelling. important aspect kernels conditional marginal distributions computed analytically, enabling eﬃciently work context probabilistic inference. interpretation density support ?-ball notion bounded noise. , assumes observations distorted noise density bounded support (instead. gaussian noise). bounded noise models signal processing community robust filtering estimation], knowledge, combine densities bounded support topology model underlying structure data. thinking set observations ..., XnS ?fuzzy noise ?-ball? naturally leads space )  balls size data points. persistent homology tool studying topological properties spaces ) emerged field computational algebraic topology recent years]. persistent homology, study clustering, periodicity generally existence ?holes? dimensions ) lying interval. starting basic observation construct kernel-based density estimator?? region support ), paper investigates interplay topological information contained ) density estimate. specifically, make contributions: prior topological information supp define topologically admissible bandwidth interval [?min ?max propose evaluate topological bandwidth selector ?top [?min ?max  prior topological information, explain persistent homology determine topologically admissible bandwidth interval.  describe additional constraints defining forbidden subset parameter-space incorporated topological bandwidth estimation framework.  provide quantitative results syntic data evaluating expected errors density estimators topologically chosen bandwidth values  {?min ?mid ?max ?top carry evaluation spherical kernels compare results asymptotically optimal bandwidth choice.  method learning demonstration] context compare results current state art Gaussian mixture regression method.  Background kernel-based density estimation Let ..., . sample arising probability density kernel-based density estimation] approach reconstructing sample means estimator  kernel function  suitably chosen probability density. context, called bandwidth. interested estimator minimizes expected norm  choice crucial, choice kernel generally important].   sequence positive bandwidth values depending sample size classical results] suﬃciently well-behaved density limn?? ? )  provided limn?? limn??  encouraging result, question determining bandwidth sample ongoing research topic interested reader referred review-depth discussion One branch methods] minimize Mean Integrated Squared error, ise) )) asymptotic analysis reveals that, mild conditions], ise approximated asymptotically ise  limn?? limn??  here, AMISE denotes Asymptotic Mean Integrated Squared error. spherical kernels symmetric functions norm kxk input variable asymptotic analysis] shows that, dimension ise(hess) x2j independent choice ,  spherical symmetry(hess)) denotes trace hessian due availability simple explicit formula amise, large class bandwidth selection methods attempt estimate minimize AMISE working MISE directly. finds AMISE minimized  ?amise) (hess Since assumed unknown real world examples, called plug methods approximate ?amise]. paper, work syntic examples densities compute ?amise numerically order benchmark topological bandwidth selection procedure. experiments, choose spherical kernels defined unit ball) defined vol (uniform)?   kxk) (conic??      kxk2 (truncated gaussian) kxk kernels defined dimension spherical. functions radial distance origin enables eﬃciently evaluate sample estimator  dimension large. denote standard spherical Gaussian??   ) Figure) ) kxk2  kernels estimator sample points. persistent homology Consider point cloud shown Figure). human observer, noticeable ?circular?. reformulate existence ?hole? figure) mamatically precise persistent homology] recently gained increasing traction tool analysis structure point-cloud data]. ) Figure Noisy data concentrated circle) barcodes dimension). ), display   toger VietorisRips complex? approximating topology  vertical axis ith barcode special meaning, horizontal axis displays parameter?  fixed value, number bars lying equal ith Betti number?  shaded region highlights ?-interval? connected component. ? ) single ?circle? . ? detected). approach], starts subset  assumes exists probability density concentrated . sample   probability distribution, aims persistent homology setting recover topological structure  homology groups (?,    sample Each (?, vector space dimension (?) called ith Betti number. properties homology homology groups invariant large class deformations. homotopies) underlying topological space. popular deformation teacup continuously deformed doughnut. (?) measuring number connected components while, roughly, describes number-dimensional holes closed curve selfintersect classified connected component topologically circle). reader encouraged consult] rigorous introduction homotopies related concepts. given discrete sample distance parameter set    ,  )  ?}. figure) set displayed increasing values.  ) topological space and, case smooth compact submanifold restrictive class densities support small tubular neighbourhood] proven results showing ) homotopy equivalent high probability large sample sizes. key insight persistent homology study homology ) fixed   , simultaneously. idea study homology groups (?? ), change records Betti number barcode] (see. figure)).  computing barcode (?? ), directly (via Cech complex covering balls     ]) computationally expensive computes barcode homology groups VietorisRips complex? ). complex abstract complex vertices elements insert-simplex set distinct elements distance (see]). homology groups? ) necessarily isomorphic homology groups ), serve approximation due interleaving property vietoris-rips Cech complex. prop]. computation barcodes, javaplex software]. computed ith barcode records birth death times topological features? dimension increase maximal called maximal filtration value. our framework Given dataset    sampled. fashion underlying probability distribution density bounded support propose recover kernel density estimator  \\x0cway respects algebraic topology this,  based kernels supp), particular, experiment kernels, supp    topological features approximate computing barcodes?  prior information topological features given, inspect barcodes search large intervals Betti numbers change. approach], demonstrated topological features data discovered way. alternatively, prior information Betti numbers. knowledge periodicity, number clusters, inequalities involving Betti numbers) incorporate searching ?intervals constraints satisfied. geometric constraints data additionally incorporated restricting allowable ?-intervals values ) ?forbidden regions?. robotics setting, frequently encountered examples forbidden regions singular points joint space robot, positions space collisions environment. assume constraints Betti numbers sample compute barcodes? dimension ,   large maximal javaplex] determine set admissible values. empty, topological reconstruction failed. happen, example, assumptions data incorrect, samples reconstruct non-empty, attempt determine finite union disjoint intervals Betti numbers constraints satisfied. since, experiments, interval [?min), ?max)] (determined fixed precision) smallest ?min) coincided largest interval cases (indicating stable topological features), decided investigate furr analysis.   [?min), ?max)], resulting density  support region ) correct Betti numbers approximated?  note elementary observation: Lemma.  ?min), ?max)  suppose limn?? ?min) exists ?max) ?min) ?max) min)  ?top) ?min) ?max)?? satisfies ?top) ?mid) ?top) min) limn?? ?top) [?min), ?mid)] define ?mid) ?max)+? iii) limn?? ?top) intuition that, large class constraints Betti numbers tame densities (such densities concentrated neighbourhood compact submanifold]), ?min) ?max) exist large sample sizes high probability conditions Lemma satisfied. case, Lemma motivation choosing {?top)}?  topological bandwidth selector diﬃcult analyse ?min) asymptotically summand ?top) asymptotics optimal AMISE solution. furrmore, choice bandwidth corresponds support region ?top) correct Betti numbers approximated vietoris-rips \\x0ccomplex) ?top) [?min), ?max)]. finally) iii) imply that, point-wise, limn?? ? ?top)  due results]. note methods choosing ) [?min), ?max)] considered. topologically admissible interval [?min), ?max)] determined constraint connected components supp), ?max) increase shift connected components supp furr apart. ?top) increases yield good error results small sample sizes anymore. case, estimator ?top) [?min), ?max)] closer ?min) choice. give initial overview, display results ?min), ?mid), ?max) experiments. note error quality measure applications topological features supp important illustrate situation racetrack data experiment. show absence furr problem-specific knowledge ?top) yields good bandwidth estimate respect error examples. experiments Results probability density displayed grey graphs Figure benchmark performance topological bandwidth estimators, compute amise-optimal bandwidth parameter ?amise numerically analytic formula here, include Gaussian kernel comparison purposes only. ?top ?amise?top ,2500 Figure Density (grey) reconstructions (black) sample size, bandwidth kernel. order topologically reconstruct assume knowledge points sampled (supp furr information. assume sample support region components. find ?top) computing topologically admissible interval [?min), ?max)] barcode sample. evaluate quality bandwidth parameters chosen inside [?min), ?max)], sample sampling sizes compute errors resulting density estimator ?top ?min ?max ?mid (?max ?min spherical kernels compare results ?amise set  results, summarized Figure show ?top performs level comparable ?amise experiments. note ?amise computed true density known, while, ?top ?top ?min ?mid ?max top amise min mid max top amise min mid max top amise min mid max top amise min mid max ?amise ?amise, ?amise ?amise) bandwidth values) ) ) Figure generate samples density rejection sampling sample sizes 100 increments (small scale) 250 5000 increments 250 (larger scale), resulting increasing sample sizes   n30 order obtain stable results, perform sampling sampling size 1000 times (small scale), 100 times (for 250, 500, 750, 1000) times (for 1000) respectively. compute kernel density estimators  norm   figures) display errors (vertical axis) kernel function bandwidth selectors. figure) displays bandwidth values (vertical axis) bandwidth selectors. plots, horizontal coordinate , } corresponds sample size   ) density) 100 samples ?top grey. ) ?top ,100 100 samples) barcode) barcode Figure density, samples inferred support region ?top topological reconstruction (using barcodes [?min ?max highlighted. required information (supp experiments (sample sizes), determine valid interval [?min), ?max)] cases encounter case topological reconstruction impossible. results here, density displayed Figure). chose representative problems arising robotics, localization robot modelled depending probability prior encodes space occupied objects probability. scenarios, obtain topological information unobstructed space knowing number components holes information valuable case deformable obstacles homology stays invariant continuous deformations homotopies. set current experiment fashion similar experiments. iterate sampling density sample \\x0csizes compute resulting errors evaluate results. figure results bandwidths  [?min ?max yield errors comparable AMISE optimal bandwidth choice. ?top perform previous experiment, observe errors noneless follow decreasing trend. note, ?top yields good error results standard spherical Gaussian kernel here. applications probabilistic motion planning, inferred structure supp importance. path-connectedness supp important), making bounded support kernel preferable choice (see racetrack example).  ?top ?mid ?max top amise min mid max ?min top amise min mid max top amise min mid max top amise min mid max ?amise, ?amise ?amise ?amise 100 500 1000 1500) bandwidth values 100 500 1000)  1500 100 500 1000)  1500 100 500 1000) 1500 100 500 1000 1500) Figure generate samples density rejection sampling sample sizes 100 1500 increments 100. perform sampling times sample size compute kernelbased density estimator  norm   figures) display errors (vertical axis) sample size (horizontal axis) kernel function. figure) displays bandwidth values (vertical axis) sample size (horizontal axis). 200 200 ...... ..................... .... ...... ... ............. ...... ......... ... .... ........ ......... ..... ........ .... ....................... .......... ... ............ ... ....... .... ....... ........ ......... .... .............. ....... ...  ...................... ... ........... .... .............. ...... ................ ....................................   .................         ...........       .... ... ...... .... .... ....................... ..... ..... ....... ............... ......... ... ......... ....... .... ......     ......... ..... .... ..... .......... ....... ........ ........... ........ ........ ............................. .......   ......... ...................... ..... ........... .............. .......... ................. ..........    ... .....  ...... ............... ............. ...........  ..... .......... .... ......... .... ........ ....... ....... ....... ....... ..... ..... ..... ....                        ........ ......... .............. ......... .................................. ......  ....... ......... ................. .............. .... ........ ......... ...............    ... .... ...... ...... ....... ................. ..... ...... .... ..... ............ ............ ............. ....... ............ ...... ... ..... ....... ...... ...... ........... ...... ..... ... ............. .... ...... .......... ........... ....... ......... ... ..... ........ .............. ...... ......... ............ .........           ..... ......... ..................... ... .... .... ........... ......... .... .............. ..... ..... ..... ............ ................... ....................... .... ................................ ............................. .......... ...... .... ...        150 100 100 150 150 100 200) Position component racetrack data 100 150 200) Projection inferred support region, generated vector field sample trajectories) Inferred vector field, position likelihood sample trajectories gmr. Figure Figure) shows positions race car driving laps racetrack. ), results proposed method displayed Figure) shows standard GMR approach. exploit topological information racetrack connected ?circular? learning density. seen, model correctly infers region support track (grey). gmr, hand, non-zero probability assigned location. observe probable regions lying track (black likely). however, sampling trajectories learned density, that, trajectories method confined track, GMM results undesirable trajectories. application regression framework applied learn complex dynamics topological constraint. gps/timestamp data laps race car driving racetrack provided]. dataset (see Figure)), information boundaries racetrack are. state art approach modelling data employ learning demonstration] technique prominent context robotics, attempts learn motion patterns observing demonstrations. , data points r2n   }, describes position velocity position. order model dynamics, employ Gaussian mixture model] R2n learn probability density dataset (usually-algorithm). position associate velocity vector respect learned density  idea Gaussian mixture regression (gmr). resulting vector field numerically integrated yield trajectories.  Gaussian mixture model computed easily, method applied high-dimensional spaces. considered strength GMR approach infer examples vector field non-zero dense subset problematic geometric topological constraints naturally part approach easily encode fact vector-field non-zero racetrack. gps/timestamp data, compute velocity vectors data-point embed data manner experimented software] model racetrack data mixture varying number gaussians. model brakes completely low number gaussians, interesting behaviour observed case mixture model Gaussians displayed Figure). display resulting velocity vector field toger newly synsized trajectories. observe undesired periodic trajectory trajectory completely traverses racetrack converging attractor. likelihood position additionally displayed) black likely. positions occur racetrack, mixture model provide natural determining boundaries track lie. topmost trajectory), example, starts highly position. apply density estimation techniques paper case. racetrack closed curve, assume data modelled probability density support region single component (?)  topologically circle (?) ). order velocities differing laps track lie topology racetrack dominates rescale velocity components data lie inside in0 terval]. figure displays barcode data. ) procedure, compute [?min ?max] bandwidth interval topological constraints defined satfigure Barcodes isfied. kernel density dimension) estimator obtain ?top ?top correct topological propand) shaded [?min ?max interval erties. figure) displays projection ?top racetrack. step, suggest follow idea GMR approach compute posterior expectation), time density ?top definition kernel-based estimator that,  ?top  top  find reftop erence computation marginals spherical truncated gaussians, simple calculation shows fact computed analytically arbitrary dimension: Lemma.     denote spherical truncated Gaussian parameter  ?kxk kxk22  ??  kxk orwise. here,  ) ) denotes normalized Gamma function. point projection ?top position coordinates, compute velocity generate motion trajectories. points support region, postulate velocity. figure) displays resulting vector-field sample trajectories. see, follow trajectory data points Figure) well. time, displayed support region choice position racetrack. conclusion paper, presented method learning density models bounded support. proposed topological bandwidth selection approach incorporate topological constraints probabilistic modelling framework combining algebraic-topological information obtained terms persistent homology tools kernel-based density estimation. provided evaluation errors syntic data exemplified practical approach application learning demonstration scenario. Probabilistic methods based Gaussian densities celebrated successes machine learning. crucial ingredient Gaussian mixture models (gmm], Gaussian processes] Gaussian mixture regression (gmr] found applications fields robotics, speech recognition computer vision, few. while Gaussian distributions convenient work oretical practical reasons central limit orem, easy computation means marginals, etc.) fall class densities supp. assign non-zero probability subset non-zero volume this property Gaussians problematic application dictates \\x0csubsets space constitute ?forbidden? region probability mass. simple probabilistic model admissible positions robot indoor environment, assign rar ?low?  probability positions collisions environment. encoding constraints. Gaussian mixture model natural assigns potentially low, non-zero probability mass portion space. contrast Gaussian models, non-parametric density estimators based spherical kernels bounded support. explain, enables study topological properties support region estimators. kernel-based density estimators wellestablished statistical literature] basic idea put rescaled version model density observed datapoint obtain estimate probability density data sampled. choice rescaling ?bandwidth?   studied respect standard error active area research]. focus spherical truncated Gaussian kernels some? this work supported projects FLEXBOT (fp7erc-279933) TOMSY (ist-fp7270436) Swedish Foundation Strategic Research overlooked tool probabilistic modelling. important aspect kernels conditional marginal distributions computed analytically, enabling eﬃciently work context probabilistic inference. interpretation density support ?-ball notion bounded noise. , assumes observations distorted noise density bounded support (instead. gaussian noise). bounded noise models signal processing community robust filtering estimation], knowledge, combine densities bounded support topology model underlying structure data. thinking set observations ..., XnS ?fuzzy noise ?-ball? naturally leads space )  balls size data points. persistent homology tool studying topological properties spaces ) emerged field computational algebraic topology recent years]. using persistent homology, study clustering, periodicity generally existence ?holes? dimensions ) lying interval. starting basic observation construct kernel-based density estimator?? region support ), paper investigates interplay topological information contained ) density estimate. specifically, make contributions: given prior topological information supp define topologically admissible bandwidth interval [?min ?max propose evaluate topological bandwidth selector ?top [?min ?max  given prior topological information, explain persistent homology determine topologically admissible bandwidth interval.   describe additional constraints defining forbidden subset parameter-space incorporated topological bandwidth estimation framework.  provide quantitative results syntic data evaluating expected errors density estimators topologically chosen bandwidth values  {?min ?mid ?max ?top carry evaluation spherical kernels compare results asymptotically optimal bandwidth choice.  method learning demonstration] context compare results current state art Gaussian mixture regression method.  Background kernel-based density estimation Let ..., . sample arising probability density kernel-based density estimation] approach reconstructing sample means estimator  kernel function  suitably chosen probability density. context, called bandwidth. interested estimator minimizes expected norm  choice crucial, choice kernel generally important]. let  sequence positive bandwidth values depending sample size classical results] suﬃciently well-behaved density limn?? ? )  provided limn?? limn??  despite encouraging result, question determining bandwidth sample ongoing research topic interested reader referred review-depth discussion One branch methods] minimize Mean Integrated Squared error, ise) )) asymptotic analysis reveals that, mild conditions], ise approximated asymptotically ise  limn?? limn??  here, AMISE denotes Asymptotic Mean Integrated Squared error. spherical kernels symmetric functions norm kxk input variable asymptotic analysis] shows that, dimension ise(hess) x2j independent choice ,  spherical symmetry(hess)) denotes trace hessian due availability simple explicit formula amise, large class bandwidth selection methods attempt estimate minimize AMISE working MISE directly. one finds AMISE minimized  ?amise) (hess Since assumed unknown real world examples, called plug methods approximate ?amise]. paper, work syntic examples densities compute ?amise numerically order benchmark topological bandwidth selection procedure. for experiments, choose spherical kernels defined unit ball) defined vol (uniform)?   kxk) (conic??      kxk2 (truncated gaussian) kxk kernels defined dimension spherical. functions radial distance origin enables eﬃciently evaluate sample estimator  dimension large. denote standard spherical Gaussian??   ) Figure) ) kxk2  kernels estimator sample points. persistent homology Consider point cloud shown Figure). for human observer, noticeable ?circular?. one reformulate existence ?hole? Figure) mamatically precise persistent homology] recently gained increasing traction tool analysis structure point-cloud data]. ) Figure Noisy data concentrated circle) barcodes dimension). ), display   toger VietorisRips complex? approximating topology  while vertical axis ith barcode special meaning, horizontal axis displays parameter?  fixed value, number bars lying equal ith Betti number?  shaded region highlights ?-interval? connected component. ? ) single ?circle? . ? detected).  approach], starts subset  assumes exists probability density concentrated given. sample   probability distribution, aims persistent homology setting recover topological structure  homology groups (?,    sample Each (?, vector space dimension (?) called ith Betti number. one properties homology homology groups invariant large class deformations. homotopies) underlying topological space. popular deformation teacup continuously deformed doughnut. one (?) measuring number connected components while, roughly, describes number-dimensional holes closed curve selfintersect classified connected component topologically circle). reader encouraged consult] rigorous introduction homotopies related concepts. Given discrete sample distance parameter set    ,  )  ?}. Figure) set displayed increasing values.  ) topological space and, case smooth compact submanifold restrictive class densities support small tubular neighbourhood] proven results showing ) homotopy equivalent high probability large sample sizes. key insight persistent homology study homology ) fixed   , simultaneously. idea study homology groups (?? ), change records Betti number barcode] (see. figure)).  computing barcode (?? ), directly (via Cech complex covering balls     ]) computationally expensive computes barcode homology groups VietorisRips complex? ). this complex abstract complex vertices elements insert-simplex set distinct elements distance (see]). homology groups? ) necessarily isomorphic homology groups ), serve approximation due interleaving property vietoris-rips Cech complex. prop]. for computation barcodes, javaplex software]. computed ith barcode records birth death times topological features? dimension increase maximal called maximal filtration value. Our framework Given dataset    sampled. fashion underlying probability distribution density bounded support propose recover kernel density estimator  \\x0cway respects algebraic topology for this,  based kernels supp), particular, experiment for kernels, supp    topological features approximate computing barcodes?  prior information topological features given, inspect barcodes search large intervals Betti numbers change. this approach], demonstrated topological features data discovered way. alternatively, prior information Betti numbers. knowledge periodicity, number clusters, inequalities involving Betti numbers) incorporate searching ?intervals constraints satisfied. geometric constraints data additionally incorporated restricting allowable ?-intervals values ) ?forbidden regions?. robotics setting, frequently encountered examples forbidden regions singular points joint space robot, positions space collisions environment. let assume constraints Betti numbers for sample compute barcodes? dimension ,   large maximal javaplex] determine set admissible values. empty, topological reconstruction failed. this happen, example, assumptions data incorrect, samples reconstruct non-empty, attempt determine finite union disjoint intervals Betti numbers constraints satisfied. since, experiments, interval [?min), ?max)] (determined fixed precision) smallest ?min) coincided largest interval cases (indicating stable topological features), decided investigate furr analysis. for  [?min), ?max)], resulting density  support region ) correct Betti numbers approximated?  note elementary observation: Lemma. let ?min), ?max)  Suppose limn?? ?min) exists ?max) ?min) ?max) min)  ?top) ?min) ?max)?? satisfies ?top) ?mid) ?top) min) limn?? ?top) [?min), ?mid)] define ?mid) ?max)+? iii) limn?? ?top) intuition that, large class constraints Betti numbers tame densities (such densities concentrated neighbourhood compact submanifold]), ?min) ?max) exist large sample sizes high probability conditions Lemma satisfied. case, Lemma motivation choosing {?top)}?  topological bandwidth selector diﬃcult analyse ?min) asymptotically summand ?top) asymptotics optimal AMISE solution. furrmore, choice bandwidth corresponds support region ?top) correct Betti numbers approximated vietoris-rips \\x0ccomplex) ?top) [?min), ?max)]. finally) iii) imply that, point-wise, limn?? ? ?top)  due results]. note methods choosing ) [?min), ?max)] considered. topologically admissible interval [?min), ?max)] determined constraint connected components supp), ?max) increase shift connected components supp furr apart. ?top) increases yield good error results small sample sizes anymore. case, estimator ?top) [?min), ?max)] closer ?min) choice. give initial overview, display results ?min), ?mid), ?max) experiments. note error quality measure applications topological features supp important illustrate situation racetrack data experiment. show absence furr problem-specific knowledge ?top) yields good bandwidth estimate respect error examples. Experiments Results probability density displayed grey graphs Figure benchmark performance topological bandwidth estimators, compute amise-optimal bandwidth parameter ?amise numerically analytic formula here, include Gaussian kernel comparison purposes only. ?top ?amise?top ,2500 Figure Density (grey) reconstructions (black) sample size, bandwidth kernel. order topologically reconstruct assume knowledge points sampled (supp furr information. assume sample support region components. find ?top) computing topologically admissible interval [?min), ?max)] barcode sample. evaluate quality bandwidth parameters chosen inside [?min), ?max)], sample sampling sizes compute errors resulting density estimator ?top ?min ?max ?mid (?max ?min spherical kernels compare results ?amise set  results, summarized Figure show ?top performs level comparable ?amise experiments. note ?amise computed true density known, while, ?top ?top ?min ?mid ?max top amise min mid max top amise min mid max top amise min mid max top amise min mid max ?amise ?amise, ?amise ?amise) bandwidth values) ) ) Figure generate samples density rejection sampling sample sizes 100 increments (small scale) 250 5000 increments 250 (larger scale), resulting increasing sample sizes   n30 order obtain stable results, perform sampling sampling size 1000 times (small scale), 100 times (for 250, 500, 750, 1000) times (for 1000) respectively. compute kernel density estimators  norm   figures) display errors (vertical axis) kernel function bandwidth selectors. figure) displays bandwidth values (vertical axis) bandwidth selectors. plots, horizontal coordinate , } corresponds sample size   ) density) 100 samples ?top grey. ) ?top ,100 100 samples) barcode) barcode Figure density, samples inferred support region ?top topological reconstruction (using barcodes [?min ?max highlighted. required information (supp experiments (sample sizes), determine valid interval [?min), ?max)] cases encounter case topological reconstruction impossible. results here, density displayed Figure). chose representative problems arising robotics, localization robot modelled depending probability prior encodes space occupied objects probability. scenarios, obtain topological information unobstructed space knowing number components holes such information valuable case deformable obstacles homology stays invariant continuous deformations homotopies. set current experiment fashion similar experiments. iterate sampling density sample \\x0csizes compute resulting errors evaluate results. Figure results bandwidths  [?min ?max yield errors comparable AMISE optimal bandwidth choice. while ?top perform previous experiment, observe errors noneless follow decreasing trend. note, ?top yields good error results standard spherical Gaussian kernel here. applications probabilistic motion planning, inferred structure supp importance. path-connectedness supp important), making bounded support kernel preferable choice (see racetrack example).  ?top ?mid ?max top amise min mid max ?min top amise min mid max top amise min mid max top amise min mid max ?amise, ?amise ?amise ?amise 100 500 1000 1500) bandwidth values 100 500 1000)  1500 100 500 1000)  1500 100 500 1000) 1500 100 500 1000 1500) Figure generate samples density rejection sampling sample sizes 100 1500 increments 100. perform sampling times sample size compute kernelbased density estimator  norm   figures) display errors (vertical axis) sample size (horizontal axis) kernel function. figure) displays bandwidth values (vertical axis) sample size (horizontal axis). 200 200 ...... ..................... .... ...... ... ............. ...... ......... ... .... ........ ......... ..... ........ .... ....................... .......... ... ............ ... ....... .... ....... ........ ......... .... .............. ....... ...  ...................... ... ........... .... .............. ...... ................ ....................................   .................         ...........       .... ... ...... .... .... ....................... ..... ..... ....... ............... ......... ... ......... ....... .... ......     ......... ..... .... ..... .......... ....... ........ ........... ........ ........ ............................. .......   ......... ...................... ..... ........... .............. .......... ................. ..........    ... .....  ...... ............... ............. ...........  ..... .......... .... ......... .... ........ ....... ....... ....... ....... ..... ..... ..... ....                        ........ ......... .............. ......... .................................. ......  ....... ......... ................. .............. .... ........ ......... ...............    ... .... ...... ...... ....... ................. ..... ...... .... ..... ............ ............ ............. ....... ............ ...... ... ..... ....... ...... ...... ........... ...... ..... ... ............. .... ...... .......... ........... ....... ......... ... ..... ........ .............. ...... ......... ............ .........           ..... ......... ..................... ... .... .... ........... ......... .... .............. ..... ..... ..... ............ ................... ....................... .... ................................ ............................. .......... ...... .... ...        150 100 100 150 150 100 200) Position component racetrack data 100 150 200) Projection inferred support region, generated vector field sample trajectories) Inferred vector field, position likelihood sample trajectories gmr. Figure Figure) shows positions race car driving laps racetrack. ), results proposed method displayed Figure) shows standard GMR approach. exploit topological information racetrack connected ?circular? learning density. seen, model correctly infers region support track (grey). using gmr, hand, non-zero probability assigned location. observe probable regions lying track (black likely). however, sampling trajectories learned density, that, trajectories method confined track, GMM results undesirable trajectories. application regression framework applied learn complex dynamics topological constraint. gps/timestamp data laps race car driving racetrack provided]. for dataset (see Figure)), information boundaries racetrack are. one state art approach modelling data employ learning demonstration] technique prominent context robotics, attempts learn motion patterns observing demonstrations. , data points r2n   }, describes position velocity position. order model dynamics, employ Gaussian mixture model] R2n learn probability density dataset (usually-algorithm). position associate velocity vector respect learned density  idea Gaussian mixture regression (gmr). resulting vector field numerically integrated yield trajectories. since Gaussian mixture model computed easily, method applied high-dimensional spaces. while considered strength GMR approach infer examples vector field non-zero dense subset problematic geometric topological constraints naturally part approach easily encode fact vector-field non-zero racetrack. from gps/timestamp data, compute velocity vectors data-point embed data manner experimented software] model racetrack data mixture varying number gaussians. while model brakes completely low number gaussians, interesting behaviour observed case mixture model Gaussians displayed Figure). display resulting velocity vector field toger newly synsized trajectories. observe undesired periodic trajectory trajectory completely traverses racetrack converging attractor. likelihood position additionally displayed) black likely. while positions occur racetrack, mixture model provide natural determining boundaries track lie. topmost trajectory), example, starts highly position. let apply density estimation techniques paper case. given racetrack closed curve, assume data modelled probability density support region single component (?)  topologically circle (?) ). order velocities differing laps track lie topology racetrack dominates rescale velocity components data lie inside in0 terval]. figure displays barcode data. using) procedure, compute [?min ?max] bandwidth interval topological constraints defined satfigure Barcodes isfied. using kernel density dimension) estimator obtain ?top ?top correct topological propand) shaded [?min ?max interval erties. figure) displays projection ?top racetrack. step, suggest follow idea GMR approach compute posterior expectation), time density ?top definition kernel-based estimator that,  ?top  top  while find reftop erence computation marginals spherical truncated gaussians, simple calculation shows fact computed analytically arbitrary dimension: Lemma. consider   let denote spherical truncated Gaussian parameter  ?kxk kxk22  ??  kxk orwise. here,  ) ) denotes normalized Gamma function. for point projection ?top position coordinates, compute velocity generate motion trajectories. for points support region, postulate velocity. figure) displays resulting vector-field sample trajectories. see, follow trajectory data points Figure) well. time, displayed support region choice position racetrack. Conclusion paper, presented method learning density models bounded support. proposed topological bandwidth selection approach incorporate topological constraints probabilistic modelling framework combining algebraic-topological information obtained terms persistent homology tools kernel-based density estimation. provided evaluation errors syntic data exemplified practical approach application learning demonstration scenario.',\n",
       " 'PP4790': 'mixture distributions widely statistical modeling complex data. classical formulations number components priori, leading diﬃculties situations number eir unknown hard estimate advance. bayesian nonparametric models, notably based Dirichlet processes (dps], emerged important method address issue. basic idea mixture models sample, distribution countably infinite set, prior component parameters. significant assumption underlying mixture model observations infinitely exchangeable. assumption hold cases multiple groups data, samples groups generally exchangeable. approaches issue, hierarchical Dirichlet processes (hdps], organize DPs tree parents acting base measure children, popular. hdps extended variety ways. kim Smyth] incorporated group-specific random perturbations, allowing component parameters] proposed dynamic hdps, vary groups. ren. combine previous time step current time step. methods developed. maceachern] proposed DDP model parameters vary stochastic process. griffin Steel] proposed order-based ddp, atoms weighted differently permutation Beta variables stick-breaking. chung Dunson] carried approach furr, local predictors select subsets atoms. recently, connections poisson, gamma, Dirichlet processes exploited. rao Teh] proposed spatially normalized Gamma process, set dependent DPs derived normalizing restricted projections auxiliary Gamma process overlapping sub-regions. lin] proposed construction dependent dps, supports dynamic evolution operations underlying Poisson processes. primary goal describe multiple groups data coupled mixture models. sharing statistical properties groups reliable model estimation, observed samples group limited noisy. probabilistic standpoint, framework obtained devising joint stochastic process generates DPs mutual dependency. particularly, desirable design satisfies properties) Sharing mixture components (atoms) groups. ) marginal distribution atoms group remains. ) Flexible configuration inter-group dependencies. example, prior weight common atom vary groups. achieving goals simultaneously nontrivial. existing constructions] meet properties, impose restrictions model structure. groups arranged tree chain). present framework address issue. specifically, express mixture models group stochastic combination set latent dps. multi-multi association data groups latent DPs greater ﬂexibility model configurations, opposed prior work provide detailed comparison section). derive MCMC sampling method infer model parameters grouped observations. background provide review Dirichlet processes order lay oretical foundations method herein. discuss related construction dependent DPs proposed], exploits connection Poisson Dirichlet processes support operations. dirichlet process, denoted), distribution probability measures, characterized concentration parameter base measure underlying space sample path ) distribution sethuraman] showed surely discrete (with countably infinite support), expressed     beta, ?). ) This stick breaking representation. discrete nature makes suited serve prior component parameters mixture models. generally, mixture model, data sample considered generated component model parameter denoted component parameters samples realization. formulation ),     ) infinite series, infeasible instantiate such, Chinese restaurant process. directly sample component parameters, integrated out.  )      ) here, denotes component parameters denotes number distinct atoms) denotes number occurrences atom given, likelihood generate conditioned incorporated, resulting modulated sampling scheme below.  denote likelihood generate. (?  denote marginal likelihood. parameter prior probability proportional set probability proportional), draw atom posterior parameter distribution recently, Lin. ] proposed construction DPs based connections poisson, gamma, Dirichlet processes. construction operations derive DPs depending existing ones, develop coupled model. here, provide review operations. ) superposition.   independent DPs    dir    stochastic convex combination DPs remains:          ) q11 q21 q31 q22 q32 x1i x3i Groups q42 x2i Latent DPs zti rtk xti Atoms x4i Figure This shows graphical model coupled formulation case groups latent dps. mixture model inherits atoms probability qts resulting). figure reformulated model Gibbs sampling latent dps, groups data, atoms. sample xti attached label zti assigns atom ?zti generate zti draw latent (from mult choose label refrom. sampling, integrated out, resulting mutual dependency zti Chinese restaurant process.  ) sub-sampling.   ). obtains sub-sampling independent Bernoulli trials. sub-sampling probability draws binary atom decide wher retain, resulting  ).  here, denotes sub-sampling operation(with probability-normalized coeﬃcient  ) transition.   ), perturbing locations atom folp? lowing probabilistic transition kernel yields operations originally developed evolve Markov chain, show section utilized construct models structures. coupled Nonparametric Mixture Models Our primary goal develop joint formulation group-wise mixture models components shared groups weights parameters shared components vary groups. propose construction illustrated Figure suppose groups data, mixture model. coupled latent dps. generative formulation follows: first, generate latent DPs independently, ),    ) second, generate dependent dps, group data, combining sub-sampled versions latent DPs stochastic convex combination.     cts Sqts (ct1   ctml dir qt1    qtml ) intuitively, group data (say), choose subset atoms latent source bring toger generate here, qts prior probability Patom inherited note formulation furr extended cts (sqts )). here, probabilistic transition kernel. transition operation, extension parameters vary groups. particularly, atom parameter adapted version itself, atom inherited third, generate component parameters data samples standard way        ) here data sample group atom parameter.  oretical Analysis orems (proofs provided supplementary material) demonstrate that, result construction above, marginal distribution: PML qts orem stochastic process. )  show dependent, covariance orem below. orem measurable subset cov(dt1 Dt2 qt1 qt2  )).  qt1 qt2) hyper-parameters inﬂuence model characteristics ways. inheritance probabilities. -values) control closely models coupled. models strongly coupled, exists subset latent dps, inherit atoms high probabilities, coupling weaker-values set differently. latent concentration parameters. values control frequently atoms created. generally, higher values lead atoms data, resulting finer clusters. anor important factor number latent dps. large number latent DPs fine-grained control model configuration cost increased complexity.  Comparison Models review related approaches discuss differences proposed here. similar work, HDPs] model grouped data. models arranged tree. child parent. model mixture model group inherit multiple sources, making applicable general contexts. worth emphasizing enabling inheritance multiple parents straightforward extension, entails oretical practical challenges: first, combine atoms multiple DPs guaranteeing resultant process remains requires careful design formulation. combination coeﬃcients Dirichlet distribution, parent properly sub-sampled). second, sampling procedure determine source atom, which, again, nontrivial special algorithmic design (see section maintain detailed balance. ] defines gamma process extended space. group derived normalized restriction measurable subset. DPs derived overlapped subsets dependent Though motivated differently, construction reduced formulation form ctj subset latent DPs compared. ), essentially special case present construction sub-sampling. -values equal). consequently, combination coeﬃcients satisfy (ctj dir implying relative weights latent sources restricted groups inherit both. contrast, approach weights latent DPs vary groups. also doesn atom parameters vary groups. sampling Algorithm This section introduces Gibbs sampling algorithm jointly estimate mixture models multiple groups. overall, algorithm extension Chinese restaurant process, aspects) conditional probability labels depend total number samples entire corpus (instead specific group). note differs hdp, probabilities depend number tables. ) Each group maintains distribution latent DPs choose from, reﬂects contributions sources. ) leverages sub-sampling operation explicitly control model complexity. particular, group maintains indicators wher atoms inherited, \\x0cand consequence, deemed irrelevant put scope. ) multiple latent dps, atom, uncertainty from. specific step takes account, reassigning atom sources. set notations. recall groups data, latent DPs link observations group xt1   xtnt denote atom. note index globally unique identifier atom, changed atom relocation. atom correspond multiple data samples. instantiating parameter data sample xti attach xti indicator zti associates sample atom. equivalent setting ?zti facilitate sampling process, atom maintain indicator latent, set counters {mtk mtk equals number data samples group. maintain set latent), indices atoms rein. model. ) reformulated, shown Fig consists steps) Generate latent dps:   draw ). ) Generate combination coeﬃcients: group draw (ct1   ctml dir qt1    qtml group-specific prior sources group. ) Decide inheritance: atom draw binary variable rtk(rtk qtsk wher inherited group. index latent from. ) Generate data: generate xti choose latent drawing mult(ct1 ctml draw atom produce xti based formulation, derive Gibbs sampling steps update atom parameters hidden variables. ) Update labels. recall data sample xti label variable zti atom accounting xti draw zti choose latent source denote index uti  denote labels zti denote inheritance indicators. likelihood xti (with integrated out) (xti —uti (xti qts (xti ) wst qts :rtk  here total number samples groups (except xti wst :rtk (xti pdf xti. (xti (xti?. derivations formulas sampling supplemental document. hence(uti—ors) (uti(xti —uti cts(xti —uti ) here, (ct1   ctml group-specific prior latent sources. latent chosen (using formula above), draw atom. similar Chinese restaurant process: probability proportional (xti set zti probability proportional qts (xti), draw atom atoms contained rtk (inherited drawn step. modify relevant quantities accordingly, \\x0csuch mtk label zti changed. moreover, atom created, initially assigned latent generates. setting uti ) Update inheritance indicators. atom data group, inherited set rtk however, observed, doesn imply rtk atom (suppose (rtk—ors) qts  —rtk ors) qts (rtk—ors) qts  —rtk ors) qts  here qts  number samples groups (excluding group).  function defined  ) ?(? )/?(?  intuitively large (indicating appears frequently HDP (train) HDP (test?ldp (train?ldp (test) SNGP (train) SNGP (test?ldp (train?ldp (test) 4500 4000 perplexity 3500 3000 2500 2800 1500 Figure model structures. 2400 2200 2000 2000-ldp train docs 400 train docs 800 train docs 1200 2600 perplexity HDP-ldp 1800 200 400 600 800 training docs 1000 1200 figure results NIPS data obfigure results NIPS data ustained training sets sizes. ing-ldp, values. groups) large group inherited. circumstances, seen, inherited. ) Update combination coeﬃcients. coeﬃcients (ct1   ctml reﬂect relative contribution latent group. dirichlet distribution priori (see). labels samples group,   dir qt1 mtk    qtml mtk   here?iml mtk total number samples group ) Update atom parameters. labels, update atoms-drawing parameters posterior distributions. denote set data samples atom, draw  denotes posterior distribution conditioned pdf (?)  ?). ) Reassign atoms. model, atom surely unique latent. distinct sources). leads important question: How assign atoms latent dps? initially, atom assigned latent generated. necessarily optimal. here, treat assignment atom variable. atom indicating source. —ors) qts qts :rtk:rtk When atom reassigned Hs0 move index Is0 experiments framework developed paper generic tool model grouped data. section, present experiments applications: document analysis scene modeling. primary goal demonstrate key distinctions proposed approach nonparametric methods, study design inﬂuences empirical performance.  Document Analysis Topic models] widely statistical analysis documents. general, topic model comprises set topics, multinomial distribution, words independently generated. here, formulate Coupled Topic Model extending LDA] model multiple groups documents. specifically, associates group mixture topics, characterized sample given, words document generated independently, topic drawn exploit statistical dependency groups, furr introduce set latent DPs link mixtures, above. nips) database], 2484 papers published 1987 2003, experiments. clean data removing words occur fewer times corpus 2000 papers, resulting reduced vocabulary comprised 11729 \\x0cwords. data divided groups, year. perform experiments configurations, ways connect latent sources data groups, illustrated Figure ) Single Latent-ldp): latent connecting groups-values set. structure similar hdp, formulation different: HDP generates group-specific mixtures latent base measure, model involves explicit sub-sampling. ) Multi Latent-ldp): types latent DPs local global ones. local latent DPs introduced sharing statistical strength groups close, capture intuition papers published consecutive years share topics published distant years. inheritance probability local latent set qts exp —/?). also, recognizing topics shared entire corpus, introduce global latent, group inherit atoms probability, distant groups connected. design illustrates ﬂexibility proposed framework leverage ﬂexibility address practical needs. comparison, anor setting-values-ldp structure: set qts   connect  qts special setting, formulation reduces]. test HDP settings]: gamma)  gamma). design parameters set below. place weak prior latent, gamma), periodically update value. base distribution assumed dir), uniform distribution probability simplex. experiment compare methods training sets sizes. divide papers disjoint halves, training testing. test, models estimated subset specific size randomly chosen training corpus. learned models tested training subset held-out testing set, study gap empirical generalized performance, measured terms perplexity. Figure observe) general, training set size increases, perplexity evaluated training set increases testing set decreases. however, convergence faster local coupling. -ldp). suggests sharing statistical strength local latent DPs improves reliability estimation, training data limited. ) Even training set size increased 1200, methods local coupling yield lower perplexity ors. partly ascribed model structure. example, papers published consecutive years tend share lots topics, however, topics similar compare papers published recently decade ago. set local latent DPs capture relations effectively single global one. ) proposed method-ldp setting outperforms methods, including. -ldp, contribution decreases gracefully increases. encourages latent locally focused, allowing atoms rein \\x0cshared entire corpus. enabled explicit subsampling. , instead, mechanism vary contributions latent dps, make hard limit spans achieve locality. issue addressed multiple level latent nodes spans, increase complexity, risk overfitting. -ldp, recall set qts exp —/?). here, important design parameter controls range local coupling. results acquired values shown Figure optimal performance attained choice balances share atoms desire latent DPs locally focused. generally, optimum depends data. training set limited, increase enlarge coupling range.  Scene Modeling Scene modeling important task computer vision. approaches, topic models build bag-features image representation] increasingly popular 100 HDP (train) HDP (test?ldp (train?ldp (test) SNGP (train) SNGP (test?ldp (train?ldp (test) water cascade coast hill ocean sky perplexity mountain snowy boardwalk swamp Figure This figure shows images categories selected experiment. 100 150 200 250 300 350 training images 400 450 500 Figure results SUN data, training sets sizes. widely statistical modeling visual scenes. trend, Dirichlet processes employed discover visual topics observed scenes]. apply proposed method jointly model topics multiple scene categories. rar pursuing optimal scene model, primarily aimed comparing nonparametric methods mixture model estimation, reasonable setting. choose subset SUN database]. selected set comprises outdoor categories: mountain snowy, hill, boardwalk, swamp, water cascade, ocean, coast sky. number images category ranges 100. figure shows images. categories similar. ocean coast, boardwalk swamp, etc), ors largely different. derive image representation, pca-sift] descriptors densely extracted training image, pooled toger quantized Kmeans 512 visual words. way, image represented histogram 512 bins. methods mentioned compared. mldp, introduce global latent capture common topics-values set uniformly, set local latent dps, category. prior probability inheriting latent, local DPs. prior knowledge similarity categories assumed, latent DPs incorporated provide mechanism local coupling. , latent dps, connected pair categories. again, divide data disjoint halves, training testing, evaluate performance terms perplexity. results shown Figure observe trends similar NIPS data: local coupling helps model estimation, model-ldp setting furr reduces perplexity (from, compared). due ﬂexible configure local coupling weights latent DPs vary. conclusion presented principled approach modeling grouped data, mixture models groups coupled set latent dps. proposed framework mixture model inherit multiple latent dps, latent contribute differently groups, providing great ﬂexibility model design. experiments document analysis image modeling demonstrated utility ﬂexibility. particularly, proposed method makes make modeling choices. latent DPs connection patterns, substantially improving effectiveness estimated models. -values treated design parameters, extend framework incorporate prior models parameters. extensions lead constructions richer structure capable addressing complex problems. acknowledgements This research partially supported Oﬃce Naval Research Multidisciplinary Research Initiative (muri) program, award N000141110688 DARPA award fa8650-7154. Mixture distributions widely statistical modeling complex data. classical formulations number components priori, leading diﬃculties situations number eir unknown hard estimate advance. bayesian nonparametric models, notably based Dirichlet processes (dps], emerged important method address issue. basic idea mixture models sample, distribution countably infinite set, prior component parameters. one significant assumption underlying mixture model observations infinitely exchangeable. this assumption hold cases multiple groups data, samples groups generally exchangeable. among approaches issue, hierarchical Dirichlet processes (hdps], organize DPs tree parents acting base measure children, popular. hdps extended variety ways. kim Smyth] incorporated group-specific random perturbations, allowing component parameters] proposed dynamic hdps, vary groups. ren. combine previous time step current time step. methods developed. maceachern] proposed DDP model parameters vary stochastic process. griffin Steel] proposed order-based ddp, atoms weighted differently permutation Beta variables stick-breaking. chung Dunson] carried approach furr, local predictors select subsets atoms. recently, connections poisson, gamma, Dirichlet processes exploited. rao Teh] proposed spatially normalized Gamma process, set dependent DPs derived normalizing restricted projections auxiliary Gamma process overlapping sub-regions. lin] proposed construction dependent dps, supports dynamic evolution operations underlying Poisson processes. our primary goal describe multiple groups data coupled mixture models. sharing statistical properties groups reliable model estimation, observed samples group limited noisy. from probabilistic standpoint, framework obtained devising joint stochastic process generates DPs mutual dependency. particularly, desirable design satisfies properties) Sharing mixture components (atoms) groups. ) marginal distribution atoms group remains. ) Flexible configuration inter-group dependencies. for example, prior weight common atom vary groups. achieving goals simultaneously nontrivial. whereas existing constructions] meet properties, impose restrictions model structure. groups arranged tree chain). present framework address issue. specifically, express mixture models group stochastic combination set latent dps. multi-multi association data groups latent DPs greater ﬂexibility model configurations, opposed prior work provide detailed comparison section). derive MCMC sampling method infer model parameters grouped observations. Background provide review Dirichlet processes order lay oretical foundations method herein. discuss related construction dependent DPs proposed], exploits connection Poisson Dirichlet processes support operations. Dirichlet process, denoted), distribution probability measures, characterized concentration parameter base measure underlying space each sample path ) distribution sethuraman] showed surely discrete (with countably infinite support), expressed     beta, ?). ) This stick breaking representation. this discrete nature makes suited serve prior component parameters mixture models. generally, mixture model, data sample considered generated component model parameter denoted component parameters samples realization. formulation ),     ) infinite series, infeasible instantiate such, Chinese restaurant process. directly sample component parameters, integrated out.  )      ) here, denotes component parameters denotes number distinct atoms) denotes number occurrences atom when given, likelihood generate conditioned incorporated, resulting modulated sampling scheme below. let denote likelihood generate. (?  denote marginal likelihood. parameter prior probability proportional set probability proportional), draw atom posterior parameter distribution recently, Lin. ] proposed construction DPs based connections poisson, gamma, Dirichlet processes. construction operations derive DPs depending existing ones, develop coupled model. here, provide review operations. ) superposition. let  independent DPs    dir    stochastic convex combination DPs remains:          ) q11 q21 q31 q22 q32 x1i x3i Groups q42 x2i Latent DPs zti rtk xti Atoms x4i Figure This shows graphical model coupled formulation case groups latent dps. each mixture model inherits atoms probability qts resulting). figure reformulated model Gibbs sampling latent dps, groups data, atoms. each sample xti attached label zti assigns atom ?zti generate zti draw latent (from mult choose label refrom. sampling, integrated out, resulting mutual dependency zti Chinese restaurant process.  ) sub-sampling. let  ). one obtains sub-sampling independent Bernoulli trials. given sub-sampling probability draws binary atom decide wher retain, resulting  ).  here, denotes sub-sampling operation(with probability-normalized coeﬃcient  ) transition. given  ), perturbing locations atom folp? lowing probabilistic transition kernel yields while operations originally developed evolve Markov chain, show section utilized construct models structures. Coupled Nonparametric Mixture Models Our primary goal develop joint formulation group-wise mixture models components shared groups weights parameters shared components vary groups. propose construction illustrated Figure suppose groups data, mixture model. coupled latent dps. generative formulation follows: first, generate latent DPs independently, ),    ) second, generate dependent dps, group data, combining sub-sampled versions latent DPs stochastic convex combination. for    cts Sqts (ct1   ctml dir qt1    qtml ) intuitively, group data (say), choose subset atoms latent source bring toger generate here, qts prior probability Patom inherited note formulation furr extended cts (sqts )). here, probabilistic transition kernel. using transition operation, extension parameters vary groups. particularly, atom parameter adapted version itself, atom inherited third, generate component parameters data samples standard way        ) here data sample group atom parameter.  oretical Analysis orems (proofs provided supplementary material) demonstrate that, result construction above, marginal distribution: PML qts orem stochastic process. )  show dependent, covariance orem below. orem let measurable subset cov(dt1 Dt2 qt1 qt2  )).  qt1 qt2) hyper-parameters inﬂuence model characteristics ways. inheritance probabilities. -values) control closely models coupled. two models strongly coupled, exists subset latent dps, inherit atoms high probabilities, coupling weaker-values set differently. latent concentration parameters. values control frequently atoms created. generally, higher values lead atoms data, resulting finer clusters. anor important factor number latent dps. large number latent DPs fine-grained control model configuration cost increased complexity.  Comparison Models review related approaches discuss differences proposed here. similar work, HDPs] model grouped data. such models arranged tree. child parent. our model mixture model group inherit multiple sources, making applicable general contexts. worth emphasizing enabling inheritance multiple parents straightforward extension, entails oretical practical challenges: first, combine atoms multiple DPs guaranteeing resultant process remains requires careful design formulation. combination coeﬃcients Dirichlet distribution, parent properly sub-sampled). second, sampling procedure determine source atom, which, again, nontrivial special algorithmic design (see section maintain detailed balance. ] defines gamma process extended space. for group derived normalized restriction measurable subset. DPs derived overlapped subsets dependent Though motivated differently, construction reduced formulation form ctj subset latent DPs compared. ), essentially special case present construction sub-sampling. -values equal). consequently, combination coeﬃcients satisfy (ctj dir implying relative weights latent sources restricted groups inherit both. contrast, approach weights latent DPs vary groups. also doesn atom parameters vary groups. Sampling Algorithm This section introduces Gibbs sampling algorithm jointly estimate mixture models multiple groups. overall, algorithm extension Chinese restaurant process, aspects) conditional probability labels depend total number samples entire corpus (instead specific group). note differs hdp, probabilities depend number tables. ) Each group maintains distribution latent DPs choose from, reﬂects contributions sources. ) leverages sub-sampling operation explicitly control model complexity. particular, group maintains indicators wher atoms inherited, \\x0cand consequence, deemed irrelevant put scope. ) multiple latent dps, atom, uncertainty from. specific step takes account, reassigning atom sources. set notations. recall groups data, latent DPs link observations group xt1   xtnt denote atom. note index globally unique identifier atom, changed atom relocation. since atom correspond multiple data samples. instead instantiating parameter data sample xti attach xti indicator zti associates sample atom. this equivalent setting ?zti facilitate sampling process, atom maintain indicator latent, set counters {mtk mtk equals number data samples group. maintain set latent), indices atoms rein. model. ) reformulated, shown Fig consists steps) Generate latent dps:   draw ). ) Generate combination coeﬃcients: group draw (ct1   ctml dir qt1    qtml group-specific prior sources group. ) Decide inheritance: atom draw binary variable rtk(rtk qtsk wher inherited group. here index latent from. ) Generate data: generate xti choose latent drawing mult(ct1 ctml draw atom produce xti based formulation, derive Gibbs sampling steps update atom parameters hidden variables. ) Update labels. recall data sample xti label variable zti atom accounting xti draw zti choose latent source denote index uti let denote labels zti denote inheritance indicators. likelihood xti (with integrated out) (xti —uti (xti qts (xti ) wst qts :rtk  here total number samples groups (except xti wst :rtk (xti pdf xti. (xti (xti?. derivations formulas sampling supplemental document. hence(uti—ors) (uti(xti —uti cts(xti —uti ) here, (ct1   ctml group-specific prior latent sources. once latent chosen (using formula above), draw atom. this similar Chinese restaurant process: probability proportional (xti set zti probability proportional qts (xti), draw atom only atoms contained rtk (inherited drawn step. modify relevant quantities accordingly, \\x0csuch mtk label zti changed. moreover, atom created, initially assigned latent generates. setting uti ) Update inheritance indicators. atom data group, inherited set rtk however, observed, doesn imply rtk for atom (suppose (rtk—ors) qts  —rtk ors) qts (rtk—ors) qts  —rtk ors) qts  here qts  number samples groups (excluding group).  function defined  ) ?(? )/?(?  intuitively large (indicating appears frequently HDP (train) HDP (test?ldp (train?ldp (test) SNGP (train) SNGP (test?ldp (train?ldp (test) 4500 4000 perplexity 3500 3000 2500 2800 1500 Figure model structures. 2400 2200 2000 2000-ldp train docs 400 train docs 800 train docs 1200 2600 perplexity HDP-ldp 1800 200 400 600 800 training docs 1000 1200 Figure results NIPS data obfigure results NIPS data ustained training sets sizes. ing-ldp, values. groups) large group inherited. under circumstances, seen, inherited. ) Update combination coeﬃcients. coeﬃcients (ct1   ctml reﬂect relative contribution latent group. Dirichlet distribution priori (see). given labels samples group,   dir qt1 mtk    qtml mtk   here?iml mtk total number samples group ) Update atom parameters. given labels, update atoms-drawing parameters posterior distributions. let denote set data samples atom, draw  denotes posterior distribution conditioned pdf (?)  ?). ) Reassign atoms. model, atom surely unique latent. distinct sources). this leads important question: How assign atoms latent dps? initially, atom assigned latent generated. this necessarily optimal. here, treat assignment atom variable. consider atom indicating source. —ors) qts qts :rtk:rtk When atom reassigned Hs0 move index Is0 Experiments framework developed paper generic tool model grouped data. section, present experiments applications: document analysis scene modeling. primary goal demonstrate key distinctions proposed approach nonparametric methods, study design inﬂuences empirical performance.  Document Analysis Topic models] widely statistical analysis documents. general, topic model comprises set topics, multinomial distribution, words independently generated. here, formulate Coupled Topic Model extending LDA] model multiple groups documents. specifically, associates group mixture topics, characterized sample with given, words document generated independently, topic drawn exploit statistical dependency groups, furr introduce set latent DPs link mixtures, above. NIPS) database], 2484 papers published 1987 2003, experiments. clean data removing words occur fewer times corpus 2000 papers, resulting reduced vocabulary comprised 11729 \\x0cwords. data divided groups, year. perform experiments configurations, ways connect latent sources data groups, illustrated Figure ) Single Latent-ldp): latent connecting groups-values set. though structure similar hdp, formulation different: HDP generates group-specific mixtures latent base measure, model involves explicit sub-sampling. ) Multi Latent-ldp): types latent DPs local global ones. local latent DPs introduced sharing statistical strength groups close, capture intuition papers published consecutive years share topics published distant years. inheritance probability local latent set qts exp —/?). also, recognizing topics shared entire corpus, introduce global latent, group inherit atoms probability, distant groups connected. this design illustrates ﬂexibility proposed framework leverage ﬂexibility address practical needs. for comparison, anor setting-values-ldp structure: set qts   connect  qts under special setting, formulation reduces]. test HDP settings]: gamma)  gamma). design parameters set below. place weak prior latent, gamma), periodically update value. base distribution assumed dir), uniform distribution probability simplex. experiment compare methods training sets sizes. divide papers disjoint halves, training testing. test, models estimated subset specific size randomly chosen training corpus. learned models tested training subset held-out testing set, study gap empirical generalized performance, measured terms perplexity. from Figure observe) general, training set size increases, perplexity evaluated training set increases testing set decreases. however, convergence faster local coupling. -ldp). this suggests sharing statistical strength local latent DPs improves reliability estimation, training data limited. ) Even training set size increased 1200, methods local coupling yield lower perplexity ors. this partly ascribed model structure. for example, papers published consecutive years tend share lots topics, however, topics similar compare papers published recently decade ago. set local latent DPs capture relations effectively single global one. ) proposed method-ldp setting outperforms methods, including. -ldp, contribution decreases gracefully increases. this encourages latent locally focused, allowing atoms rein \\x0cshared entire corpus. this enabled explicit subsampling. , instead, mechanism vary contributions latent dps, make hard limit spans achieve locality. whereas issue addressed multiple level latent nodes spans, increase complexity, risk overfitting. for-ldp, recall set qts exp —/?). here, important design parameter controls range local coupling. results acquired values shown Figure optimal performance attained choice balances share atoms desire latent DPs locally focused. generally, optimum depends data. when training set limited, increase enlarge coupling range.  Scene Modeling Scene modeling important task computer vision. among approaches, topic models build bag-features image representation] increasingly popular 100 HDP (train) HDP (test?ldp (train?ldp (test) SNGP (train) SNGP (test?ldp (train?ldp (test) water cascade coast hill ocean sky perplexity mountain snowy boardwalk swamp Figure This figure shows images categories selected experiment. 100 150 200 250 300 350 training images 400 450 500 Figure results SUN data, training sets sizes. widely statistical modeling visual scenes. along trend, Dirichlet processes employed discover visual topics observed scenes]. apply proposed method jointly model topics multiple scene categories. rar pursuing optimal scene model, primarily aimed comparing nonparametric methods mixture model estimation, reasonable setting. choose subset SUN database]. selected set comprises outdoor categories: mountain snowy, hill, boardwalk, swamp, water cascade, ocean, coast sky. number images category ranges 100. figure shows images. categories similar. ocean coast, boardwalk swamp, etc), ors largely different. derive image representation, pca-sift] descriptors densely extracted training image, pooled toger quantized Kmeans 512 visual words. way, image represented histogram 512 bins. all methods mentioned compared. for mldp, introduce global latent capture common topics-values set uniformly, set local latent dps, category. prior probability inheriting latent, local DPs. whereas prior knowledge similarity categories assumed, latent DPs incorporated provide mechanism local coupling. for, latent dps, connected pair categories. again, divide data disjoint halves, training testing, evaluate performance terms perplexity. results shown Figure observe trends similar NIPS data: local coupling helps model estimation, model-ldp setting furr reduces perplexity (from, compared). this due ﬂexible configure local coupling weights latent DPs vary. Conclusion presented principled approach modeling grouped data, mixture models groups coupled set latent dps. proposed framework mixture model inherit multiple latent dps, latent contribute differently groups, providing great ﬂexibility model design. experiments document analysis image modeling demonstrated utility ﬂexibility. particularly, proposed method makes make modeling choices. latent DPs connection patterns, substantially improving effectiveness estimated models. while-values treated design parameters, extend framework incorporate prior models parameters. such extensions lead constructions richer structure capable addressing complex problems. acknowledgements This research partially supported Oﬃce Naval Research Multidisciplinary Research Initiative (muri) program, award N000141110688 DARPA award fa8650-7154.',\n",
       " 'PP4891': 'multitask learning exploits relationships learning tasks order improve performance, common subset features tasks hand. group lasso (glasso, naturally suited situation: feature selected task, selected tasks. restrictive applications, motivates rigid approach multitask feature selection. suppose features organized overlapping subsets notion similarity, features task similar, necessarily identical, suited tasks. words, feature task suggests subset belongs \\x0ccontain features tasks (figure). paper, introduce sparse overlapping sets lasso (soslasso), convex program recover sparsity patterns situations explained above. soslasso generalizes lasso] glasso, effectively spanning range well-known procedures. soslasso capable exploiting similarities features tasks, unlike Glasso force tasks features. produces sparse solutions, unlike lasso encourages similar patterns sparsity tasks. sparse group lasso] special case SOSlasso applies disjoint sets, significant limitation features easily partitioned, case motivating fmri. main contribution paper oretical analysis soslasso, covers sparse group lasso special case (furr differentiating]). performance SOSlasso analyzed, error bounds derived general loss functions, consistency shown squared error loss. experiments real syntic data demonstrate advantages SOSlasso relative lasso glasso.  Sparse Overlapping Sets SOSlasso encourages sparsity patterns similar, identical, tasks. accomplished decomposing features task groups   task, set features considered similar tasks. conceptually, SOSlasso selects subsets tasks, identifies unique sparse solution task drawing features selected subsets. fmri application discussed later, subsets simply clusters adjacent spatial data points (voxels) brains multiple subjects. figure shows patterns typically arise sparse multitask learning applications, rows features columns correspond tasks. past work focused recovering variables exhibit group sparsity, groups overlap], finding application genetics, handwritten character recognition] climate oceanography]. related lines, exclusive lasso] explicitly variables sets negatively correlated. ) Sparse) Group sparse) Group sparse sparse) Group sparse sparse Figure comparison sparsity patterns. ) shows standard sparsity pattern. group sparse patterns promoted Glasso] shown). ), show patterns considered]. finally), show patterns interested paper.  fmri Applications psychological studies involving fmri, multiple participants scanned subjected experimental manipulations. cognitive Neuroscientists interested identifying patterns activity cognitive states, construct model activity accu \\x0crately predicts cognitive state evoked trials. datasets, reasonable expect general areas brain respond manipulation participant. however, specific patterns activity regions vary, neural codes vary participant] brains vary size shape, rendering neuroanatomy approximate guide location relevant information individuals. short, voxel prediction participant suggests general anatomical neighborhood voxels found, precise voxel. logistic Glasso], lasso], elastic net penalty] applied neuroimaging data, methods exclusively account common macrostructure differences microstructure brains. soslasso, contrast, lends scenario, experiments.  Organization rest paper organized follows: Section outline notations formally set problem. introduce SOSlasso regularizer. derive key properties regularizer Section section specialize problem multitask linear regression setting), derive consistency rates same, leveraging ideas]. outline experiments performed simulated data Section section, perform logistic regression fmri data, argue SOSlasso yields interpretable multivariate solutions compared Glasso lasso. sparse Overlapping Sets Lasso formalize notations sequel. lowercase uppercase bold letters vectors matrices respectively. assume multitask learning framework, data matrix  task ,  assume exists vector measurements  obtained form , ).       suppose (possibly overlapping) groups, maximum, size groups sets ?similar? features, notion similarity application dependent. assume groups identically zero. active groups, furr assume fraction  , coeﬃcients group zero. optimization program paper  ) arg min)   ,      [xt1 xt2   xtt) regularizer denotes loss function, depends data matrix squares logistic loss functions. squares setting, kyt  reformulate optimization problem) squares loss arg min ?xk22) [y1t y2t   ytt block diagonal matrix formed block concatenating reformulation ease exposition (see] references rein). note     define    set groups defined formed aggregating rows originally composed groups define regularizer promotes sparsity overlapping sets similar features) inf kwg kwg.  constants balance tradeoff group norms norm. size support restricted variables indexed group set vectors, vector support restricted groups    ith coeﬃcient soslasso optimization) defined). set vectors optimal decomposition achieve inf). objective function) convex coercive. hence, optimal decomposition exists.   term redundant, reducing) overlapping group lasso penalty introduced], studied].  overlapping group lasso term vanishes) reduces lasso penalty. . results paper easily modified incorporate settings support Values kxk1 kxg (kxg kxg.602.602.602.602 Table Different instances vector norms. table insight kind sparsity patterns preferred function). optimization problems) prefer solutions small(?). consider instances r10 group lasso) function values. vector assumed made groups}. ) smallest support set sparse groups, groups selected. norm account sparsity groups, group lasso norm account sparsity groups. solve) regularizer proposed), covariate duplication method], reduce problem overlapping sparse group lasso problem. proximal point methods] conjunction MALSAR] package solve optimization problem. error Bounds SOSlasso General Loss Functions derive key properties regularizer(?) ), independent loss function used. lemma function) norm proof basic properties norms optimal decompositions imply optimal decomposition detailed proof, refer supplementary \\x0cmaterial. dual norm) bounded ) max. ) max. (kwg kwg ) max{ max.  2kwg .  kwg   ) max kug) fact constraint set) superset constraint set previous statement, kak2 kak1 ) noting maximum obtained setting?  arg maxg kug inequality) tractable? 2ku actual dual norm, derivations below. (?) norm, apply methods developed] derive consistency rates optimization problems). notations] possible. definition norm(?) decomposable respect subspace pair ) ,   lemma Let  vector decomposed (overlapping) groups withingroup sparsity.   set active groups  supp? support set subspace spanned coordinates indexed. norm) decomposable respect, result straightforward noting supports decompositions vectors overlap. defer proof supplementary material. definition Given subspace, subspace compatibility constant respect norm) } ) kxk Lemma Consider vector decomposed  active groups. suppose maximum group size assume fraction  , coordinates active group zero.  ) ?)  —kxk2 Proof For vector supp)  exists representation supports overlap.   ) (kwg kwg ?) kwg ?)  —kxk2   ?)  (lemma) upper bound subspace compatibility constant respect norm subspace \\x0cindexed support vector, contained span union groups  definition For set vector loss function ) satisfies Restricted Strong convexity(rsc) condition parameter tolerance  ?    ?  ? ?  ?k22  ?   paper, vectors lie groups, display within-group sparsity. implies tolerance ? ignore term henceforth. define set, sequel,       ?   (?) denotes projection subspace. based results above, apply result] soslasso: orem (corollary]) Consider convex differentiable loss function RSC holds constants  ), norm(?) decomposable sets. optimization program), parameter ? ? ? )), optimal solution ) satisfies   ) result shows general bound error lasso sparse overlapping sets. note regularization parameter RSC constant depend loss function ). convergence logistic regression settings derived methods]. section, squares loss), show estimate SOSlasso consistent. consistency SOSlasso Squared Error Loss bound dual norm gradient loss function, bound  ) ?xk2 gradient loss function respect        (see Section). goal find upper bound quantity ) max max matrix restricted columns indexed group prove upper bound quantity results follow.   ,    defining ?max maximum singular value ?k22  ?k22  — ?k22  chi-squared random variable degrees freedom. work tractable chi squared random variable bound dual norm. lemma helps obtain bound maximum random variables. lemma Let   chi-squared random variables degrees freedom. constant  max   exp log  Proof From chi-squared tail bound  exp ) result union bound inverting expression. lemma Consider loss function kyt   deterministic measurements corrupted AWGN variance  regularizer), dual norm gradient loss function bounded (log probability exp), maxg  proof Let  — begin upper bound obtained dual norm regularizer)   max) max )   (iii)   max  exp log ) formulation gradient loss function fact square maximum negative numbers maximum squares numbers. ), defined maxg finally, made Lemma (iii). set log obtain result. combine results developed derive consistency result SOS lasso, squares loss function. orem Suppose obtain linear measurements sparse overlapping grouped matrix   corrupted AWGN variance  suppose matrix decomposed overlapping groups maximum size active. furrmore, assume fraction  , coeﬃcients active group. vectorized SOSlasso multitask regression problem): arg min ?xk2) inf (kwg kwg.  Suppose data matrices random, loss function satisfies restricted strong (log holds convexity assumptions parameter  probability exp),    (log    define maxg ?max Proof Follows substituting orem results Lemma Lemma. ], convergence rate matches group lasso, additional multiplicative factor stems fact signal sparse structure ?embedded? group sparse structure. visualizing optimization problem solving lasso group lasso framework lends intuition result. note bound smaller standard group lasso. experiments Results Syntic data, Gaussian Linear Regression For tasks, define 2002 element vector divided 500 groups size group overlaps neighboring groups,   ,   ,   },    groups activated uniformly random, populated uniform, distribution. proportion coeﬃcients largest magnitude retained true signal. task, obtain 250 linear measurements, 250 matrix. corrupt measurement Additive White Gaussian Noise (awgn), assess signal recovery terms Mean Squared Error (mse). regularization parameter clairvoyantly picked minimize MSE range parameter values. results applying lasso, standard latent group lasso], SOSlasso data plotted Figures), varying ), varying . point Figures), average 100 trials, trial based random instance gaussian data matrices. .015 Glasso SOSlasso lasso Glasso SOSlasso.015 MSE MSE.005.005 ) Varying ?? ) Varying ) Sample pattern Figure noise increased), proposed penalty function (soslasso) recover true coeﬃcients accurately group lasso (glasso). also, alpha large, active groups sparse, standard overlapping group lasso outperforms methods. however, reduces, method propose outperforms group lasso). ) shows toy sparsity pattern, colors denoting overlapping groups SOSlasso fmri experiment, compared soslasso, lasso, Glasso analysis star-plus dataset]. subjects made judgements involved processing sentences pictures brains scanned half intervals fmri1 retained time points stimulus, \\x0cyielding 1280 measurements voxel. task distinguish, point time, stimulus subject processing. ] showed exists cross-subject consistency cortical regions prediction task. specifically, experts partitioned dataset overlapping regions interest (rois), reduced data discarding ROIs and, subject, averaging BOLD response voxels ROI showed classifier trained data subjects generalized applied data 6th. assessed wher SOSlasso leverage cross-individual consistency aid discovery predictive voxels requiring expert pre-selection rois, data reduction, alignment voxels existing raw data. note that, unlike], aim learn solution generalizes withheld subject. rar, aim discover group sparsity pattern suggests similar set voxels subjects, optimizing separate solution individual. soslasso exploit cross-individual anatomical similarity raw, coarsely-aligned data, show reduced cross-validation error relative lasso applied separately individual. solution sparse groups highly variable individuals, SOSlasso show reduced cross-validation error relative glasso. finally, SOSlasso finding cross-individual structure, features selects align expert-identified ROIs shown] carry consistent information. data documentation http://www.cmu.edu/afs.cmu.edu/project/www/ Figure Results fmri experiments. ) Aggregated sparsity patterns single brain slice. ) Crossvalidation error obtained method. lines connect data single subject. ) full sparsity pattern obtained soslasso. lasso Error Glasso lasso Glasso SOSlasso SOSlasso) Picture Sentence Picture Sentence) \\x0cmethod ROI) lasso.001 Glasso.002 SOSlasso Table Proportion selected voxels relevant ROIS aggregated subjects, two-tailed significance levels contrast lasso Glasso soslasso. trained classifiers-fold cross validation select regularization parameter, voxels preselection. group regions  voxels considered overlapping groups ?shifted? voxels dimensions Figure) shows individual error rates subjects methods. subjects, SOSlasso significantly lower cross-validation error rate individual lasso within-subjects.004 two-tailed), showing method exploit anatomical similarity subjects learn classifier each. soslasso showed significantly lower error rates glasso two-tailed), suggesting signal sparse selected regions variable subjects. figure) presents sample sparsity patterns obtained methods, aggregated subjects. red points voxels contributed positively picture classification subject, sentences; Blue points opposite interpretation. purple points voxels contributed positively picture sentence classification subjects. remaining slices SOSlasso shown Figure). things note Figure). first, Glasso solution fairly dense, voxels signaling picture sentence subjects. ?purple haze? demonstrates Glasso illsuited fmri analysis: voxel selected subject selected ors. approach succeed, likely, exists direct voxel-voxel correspondence neural code variable subjects. second, lasso solution sparse SOSlasso task-correlated voxel selected. leads higher cross-validation error, indicating ungrouped voxels inferior predictors (figure)). third, SOSlasso yields sparse solution, clustered. assess clusters align anatomical regions thought-priori involved sentence picture representation, calculated proportion selected voxels falling ROIs identified] relevant classification task (table). SOSlasso average% identified voxels fell rois, significantly lasso glasso. conclusions Extensions introduced soslasso, function recovers sparsity patterns hybrid overlapping group sparse sparse patterns regularizer convex programs, proved oretical convergence rates minimizing squares. soslasso succeeds multitask fmri analysis, makes inferences discovers oretically plausible brain regions lasso glasso. future work involves experimenting parameters group penalties, similarity groupings, functional connectivity fmri. irregular group size \\x0ccompensates voxels larger scanner coverage smaller-dimension (only slices relative xand-dimensions). Multitask learning exploits relationships learning tasks order improve performance, common subset features tasks hand. group lasso (glasso, naturally suited situation: feature selected task, selected tasks. this restrictive applications, motivates rigid approach multitask feature selection. suppose features organized overlapping subsets notion similarity, features task similar, necessarily identical, suited tasks. words, feature task suggests subset belongs \\x0ccontain features tasks (figure). paper, introduce sparse overlapping sets lasso (soslasso), convex program recover sparsity patterns situations explained above. soslasso generalizes lasso] glasso, effectively spanning range well-known procedures. soslasso capable exploiting similarities features tasks, unlike Glasso force tasks features. produces sparse solutions, unlike lasso encourages similar patterns sparsity tasks. sparse group lasso] special case SOSlasso applies disjoint sets, significant limitation features easily partitioned, case motivating fmri. main contribution paper oretical analysis soslasso, covers sparse group lasso special case (furr differentiating]). performance SOSlasso analyzed, error bounds derived general loss functions, consistency shown squared error loss. experiments real syntic data demonstrate advantages SOSlasso relative lasso glasso.  Sparse Overlapping Sets SOSlasso encourages sparsity patterns similar, identical, tasks. this accomplished decomposing features task groups   task, set features considered similar tasks. conceptually, SOSlasso selects subsets tasks, identifies unique sparse solution task drawing features selected subsets. fmri application discussed later, subsets simply clusters adjacent spatial data points (voxels) brains multiple subjects. figure shows patterns typically arise sparse multitask learning applications, rows features columns correspond tasks. past work focused recovering variables exhibit group sparsity, groups overlap], finding application genetics, handwritten character recognition] climate oceanography]. along related lines, exclusive lasso] explicitly variables sets negatively correlated. ) Sparse) Group sparse) Group sparse sparse) Group sparse sparse Figure comparison sparsity patterns. ) shows standard sparsity pattern. group sparse patterns promoted Glasso] shown). ), show patterns considered]. finally), show patterns interested paper.  fmri Applications psychological studies involving fmri, multiple participants scanned subjected experimental manipulations. cognitive Neuroscientists interested identifying patterns activity cognitive states, construct model activity accu \\x0crately predicts cognitive state evoked trials. datasets, reasonable expect general areas brain respond manipulation participant. however, specific patterns activity regions vary, neural codes vary participant] brains vary size shape, rendering neuroanatomy approximate guide location relevant information individuals. short, voxel prediction participant suggests general anatomical neighborhood voxels found, precise voxel. while logistic Glasso], lasso], elastic net penalty] applied neuroimaging data, methods exclusively account common macrostructure differences microstructure brains. soslasso, contrast, lends scenario, experiments.  Organization rest paper organized follows: Section outline notations formally set problem. introduce SOSlasso regularizer. derive key properties regularizer Section Section specialize problem multitask linear regression setting), derive consistency rates same, leveraging ideas]. outline experiments performed simulated data Section section, perform logistic regression fmri data, argue SOSlasso yields interpretable multivariate solutions compared Glasso lasso. Sparse Overlapping Sets Lasso formalize notations sequel. lowercase uppercase bold letters vectors matrices respectively. assume multitask learning framework, data matrix  task ,  assume exists vector measurements  obtained form , ). let      suppose (possibly overlapping) groups, maximum, size groups sets ?similar? features, notion similarity application dependent. assume groups identically zero. among active groups, furr assume fraction  , coeﬃcients group zero. optimization program paper  ) arg min)   ,      [xt1 xt2   xtt) regularizer denotes loss function, depends data matrix squares logistic loss functions. squares setting, kyt  reformulate optimization problem) squares loss arg min ?xk22) [y1t y2t   ytt block diagonal matrix formed block concatenating reformulation ease exposition (see] references rein). note     define    set groups defined formed aggregating rows originally composed groups define regularizer promotes sparsity overlapping sets similar features) inf kwg kwg.  constants balance tradeoff group norms norm. each size support restricted variables indexed group set vectors, vector support restricted groups    ith coeﬃcient SOSlasso optimization) defined). set vectors optimal decomposition achieve inf). objective function) convex coercive. hence, optimal decomposition exists.   term redundant, reducing) overlapping group lasso penalty introduced], studied]. when overlapping group lasso term vanishes) reduces lasso penalty. . all results paper easily modified incorporate settings Support Values kxk1 kxg (kxg kxg.602.602.602.602 Table Different instances vector norms. Table insight kind sparsity patterns preferred function). optimization problems) prefer solutions small(?). Consider instances r10 group lasso) function values. vector assumed made groups}. ) smallest support set sparse groups, groups selected. norm account sparsity groups, group lasso norm account sparsity groups. solve) regularizer proposed), covariate duplication method], reduce problem overlapping sparse group lasso problem. proximal point methods] conjunction MALSAR] package solve optimization problem. Error Bounds SOSlasso General Loss Functions derive key properties regularizer(?) ), independent loss function used. lemma function) norm proof basic properties norms optimal decompositions imply optimal decomposition for detailed proof, refer supplementary \\x0cmaterial. dual norm) bounded ) max. ) max. (kwg kwg ) max{ max.  2kwg .  kwg   ) max kug) fact constraint set) superset constraint set previous statement, kak2 kak1 ) noting maximum obtained setting?  arg maxg kug inequality) tractable? 2ku actual dual norm, derivations below. since(?) norm, apply methods developed] derive consistency rates optimization problems). notations] possible. definition norm(?) decomposable respect subspace pair ) ,   lemma Let  vector decomposed (overlapping) groups withingroup sparsity. let  set active groups  let supp? support set let subspace spanned coordinates indexed. norm) decomposable respect, result straightforward noting supports decompositions vectors overlap. defer proof supplementary material. definition Given subspace, subspace compatibility constant respect norm) } ) kxk Lemma Consider vector decomposed  active groups. suppose maximum group size assume fraction  , coordinates active group zero.  ) ?)  —kxk2 Proof For vector supp)  exists representation supports overlap.   ) (kwg kwg ?) kwg ?)  —kxk2   ?)  (lemma) upper bound subspace compatibility constant respect norm subspace \\x0cindexed support vector, contained span union groups  definition For set vector loss function ) satisfies Restricted Strong convexity(rsc) condition parameter tolerance  ?    ?  ? ?  ?k22  ?   paper, vectors lie groups, display within-group sparsity. this implies tolerance ? ignore term henceforth. define set, sequel,       ?   (?) denotes projection subspace. based results above, apply result] soslasso: orem (corollary]) Consider convex differentiable loss function RSC holds constants  ), norm(?) decomposable sets. for optimization program), parameter ? ? ? )), optimal solution ) satisfies   ) result shows general bound error lasso sparse overlapping sets. note regularization parameter RSC constant depend loss function ). convergence logistic regression settings derived methods]. section, squares loss), show estimate SOSlasso consistent. Consistency SOSlasso Squared Error Loss bound dual norm gradient loss function, bound consider ) ?xk2 gradient loss function respect        (see Section). our goal find upper bound quantity ) max max matrix restricted columns indexed group prove upper bound quantity results follow. since  ,    defining ?max maximum singular value ?k22  ?k22  — ?k22  chi-squared random variable degrees freedom. this work tractable chi squared random variable bound dual norm. lemma helps obtain bound maximum random variables. lemma Let   chi-squared random variables degrees freedom. constant  max   exp log  Proof From chi-squared tail bound  exp ) result union bound inverting expression. Lemma Consider loss function kyt   deterministic measurements corrupted AWGN variance  for regularizer), dual norm gradient loss function bounded (log probability exp), maxg  proof Let  — begin upper bound obtained dual norm regularizer)   max) max )   (iii)   max  exp log ) formulation gradient loss function fact square maximum negative numbers maximum squares numbers. ), defined maxg finally, made Lemma (iii). set log obtain result. combine results developed derive consistency result SOS lasso, squares loss function. orem Suppose obtain linear measurements sparse overlapping grouped matrix   corrupted AWGN variance  suppose matrix decomposed overlapping groups maximum size active. furrmore, assume fraction  , coeﬃcients active group. consider vectorized SOSlasso multitask regression problem): arg min ?xk2) inf (kwg kwg.  Suppose data matrices random, loss function satisfies restricted strong (log holds convexity assumptions parameter  probability exp),    (log    define maxg ?max Proof Follows substituting orem results Lemma Lemma. from], convergence rate matches group lasso, additional multiplicative factor this stems fact signal sparse structure ?embedded? group sparse structure. visualizing optimization problem solving lasso group lasso framework lends intuition result. note bound smaller standard group lasso. Experiments Results Syntic data, Gaussian Linear Regression For tasks, define 2002 element vector divided 500 groups size each group overlaps neighboring groups,   ,   ,   },    groups activated uniformly random, populated uniform, distribution. proportion coeﬃcients largest magnitude retained true signal. for task, obtain 250 linear measurements, 250 matrix. corrupt measurement Additive White Gaussian Noise (awgn), assess signal recovery terms Mean Squared Error (mse). regularization parameter clairvoyantly picked minimize MSE range parameter values. results applying lasso, standard latent group lasso], SOSlasso data plotted Figures), varying ), varying . each point Figures), average 100 trials, trial based random instance Gaussian data matrices. .015 Glasso SOSlasso lasso Glasso SOSlasso.015 MSE MSE.005.005 ) Varying ?? ) Varying ) Sample pattern Figure noise increased), proposed penalty function (soslasso) recover true coeﬃcients accurately group lasso (glasso). also, alpha large, active groups sparse, standard overlapping group lasso outperforms methods. however, reduces, method propose outperforms group lasso). ) shows toy sparsity pattern, colors denoting overlapping groups SOSlasso fmri experiment, compared soslasso, lasso, Glasso analysis star-plus dataset]. subjects made judgements involved processing sentences pictures brains scanned half intervals fmri1 retained time points stimulus, \\x0cyielding 1280 measurements voxel. task distinguish, point time, stimulus subject processing. ] showed exists cross-subject consistency cortical regions prediction task. specifically, experts partitioned dataset overlapping regions interest (rois), reduced data discarding ROIs and, subject, averaging BOLD response voxels ROI showed classifier trained data subjects generalized applied data 6th. assessed wher SOSlasso leverage cross-individual consistency aid discovery predictive voxels requiring expert pre-selection rois, data reduction, alignment voxels existing raw data. note that, unlike], aim learn solution generalizes withheld subject. rar, aim discover group sparsity pattern suggests similar set voxels subjects, optimizing separate solution individual. SOSlasso exploit cross-individual anatomical similarity raw, coarsely-aligned data, show reduced cross-validation error relative lasso applied separately individual. solution sparse groups highly variable individuals, SOSlasso show reduced cross-validation error relative glasso. finally, SOSlasso finding cross-individual structure, features selects align expert-identified ROIs shown] carry consistent information. Data documentation http://www.cmu.edu/afs.cmu.edu/project/www/ Figure Results fmri experiments. ) Aggregated sparsity patterns single brain slice. ) Crossvalidation error obtained method. lines connect data single subject. ) full sparsity pattern obtained soslasso. lasso Error Glasso lasso Glasso SOSlasso SOSlasso) Picture Sentence Picture Sentence) \\x0cmethod ROI) lasso.001 Glasso.002 SOSlasso Table Proportion selected voxels relevant ROIS aggregated subjects, two-tailed significance levels contrast lasso Glasso soslasso. trained classifiers-fold cross validation select regularization parameter, voxels preselection. group regions  voxels considered overlapping groups ?shifted? voxels dimensions Figure) shows individual error rates subjects methods. across subjects, SOSlasso significantly lower cross-validation error rate individual lasso within-subjects.004 two-tailed), showing method exploit anatomical similarity subjects learn classifier each. soslasso showed significantly lower error rates glasso two-tailed), suggesting signal sparse selected regions variable subjects. figure) presents sample sparsity patterns obtained methods, aggregated subjects. red points voxels contributed positively picture classification subject, sentences; Blue points opposite interpretation. purple points voxels contributed positively picture sentence classification subjects. remaining slices SOSlasso shown Figure). things note Figure). first, Glasso solution fairly dense, voxels signaling picture sentence subjects. ?purple haze? demonstrates Glasso illsuited fmri analysis: voxel selected subject selected ors. this approach succeed, likely, exists direct voxel-voxel correspondence neural code variable subjects. second, lasso solution sparse SOSlasso task-correlated voxel selected. leads higher cross-validation error, indicating ungrouped voxels inferior predictors (figure)). third, SOSlasso yields sparse solution, clustered. assess clusters align anatomical regions thought-priori involved sentence picture representation, calculated proportion selected voxels falling ROIs identified] relevant classification task (table). for SOSlasso average% identified voxels fell rois, significantly lasso glasso. Conclusions Extensions introduced soslasso, function recovers sparsity patterns hybrid overlapping group sparse sparse patterns regularizer convex programs, proved oretical convergence rates minimizing squares. SOSlasso succeeds multitask fmri analysis, makes inferences discovers oretically plausible brain regions lasso glasso. future work involves experimenting parameters group penalties, similarity groupings, functional connectivity fmri. irregular group size \\x0ccompensates voxels larger scanner coverage smaller-dimension (only slices relative xand-dimensions).',\n",
       " 'PP4953': 'most real-world applications structured., composed multiple random variables related. example, natural language processing, interested parsing sentences syntactically. computer vision, predict depth pixel, semantic category. computational biology, sequence proteins., lethal edema factors, protective antigen) predict docking anthrax toxin. individual variables considered independently, demonstrated taking relations account improves prediction performance significantly. prediction structured models typically performed maximizing scoring function space outcomes-hard task graphical models. traditional learning algorithms structured problems tackle supervised setting], input-output pairs structured output fully labeled. obtaining fully labeled examples might, however, cumbersome structured models involve large number random variables., semantic segmentation, label million random variables, pixel. furrmore, obtaining ground truth diﬃcult potentially requires accessing extra sensors., laser scanners case stereo. extreme medical domain, obtaining extra labels possible., tests available. thus, reducing amount labeled examples required learning scoring function key success structured prediction real-world applications. active learning setting beneficial potential considerably reduce amount supervision required learn good model, querying informative examples. structured case, active learning generalized query subparts graph example, reducing amount labeling furr. variety active learning approaches exists case classification regression, structured case popular, intrinsic computational diﬃculties deal exponentially sized output spaces. existing approaches typically case exact inference], label full output space], rely computationally expensive processes require inference outcome random variable]. computationally infeasible graphical models. contrast, paper present eﬃcient approximate approaches general graphical models exact inference intractable. particular, propose select parts label based entropy local marginal distributions. active learning algorithms exploit recently developed weakly supervised methods structured prediction], showing benefit unlabeled examples exploit marginal distributions computed learning. furrmore, computation-used active learning iteration, improving eﬃciency significantly. demonstrate effectiveness approach context room layout estimation single images, show state--art results achieved employing fewer manual interactions., labels). particular, match performance state--art task] labeling% random variables. remainder paper review learning methods structured prediction. propose active learning algorithms, show experimental evaluation discussion related work conclusions. maximum Likelihood Structure Prediction begin reviewing structured prediction approaches employ fully labeled training sets handle latent variables. interest probabilistic formulations employ entropies local probability distributions criteria deciding parts graph label active learning step.  input space., image sentence), structured labeled space interested predicting., image segmentation parse tree). define   mapping input label space -dimensional feature space. log-linear distributions) describing probability structured label space object ) exp , ) During learning, interested \\x0cestimating parameters log-linear distribution score , high ?good? label   Supervised Setting define ?good,? supervised setting training set pairs, composed input fully labeled data addition, compare fitness estimate  training sample, refer task-loss function) ). purpose enforcing distance hyperplane defined parameters respective sample popular max-margin setting. incorporate loss function learning process loss-augmented distribution) exp¿ )). ) Intuitively places probability mass parts output space high loss, forcing model adapt diﬃcult setting encountered inference, loss present. maximum likelihood learning aims finding model parameters assign highest probability training set Assuming data independent identically distributed. ourp goal minimize negative log-posterior ) ?kwkp prior model parameters. cost function refore   ?) ) kwkp exp   included parameter yield soft-max function. convex function, diﬃculty arises sum exponentially label configurations?. algorithms proposed solve task. eﬃcient computation treestructured models required convergence guarantees], approximations suggested achieve convergence working loopy models].  Dealing Latent Variables weakly supervised setting, training set pairs, composed input partially labeled data  for training pair, label space divided non-intersecting subspaces refer missing information hidden latent. before, incorporate task-loss function, define loss-augmented likelihood prediction  observing pair, ) ) ) ) )  ) ) defined.  minimization negative log-posterior results difference convex terms  kwkpp exp  ?) )     exp  ,  ) terms sum log-prior logarithm partition function. generality task-loss noting experiments. previously outlined diﬃculty exponentially sized product spaces, cost function longer convex. generally employ expectation maximization) concave-convex procedure (cccp] type approaches., linearize non-convex part current iterate taking step direction gradient convex function. specifically, follow Schwing. ] upper-bound concave part minimization set dual variables subsequently referred): kwkpp exp)  , )     ) )] deal exponential complexity notice feature frequently element vector decomposes local terms, ,? ,   represents set indexing unary potentials feature). similarly denotes set high-order variable interaction sets  feature). variable indexes observed subsumed set Similarly factors variable summarized set). leverage decomposition features approximate entropy joint distribution) local ranging marginals. furrmore, approximate marginal polytope local polytope. deal summation output space objects  convex part similar manner. end change dual space, employ entropy approximations transform resulting surrogate function back primal space obtain Lagrange multipliers enforce marginalization constraints. altoger obtain approximate primal program form: min. , ),? ?  ),   ), ) ),?    denoting probability simplexes. refer reader] specific forms functions. cccp, program optimized alternatively minimizing. local beliefs solve latent variable prediction problem, performing gradient step. weights block-coordinate descent steps update Lagrange multipliers equivalent solving supervised \\x0cconditional random field problem distribution latent variables inferred preceding latent variable prediction step. augment], return weights local beliefs represent joint distribution., distribution latent space only. summarize process alg.  note local minimum obtained solving non-convex problem. algorithm latent structured prediction input: data initial weights repeat repeat //solve latent variable prediction problem mind.  ) ) convergence //message passing update ),     //gradient step step size     convergence output: weights beliefs Active Learning previous section, defined maximum likelihood estimators learning supervised weakly supervised setting. derive active learning approaches. active learning setting, assume training set pairs, composed input partially labeled data  before, training pair, divide label space non-intersecting subspaces refer missing information hidden latent. additionally, set unlabeled examples interested answering question: part graph labeled order learn model amount supervision? goal, derive iterative algorithms select random variables labeled based local entropies. intuitive, entropy surrogate uncertainty considered application cost labeling random variable independent selection. here, algorithms iteratively query labels random variables highest uncertainty, update model parameters uncertain set variables. goal, compute entropies marginal distributions latent variable, entropy random variable unlabeled examples. general-hard, interested dealing graphical models general potentials connectivity. paper derive active learning algorithms, trade-off accuracy computational complexity. separate active: Our algorithm utilizes labeled weakly labeled examples learn iteration. parameters learned performs inference unlabeled partially labeled examples query random variable label. thus, requires separate inference step active learning iteration. shown experiments, eﬃciently convex belief propagation]. algorithm summarized alg.  joint active: Our active learning algorithm takes advantage unlabeled examples learning extra effort required compute informative random variable. note contrasts active learning algorithms typically exploit unlabeled data learning require expensive computations order select random variable labeled.  set training examples fully labeled, partially labeled unlabeled examples. iteration obtain querying label random variable labeled thus, iteration, learn weakly supervised structured prediction task solves  kwt kpp exp ¿ , )     exp   ¿ , Algorithm Separate active Algorithm Joint active input: data initial weights repeat, alg.    arg maxi  ? ? )}, ? suﬃciently output: weights input: data initial weights repeat, alg.  inference  arg maxi  ? ? )}, ? suﬃciently output: weights weights iteration. resort approximated problem. solve optimization task. entropies readily computable close form, local beliefs computed learning. thus, extra inference step necessary. local entropies  log query variable highest entropy., highest uncertainty. note computation linear number unlabeled random variables linear number states. summarize approach alg.  note algorithm expensive previous learning employs fully, weakly unlabeled examples. case pool unlabeled examples large. however, shown experimental evaluation, dramatically reduce amount labeling required learn good model. batch mode: previously defined active learning approaches computationally expensive sequential active learning step, model learned inference performed latent variables. investigate batch algorithms label random variables step algorithm. goal, simply label top uncertain variables. note approximation sequential algorithm, estimates parameters entropies updated selecting variable. -using computation: Warm starting learning algorithm active learning query important order reduce number iterations required convergence. (almost) samples involved step, extract lot information previous iterations. end-use weights messages beliefs. specifically, alg. perform inference newly selected examples update messages toger Lagrange multipliers training images current weights, perform iteration anor active step. hand, advantage unlabeled data joint active learning algorithm (alg. ), Lagrange multipliers image. furr updates directly start active step. experimental evaluation show choice results dramatic speed ups compared randomly initializing weights messages active learning iteration. note joint approach (alg. requires larger number iterations converge employs large amounts unlabeled data. iterations, convergence active learning steps improves significantly requiring time separate approach (alg. does. experimental Evaluation demonstrate performance algorithms task predicting layout rooms single image. existing approaches formulate task structured prediction problem focusing estimating box describes layout. taking advantage Manhattan world assumption., existence dominant vanishing points orthonormal), vanishing points, problem formulated inference pairwise graphical model composed random variables]. shown fig. variables represent angles encoding rays originate respective vanishing points. existing approaches], employ features based geometric context] orientation maps] image cues. features count face cuboid (given configuration layout) number pixels label probability label exists task-loss denotes pixel-wise prediction error. figure Parameterization factor graph layout prediction task. separate joint separate joint pixelwise test error \\x0cpixelwise test error random number queries separate joint random number queries) random separate joint pixelwise test error random pixelwise test error number queries) number queries) Figure Test set error function number random variables labeled, joint separate active learning. plots reﬂect scenarios top random variables labeled iteration., batch setting). left. performance measured percentage pixels correctly labeled, left-wall, rightwall, front-wall, ceiling ﬂoor. orwise stated experiments \\x0cperformed averaging runs algorithm, initial seed fully labeled images selected random. active learning: begin experimentation comparing proposed active learning algorithms., separate (alg. joint (alg. ). shown fig. ), active learning algorithms achieve lower test error algorithm selects variables label random. also, note joint algorithm takes advantage unlabeled data achieves good performance labeling variables, improving significantly separate algorithm. batch active learning: fig. shows performances active learning algorithms labeling batch random variables-learning. note batch random variables, algorithms quickly outperform random selection, illustrated fig. ). image random variable: Instead labeling random variable time, experiment algorithm labels variables image once. note setting equivalent labeling random variables image. shown fig. ), labeling full image requires labeling achieve test error performance compared labeling random variables possibly examples. importance fig. ) show performance active learning algorithms function note parameter fairly important. particular, entropy random variables large discriminative. illustrated fig. ) observe fairly uniform distribution states randomly chosen variable active learning algorithm prefers smaller values hyposize due fact small number random variables large number states. initial tests show applications number states smaller., segmentation) larger values perform better. automatic selection subject future research. complexity Separate. joint: fig. ) illustrate number CCCP iterations function number queried examples active learning algorithms. observe joint algorithm requires computation initially. active steps., converged good solution, computation requirements reduce drastically.  algorithms.  number queries) Image variable pixelwise test error pixelwise test error random: var separate: var joint: var number queries) separate probability random: img separate: img joint: img pixelwise test error number queries) joint state) Marginal distribution Figure Test set error function number random variables labeled)). marginal distribution illustrated) 100 joint separate reuse reuse 5000 4000 time] 3000 2000 1000 6000 reuse reuse time] CCCP iteration number queries) active finished) active finished) Figure Number CCCP iterations function amount queried variables) time number active iterations) (joint) (separate). reusing computation: fig. ) show number finished active learning iterations function time joint separate algorithm respectively. note reusing computation, larger number active learning iterations finishes specific time budget. related Work Active learning approaches scenarios. stream-based methods], samples considered successively decision made discard eventually pick investigated sample. contrast, poolbased methods] access large set unlabeled data. proposed approach pool-based ﬂavor. years strategies proposed context active learning algorithms decide label next. follow uncertainty sampling scheme] entropy measure, sampling schemes based expected model change] proposed. alternatives expected error \\x0creduction], variance reduction], least-confident measure] marginbased measures]. alternative classify active learning algorithms related information revealed querying label. multiarmed bandit model, algorithm chooses action/sample observes utility action. alternatively learning expert advice, utilities actions revealed]. aforementioned extremes sits coactive learning setting] subset rewards actions revealed user. approach resembles multi-armed bandit setting result newly queried sample. active learning approaches proposed context Neural Networks], Support Vector Machines], Gaussian processes], CRFs] structured max-margin formulations]. contrasting previously proposed approaches active learning extension latent structured prediction setting., extend double-loop algorithm anor layer. importantly, active learning algorithm recent ideas unify CRFs structured svms. employs convex approximations amenable general graphical models arbitrary topology energy functions. application active learning computer vision developed Kapoor. ] perform object recognition minimal supervision. context structured models] proposed conditional entropies decide image label segmentation task. ] set frames label video sequence selected based cost labeling frame cost correcting errors. unlike approach] labeled full images (not sets random variables). shown experiments requires manual interactions approach. grabcut] popularized ?active learning? figure ground segmentation, question labeled answered human interactive segmentation system. siddiquie. ] propose label variable reduces entropy entire ?system,? ., data, taking account correlations variables entropy approximation. ], region labeled selected based surrogate uncertainty., min marginals) computed eﬃciently dynamic graph cuts. this, however, suitable problems solved graph cuts., binary labeling problems modular energies). contrast, paper interested general setting arbitrary energies connectivities. entropy active learning criteria tree-structured models], marginal probabilities computed exactly. context video segmentation, Fathi. ] frame active learning semi-supervised learning problem graph. utilized entropy metric selecting superpixel label graph regularization approach. context holistic approaches, Vijayanarasimhan. ] investigated problem task label. goal derived multi-label multipleinstance approach, takes account task effort., expected time perform labeling). vezhnevets. ] resort expected change criteria select parts label graphical model. unfortunately, computing measure computationally expensive, approach feasible graphical models inference solved graph cuts. conclusions proposed active learning algorithms context structure models utilized local entropies order decide subset output space label. demonstrated effectiveness approach problem room layout prediction single image, showed state--art performance obtained employing% labelings. release source code acceptance scripts reproduce experiments paper. future, plan apply algorithms context holistic models order investigate tasks informative visual parsing. Most real-world applications structured., composed multiple random variables related. for example, natural language processing, interested parsing sentences syntactically. computer vision, predict depth pixel, semantic category. computational biology, sequence proteins., lethal edema factors, protective antigen) predict docking anthrax toxin. while individual variables considered independently, demonstrated taking relations account improves prediction performance significantly. prediction structured models typically performed maximizing scoring function space outcomes-hard task graphical models. traditional learning algorithms structured problems tackle supervised setting], input-output pairs structured output fully labeled. obtaining fully labeled examples might, however, cumbersome structured models involve large number random variables., semantic segmentation, label million random variables, pixel. furrmore, obtaining ground truth diﬃcult potentially requires accessing extra sensors., laser scanners case stereo. this extreme medical domain, obtaining extra labels possible., tests available. thus, reducing amount labeled examples required learning scoring function key success structured prediction real-world applications. active learning setting beneficial potential considerably reduce amount supervision required learn good model, querying informative examples. structured case, active learning generalized query subparts graph example, reducing amount labeling furr. while variety active learning approaches exists case classification regression, structured case popular, intrinsic computational diﬃculties deal exponentially sized output spaces. existing approaches typically case exact inference], label full output space], rely computationally expensive processes require inference outcome random variable]. computationally infeasible graphical models. contrast, paper present eﬃcient approximate approaches general graphical models exact inference intractable. particular, propose select parts label based entropy local marginal distributions. our active learning algorithms exploit recently developed weakly supervised methods structured prediction], showing benefit unlabeled examples exploit marginal distributions computed learning. furrmore, computation-used active learning iteration, improving eﬃciency significantly. demonstrate effectiveness approach context room layout estimation single images, show state--art results achieved employing fewer manual interactions., labels). particular, match performance state--art task] labeling% random variables. remainder paper review learning methods structured prediction. propose active learning algorithms, show experimental evaluation discussion related work conclusions. Maximum Likelihood Structure Prediction begin reviewing structured prediction approaches employ fully labeled training sets handle latent variables. interest probabilistic formulations employ entropies local probability distributions criteria deciding parts graph label active learning step. let input space., image sentence), structured labeled space interested predicting., image segmentation parse tree). define   mapping input label space -dimensional feature space. here log-linear distributions) describing probability structured label space object ) exp , ) During learning, interested \\x0cestimating parameters log-linear distribution score , high ?good? label   Supervised Setting define ?good,? supervised setting training set pairs, composed input fully labeled data addition, compare fitness estimate  training sample, refer task-loss function) ). its purpose enforcing distance hyperplane defined parameters respective sample popular max-margin setting. incorporate loss function learning process loss-augmented distribution) exp¿ )). ) Intuitively places probability mass parts output space high loss, forcing model adapt diﬃcult setting encountered inference, loss present. maximum likelihood learning aims finding model parameters assign highest probability training set Assuming data independent identically distributed. ourp goal minimize negative log-posterior ) ?kwkp prior model parameters. cost function refore   ?) ) kwkp exp   included parameter yield soft-max function. although convex function, diﬃculty arises sum exponentially label configurations?. different algorithms proposed solve task. while eﬃcient computation treestructured models required convergence guarantees], approximations suggested achieve convergence working loopy models].  Dealing Latent Variables weakly supervised setting, training set pairs, composed input partially labeled data  For training pair, label space divided non-intersecting subspaces refer missing information hidden latent. before, incorporate task-loss function, define loss-augmented likelihood prediction  observing pair, ) ) ) ) )  ) ) defined.  minimization negative log-posterior results difference convex terms  kwkpp exp  ?) )     exp  ,  ) terms sum log-prior logarithm partition function. for generality task-loss noting experiments. besides previously outlined diﬃculty exponentially sized product spaces, cost function longer convex. hence generally employ expectation maximization) concave-convex procedure (cccp] type approaches., linearize non-convex part current iterate taking step direction gradient convex function. more specifically, follow Schwing. ] upper-bound concave part minimization set dual variables subsequently referred): kwkpp exp)  , )     ) )] deal exponential complexity notice feature frequently element vector decomposes local terms, ,? ,   represents set indexing unary potentials feature). similarly denotes set high-order variable interaction sets  feature). all variable indexes observed subsumed set Similarly factors variable summarized set). leverage decomposition features approximate entropy joint distribution) local ranging marginals. furrmore, approximate marginal polytope local polytope. deal summation output space objects  convex part similar manner. end change dual space, employ entropy approximations transform resulting surrogate function back primal space obtain Lagrange multipliers enforce marginalization constraints. altoger obtain approximate primal program form: min. , ),? ?  ),   ), ) ),?    denoting probability simplexes. refer reader] specific forms functions. following cccp, program optimized alternatively minimizing. local beliefs solve latent variable prediction problem, performing gradient step. weights block-coordinate descent steps update Lagrange multipliers equivalent solving supervised \\x0cconditional random field problem distribution latent variables inferred preceding latent variable prediction step. augment], return weights local beliefs represent joint distribution., distribution latent space only. summarize process alg.  note local minimum obtained solving non-convex problem. Algorithm latent structured prediction input: data initial weights repeat repeat //solve latent variable prediction problem mind.  ) ) convergence //message passing update ),     //gradient step step size     convergence output: weights beliefs Active Learning previous section, defined maximum likelihood estimators learning supervised weakly supervised setting. derive active learning approaches. active learning setting, assume training set pairs, composed input partially labeled data  before, training pair, divide label space non-intersecting subspaces refer missing information hidden latent. additionally, set unlabeled examples interested answering question: part graph labeled order learn model amount supervision? towards goal, derive iterative algorithms select random variables labeled based local entropies. this intuitive, entropy surrogate uncertainty considered application cost labeling random variable independent selection. here, algorithms iteratively query labels random variables highest uncertainty, update model parameters uncertain set variables. towards goal, compute entropies marginal distributions latent variable, entropy random variable unlabeled examples. this general-hard, interested dealing graphical models general potentials connectivity. paper derive active learning algorithms, trade-off accuracy computational complexity. separate active: Our algorithm utilizes labeled weakly labeled examples learn iteration. once parameters learned performs inference unlabeled partially labeled examples query random variable label. thus, requires separate inference step active learning iteration. shown experiments, eﬃciently convex belief propagation]. algorithm summarized alg.  joint active: Our active learning algorithm takes advantage unlabeled examples learning extra effort required compute informative random variable. note contrasts active learning algorithms typically exploit unlabeled data learning require expensive computations order select random variable labeled. let set training examples fully labeled, partially labeled unlabeled examples. iteration obtain querying label random variable labeled thus, iteration, learn weakly supervised structured prediction task solves  kwt kpp exp ¿ , )     exp   ¿ , Algorithm Separate active Algorithm Joint active input: data initial weights repeat, alg.    arg maxi  ? ? )}, ? suﬃciently output: weights input: data initial weights repeat, alg.  inference  arg maxi  ? ? )}, ? suﬃciently output: weights weights iteration. resort approximated problem. solve optimization task. entropies readily computable close form, local beliefs computed learning. thus, extra inference step necessary. local entropies  log query variable highest entropy., highest uncertainty. note computation linear number unlabeled random variables linear number states. summarize approach alg.  note algorithm expensive previous learning employs fully, weakly unlabeled examples. this case pool unlabeled examples large. however, shown experimental evaluation, dramatically reduce amount labeling required learn good model. batch mode: previously defined active learning approaches computationally expensive sequential active learning step, model learned inference performed latent variables. investigate batch algorithms label random variables step algorithm. towards goal, simply label top uncertain variables. note approximation sequential algorithm, estimates parameters entropies updated selecting variable. -using computation: Warm starting learning algorithm active learning query important order reduce number iterations required convergence. since (almost) samples involved step, extract lot information previous iterations. end-use weights messages beliefs. more specifically, alg. perform inference newly selected examples update messages only toger Lagrange multipliers training images current weights, perform iteration anor active step. hand, advantage unlabeled data joint active learning algorithm (alg. ), Lagrange multipliers image. without furr updates directly start active step. experimental evaluation show choice results dramatic speed ups compared randomly initializing weights messages active learning iteration. note joint approach (alg. requires larger number iterations converge employs large amounts unlabeled data. after iterations, convergence active learning steps improves significantly requiring time separate approach (alg. does. Experimental Evaluation demonstrate performance algorithms task predicting layout rooms single image. existing approaches formulate task structured prediction problem focusing estimating box describes layout. taking advantage Manhattan world assumption., existence dominant vanishing points orthonormal), vanishing points, problem formulated inference pairwise graphical model composed random variables]. shown fig. variables represent angles encoding rays originate respective vanishing points. following existing approaches], employ features based geometric context] orientation maps] image cues. our features count face cuboid (given configuration layout) number pixels label probability label exists task-loss denotes pixel-wise prediction error. Figure Parameterization factor graph layout prediction task. separate joint separate joint pixelwise test error \\x0cpixelwise test error random number queries separate joint random number queries) random separate joint pixelwise test error random pixelwise test error number queries) number queries) Figure Test set error function number random variables labeled, joint separate active learning. plots reﬂect scenarios top random variables labeled iteration., batch setting). from left. performance measured percentage pixels correctly labeled, left-wall, rightwall, front-wall, ceiling ﬂoor. unless orwise stated experiments \\x0cperformed averaging runs algorithm, initial seed fully labeled images selected random. active learning: begin experimentation comparing proposed active learning algorithms., separate (alg. joint (alg. ). shown fig. ), active learning algorithms achieve lower test error algorithm selects variables label random. also, note joint algorithm takes advantage unlabeled data achieves good performance labeling variables, improving significantly separate algorithm. batch active learning: fig. shows performances active learning algorithms labeling batch random variables-learning. note batch random variables, algorithms quickly outperform random selection, illustrated fig. ). image random variable: Instead labeling random variable time, experiment algorithm labels variables image once. note setting equivalent labeling random variables image. shown fig. ), labeling full image requires labeling achieve test error performance compared labeling random variables possibly examples. importance fig. ) show performance active learning algorithms function note parameter fairly important. particular, entropy random variables large discriminative. this illustrated fig. ) observe fairly uniform distribution states randomly chosen variable our active learning algorithm prefers smaller values hyposize due fact small number random variables large number states. our initial tests show applications number states smaller., segmentation) larger values perform better. automatic selection subject future research. complexity Separate. joint: fig. ) illustrate number CCCP iterations function number queried examples active learning algorithms. observe joint algorithm requires computation initially. but active steps., converged good solution, computation requirements reduce drastically. here algorithms.  number queries) Image variable pixelwise test error pixelwise test error random: var separate: var joint: var number queries) separate probability random: img separate: img joint: img pixelwise test error number queries) joint state) Marginal distribution Figure Test set error function number random variables labeled)). marginal distribution illustrated) 100 joint separate reuse reuse 5000 4000 time] 3000 2000 1000 6000 reuse reuse time] CCCP iteration number queries) active finished) active finished) Figure Number CCCP iterations function amount queried variables) time number active iterations) (joint) (separate). reusing computation: fig. ) show number finished active learning iterations function time joint separate algorithm respectively. note reusing computation, larger number active learning iterations finishes specific time budget. Related Work Active learning approaches scenarios. stream-based methods], samples considered successively decision made discard eventually pick investigated sample. contrast, poolbased methods] access large set unlabeled data. clearly proposed approach pool-based ﬂavor. over years strategies proposed context active learning algorithms decide label next. while follow uncertainty sampling scheme] entropy measure, sampling schemes based expected model change] proposed. alternatives expected error \\x0creduction], variance reduction], least-confident measure] marginbased measures]. alternative classify active learning algorithms related information revealed querying label. multiarmed bandit model, algorithm chooses action/sample observes utility action. alternatively learning expert advice, utilities actions revealed]. between aforementioned extremes sits coactive learning setting] subset rewards actions revealed user. our approach resembles multi-armed bandit setting result newly queried sample. active learning approaches proposed context Neural Networks], Support Vector Machines], Gaussian processes], CRFs] structured max-margin formulations]. contrasting previously proposed approaches active learning extension latent structured prediction setting., extend double-loop algorithm anor layer. importantly, active learning algorithm recent ideas unify CRFs structured svms. employs convex approximations amenable general graphical models arbitrary topology energy functions. application active learning computer vision developed Kapoor. ] perform object recognition minimal supervision. context structured models] proposed conditional entropies decide image label segmentation task. ] set frames label video sequence selected based cost labeling frame cost correcting errors. unlike approach] labeled full images (not sets random variables). shown experiments requires manual interactions approach. grabcut] popularized ?active learning? figure ground segmentation, question labeled answered human interactive segmentation system. siddiquie. ] propose label variable reduces entropy entire ?system,? ., data, taking account correlations variables entropy approximation. ], region labeled selected based surrogate uncertainty., min marginals) computed eﬃciently dynamic graph cuts. this, however, suitable problems solved graph cuts., binary labeling problems modular energies). contrast, paper interested general setting arbitrary energies connectivities. entropy active learning criteria tree-structured models], marginal probabilities computed exactly. context video segmentation, Fathi. ] frame active learning semi-supervised learning problem graph. utilized entropy metric selecting superpixel label graph regularization approach. context holistic approaches, Vijayanarasimhan. ] investigated problem task label. towards goal derived multi-label multipleinstance approach, takes account task effort., expected time perform labeling). vezhnevets. ] resort expected change criteria select parts label graphical model. unfortunately, computing measure computationally expensive, approach feasible graphical models inference solved graph cuts. Conclusions proposed active learning algorithms context structure models utilized local entropies order decide subset output space label. demonstrated effectiveness approach problem room layout prediction single image, showed state--art performance obtained employing% labelings. release source code acceptance scripts reproduce experiments paper. future, plan apply algorithms context holistic models order investigate tasks informative visual parsing.',\n",
       " 'PP4983': 'gaussian mixture models provide simple framework machine learning problems including clustering, density estimation classification. mixtures appealing high dimensional problems. common Gaussian mixtures clustering. course, statistical (and computational) behavior methods degrade high dimensions. inspired success variable selection methods regression, authors considered variable selection clustering. however, appears oretical results justifying advantage variable selection high dimensional setting. sort variable selection useful, clustering subjects vector genes subject. typically larger suggests statistical clustering methods perform poorly. however, case small number relevant genes case expect behavior focusing small set relevant genes. purpose paper provide precise bounds clustering error mixtures gaussians. general case features relevant, special case subset features relevant. mamatically, model irrelevant feature requiring feature clusters, feature serve differentiate groups. paper, probability misclustering observation, relative optimal clustering true distribution, loss function. akin excess risk classification. paper makes contributions: provide information oretic bounds sample complexity learning mixture isotropic Gaussians equal weight small separation setting precisely captures dimension dependence, matches sample complexity requirements existing algorithms. debunks myth gap statistical computational complexity learning mixture isotropic Gaussians small separation. bounds require non-standard arguments loss function satisfy triangle inequality.  high-dimensional setting subset relevant dimensions determine separation mixture components show learning substantially easier sample complexity depends sparse set relevant dimensions. oretical basis feature selection approaches clustering.  show simple computationally feasible procedure achieves information oretic sample complexity high-dimensional sparse separation settings. related work. long continuing history research mixtures gaussians. complete review feasible mention highlights work related ours. popular method estimating mixture distribution maximum likelihood. unfortunately, maximizing likelihood-hard. led stream work alternative methods estimating mixtures. algorithms pairwise distances, spectral methods method moments. pairwise methods developed Dasgupta (1999), Schulman Dasgupta (2000) Arora Kannan (2001). methods require separation increase dimension.  requires separation improve avoid problem, Vempala Wang (2004) introduced idea spectral methods estimating mixtures spherical Gaussians makes separation independent dimension. assumption components spherical removed Brubaker Vempala (2008). method requires components separated hyperplane runs polynomial time, requires  log samples. spectral methods include Kannan. (2005), Achlioptas McSherry (2005) Hsu Kakade (2013). clever spectral decompositions toger method moments derive effective algorithm. kalai. (2012) method moments estimates requiring separation components mixture components. similar approach Belkin Sinha (2010). chaudhuri. (2009) give modified-means algorithm estimating mixture Gaus2 sians. large separation setting chaudhuri. (2009) show/? samples \\x0cneeded. provide information oretic bound sample complexity algorithm matches sample complexity method log factors)  separation small show/? samples suﬃcient. results small separation setting give matching condition. assuming separation component means sparse, Chaudhuri Rao (2008) provide algorithm learning mixture polynomial computational sample complexity. papers concerned computational eﬃciency give precise, statistical minimax upper lower bounds. deal case interested, namely, high dimensional mixture sparse separation. point results papers necessarily comparable authors loss functions. paper probability misclassifying future observation, relative correct distribution clusters observation, loss function. confused probability attributing observation wrong component mixture. loss typically tend sample size increases. loss similar excess risk classification compare misclassification rate classifier misclassification rate Bayes optimal classifier. finally, remind reader motivation studying sparsely separated mixtures model variable selection clustering problems. relevant recent papers problem high-dimensional setting. pan Shen (2007) penalized mixture models variable selection clustering simultaneously. witten Tibshirani (2010) develop penalized version-means clustering. related methods include Raftery Dean (2006); Sun. (2012) Guo. (2010). applied bioinformatics literature huge number heuristic methods problem. papers provide minimax bounds clustering error provide oretical evidence benefit variable selection unsupervised problems clustering. problem Setup paper, simple setting learning mixture isotropic Gaussians equal mixing weights data points    drawn. -dimensional mixture density function ; ; ),  density (?,  fixed constant,   classes parameters:            class defines mixtures components separation  class defines mixtures separation sparse set ,   dimensions. also, denote probability measure  mixture parameter Bayes optimal classification, assignment point correct mixture component, function ) argmax; ).  } Given candidate assignment function }, define loss incurred min  ) ))}) minimum permutations , }. probability misclustering relative oracle true distribution optimal clustering. denote Fbn assignment function learned data   referred estimator. goal paper quantify minimax expected loss (worst case expected loss estimator) inf  (fbn ??? scales number samples dimension feature space number relevant dimensions signal-noise ratio defined ratio separation standard deviation ?/?. demonstrate specific estimator achieves minimax scaling. purposes paper, feature irrelevant). orwise feature relevant. minimax Bounds Small separation setting sparsity begin assuming sparsity, features relevant. case, comparing projections data projection sample principal component suﬃces achieve minimax optimal sample complexity clustering loss. orem (upper bound). define   btn ) orwise.     sample  sample mean,  denotes eigenvector largest eigenvalue  covariance  max), log)  ) 600 max ???? results hold unequal mixture weight setting major modifications. furrmore,    max),   exp exp  ? ???? note estimator orem (and orem knowledge  orem (lower bound). assume   .  log   inf  (fbn min ???? 500 constants (including lower bound exact upper bound ?/?) tightened, results demonstrate matching scaling behavior clustering error ?/?. thus, (ignoring constants log terms) equivalently constant target    result intuitive: dependence dimension expected. rate depends precise signal-noise ratio ?/?. particular, results imply modern high-dimensional datasets. large number features samples. however, inference \\x0cusually tractable features relevant learning task hand. sparsity relevant feature set successfully exploited supervised learning problems regression classification. show true clustering Gaussian mixture model.  Sparse small separation setting Now case relevant features. denote set relevant features. begin constructing estimator Sbn follows. define+?  min    } log) log) now sbn ,  method before, features Sbn identified relevant. orem (upper bound). define    xtsb btsb fbn) orwise sample xsbn coordinates restricted Sbn bsbn covariance data restricted Sbn  max),    ? log) log)   603 max 220  ???  Next find lower bound. orem (lower bound). assume   , ,    inf   min log ???  600 remark constants bounds tightened, results suggest log log  log constant target  case, gap upper lower bounds clustering loss. also, sample complexity possibly improved scale (instead method. however, notice dimension enters logarithmically. number relevant dimensions small expect good rates. justification feature selection. conjecture lower bound tight gap closed sparse principal component method Lei (2012) find relevant features. however, method combinatorial computationally eﬃcient method implementing similar guarantees. note upper bound achieved two-stage method finds relevant dimensions estimates clusters. contrast methods introduction clustering variable selection simultaneously. raises interesting question: achieve minimax rate two-stage procedure cases simultaneous method outperforms two-stage procedure? indeed, case general covariance matrices (non-spherical) twostage methods fail. hope address question future work. proofs Lower Bounds lower bounds estimation problems rely standard reduction expected error hyposis testing assumes loss function semidistance, clustering loss isn. however, local triangle inequalitytype bound shown (proposition). weaker condition lower-bound expected loss, stated Proposition (which easily fano inequality). proof techniques sparse non-sparse lower bounds identical. main difference non-sparse case, varshamov?gilbert bound (lemma construct set suﬃciently dissimilar hyposes, sparse case analogous result sparse hypercubes (lemma). supplementary material complete proofs results. section next,  denote univariate standard normal PDF cdf. lemma (varshamov?gilbert bound).    exists subset ...,  ),      denotes Hamming distance vectors (tsybakov (2009)). lemma     integers  .  exist ...,      (massart (2007), Lemma). proposition ...,     ,      log) implies)    clusterings inf Fbn maxi?  (fbn ?. proposition   clustering,  ? .    ,    )   results.    )  cos   proposition )(? ) )).  sin cos    tan   proposition    ?    cos ?).   define log    define?      ), ..., (where standard basis   ? ?     ))  ? )  proof orem  min Proposition?? ??    cos ??,? cos ??,?  ?(?,?)    proposition cos ??,?   hamming distance?? ??   ) cos ??,?  ?? ??  tan ??,?   cos ??,?    cos ??,?     (?) cos ??,?  cos ??,?  (?)  (?,  )(? ) )). lemma exist ...,        simplicity notation,             (?)     ?? ??  (?) sin ??,? cos ??,? define (?)       )  (?)       because,  , definition    (?)         log, Proposition)  also  )?    log because, definition  72n proposition fact  ,  log min inf max (fbn ?    500 complete proof sup????   (fbn maxi?  (fbn Fbn  proof orem simplicity, construction qwe state  assuming     Let  define min  define    log  }.   ), ...,  ))    (where standard basis   ? ?    lemma exist ...,           remainder proof analogous orem (?)    proofs Upper Bounds Propositions bound error estimating principal direction, obtained standard concentration bounds variant davis?kahan orem. proposition relates errors clustering loss. sparse case, Propositions bound added error induced support estimation procedure. supplementary material proof details. . proposition     ...,   max log log  probability ?.      . proposition      ...,  )—.    . define cos    max log   max 160 probability ?  exp    sin  max log max log   proposition     kvk) xt0 orwise. define cos .      sin    exp max sin sin      proof. (x0cos clustering loss invariant rotation translation,   — tan  — tan                — tan .  )    tan            max   defining     )   — tan  )? )dydx    tan  cos sin tan )? )dudv  ) tan sin sin cos   max sin tan   cos  sin sin cos step. bound easily. proof orem Propositions  Proposition fact exp(? max, ) exp(? max, ) log)   600 max  easy verify bounds decreasing bound supremum). case Proposition?need applied, principal directionsagree trivially. bound  max) shown similarly, exp  . proposition      ...,   log probability ?, ],  log log log  ,      ?—? )—   . proposition      ...,   define (?)  ] (?)  ] )—  ?}. assume     (?)  sbn (?) probability  proof. proposition probability log) log) log,   )   ?—? )—   ]. assume event holds. (?) ], (?). orwise, ?)? clear \\x0csbn (?).  (?  ?)?   remainder proof trivial(?)  (?)  assume orwise.  (? ,  ?)?  log)  )—.   +?) , sbn ignore strict  definition, )—    (? ?? equality measure event. (?)  sbn concludes proof.  proof orem define(?)  ] (?)  ] )—  ?}. assume(?)  sbn (?)  Proposition holds probability  (?) simply  (fbn   )—, cos  )—, cos assume(?)  cos     simplicity define    respectively, elsewhere. sin  sin sin    (?)   (?)   (?  (?      sin  argument proof orem long bound smaller    log) 104   ) 600 max      using fact  always,  implies log) bound follows.  conclusion provided minimax lower upper bounds estimating high dimensional mixtures. bounds show explicitly statistical diﬃculty problem depends dimension sample size separation sparsity level For clarity, focused special case spherical components equal mixture weights. future work, plan extend results general mixtures gaussians. motivations work recent interest variable selection methods facilitate clustering high dimensional problems. existing methods Pan Shen (2007); Witten Tibshirani (2010); Raftery Dean (2006); Sun. (2012) Guo. (2010) provide promising numerical evidence variable selection improve high dimensional clustering. results provide oretical basis idea. however, gap results paper methodology papers. indeed, now, rigorous proof methods papers outperform stage approach stage screens relevant features stage applies standard clustering methods features found stage. conjecture conditions simultaneous feature selection clustering outperforms stage method. settling question require aforementioned extension results general mixture case. acknowledgements This research supported part NSF grants IIS1116458 CAREER award iis-1252412, NSF Grant dms-0806009 Air Force Grant fa95500910373. Gaussian mixture models provide simple framework machine learning problems including clustering, density estimation classification. mixtures appealing high dimensional problems. perhaps common Gaussian mixtures clustering. course, statistical (and computational) behavior methods degrade high dimensions. inspired success variable selection methods regression, authors considered variable selection clustering. however, appears oretical results justifying advantage variable selection high dimensional setting. sort variable selection useful, clustering subjects vector genes subject. typically larger suggests statistical clustering methods perform poorly. however, case small number relevant genes case expect behavior focusing small set relevant genes. purpose paper provide precise bounds clustering error mixtures gaussians. both general case features relevant, special case subset features relevant. mamatically, model irrelevant feature requiring feature clusters, feature serve differentiate groups. throughout paper, probability misclustering observation, relative optimal clustering true distribution, loss function. this akin excess risk classification. this paper makes contributions: provide information oretic bounds sample complexity learning mixture isotropic Gaussians equal weight small separation setting precisely captures dimension dependence, matches sample complexity requirements existing algorithms. this debunks myth gap statistical computational complexity learning mixture isotropic Gaussians small separation. our bounds require non-standard arguments loss function satisfy triangle inequality.  high-dimensional setting subset relevant dimensions determine separation mixture components show learning substantially easier sample complexity depends sparse set relevant dimensions. this oretical basis feature selection approaches clustering.  show simple computationally feasible procedure achieves information oretic sample complexity high-dimensional sparse separation settings. related work. long continuing history research mixtures gaussians. complete review feasible mention highlights work related ours. perhaps popular method estimating mixture distribution maximum likelihood. unfortunately, maximizing likelihood-hard. this led stream work alternative methods estimating mixtures. algorithms pairwise distances, spectral methods method moments. pairwise methods developed Dasgupta (1999), Schulman Dasgupta (2000) Arora Kannan (2001). methods require separation increase dimension.  requires separation improve avoid problem, Vempala Wang (2004) introduced idea spectral methods estimating mixtures spherical Gaussians makes separation independent dimension. assumption components spherical removed Brubaker Vempala (2008). method requires components separated hyperplane runs polynomial time, requires  log samples. spectral methods include Kannan. (2005), Achlioptas McSherry (2005) Hsu Kakade (2013). clever spectral decompositions toger method moments derive effective algorithm. kalai. (2012) method moments estimates requiring separation components mixture components. similar approach Belkin Sinha (2010). chaudhuri. (2009) give modified-means algorithm estimating mixture Gaus2 sians. for large separation setting Chaudhuri. (2009) show/? samples \\x0cneeded. provide information oretic bound sample complexity algorithm matches sample complexity method log factors)  separation small show/? samples suﬃcient. our results small separation setting give matching condition. assuming separation component means sparse, Chaudhuri Rao (2008) provide algorithm learning mixture polynomial computational sample complexity. most papers concerned computational eﬃciency give precise, statistical minimax upper lower bounds. none deal case interested, namely, high dimensional mixture sparse separation. point results papers necessarily comparable authors loss functions. paper probability misclassifying future observation, relative correct distribution clusters observation, loss function. this confused probability attributing observation wrong component mixture. loss typically tend sample size increases. our loss similar excess risk classification compare misclassification rate classifier misclassification rate Bayes optimal classifier. finally, remind reader motivation studying sparsely separated mixtures model variable selection clustering problems. relevant recent papers problem high-dimensional setting. pan Shen (2007) penalized mixture models variable selection clustering simultaneously. witten Tibshirani (2010) develop penalized version-means clustering. related methods include Raftery Dean (2006); Sun. (2012) Guo. (2010). applied bioinformatics literature huge number heuristic methods problem. none papers provide minimax bounds clustering error provide oretical evidence benefit variable selection unsupervised problems clustering. Problem Setup paper, simple setting learning mixture isotropic Gaussians equal mixing weights data points    drawn. -dimensional mixture density function ; ; ),  density (?,  fixed constant,   classes parameters:            class defines mixtures components separation  class defines mixtures separation sparse set ,   dimensions. also, denote probability measure  for mixture parameter Bayes optimal classification, assignment point correct mixture component, function ) argmax; ).  } Given candidate assignment function }, define loss incurred min  ) ))}) minimum permutations , }. this probability misclustering relative oracle true distribution optimal clustering. denote Fbn assignment function learned data   referred estimator. goal paper quantify minimax expected loss (worst case expected loss estimator) inf  (fbn ??? scales number samples dimension feature space number relevant dimensions signal-noise ratio defined ratio separation standard deviation ?/?. demonstrate specific estimator achieves minimax scaling. for purposes paper, feature irrelevant). orwise feature relevant. Minimax Bounds Small separation setting sparsity begin assuming sparsity, features relevant. case, comparing projections data projection sample principal component suﬃces achieve minimax optimal sample complexity clustering loss. orem (upper bound). define   btn ) orwise.     sample  sample mean,  denotes eigenvector largest eigenvalue  covariance  max), log)  ) 600 max ???? results hold unequal mixture weight setting major modifications. furrmore,    max),   exp exp  ? ???? note estimator orem (and orem knowledge  orem (lower bound). assume   .  log   inf  (fbn min ???? 500 constants (including lower bound exact upper bound ?/?) tightened, results demonstrate matching scaling behavior clustering error ?/?. thus, (ignoring constants log terms) equivalently constant target    result intuitive: dependence dimension expected. also rate depends precise signal-noise ratio ?/?. particular, results imply modern high-dimensional datasets. large number features samples. however, inference \\x0cusually tractable features relevant learning task hand. this sparsity relevant feature set successfully exploited supervised learning problems regression classification. show true clustering Gaussian mixture model.  Sparse small separation setting Now case relevant features. let denote set relevant features. begin constructing estimator Sbn follows. define+?  min    } log) log) Now sbn ,  now method before, features Sbn identified relevant. orem (upper bound). define    xtsb btsb Fbn) orwise sample xsbn coordinates restricted Sbn bsbn covariance data restricted Sbn  max),    ? log) log)   603 max 220  ???  Next find lower bound. orem (lower bound). assume   , ,    inf   min log ???  600 remark constants bounds tightened, results suggest log log  log constant target  case, gap upper lower bounds clustering loss. also, sample complexity possibly improved scale (instead method. however, notice dimension enters logarithmically. number relevant dimensions small expect good rates. this justification feature selection. conjecture lower bound tight gap closed sparse principal component method Lei (2012) find relevant features. however, method combinatorial computationally eﬃcient method implementing similar guarantees. note upper bound achieved two-stage method finds relevant dimensions estimates clusters. this contrast methods introduction clustering variable selection simultaneously. this raises interesting question: achieve minimax rate two-stage procedure cases simultaneous method outperforms two-stage procedure? indeed, case general covariance matrices (non-spherical) twostage methods fail. hope address question future work. proofs Lower Bounds lower bounds estimation problems rely standard reduction expected error hyposis testing assumes loss function semidistance, clustering loss isn. however, local triangle inequalitytype bound shown (proposition). this weaker condition lower-bound expected loss, stated Proposition (which easily fano inequality). proof techniques sparse non-sparse lower bounds identical. main difference non-sparse case, varshamov?gilbert bound (lemma construct set suﬃciently dissimilar hyposes, sparse case analogous result sparse hypercubes (lemma). see supplementary material complete proofs results. section next,  denote univariate standard normal PDF cdf. lemma (varshamov?gilbert bound). let   exists subset ...,  ),      denotes Hamming distance vectors (tsybakov (2009)). lemma let    integers  .  exist ...,      (massart (2007), Lemma). proposition let ...,     ,      log) implies)    clusterings inf Fbn maxi?  (fbn ?. Proposition for  clustering,  ? .    ,    )   results. let   )  cos   proposition let)(? ) )).  sin cos    tan   proposition let   ?    cos ?).   define log    define?  let  for  ), ..., (where standard basis let  ? ?     ))  ? )  proof orem let min Proposition?? ??    cos ??,? cos ??,?  ?(?,?)    Proposition cos ??,?   Hamming distance?? ??   ) cos ??,?  ?? ??  tan ??,?   cos ??,?    cos ??,?     (?) cos ??,?  cos ??,?  (?)  (?,  )(? ) )). Lemma exist ...,        for simplicity notation,             (?)     ?? ??  (?) sin ??,? cos ??,? define (?)       )  (?)       because,  , definition    (?)         log, Proposition)  also  )?    log because, definition  72n Proposition fact  ,  log min inf max (fbn ?    500 complete proof sup????   (fbn maxi?  (fbn Fbn  proof orem for simplicity, construction qwe state  assuming     Let  define min  define  let  log  }. for  ), ...,  ))    (where standard basis let  ? ?    Lemma exist ...,           remainder proof analogous orem (?)    Proofs Upper Bounds Propositions bound error estimating principal direction, obtained standard concentration bounds variant davis?kahan orem. proposition relates errors clustering loss. for sparse case, Propositions bound added error induced support estimation procedure. see supplementary material proof details. . Proposition let    ...,   for max log log  probability ?.      . proposition let     ...,  )—. for   . define cos    max log   max 160 probability ?  exp    sin  max log max log   proposition let    kvk) xt0 orwise. define cos .      sin    exp max sin sin      proof. let (x0cos since clustering loss invariant rotation translation,   — tan  — tan                — tan .  )    since tan            max   defining     )   — tan  )? )dydx    tan  cos sin tan )? )dudv  ) tan sin sin cos   max sin tan   cos  sin sin cos step. bound easily. proof orem using Propositions  Proposition fact exp(? max, ) exp(? max, ) log)   600 max  easy verify bounds decreasing bound supremum). case Proposition?need applied, principal directionsagree trivially. bound  max) shown similarly, exp  . proposition let     ...,   for log probability ?, ],  log log log  ,      ?—? )—   . proposition let     ...,   define (?)  ] (?)  ] )—  ?}. Assume     (?)  sbn (?) probability  proof. Proposition probability log) log) log,   )   ?—? )—   ]. assume event holds. (?) ], (?). orwise, ?)? clear \\x0csbn (?).  (?  ?)?   remainder proof trivial(?)  (?)  assume orwise. for (? ,  ?)?  log)  )—.   +?) , sbn ignore strict  definition, )—    (? ?? equality measure event. (?)  sbn concludes proof.  Proof orem define(?)  ] (?)  ] )—  ?}. Assume(?)  sbn (?)  Proposition holds probability  (?) simply  (fbn   )—, cos  )—, cos Assume(?)  let cos     simplicity define    respectively, elsewhere. sin  sin sin    (?)   (?)   (?  (?      sin  using argument proof orem long bound smaller    log) 104   ) 600 max      Using fact  always,  implies log) bound follows.  Conclusion provided minimax lower upper bounds estimating high dimensional mixtures. bounds show explicitly statistical diﬃculty problem depends dimension sample size separation sparsity level For clarity, focused special case spherical components equal mixture weights. future work, plan extend results general mixtures gaussians. one motivations work recent interest variable selection methods facilitate clustering high dimensional problems. existing methods Pan Shen (2007); Witten Tibshirani (2010); Raftery Dean (2006); Sun. (2012) Guo. (2010) provide promising numerical evidence variable selection improve high dimensional clustering. our results provide oretical basis idea. however, gap results paper methodology papers. indeed, now, rigorous proof methods papers outperform stage approach stage screens relevant features stage applies standard clustering methods features found stage. conjecture conditions simultaneous feature selection clustering outperforms stage method. settling question require aforementioned extension results general mixture case. acknowledgements This research supported part NSF grants IIS1116458 CAREER award iis-1252412, NSF Grant dms-0806009 Air Force Grant fa95500910373.',\n",
       " 'PP4992': 'develop methods Gaussian Poisson noise models, show low-rank estimates substantially outperform full rank estimates accuracy speed neural data retina.” receptive field) sensory neuron describes neuron integrates sensory stimuli time space. typical experiments naturalistic ﬂickering spatiotemporal stimuli, RFs high-dimensional, due large number coeﬃcients needed integration profile time \\x0cand space. estimating coeﬃcients small amounts data poses variety challenging statistical computational problems. address challenges developing Bayesian reduced rank regression methods estimation. corresponds modeling sum space-time separable., rank) filters. approach substantially reduces number parameters needed-10k mere 100s examples consider, confers substantial benefits statistical power computational eﬃciency. introduce prior low-rank RFs restriction matrix normal prior manifold low-rank matrices, ?localized? row column covariances obtain sparse, smooth, localized estimates spatial temporal components. develop methods inference resulting hierarchical model) fully Bayesian method blocked-gibbs sampling) fast, approximate method employs alternating ascent conditional marginal likelihoods. develop methods Gaussian Poisson noise models, show low-rank estimates substantially outperform full rank estimates neural data retina. introduction neuron linear receptive field) filter maps high-dimensional sensory stimuli one-dimensional variable underlying neuron spike rate. white noise reverse-correlation experiments, dimensionality determined number stimulus elements spatiotemporal window inﬂuencing neuron probability spiking. stimulus movie pixels frame, coeﬃcients, (experimenter-determined) number movie frames neuron temporal integration window. typical neurophysiology experiments, result RFs hundreds thousands parameters, meaning vector high dimensional space. high dimensional settings, traditional estimators whitened spike-triggered average (sta) exhibit large errors, naturalistic correlated stimuli. substantial literature refore focused methods regularizing estimates improve accuracy face limited experimental data. bayesian approach regularization involves prior distribution assigns higher probability RFs kinds structure. popular methods involved priors impose smallness, sparsity, smoothness, localized structure coeﬃcients]. here develop regularization method exploit fact neural RFs modeled low-rank matrices tensors). approach justified observation RFs summing small number space-time separable filters]. moreover, substantially reduce number parameters: rank receptive field dimensions requires parameters, single space-time separable filter spatial coeﬃcients temporal coeﬃcients., temporal unit vector). min commonly occurs experimental settings, parametrization yields considerable \\x0csavings. statistics literature, problem estimating low-rank matrix regression coeﬃcients reduced rank regression]. problem received considerable attention econometrics literature, Bayesian formulations tended focus non-informative minimally informative priors]. formulate prior reduced rank regression restriction matrix normal distribution] manifold low-rank matrices. results marginally Gaussian prior coefficients, puts equal footing ?ridge?, ar1, Gaussian priors. moreover, linear-gaussian response model, posterior rows columns conditionally gaussian, leading fast eﬃcient sampling-based inference methods. ?localized? form row column covariances matrix normal prior, hyperparameters governing smoothness locality components space time]. addition fully Bayesian sampling-based inference, develop fast approximate inference method coordinate ascent conditional marginal likelihoods temporal (column) spatial (row) hyperparameters. apply method linear-gaussian linear-nonlinear-poisson encoding models, show performance neural data. paper organized follows. sec. describe low-rank model localized priors. sec. describe fully Bayesian inference method blocked-gibbs sampling interleaved Metroplis Hastings steps. sec. introduce fast method approximate inference conditional empirical Bayesian hyperparameter estimates. sec. extend estimator linear-nonlinear Poisson encoding model. finally, sec. show applications simulated real neural datasets retina.  Hierarchical low-rank receptive field model Response model (likelihood) begin defining probabilistic encoding models provide likelihood functions inference. denote number spikes occur response matrix stimulus denote number temporal spatial elements, respectively. denote neuron matrix receptive field. consider, first, linear Gaussian encoding model ¿ ) vec vec) denote vectorized stimulus vectorized, respectively, variance response noise, bias term. second, linear-nonlinear-poisson (lnp) encoding model poiss)). ) denotes nonlinearity. examples include exponential soft rectifying function, log(exp(?) ), give rise concave log-likelihood].  Prior low rank receptive field represent rank factorization columns matrix  rdx spatial filters. ¿ temporal filters columns matrix) define prior rank matrices restriction matrix normal distribution, prior written exp ) normalizer involves integration space rank matrices, closed-form expression. prior controlled ?column? covariance matrix rdt ?row? covariance matrix rdx govern temporal spatial components, respectively. express factorized form. ), rewrite prior exp  ) This formulation makes clear conditionally Gaussian priors   ) pdt pdx denotes Kronecker product, vec vec define define parametric form controlled hyperparameters respectively. form adopted ?automatic locality determination? (ald) prior introduced]. ald prior, covariance matrix encodes tendency RFs localized space-time spatiotemporal frequency. spatial covariance matrix hyperparameters {?, scalar determining scale covariance; length vectors center location support space spatial frequency, (where number spatial dimensions? standard visual pixel stimuli). positive definite matrices determine size local region support space spatial frequency]. temporal covariance matrix hyperparameters directly analogous determine localized structure time temporal frequency. finally, place zero-mean Gaussian prior (scalar) bias term:  posterior inference Markov Chain Monte Carlo For complete dataset}, ?  design matrix, vector responses, goal infer joint posterior)  ) develop eﬃcient Markov chain Monte Carlo (mcmc) sampling method blocked-gibbs sampling. blocked-gibbs sampling closed-form conditional priors. gaussian likelihood yields closed-form ?conditional marginal likelihood? ), respectively1 blocked-gibbs samples conditional evidence simultaneously sample conditional posterior. samples sample similarly. sampling conditional evidence, Metropolis Hastings) algorithm sample low dimensional space hyperparameters. sampling, closed-form formula (will introduced shortly) conditional posterior. details algorithm follows. step Given samples draw ith samples ) ) ) ), section sec, fix likelihood Gaussian. ). extension Poisson likelihood model. sec. divided parts2 sample conditional posterior  ,  )dbdkt  —mx0, Cwt )dwt) vector ktt Mx0 concatenation vector matrix generated projecting stimulus stacking row, meaning row [vec )]¿ Cwt block diagonal matrix diagonal  standard formula product gaussians, obtain closed form conditional evidence?  ? ?cwt exp  ) covariance conditional posterior Mx0T Mx0T ) algorithm search low dimensional hyperparameter space, conditional evidence. target distribution, uniform hyperprior ?).  sample, conditional posterior. . step Given ith samples draw ith samples) ) ) ), divided parts: sample conditional posterior, ,  )dkx)  )dkx matrix generated projecting stimulus stacking row, meaning row [vec¿ ])]¿ standard formula product gaussians, obtain closed form conditional evidence? ? ?  exp    ) covariance conditional posterior¿   ) Step uniform hyperprior conditional evidence target distribution algorithm.  sample conditional posterior. . summary algorithm Algorithm omit sample index, superscript), notational clean ness. algorithm fully Bayesian low-rank inference blocked-gibbs sampling Given data conditioned samples variables, iterate following: sample conditional evidence . conditional posterior. ).  sample conditional evidence. ) conditional posterior. ). convergence. approximate algorithm fast posterior inference Here develop alternative, approximate algorithm fast posterior inference. integrating hyperparameters, attempt find point estimates maximize conditional marginal likelihood. resembles empirical Bayesian inference, hyperparameters set maximizing full marginal likelihood. model, evidence closed form; however, conditional evidence  conditional evidence closed form. . ). thus, alternate) maximizing conditional evidence set finding MAP estimates) maximizing conditional evidence set finding MAP estimates,      arg max    ), arg max,   )     arg max)    arg max ).  approximate algorithm works conditional evidence tightly concentrated maximum. note hyperparameters fixed, iterative updates, amount alternating coordinate ascent posterior). extension Poisson likelihood When likelihood non-gaussian, blocked-gibbs sampling tractable, closed form expression conditional evidence. here, introduce fast, approximate inference algorithm low-rank model LNP likelihood. basic steps approximate algorithm (sec). however, make Gaussian approximation conditional posterior, Laplace approximation. approximate conditional evidence posterior mode, details follows. conditional evidence poiss(mx0, Cwt )dwt) integrand proportional conditional posterior approximate Gaussian distribution Laplace approximation  ) conditional MAP estimate obtained numerically maximizing logwhere conditional posterior., newton method. Appendix), log log(mx0 (mx0 ) covariance conditional posterior obtained derivative log conditional posterior mode hessian negative logt likelihood denoted  log true low-rank Gibbs low-rank fast space MSE 250 samples time full-rank 2000 samples full-rank low-rank (fast) low-rank (gibbs.003 250 500 1000 2000 training data Figure Simulated data. data generated linear Gaussian response model rank pixels: 1024 parameters full-rank model; 160 rank model).  true rank (left). estimates obtained, full-rank ald, low-rank approximate method, blocked-gibbs sampling, 250 samples (top), 2000 samples (bottom), respectively.  average squared error estimate method (average independent repetitions). Gaussian posterior. ), log conditional evidence (log. ) posterior simply mode log   log Mx0 log —cwt maximize set due space limit, omit derivations conditional posterior conditional evidence, (see Appendix). results Simulations tested performance blocked-gibbs sampling fast approximate algorithm simulated Gaussian neuron rank temporal bins spatial pixels shown fig. . compared methods maximum likelihood estimate full-rank ALD estimate. fig. shows low-rank estimates obtained blocked-gibbs sampling approximate algorithm perform similarly, achieve lower squared error full-rank estimates. linear Gaussian full-rank low-rank Linear Nonlinear Poisson full-rank low-rank Gaussian full-rank low-rank 250 samples MSE LNP full-rank low-rank 2000 samples \\x0c250 500 1000 2000 training data Figure Simulated data. data generated linear-nonlinear Poisson (lnp) response model rank (shown fig. ) ?softrect? nonlinearity.  estimates obtained, fullrank ald, low-rank approximate method linear Gaussian model, methods LNP model, 250 (top) 2000 (bottom) samples, respectively.  average squared error estimate (from independent repetitions). low-rank estimates LNP model perform linear Gaussian model. tested performance methods simulated linear-nonlinear Poisson (lnp) neuron softrect nonlinearity. estimated method linear Gaussian model LNP model. fig. shows low-rank rank low-rank (gibbs) relative likelihood stimulus rank space low-rank STA rank low-rank (fast) low-rank (gibbs) rank rank space low-rank STA rank simple cell time rank low-rank (fast) time simple cell relative likelihood stimulus rank Figure Comparison low-rank estimates simple cells (using white noise ﬂickering bars stimuli]). relative likelihood test stimulus (left) low-rank estimates ranks (right). relative likelihood ratio test likelihood rank STA estimates. minutes training data, rank estimates obtained blocked-gibbs sampling approximate method achieve highest test likelihood (estimates shown top row), rank STA achieves highest test likelihood, noise added low-rank STA rank increases (estimates shown bottom row). relative likelihood full rank ALD. similar plot anor simple cell. rank estimates obtained blocked-gibbs sampling approximate method achieve highest test likelihood cell. relative likelihood full rank ALD. estimates perform full-rank estimates model, low-rank estimates LNP model achieved lowest mse.  Application neural data applied methods estimate RFs simple cells retinal ganglion cells (rgcs). details data collection]. performed-fold cross-validation minute training minutes test data. fig. fig. show average test likelihood function rank linear Gaussian model. show low-rank estimates obtained methods low-rank sta. low-rank STA (rank)  singular value left computed singular vectors, respectively. stimulus distribution non-gaussian, low-rank STA larger bias low-rank ALD estimate. RGC off-cell spatial extent temporal extent relative likelihood stimulus 1st low-rank (gibbs 2nd low-rank STA 3rd 2nd low-rank (fast) rank 3rd spatial extent temporal extent 3rd low-rank (gibbs) relative likelihood stimulus 1st RGC-cell 1st 2nd low-rank (fast low-rank STA rank Figure Comparison low-rank estimates retinal data (using binary white noise stimuli]). consists spatial pixels temporal bins (2500 coeﬃcients). relative likelihood test stimulus (left), top left singular vectors (middle) singular vectors (right) estimated off-rgc cell. samplingbased estimate benefits rank representation, making distinct spatial temporal components, performance low-rank STA degrades rank relative likelihood full rank ALD.0146. similar plot-rgc cell. relative likelihood full rank ALD.006. estimates perform rank space16 min. rank (lnp) Gaussian full-rank rank(fast) rank(gibbs LNP full-rank rank minutes training data runtime (sec) time sec. rank (gaussian) (gaussian) prediction error 103 minutes training data Figure estimates simple cell. (data]). estimates obtained (left) low-rank blocked-gibbs sampling linear Gaussian model (middle), low-rank approximate algorithm LNP model (right), amounts training data sec. min.). consists temporal spatial dimensions (256 coeﬃcients). average prediction spike count) error-subset data. low-rank estimates LNP model achieved lowest prediction error methods. runtime method. low-rank approximate algorithms sec., full-rank inference methods 100 times longer. finally, applied methods estimate simple cell amounts training data minutes) computed prediction error estimate linear Gaussian LNP models. fig. show estimates sec. min. training data. computed test likelihood estimate set rank found rank \\x0cestimates achieved highest test likelihood. terms average prediction error, low-rank estimates obtained fast approximate algorithm achieved lowest error, runtime algorithm significantly lower full-rank inference methods. conclusion hierarchical model low-rank rfs. introduced prior low-rank matrices based restricted matrix normal distribution, feature preserving marginally Gaussian prior regression coeﬃcients. ?localized? form define row column covariance matrices matrix normal prior, model ﬂexibly learn smooth sparse structure spatial temporal components. developed inference methods: exact based MCMC blocked-gibbs sampling approximate based alternating evidence optimization. applied model neural data Gaussian Poisson noise models, found Poisson lnp) model performed increased reliance approximate inference. overall, found low-rank estimates achieved higher prediction accuracy significantly lower computation time compared full-rank estimates. localized, low-rank model high-dimensional settings, cases stimulus covariance matrix fit memory. future work, develop fully Bayesian inference methods low-rank RFs LNP noise model, quantify accuracy approximate method. secondly, examine methods inferring rank, number space-time separable components determined automatically data. acknowledgments Rust movshon data, chichilnisky, shlens, . litke, Sher retinal data. work supported Sloan Research fellowship, McKnight scholar award, NSF CAREER Award iis-1150186. develop methods Gaussian Poisson noise models, show low-rank estimates substantially outperform full rank estimates accuracy speed neural data retina.” receptive field) sensory neuron describes neuron integrates sensory stimuli time space. typical experiments naturalistic ﬂickering spatiotemporal stimuli, RFs high-dimensional, due large number coeﬃcients needed integration profile time \\x0cand space. estimating coeﬃcients small amounts data poses variety challenging statistical computational problems. here address challenges developing Bayesian reduced rank regression methods estimation. this corresponds modeling sum space-time separable., rank) filters. this approach substantially reduces number parameters needed-10k mere 100s examples consider, confers substantial benefits statistical power computational eﬃciency. introduce prior low-rank RFs restriction matrix normal prior manifold low-rank matrices, ?localized? row column covariances obtain sparse, smooth, localized estimates spatial temporal components. develop methods inference resulting hierarchical model) fully Bayesian method blocked-gibbs sampling) fast, approximate method employs alternating ascent conditional marginal likelihoods. develop methods Gaussian Poisson noise models, show low-rank estimates substantially outperform full rank estimates neural data retina. Introduction neuron linear receptive field) filter maps high-dimensional sensory stimuli one-dimensional variable underlying neuron spike rate. white noise reverse-correlation experiments, dimensionality determined number stimulus elements spatiotemporal window inﬂuencing neuron probability spiking. for stimulus movie pixels frame, coeﬃcients, (experimenter-determined) number movie frames neuron temporal integration window. typical neurophysiology experiments, result RFs hundreds thousands parameters, meaning vector high dimensional space. high dimensional settings, traditional estimators whitened spike-triggered average (sta) exhibit large errors, naturalistic correlated stimuli. substantial literature refore focused methods regularizing estimates improve accuracy face limited experimental data. Bayesian approach regularization involves prior distribution assigns higher probability RFs kinds structure. popular methods involved priors impose smallness, sparsity, smoothness, localized structure coeﬃcients]. Here develop regularization method exploit fact neural RFs modeled low-rank matrices tensors). this approach justified observation RFs summing small number space-time separable filters]. moreover, substantially reduce number parameters: rank receptive field dimensions requires parameters, single space-time separable filter spatial coeﬃcients temporal coeﬃcients., temporal unit vector). when min commonly occurs experimental settings, parametrization yields considerable \\x0csavings. statistics literature, problem estimating low-rank matrix regression coeﬃcients reduced rank regression]. this problem received considerable attention econometrics literature, Bayesian formulations tended focus non-informative minimally informative priors]. here formulate prior reduced rank regression restriction matrix normal distribution] manifold low-rank matrices. this results marginally Gaussian prior coefficients, puts equal footing ?ridge?, ar1, Gaussian priors. moreover, linear-gaussian response model, posterior rows columns conditionally gaussian, leading fast eﬃcient sampling-based inference methods. ?localized? form row column covariances matrix normal prior, hyperparameters governing smoothness locality components space time]. addition fully Bayesian sampling-based inference, develop fast approximate inference method coordinate ascent conditional marginal likelihoods temporal (column) spatial (row) hyperparameters. apply method linear-gaussian linear-nonlinear-poisson encoding models, show performance neural data. paper organized follows. sec. describe low-rank model localized priors. sec. describe fully Bayesian inference method blocked-gibbs sampling interleaved Metroplis Hastings steps. sec. introduce fast method approximate inference conditional empirical Bayesian hyperparameter estimates. sec. extend estimator linear-nonlinear Poisson encoding model. finally, sec. show applications simulated real neural datasets retina.  Hierarchical low-rank receptive field model Response model (likelihood) begin defining probabilistic encoding models provide likelihood functions inference. let denote number spikes occur response matrix stimulus denote number temporal spatial elements, respectively. let denote neuron matrix receptive field. consider, first, linear Gaussian encoding model ¿ ) vec vec) denote vectorized stimulus vectorized, respectively, variance response noise, bias term. second, linear-nonlinear-poisson (lnp) encoding model poiss)). ) denotes nonlinearity. examples include exponential soft rectifying function, log(exp(?) ), give rise concave log-likelihood].  Prior low rank receptive field represent rank factorization columns matrix  rdx spatial filters. ¿ temporal filters columns matrix) define prior rank matrices restriction matrix normal distribution, prior written exp ) normalizer involves integration space rank matrices, closed-form expression. prior controlled ?column? covariance matrix rdt ?row? covariance matrix rdx govern temporal spatial components, respectively. express factorized form. ), rewrite prior exp  ) This formulation makes clear conditionally Gaussian priors   ) pdt pdx denotes Kronecker product, vec vec define define parametric form controlled hyperparameters respectively. this form adopted ?automatic locality determination? (ald) prior introduced]. ALD prior, covariance matrix encodes tendency RFs localized space-time spatiotemporal frequency. for spatial covariance matrix hyperparameters {?, scalar determining scale covariance; length vectors center location support space spatial frequency, (where number spatial dimensions? standard visual pixel stimuli). positive definite matrices determine size local region support space spatial frequency]. temporal covariance matrix hyperparameters directly analogous determine localized structure time temporal frequency. finally, place zero-mean Gaussian prior (scalar) bias term:  Posterior inference Markov Chain Monte Carlo For complete dataset}, ?  design matrix, vector responses, goal infer joint posterior)  ) develop eﬃcient Markov chain Monte Carlo (mcmc) sampling method blocked-gibbs sampling. blocked-gibbs sampling closed-form conditional priors. Gaussian likelihood yields closed-form ?conditional marginal likelihood? ), respectively1 blocked-gibbs samples conditional evidence simultaneously sample conditional posterior. given samples sample similarly. for sampling conditional evidence, Metropolis Hastings) algorithm sample low dimensional space hyperparameters. for sampling, closed-form formula (will introduced shortly) conditional posterior. details algorithm follows. step Given samples draw ith samples ) ) ) ), section sec, fix likelihood Gaussian. ). extension Poisson likelihood model. sec. divided parts2 sample conditional posterior  ,  )dbdkt  —mx0, Cwt )dwt) vector ktt Mx0 concatenation vector matrix generated projecting stimulus stacking row, meaning row [vec )]¿ Cwt block diagonal matrix diagonal  using standard formula product gaussians, obtain closed form conditional evidence?  ? ?cwt exp  ) covariance conditional posterior Mx0T Mx0T ) algorithm search low dimensional hyperparameter space, conditional evidence. target distribution, uniform hyperprior ?).  sample, conditional posterior. . step Given ith samples draw ith samples) ) ) ), divided parts: sample conditional posterior, ,  )dkx)  )dkx matrix generated projecting stimulus stacking row, meaning row [vec¿ ])]¿ using standard formula product gaussians, obtain closed form conditional evidence? ? ?  exp    ) covariance conditional posterior¿   and) Step uniform hyperprior conditional evidence target distribution algorithm.  sample conditional posterior. . summary algorithm Algorithm omit sample index, superscript), notational clean ness. Algorithm fully Bayesian low-rank inference blocked-gibbs sampling Given data conditioned samples variables, iterate following: sample conditional evidence . conditional posterior. ).  sample conditional evidence. ) conditional posterior. ). until convergence. Approximate algorithm fast posterior inference Here develop alternative, approximate algorithm fast posterior inference. instead integrating hyperparameters, attempt find point estimates maximize conditional marginal likelihood. this resembles empirical Bayesian inference, hyperparameters set maximizing full marginal likelihood. model, evidence closed form; however, conditional evidence  conditional evidence closed form. . ). thus, alternate) maximizing conditional evidence set finding MAP estimates) maximizing conditional evidence set finding MAP estimates,      arg max    ), arg max,   )     arg max)    arg max ).  approximate algorithm works conditional evidence tightly concentrated maximum. note hyperparameters fixed, iterative updates, amount alternating coordinate ascent posterior). Extension Poisson likelihood When likelihood non-gaussian, blocked-gibbs sampling tractable, closed form expression conditional evidence. here, introduce fast, approximate inference algorithm low-rank model LNP likelihood. basic steps approximate algorithm (sec). however, make Gaussian approximation conditional posterior, Laplace approximation. approximate conditional evidence posterior mode, details follows. conditional evidence poiss(mx0, Cwt )dwt) integrand proportional conditional posterior approximate Gaussian distribution Laplace approximation  ) conditional MAP estimate obtained numerically maximizing logwhere conditional posterior., newton method. see Appendix), log log(mx0 (mx0 ) covariance conditional posterior obtained derivative log conditional posterior mode Hessian negative logt likelihood denoted  log true low-rank Gibbs low-rank fast space MSE 250 samples time full-rank 2000 samples full-rank low-rank (fast) low-rank (gibbs.003 250 500 1000 2000 training data Figure Simulated data. data generated linear Gaussian response model rank pixels: 1024 parameters full-rank model; 160 rank model).  true rank (left). estimates obtained, full-rank ald, low-rank approximate method, blocked-gibbs sampling, 250 samples (top), 2000 samples (bottom), respectively.  average squared error estimate method (average independent repetitions). under Gaussian posterior. ), log conditional evidence (log. ) posterior simply mode log   log Mx0 log —cwt maximize set due space limit, omit derivations conditional posterior conditional evidence, (see Appendix). Results Simulations tested performance blocked-gibbs sampling fast approximate algorithm simulated Gaussian neuron rank temporal bins spatial pixels shown fig. . compared methods maximum likelihood estimate full-rank ALD estimate. fig. shows low-rank estimates obtained blocked-gibbs sampling approximate algorithm perform similarly, achieve lower squared error full-rank estimates. linear Gaussian full-rank low-rank Linear Nonlinear Poisson full-rank low-rank Gaussian full-rank low-rank 250 samples MSE LNP full-rank low-rank 2000 samples \\x0c250 500 1000 2000 training data Figure Simulated data. data generated linear-nonlinear Poisson (lnp) response model rank (shown fig. ) ?softrect? nonlinearity.  estimates obtained, fullrank ald, low-rank approximate method linear Gaussian model, methods LNP model, 250 (top) 2000 (bottom) samples, respectively.  average squared error estimate (from independent repetitions). low-rank estimates LNP model perform linear Gaussian model. tested performance methods simulated linear-nonlinear Poisson (lnp) neuron softrect nonlinearity. estimated method linear Gaussian model LNP model. fig. shows low-rank rank low-rank (gibbs) relative likelihood stimulus rank space low-rank STA rank low-rank (fast) low-rank (gibbs) rank rank space low-rank STA rank simple cell time rank low-rank (fast) time simple cell relative likelihood stimulus rank Figure Comparison low-rank estimates simple cells (using white noise ﬂickering bars stimuli]). Relative likelihood test stimulus (left) low-rank estimates ranks (right). relative likelihood ratio test likelihood rank STA estimates. using minutes training data, rank estimates obtained blocked-gibbs sampling approximate method achieve highest test likelihood (estimates shown top row), rank STA achieves highest test likelihood, noise added low-rank STA rank increases (estimates shown bottom row). relative likelihood full rank ALD. Similar plot anor simple cell. rank estimates obtained blocked-gibbs sampling approximate method achieve highest test likelihood cell. relative likelihood full rank ALD. estimates perform full-rank estimates model, low-rank estimates LNP model achieved lowest mse.  Application neural data applied methods estimate RFs simple cells retinal ganglion cells (rgcs). details data collection]. performed-fold cross-validation minute training minutes test data. fig. fig. show average test likelihood function rank linear Gaussian model. show low-rank estimates obtained methods low-rank sta. low-rank STA (rank)  singular value left computed singular vectors, respectively. stimulus distribution non-gaussian, low-rank STA larger bias low-rank ALD estimate. RGC off-cell spatial extent temporal extent relative likelihood stimulus 1st low-rank (gibbs 2nd low-rank STA 3rd 2nd low-rank (fast) rank 3rd spatial extent temporal extent 3rd low-rank (gibbs) relative likelihood stimulus 1st RGC-cell 1st 2nd low-rank (fast low-rank STA rank Figure Comparison low-rank estimates retinal data (using binary white noise stimuli]). consists spatial pixels temporal bins (2500 coeﬃcients). Relative likelihood test stimulus (left), top left singular vectors (middle) singular vectors (right) estimated off-rgc cell. samplingbased estimate benefits rank representation, making distinct spatial temporal components, performance low-rank STA degrades rank relative likelihood full rank ALD.0146. Similar plot-rgc cell. relative likelihood full rank ALD.006. both estimates perform rank space16 min. rank (lnp) Gaussian full-rank rank(fast) rank(gibbs LNP full-rank rank minutes training data runtime (sec) time sec. rank (gaussian) (gaussian) prediction error 103 minutes training data Figure estimates simple cell. (data]). estimates obtained (left) low-rank blocked-gibbs sampling linear Gaussian model (middle), low-rank approximate algorithm LNP model (right), amounts training data sec. min.). consists temporal spatial dimensions (256 coeﬃcients). Average prediction spike count) error-subset data. low-rank estimates LNP model achieved lowest prediction error methods. Runtime method. low-rank approximate algorithms sec., full-rank inference methods 100 times longer. finally, applied methods estimate simple cell amounts training data minutes) computed prediction error estimate linear Gaussian LNP models. fig. show estimates sec. min. training data. computed test likelihood estimate set rank found rank \\x0cestimates achieved highest test likelihood. terms average prediction error, low-rank estimates obtained fast approximate algorithm achieved lowest error, runtime algorithm significantly lower full-rank inference methods. Conclusion hierarchical model low-rank rfs. introduced prior low-rank matrices based restricted matrix normal distribution, feature preserving marginally Gaussian prior regression coeﬃcients. ?localized? form define row column covariance matrices matrix normal prior, model ﬂexibly learn smooth sparse structure spatial temporal components. developed inference methods: exact based MCMC blocked-gibbs sampling approximate based alternating evidence optimization. applied model neural data Gaussian Poisson noise models, found Poisson lnp) model performed increased reliance approximate inference. overall, found low-rank estimates achieved higher prediction accuracy significantly lower computation time compared full-rank estimates. localized, low-rank model high-dimensional settings, cases stimulus covariance matrix fit memory. future work, develop fully Bayesian inference methods low-rank RFs LNP noise model, quantify accuracy approximate method. secondly, examine methods inferring rank, number space-time separable components determined automatically data. acknowledgments Rust Movshon data, chichilnisky, shlens, . litke, Sher retinal data. this work supported Sloan Research fellowship, McKnight scholar award, NSF CAREER Award iis-1150186.',\n",
       " 'PP5029': 'learning classifiers generalize hard problem training examples available. example, images cheetah, hard train classifier good distinguishing cheetahs hundreds classes, working pixels alone. powerful machine learning model severely overfit examples, held back strong regularizers. paper based idea performance improved natural structure inherent set classes. example, cheetahs related tigers, lions, jaguars leopards. labeled examples related classes make task learning cheetah examples easier. knowing class structure borrow ?knowledge? relevant classes distinctive features specific cheetahs learned. least, \\x0cmodel confuse cheetahs animals rar completely unrelated classes, cars lamps. aim develop methods transferring knowledge related tasks learning task. endeavour scale machine learning algorithms, imperative good ways transferring knowledge related problems. finding relatedness hard problem. absence prior knowledge, order find classes related, classes., good model learn good model, classes related. creates cyclic dependency. circumvent external knowledge source, human, class structure hand. anor resolve dependency iteratively learn model classes relationships exist improve. paper, follow bootstrapping approach. this paper proposes learning class structure classifier parameters context deep neural networks. aim improve classification accuracy classes examples. deep neural networks trained discriminatively back propagation achieved state--art performance diﬃcult classification problems large amounts labeled data]. case smaller amounts data datasets rare classes studied. address shortcoming, model augments neural networks tree-based prior layer weights. structure prior related classes share prior. shared prior captures features common members superclass. refore, class examples, model orwise unable learn good features for, access good features virtue belonging superclass. learning hierarchical structure classes extensively studied machine learning, statistics, vision communities. large class models based hierarchical Bayesian models transfer learning]. hierarchical topic model image features Bart. ] discover visual taxonomies unsupervised fashion large datasets designed rapid learning categories. fei-fei. ] developed hierarchical Bayesian model visual categories, prior parameters categories induced categories. however, approach well-suited generic approach transfer learning learned single prior shared categories. number models based hierarchical Dirichlet processes transfer learning]. however, above-mentioned models generative nature. models typically resort MCMC approaches inference, hard scale large datasets. furrmore, tend perform worse discriminative approaches, number labeled examples increases. large class discriminative models] transfer learning enable discovering sharing information related classes. similar work] defined generative prior classifier parameters prior tree structures identify \\x0crelevant categories. however, work focused specific object detection task SVM model pre-defined HOG features input. paper, demonstrate method deep architectures) convolutional nets pixels input singlelabel softmax outputs) fully connected nets pretrained deep Boltzmann machines image features text tokens input multi-label logistic outputs. model improves performance strong baselines cases, lending measure universality approach. essence, model learns low-level features, high-level features, hierarchy classes end-end way. model Description Let     set data points set labels, label dimensional vector targets. targets binary, one, real-valued. setting, image one encoding label. model multi-layer neural network (see fig. ). denote set parameters network (weights biases layers), excluding top-level weights, denote separately   represents number hidden units hidden layer. conditional distribution expressed  (?)dwd?. ,? general, integral intractable, typically resort MAP estimation determine values model parameters maximize log log) log (?). here, log log-likelihood function terms priors model parameters. typical choice prior Gaussian distribution diagonal covariance:  ,   }.   denotes classifier parameters class Note prior assumes independent. words-priori, weights label related ??? predictions ?car ?tiger high level features ??? low level features) ?vehicle Input ?car ?animal ?truck \\x0c?tiger ??? ?cheetah) Figure Our model: deep neural network priors classification parameters. priors derived hierarchy classes. label weights. reasonable assumption labels. works applications large number labeled examples class. however, classes related anor, priors respect relationships suitable. priors crucial classes handful training examples, effect prior pronounced. work, focus developing prior.  Learning With Fixed Tree Hierarchy Let assume classes organized fixed tree hierarchy. relax assumption placing hierarchical non-parametric prior tree structures. ease exposition, two-level hierarchy1 shown fig. . leaf nodes classes. connected super-classes group toger similar basic-level classes. leaf node weight vector  super-class node vector ..., define generative model   ?parent) ) This prior expresses relationships classes. example, asserts ?car ?truck deviations ?vehicle similarly, ?cat ?dog deviations ?animal . -written include   (?—? (?)dwd?. ,?,? perform MAP inference determine values, maximize log log) log (?—?) log (?). terms loss function, minimize,  log  log) log (?—?)  log (?)  log  ?parent  Note fixing loss function recovers standard loss function. choice normal distributions. leads nice property maximization closed form. amounts taking (scaled) average children —parent?   model easily generalized deeper hierarchies. : given: classes superclasses initial Initialize repeat van Optimize fixed  sgd steps. Optimize fixed=vehicle randompermute) {snew car truck van  sgd steps. end choosebestsuperclass(?            end convergence Algorithm Procedure learning tree. =snew=animal cat dog van van refore, loss function. optimized iteratively performing steps. step, maximize keeping fixed. standard stochastic gradient descent (sgd). maximize keeping fixed. closed form.  practical terms, step instantaneous performed gradient descent steps-100. refore, learning identical standard gradient descent. exploit structure labels nominal cost terms computational time.  Learning Tree Hierarchy assumed model fixed tree hierarchy. now, show tree structure learned training. -length vector specifies tree structure, class child super-class place non-parametric Chinese Restaurant Process (crp) prior prior) model ﬂexibility number superclasses. crp prior extends partition classes class adding class eir existing superclasses superclass. probability number children superclass probability adding superclass+?  creating superclass+?  essence, prefers add node existing large superclass spawning one. strength preference controlled equipped CRP prior conditional takes form  (?)dwd? ). ,?,? map inference model leads optimization problem max log log) log (?—?, log (?) log).  Maximization problematic domain huge discrete set. fortunately, approximated simple parallelizable search procedure. initialize tree sensibly. hand extracting semantic tree WordNet]. number superclasses tree optimize, steps \\x0cthis tree. leaf node picked uniformly random tree tree proposals generated follows. proposals generated attaching leaf node superclasses. additional proposal generated creating super-class attaching label. process shown Algorithm -estimate {?, trees steps. note optimization problems performed independently, parallel. tree picked validation set. process repeated picking anor node locations. node picked potentially repositioned, resulting tree back whale dolphin willow tree oak tree lamp clock leopard tiger ray ﬂatfish Figure Examples cifar-100. randomly chosen examples 100 classes shown. classes row belong superclass. optimizing newly learned tree place tree. position class tree change full pass classes, hierarchy discovery converged. training model cifar-100, amounts interrupting stochastic gradient descent,000 steps find tree. amount time spent learning tree small fraction total time (about%). experiments cifar-100 cifar-100 dataset] consists color images belonging 100 classes. classes divided groups each. example, superclass fish aquarium fish, ﬂatfish, ray, shark trout; superclass ﬂowers orchids, poppies, roses, sunﬂowers tulips. examples dataset shown fig.  chose dataset large number classes examples each, making ideal demonstrating utility transfer learning. 600 examples class 500 training set 100 test set. preprocessed images global contrast normalization ZCA whitening.  Model Architecture Training Details convolutional neural network convolutional hidden layers fully connected hidden layers. hidden units rectified linear activation function. convolutional layer max-pooling layer. dropout] applied layers network probability retaining hidden unit) layers network (going input convolutional layers fully connected layers). max-norm regularization] weights convolutional fully connected layers. initial tree chosen based superclass structure data set. learned tree Algorithm, 000 100. final learned tree provided supplementary material.  Experiments Few Examples Class set experiments, worked scenario class examples. aim assess wher proposed model related classes borrow information. baseline, standard convolutional neural network architecture model. extremely strong baseline achieved excellent results, outperforming previously reported results dataset shown Table created subsets data randomly choosing, 100 250 examples class, trained models subset. baseline. model tree structure (100 classes grouped superclasses) fixed training. fourth models learned tree structure. initialized random tree fourth tree. random tree constructed drawing sample CRP prior randomly assigning classes leaf nodes. test performance models compared fig. . observe number examples class small, fixed tree model significant improvement baseline. improvement diminishes number examples increases eventually performance falls baseline%). however, learned tree model Improvement Test classification accuracy Baseline Fixed Tree Learned Tree 100 250 Number training examples label Baseline Fixed Tree Learned Tree 500) Sorted classes 100) Figure Classification results cifar-100. left: Test set classification accuracy number training examples class. right: Improvement baseline trained examples class. learned tree models initialized tree. method Test Accuracy Conv Net max pooling Conv Net stochastic pooling] Conv Net maxout] Conv Net max pooling dropout (baseline) Baseline fixed tree Baseline learned tree (initialized randomly) Baseline learned tree (initialized tree      Table Classification results cifar-100. models trained full training set. better. examples class, accuracy% compared baseline model% fixed tree model%. model% relative improvement examples available. number examples increases, relative improvement decreases. however, 500 examples class, learned tree model improves baseline, achieving classification accuracy%. note initializing model random tree decreases model performance, shown Table next, analyzed learned tree model find source improvements. model trained examples class looked classification accuracy separately class. aim find classes gain suffer most. fig. shows improvement obtained classes baseline, classes sorted improvement baseline. observe classes benefit degrees learning hierarchy parameter sharing, classes perform worse result transfer. learned tree model, classes improve willow tree%) orchid%). classes lose transfer ray%) lamp%). hyposize reason classes gain lot similar classes superclass stand gain lot transferring knowledge. example, superclass willow tree trees, maple tree oak tree. however, ray belongs superclass fish typical examples fish dissimilar appearance. fixed tree, transfer hurts performance (ray worse%). however, tree learned, \\x0cclass split fish superclass join superclass suffer much. similarly, lamp household electrical devices keyboard clock. putting kinds electrical devices superclass makes semantic sense visual recognition tasks. highlights key limitation hierarchies based semantic knowledge advocates learn hierarchy relevant task hand. full learned tree provided supplementary material.  Experiments Few Examples One Class set experiments, worked scenario lots examples classes, examples class. aim wher model transfers information classes learned ?rare? class. constructed training sets randomly drawing eir, 100, 250 500 examples dolphin Baseline Fixed Tree Learned Tree Baseline Fixed Tree Learned Tree Test classification accuracy Test classification accuracy 100 250 Number training cases dolphin 500 100 250 Number training cases dolphin) 500) Figure Results cifar-100 examples dolphin class. left: Test set classification accuracy number examples. right: Accuracy classifying dolphin whale shark considered correct. classes baby, female, people, portrait \\x0cplant life, river, water clouds, sea, sky, transport, water animals, dog, food clouds, sky, structures claudia text barco, pesca, boattosail, navegac? watermelon, dog, hilarious, chihuahua colors, cores, centro, commercial, building Images Tags Figure Some examples mir-flickr dataset. instance dataset image textual tags. image multiple classes. class 500 training examples classes. trained baseline, fixed tree learned tree models datasets. objective special attention paid dolphin class. fig. shows test accuracy correctly predicting dolphin class. transfer learning helped tremendously. example, cases, baseline accuracy transfer learning model%. 250 cases, learned tree model significant improvements%). repeated experiment classes dolphin found similar improvements. supplementary material detailed description. addition performing class examples, errors sensible. check case, evaluated performance models treating classification dolphin shark whale correct, reasonable mistakes. fig. shows classification accuracy assumption models. observe transfer learning methods provide significant improvements baseline. dolphin, accuracy jumps%. experiments MIR Flickr Multimedia Information Retrieval Flickr Data set] consists million images collected social photography website Flickr user assigned tags. million images,000 annotated labels. labels include object categories, bird, tree, people, scene categories, indoor, sky night. image multiple labels. examples shown fig.  dataset cifar-100 ways. cifar-100 dataset, model trained image pixels input image belonged class. mir-flickr multimodal dataset standard computer vision image features word counts inputs. cifar-100 models multi-layer convolutional network, dataset fully connected neural network initialized unrolling Deep Boltzmann Machine (dbm]. moreover, dataset offers natural class distribution classes occur ors. example, sky occurs \\x0cover% instances, baby occurs fewer%. 975,000 unlabeled images unsupervised training dbm. publicly features train-test splits].  Baseline Fixed Tree Learned Tree Improvement Average Precision Improvement Average Precision Sorted classes) class-wise improvement Fraction instances class) Improvement. number examples Figure Results MIR flickr. left: Improvement Average Precision baseline methods. right: Improvement learned tree model baseline classes fraction test cases class. dot corresponds class. classes examples (towards left plot) significant improvements. method MAP Logistic regression Multimodal DBM] Multiple Kernel Learning SVMs] TagProp] Multimodal DBM finetuning dropout (baseline) Baseline fixed tree Baseline learned tree (initialized tree.609.623.640.641 .004.648 .004.651 .005 Table Mean Average Precision obtained models MIR Flickr data set.  \\x0cmodel Architecture Training Details order make results directly comparable], network architecture rein. authors dataset] provided high-level categorization classes create initial tree. tree structure learned model shown supplementary material. algorithm 500 100.  Classification Results For baseline Multimodal DBM model finetuning discriminatively dropout. model achieves state--art results, making strong baseline. results experiment summarized Table baseline achieved MAP.641, model fixed tree improved.647. learning tree structure furr pushed.651. dataset, learned tree significantly tree. refore, expected improvement learning tree marginal. however, improvement baseline significant, showing transferring information related classes helped. closely source gains, found similar cifar-100, classes gain ors lose shown fig. . encouraging note classes occur rarely dataset improve most. fig. plots improvements learned tree model baseline fraction test instances class. example, average precision baby occurs% test cases improves.173 (baseline.205 (learned tree). class borrows people portrait occur frequently. performance sky occurs% test cases stays same. conclusion proposed model augments standard neural networks treebased priors classification parameters. priors follow hierarchical structure classes enable model transfer knowledge related classes. proposed learning hierarchical structure. experiments show model achieves excellent results challenging datasets. Learning classifiers generalize hard problem training examples available. for example, images cheetah, hard train classifier good distinguishing cheetahs hundreds classes, working pixels alone. any powerful machine learning model severely overfit examples, held back strong regularizers. this paper based idea performance improved natural structure inherent set classes. for example, cheetahs related tigers, lions, jaguars leopards. having labeled examples related classes make task learning cheetah examples easier. knowing class structure borrow ?knowledge? relevant classes distinctive features specific cheetahs learned. least, \\x0cmodel confuse cheetahs animals rar completely unrelated classes, cars lamps. our aim develop methods transferring knowledge related tasks learning task. endeavour scale machine learning algorithms, imperative good ways transferring knowledge related problems. finding relatedness hard problem. this absence prior knowledge, order find classes related, classes., good model but learn good model, classes related. this creates cyclic dependency. one circumvent external knowledge source, human, class structure hand. anor resolve dependency iteratively learn model classes relationships exist improve. paper, follow bootstrapping approach. This paper proposes learning class structure classifier parameters context deep neural networks. aim improve classification accuracy classes examples. deep neural networks trained discriminatively back propagation achieved state--art performance diﬃcult classification problems large amounts labeled data]. case smaller amounts data datasets rare classes studied. address shortcoming, model augments neural networks tree-based prior layer weights. structure prior related classes share prior. this shared prior captures features common members superclass. refore, class examples, model orwise unable learn good features for, access good features virtue belonging superclass. learning hierarchical structure classes extensively studied machine learning, statistics, vision communities. large class models based hierarchical Bayesian models transfer learning]. hierarchical topic model image features Bart. ] discover visual taxonomies unsupervised fashion large datasets designed rapid learning categories. fei-fei. ] developed hierarchical Bayesian model visual categories, prior parameters categories induced categories. however, approach well-suited generic approach transfer learning learned single prior shared categories. number models based hierarchical Dirichlet processes transfer learning]. however, above-mentioned models generative nature. models typically resort MCMC approaches inference, hard scale large datasets. furrmore, tend perform worse discriminative approaches, number labeled examples increases. large class discriminative models] transfer learning enable discovering sharing information related classes. most similar work] defined generative prior classifier parameters prior tree structures identify \\x0crelevant categories. however, work focused specific object detection task SVM model pre-defined HOG features input. paper, demonstrate method deep architectures) convolutional nets pixels input singlelabel softmax outputs) fully connected nets pretrained deep Boltzmann machines image features text tokens input multi-label logistic outputs. our model improves performance strong baselines cases, lending measure universality approach. essence, model learns low-level features, high-level features, hierarchy classes end-end way. Model Description Let     set data points set labels, label dimensional vector targets. targets binary, one, real-valued. setting, image one encoding label. model multi-layer neural network (see fig. ). let denote set parameters network (weights biases layers), excluding top-level weights, denote separately   here represents number hidden units hidden layer. conditional distribution expressed  (?)dwd?. ,? general, integral intractable, typically resort MAP estimation determine values model parameters maximize log log) log (?). here, log log-likelihood function terms priors model parameters. typical choice prior Gaussian distribution diagonal covariance:  ,   }.  here denotes classifier parameters class Note prior assumes independent. words-priori, weights label related ??? predictions ?car ?tiger High level features ??? low level features) ?vehicle Input ?car ?animal ?truck \\x0c?tiger ??? ?cheetah) Figure Our model: deep neural network priors classification parameters. priors derived hierarchy classes. label weights. this reasonable assumption labels. works applications large number labeled examples class. however, classes related anor, priors respect relationships suitable. such priors crucial classes handful training examples, effect prior pronounced. work, focus developing prior.  Learning With Fixed Tree Hierarchy Let assume classes organized fixed tree hierarchy. relax assumption placing hierarchical non-parametric prior tree structures. for ease exposition, two-level hierarchy1 shown fig. . leaf nodes classes. connected super-classes group toger similar basic-level classes. each leaf node weight vector  each super-class node vector ..., define generative model   ?parent) ) This prior expresses relationships classes. for example, asserts ?car ?truck deviations ?vehicle similarly, ?cat ?dog deviations ?animal . -written include   (?—? (?)dwd?. ,?,? perform MAP inference determine values, maximize log log) log (?—?) log (?). terms loss function, minimize,  log  log) log (?—?)  log (?)  log  ?parent  Note fixing loss function recovers standard loss function. choice normal distributions. leads nice property maximization closed form. amounts taking (scaled) average children let—parent?   model easily generalized deeper hierarchies. : given: classes superclasses initial Initialize repeat van Optimize fixed  sgd steps. // Optimize fixed=vehicle randompermute) {snew car truck van  sgd steps. end choosebestsuperclass(?            end convergence Algorithm Procedure learning tree. =snew=animal cat dog van van refore, loss function. optimized iteratively performing steps. step, maximize keeping fixed. this standard stochastic gradient descent (sgd). maximize keeping fixed. this closed form.  practical terms, step instantaneous performed gradient descent steps-100. refore, learning identical standard gradient descent. exploit structure labels nominal cost terms computational time.  Learning Tree Hierarchy assumed model fixed tree hierarchy. now, show tree structure learned training. let-length vector specifies tree structure, class child super-class place non-parametric Chinese Restaurant Process (crp) prior this prior) model ﬂexibility number superclasses. CRP prior extends partition classes class adding class eir existing superclasses superclass. probability number children superclass probability adding superclass+?  creating superclass+?  essence, prefers add node existing large superclass spawning one. strength preference controlled equipped CRP prior conditional takes form  (?)dwd? ). ,?,? map inference model leads optimization problem max log log) log (?—?, log (?) log).  Maximization problematic domain huge discrete set. fortunately, approximated simple parallelizable search procedure. initialize tree sensibly. this hand extracting semantic tree WordNet]. let number superclasses tree optimize, steps \\x0cthis tree. leaf node picked uniformly random tree tree proposals generated follows. proposals generated attaching leaf node superclasses. one additional proposal generated creating super-class attaching label. this process shown Algorithm -estimate {?, trees steps. note optimization problems performed independently, parallel. tree picked validation set. this process repeated picking anor node locations. after node picked potentially repositioned, resulting tree back whale dolphin willow tree oak tree lamp clock leopard tiger ray ﬂatfish Figure Examples cifar-100. five randomly chosen examples 100 classes shown. classes row belong superclass. optimizing newly learned tree place tree. position class tree change full pass classes, hierarchy discovery converged. when training model cifar-100, amounts interrupting stochastic gradient descent,000 steps find tree. amount time spent learning tree small fraction total time (about%). Experiments cifar-100 cifar-100 dataset] consists color images belonging 100 classes. classes divided groups each. for example, superclass fish aquarium fish, ﬂatfish, ray, shark trout; superclass ﬂowers orchids, poppies, roses, sunﬂowers tulips. some examples dataset shown fig.  chose dataset large number classes examples each, making ideal demonstrating utility transfer learning. 600 examples class 500 training set 100 test set. preprocessed images global contrast normalization ZCA whitening.  Model Architecture Training Details convolutional neural network convolutional hidden layers fully connected hidden layers. all hidden units rectified linear activation function. each convolutional layer max-pooling layer. dropout] applied layers network probability retaining hidden unit) layers network (going input convolutional layers fully connected layers). max-norm regularization] weights convolutional fully connected layers. initial tree chosen based superclass structure data set. learned tree Algorithm, 000 100. final learned tree provided supplementary material.  Experiments Few Examples Class set experiments, worked scenario class examples. aim assess wher proposed model related classes borrow information. for baseline, standard convolutional neural network architecture model. this extremely strong baseline achieved excellent results, outperforming previously reported results dataset shown Table created subsets data randomly choosing, 100 250 examples class, trained models subset. baseline. model tree structure (100 classes grouped superclasses) fixed training. fourth models learned tree structure. initialized random tree fourth tree. random tree constructed drawing sample CRP prior randomly assigning classes leaf nodes. test performance models compared fig. . observe number examples class small, fixed tree model significant improvement baseline. improvement diminishes number examples increases eventually performance falls baseline%). however, learned tree model Improvement Test classification accuracy Baseline Fixed Tree Learned Tree 100 250 Number training examples label Baseline Fixed Tree Learned Tree 500) Sorted classes 100) Figure Classification results cifar-100. left: Test set classification accuracy number training examples class. right: Improvement baseline trained examples class. learned tree models initialized tree. method Test Accuracy Conv Net max pooling Conv Net stochastic pooling] Conv Net maxout] Conv Net max pooling dropout (baseline) Baseline fixed tree Baseline learned tree (initialized randomly) Baseline learned tree (initialized tree      Table Classification results cifar-100. all models trained full training set. better. even examples class, accuracy% compared baseline model% fixed tree model%. thus model% relative improvement examples available. number examples increases, relative improvement decreases. however, 500 examples class, learned tree model improves baseline, achieving classification accuracy%. note initializing model random tree decreases model performance, shown Table next, analyzed learned tree model find source improvements. model trained examples class looked classification accuracy separately class. aim find classes gain suffer most. fig. shows improvement obtained classes baseline, classes sorted improvement baseline. observe classes benefit degrees learning hierarchy parameter sharing, classes perform worse result transfer. for learned tree model, classes improve willow tree%) orchid%). classes lose transfer ray%) lamp%). hyposize reason classes gain lot similar classes superclass stand gain lot transferring knowledge. for example, superclass willow tree trees, maple tree oak tree. however, ray belongs superclass fish typical examples fish dissimilar appearance. with fixed tree, transfer hurts performance (ray worse%). however, tree learned, \\x0cclass split fish superclass join superclass suffer much. similarly, lamp household electrical devices keyboard clock. putting kinds electrical devices superclass makes semantic sense visual recognition tasks. this highlights key limitation hierarchies based semantic knowledge advocates learn hierarchy relevant task hand. full learned tree provided supplementary material.  Experiments Few Examples One Class set experiments, worked scenario lots examples classes, examples class. aim wher model transfers information classes learned ?rare? class. constructed training sets randomly drawing eir, 100, 250 500 examples dolphin Baseline Fixed Tree Learned Tree Baseline Fixed Tree Learned Tree Test classification accuracy Test classification accuracy 100 250 Number training cases dolphin 500 100 250 Number training cases dolphin) 500) Figure Results cifar-100 examples dolphin class. left: Test set classification accuracy number examples. right: Accuracy classifying dolphin whale shark considered correct. classes baby, female, people, portrait \\x0cplant life, river, water clouds, sea, sky, transport, water animals, dog, food clouds, sky, structures claudia text barco, pesca, boattosail, navegac? watermelon, dog, hilarious, chihuahua colors, cores, centro, commercial, building Images Tags Figure Some examples mir-flickr dataset. each instance dataset image textual tags. each image multiple classes. class 500 training examples classes. trained baseline, fixed tree learned tree models datasets. objective special attention paid dolphin class. fig. shows test accuracy correctly predicting dolphin class. transfer learning helped tremendously. for example, cases, baseline accuracy transfer learning model%. even 250 cases, learned tree model significant improvements%). repeated experiment classes dolphin found similar improvements. see supplementary material detailed description. addition performing class examples, errors sensible. check case, evaluated performance models treating classification dolphin shark whale correct, reasonable mistakes. fig. shows classification accuracy assumption models. observe transfer learning methods provide significant improvements baseline. even dolphin, accuracy jumps%. Experiments MIR Flickr Multimedia Information Retrieval Flickr Data set] consists million images collected social photography website Flickr user assigned tags. among million images,000 annotated labels. labels include object categories, bird, tree, people, scene categories, indoor, sky night. each image multiple labels. some examples shown fig.  this dataset cifar-100 ways. cifar-100 dataset, model trained image pixels input image belonged class. mir-flickr multimodal dataset standard computer vision image features word counts inputs. cifar-100 models multi-layer convolutional network, dataset fully connected neural network initialized unrolling Deep Boltzmann Machine (dbm]. moreover, dataset offers natural class distribution classes occur ors. for example, sky occurs \\x0cover% instances, baby occurs fewer%. 975,000 unlabeled images unsupervised training dbm. publicly features train-test splits].  Baseline Fixed Tree Learned Tree Improvement Average Precision Improvement Average Precision Sorted classes) class-wise improvement Fraction instances class) Improvement. number examples Figure Results MIR flickr. left: Improvement Average Precision baseline methods. right: Improvement learned tree model baseline classes fraction test cases class. each dot corresponds class. classes examples (towards left plot) significant improvements. method MAP Logistic regression Multimodal DBM] Multiple Kernel Learning SVMs] TagProp] Multimodal DBM finetuning dropout (baseline) Baseline fixed tree Baseline learned tree (initialized tree.609.623.640.641 .004.648 .004.651 .005 Table Mean Average Precision obtained models MIR Flickr data set.  \\x0cmodel Architecture Training Details order make results directly comparable], network architecture rein. authors dataset] provided high-level categorization classes create initial tree. this tree structure learned model shown supplementary material. Algorithm 500 100.  Classification Results For baseline Multimodal DBM model finetuning discriminatively dropout. this model achieves state--art results, making strong baseline. results experiment summarized Table baseline achieved MAP.641, model fixed tree improved.647. learning tree structure furr pushed.651. for dataset, learned tree significantly tree. refore, expected improvement learning tree marginal. however, improvement baseline significant, showing transferring information related classes helped. looking closely source gains, found similar cifar-100, classes gain ors lose shown fig. . encouraging note classes occur rarely dataset improve most. this fig. plots improvements learned tree model baseline fraction test instances class. for example, average precision baby occurs% test cases improves.173 (baseline.205 (learned tree). this class borrows people portrait occur frequently. performance sky occurs% test cases stays same. Conclusion proposed model augments standard neural networks treebased priors classification parameters. priors follow hierarchical structure classes enable model transfer knowledge related classes. proposed learning hierarchical structure. experiments show model achieves excellent results challenging datasets.',\n",
       " 'PP5031': 'deep architectures strong representational power due hierarchical structures. capable encoding highly varying functions capture complex relationships high-level abstractions high-dimensional data]. traditionally, multilayer perceptron optimize hierarchical models based discriminative criterion models) error backpropagating gradient descent]. however, architecture deep, challenging train entire network supervised learning due large number parameters, non-convex optimization problem dilution error signal layers. optimization lead worse performances compared shallower networks]. recent developments unsupervised feature learning deep learning algorithms made learn deep feature hierarchies. deep learning current form, typically involves consecutive learning phases. phase greedily learns unsupervised modules layer-layer bottom]. common criteria unsupervised learning include maximum likelihood models] input reconstruction error vector]. subsequently supervised phase fine-tunes network supervised, discriminative algorithm, supervised error backpropagation. unsupervised learning phase initializes parameters taking account ultimate task interest, classification. phase assumes entire burden modifying model fit task. work, propose gradual transition fully-unsupervised learning highlydiscriminative optimization. adding intermediate training phase existing deep learning phases, enhances unsupervised representation incorporating top-down information. realize notion, introduce global (non-greedy) optimization hanlin Goh Institute Infocomm research*star, Singapore Image Pervasive Access lab, CNRS UMI 2955, Singapore france.  joo-hwee Lim Image Pervasive Access lab, CNRS UMI 2955, Singapore france. regularizes deep belief network (dbn) top-down. retain gradient descent procedure updating parameters DBN unsupervised learning phase. regularization method deep learning strategy applied handwritten digit recognition dictionary learning object recognition, competitive empirical results. related Work Input layer! ¿   probability assigned) exp)), input units! restricted Boltzmann machines. restricted Boltzmann machine (rbm] bipartite Markov random field input layer latent layer (see Figure). layers connected undirected weights  unit receives input bias parameter joint configuration binary states, energy:   ) Latent layer!  latent units! figure Structure rbm. exp) \\x0cwhere partition function, normalizes) valid distribution. units layer conditionally independent distributions logistic functions exp¿  exp )). ) This enables model sampled alternating Gibbs sampling layers. estimate maximum likelihood data distribution), RBM trained taking gradient log probability input data respect parameters: log) hxi hxi ?wij denotes expectation distribution sampling Markov chain. term samples data distribution term approximates equilibrium distribution contrastive divergence method] small finite number sampling steps obtain distribution reconstructed states rbms regularized produce sparse representations]. conditional distribution concatenated vector now) Equation outputs eir logistic units softmax units. rbm trained contrastive divergence algorithm] approximate maximum likelihood joint distribution.  latent layer!  latent units! ¿    ) inputs!  output units! input units! supervised Restricted Boltzmann machines. introduce class labels rbm, one-hot coded output vector defined, iff class index. anor set weights  connects vectors concatenated form input vector, rbm, linked¿ shown Figure supervised RBM models joint distribution). energy function model extended  classes! concatenated! layer! figure supervised RBM jointly models inputs outputs. biases omitted simplicity. inference, set neutral value, makes part RBM ?noisy?. objective ?denoise? obtain prediction. iterations alternating \\x0cgibbs sampling. number classes huge, number input units huge maintain high signal noise ratio. larochelle Bengio] suggested couple generative model, discriminative model), alleviate issue. however, objective train deep network, layer, previous discarded retrained. desirable discriminative criterion directly outputs, initial layers network. deep Belief networks. deep belief networks (dbn] probabilistic graphical models made hierarchy stochastic latent variables. universal approximators], applied variety problems image video recognition], dimension reduction]. two-phase training strategy unsupervised greedy pre-training supervised fine-tuning. unsupervised pre-training, stack RBMs trained greedily bottom, latent activations layer inputs rbm. layer RBM models data distribution), higher-level layers suﬃciently large, variational bound likelihood improves]. popular method supervised fine-tuning backpropagates error) update network parameters. shown perform initialized learning model input data unsupervised pre-training]. alternative supervised method generative model implements supervised RBM (figure models, top layer. training, network employs-down backfitting algorithm]. algorithm initialized untying network recognition generative weights. first, stochastic bottom pass performed generative weights adjusted good reconstructing layer below. next, iterations alternating sampling respective conditional probabilities top-level supervised RBM concatenated vector latent layer. contrastive divergence RBM updated fitting posterior distribution. finally, stochastic top-down pass adjusts bottom recognition weights reconstruct activations layer above. work, extend existing DBN training strategy additional supervised training phase discriminative error backpropagation. top-down regularization network parameters proposed. network optimized globally inputs gradually map output layers. retain simple method gradient descent update weights RBMs retain convention generative RBM learning. top-down RBM regularization: Building Block regularize RBM learning targets obtained sampling higherlevel representations. generic cross-entropy regularization. aim construct top-down regularized building block deep networks, combining optimization criteria directly], supervised RBM model (figure). give control individual elements latent vector, manipulate representations point-wise bias \\x0cactivations latent variable]. training dataset Dtrain regularizer based cross-entropy loss defined penalize difference latent vector target vector —dtrain LRBM +reg (dtrain  log  —dtrain log zjk —zjk ) update rule cross-entropy-regularized RBM modified: ?wij hxi hxi)   ) merger latent target activations update parameters. here, inﬂuences regulated parameter  activationes match.  parameter update original contrastive divergence learning algorithm. building block. principle regularizing latent activations combine signals bottom top-down. forms building block optimizing DBN top-down regularization. basic building block three-layer structure consisting consecutive layers: previous current  layers. layers connected sets weight parameters previous layers respectively. current layer bottom representations sampled previous layer weighted connections with exp ) terms subscripts sampled representation zdest,src refer destination (dest) source (src) layers respectively. meanwhile, sampling layer weights drives top-down representations exp  )). ) objective learn RBM parameters map previous layer current latent layer maximizing likelihood previous layer top-down samples layer target representations. loss function network layers broken: LDBN +topdown,rbm +topdown) cross-entropy regularization loss function layer —dtrain,rbm +topdown  log  —dtrain log ) This results gradient descent hzl hzl ) bottom top-down merged representation bottom top-down signals (see Figure), weighted hyperparameter bias source signal adjusted selecting additionally, alternating Gibbs sampling, contrastive divergence updates, performed unbiased bottom samples Equation symmetric decoder exp  )). previous layer! intermediate layer! ) Next layer! -step!  merged! bottom! top-down! figure basic building block learns bottom latent representation regularized topdown signals. bottom top-down latent activations sampled respectively. merged modified activations parameter updates. reconstructions independently driven input signals form Gibbs sampling Markov chain. globally-optimized Deep Belief Networks forward-backward Learning strategy. dbn, RBMs stacked bottom greedy layer-wise manner, layer modeling posterior distribution previous layer. similarly, regularized building blocks construct regularized DBN (figure). network, illustrated Figure), comprises total rbms. network trained forward backward strategy (figure)). integrates top-down regularization contrastive divergence learning, alternating Gibbs sampling layers (figure)). input! layer layer layer output! ) top-down regularized deep belief network. forward pass!  Backward pass! merged! ) Forward backward passes top-down regularization. -step! ) Alternating Gibbs sampling chains contrastive divergence learning. figure Constructing top-down regularized deep belief network (dbn). restricted Boltzmann machines (rbm) make network concurrently optimized. ) building blocks connected layer-wise. bottom top-down activations training network. ) Activations top-down regularization obtained sampling merging forward pass backward pass. ) From activations forward pass, reconstructions obtained performing alternating Gibbs sampling previous layer. forward pass, input features, layer sampled bottom, based representation previous layer (equation). top-level vector activated softmax function. reaching output layer, backward pass begins. activations combined output labels produce  yck) merged activations (equation), parameter updates, role activating lower layer top-down exp  repeated layer reached computed. ) top-down sampling encourages class-based invariance bottom representations. however, sampling top-down, output vector source result activation pattern class. undesirable, bottom layers, representations heavily inﬂuenced bottom data. merging top-down representations bottom ones, representations encode instance-based variations class-based variations. layer, typically set final RBM learns map class labels Backward activation class-based invariant representation obtained regularize backward activations point onwards based merged representation instanceand class-based representations. three-phase Learning procedure. greedy learning models) top-down regularized forward-backward learning executed. eventual goal network give prediction). suggest network adopt three-phase strategy \\x0cfor training, parameters learned phase initializes next, follows: phase unsupervised greedy. network constructed greedily learning unsupervised RBM top existing network. enhance representations, regularizations, sparsity], applied. stacking process repeated rbms, layer added network.  phase supervised regularized. phase begins connecting final layer, activated softmax activation function classification problem. one-hot coded output vector target activations setting RBM learned associative memory update hzl hzl ) This final rbm, toger RBMs learned Phase form initialization top-down regularized forward-backward learning algorithm. phase fine-tune network generative learning, binds layers toger aligning parameters network outputs.  phase supervised discriminative. finally, supervised error backpropagation algorithm improve class discrimination representations. backpropagation passes. forward pass, layer activated bottom obtain class predictions. classification error computed based groundtruth backward pass performs gradient descent parameters backpropagating errors layers top-down. Phase Phase form parameter update rule based gradient descent change. top-down signals account. essentially, phases performing variant contrastive divergence algorithm. meanwhile, Phase Phase inputs phases change, optimization function modified performing regularization completely discriminative. empirical Evaluation work, proposed deep learning strategy top-down regularization method evaluated analyzed MNIST handwritten digit dataset] caltech-101 object recognition dataset].  MNIST Handwritten Digit Recognition MNIST dataset images handwritten digits. task recognize digit pixel image. dataset split, 000 images training, 000 test images. methods dataset perform evaluation classification performances, specifically DBNN]. basic version dataset, neir preprocessing enhancements, evaluation. fivelayer DBN setup topography evaluated]. number units layer, layer, 784, 500, 500, 2000, order. architectural setups tested:     Stacked RBMs-down learning (original DBN reported]), Stacked \\x0crbms forward-backward learning backpropagation, Stacked sparse RBMs] forward-backward learning backpropagation, Stacked sparse RBMs] backpropagation, forward-backward learning random weights. phases evaluation procedure Hinton. ] initially, 000 training, 000 validation images train network retraining full training set. phase sets, 000, 000 images initial training validation sets. model selection, network retrained training set, 000 images. simplify parameterization forward-backward learning phase top-down modulation parameter layers controlled single parameter function: —?  —?   —?  )  top-down inﬂuence layer dependent relative position network. function assigns layers nearer input stronger inﬂuences input, layers output biased output. distancebased modulation inﬂuences enables gradual mapping input output layers. performance obtained setting error rate% test set. figure shows wrongly classified test examples setting. initialized conventional RBMs fine-tuned forward-backward learning error backpropagation, score%. comparison, conventional DBN obtained error rate%. directly optimizing network random weights produced error%, fairly decent, network optimized globally scratch. setup, intermediate results training phase reported Table overall, results achieved competitive methods complexity rely neir convolution image distortions normalization. variant dbn, focused learning nonlinear transformations feature space nearest neighbor classification], error rate%. deep convex net], utilized complex convex-optimized modules building blocks perform fine-tuning global network level, score%. time writing, performing model dataset gave error rate% heavy architecture committee deep convolutional neural nets elastic distortions image normalization]. Table observe learning phases helped improve performance networks. forward-backward algorithm outperforms-down learning original dbn. sparse RBMs] backpropagation, furr improve recognition performances. forward-backward learning effective bridge phases, improvement% setup phase method standalone algorithm, demonstrating potential learning randomly initialized weights. table Results MNIST phases training process. setup Learning algorithm* Classifica \\x0ction error rate Phase Phase Phase Phase Phase Deep belief network (reported]) rbms-down% proposed top-down regularized deep belief network rbms forward-backward% sparse RBMs forward-backward% sparse RBMs %  random weights forward-backward % *phase runs error backpropagation algorithm employed. % figure wrongly classified test examples MNIST dataset.  caltech-101 Object Recognition caltech-101 dataset] popular datasets object recognition evaluation. 144 images belonging 101 object categories background class. images resized retaining original aspect ratios, longer spatial dimension 300 pixels. sift descriptors] extracted densely sampled patches pixel intervals. sift descriptors -normalized constraining descriptor vector sum maximum one, resulting quasi-binary feature. additionally, SIFT descriptors spatial neighborhood concatenated form macrofeature]. dbn setup learn dictionary map local macrofeatures mid-level representation. layers RBMs stacked model macrofeatures. RBMs regularized population lifetime sparseness training]. single rbm, 1024 latent variables, trained macrofeature. set 200, 000 randomly selected macrofeatures training layer. resulting representations RBM concatenated spatial neighborhood  rbm modeled spatially aggregated representation higher-level representation. anor set 200, 000 randomly selected spatially aggregated representations training rbm. higher-level RBM representation image label. experimental trial, set training examples class (totaling 3060) randomly selected supervised learning. forward-backward learning algorithm regularize learning finetuning network. finally, error backpropagation performed furr optimize dictionary. representations, max-pooling spatial regions defined spatial pyramid employed] obtain single vector representing image. employ advanced pooling schemes]. linear SVM classifier trained, train-test split previous supervised learning phase. table shows average class-wise classification accuracy, averaged 102 classes experimental trials. results demonstrate consistent improvement moving Phase phase final accuracy obtained%. outperforms existing dictionary learning methods based single image descriptors% improvement previous state--art results]. comparison, existing reported dictionary learning methods encode sift-based local descriptors included Table table Classification accuracy caltech-101. method Training phase Accuracy Proposed top-down regularized DBN Phase Unsupervised stacking Phase top-down regularization Phase Error backpropagation% Sparse coding max-pooling] Extended HMAX] Convolutional RBM] Unsupervised supervised RBM] Gated Convolutional RBM% Conclusion proposed notion deep learning gradually transitioning fully unsupervised strongly discriminative. achieved introduction intermediate phase unsupervised supervised learning phases. notion implemented incorporating top-down information DBNs regularization. method easily integrated intermediate learning phase based simple building blocks. performed complement greedy layer-wise unsupervised learning discriminative optimization error backpropagation. empirical evaluation show method leads competitive results handwritten digit recognition object recognition datasets. Deep architectures strong representational power due hierarchical structures. capable encoding highly varying functions capture complex relationships high-level abstractions high-dimensional data]. traditionally, multilayer perceptron optimize hierarchical models based discriminative criterion models) error backpropagating gradient descent]. however, architecture deep, challenging train entire network supervised learning due large number parameters, non-convex optimization problem dilution error signal layers. this optimization lead worse performances compared shallower networks]. recent developments unsupervised feature learning deep learning algorithms made learn deep feature hierarchies. deep learning current form, typically involves consecutive learning phases. phase greedily learns unsupervised modules layer-layer bottom]. some common criteria unsupervised learning include maximum likelihood models] input reconstruction error vector]. this subsequently supervised phase fine-tunes network supervised, discriminative algorithm, supervised error backpropagation. unsupervised learning phase initializes parameters taking account ultimate task interest, classification. phase assumes entire burden modifying model fit task. work, propose gradual transition fully-unsupervised learning highlydiscriminative optimization. this adding intermediate training phase existing deep learning phases, enhances unsupervised representation incorporating top-down information. realize notion, introduce global (non-greedy) optimization hanlin Goh Institute Infocomm research*star, Singapore Image Pervasive Access lab, CNRS UMI 2955, Singapore france.  joo-hwee Lim Image Pervasive Access lab, CNRS UMI 2955, Singapore france. regularizes deep belief network (dbn) top-down. retain gradient descent procedure updating parameters DBN unsupervised learning phase. regularization method deep learning strategy applied handwritten digit recognition dictionary learning object recognition, competitive empirical results. related Work Input layer! ¿   probability assigned) exp)), input units! restricted Boltzmann machines. restricted Boltzmann machine (rbm] bipartite Markov random field input layer latent layer (see Figure). layers connected undirected weights  each unit receives input bias parameter joint configuration binary states, energy:   ) Latent layer!  latent units! Figure Structure rbm. exp) \\x0cwhere partition function, normalizes) valid distribution. units layer conditionally independent distributions logistic functions exp¿  exp )). ) This enables model sampled alternating Gibbs sampling layers. estimate maximum likelihood data distribution), RBM trained taking gradient log probability input data respect parameters: log) hxi hxi ?wij denotes expectation distribution sampling Markov chain. term samples data distribution term approximates equilibrium distribution contrastive divergence method] small finite number sampling steps obtain distribution reconstructed states rbms regularized produce sparse representations]. conditional distribution concatenated vector now) Equation outputs eir logistic units softmax units. RBM trained contrastive divergence algorithm] approximate maximum likelihood joint distribution.  latent layer!  latent units! ¿    ) inputs!  output units! input units! supervised Restricted Boltzmann machines. introduce class labels rbm, one-hot coded output vector defined, iff class index. anor set weights  connects vectors concatenated form input vector, rbm, linked¿ shown Figure this supervised RBM models joint distribution). energy function model extended  classes! concatenated! layer! figure supervised RBM jointly models inputs outputs. biases omitted simplicity. during inference, set neutral value, makes part RBM ?noisy?. objective ?denoise? obtain prediction. this iterations alternating \\x0cgibbs sampling. number classes huge, number input units huge maintain high signal noise ratio. larochelle Bengio] suggested couple generative model, discriminative model), alleviate issue. however, objective train deep network, layer, previous discarded retrained. desirable discriminative criterion directly outputs, initial layers network. deep Belief networks. deep belief networks (dbn] probabilistic graphical models made hierarchy stochastic latent variables. being universal approximators], applied variety problems image video recognition], dimension reduction]. two-phase training strategy unsupervised greedy pre-training supervised fine-tuning. for unsupervised pre-training, stack RBMs trained greedily bottom, latent activations layer inputs rbm. each layer RBM models data distribution), higher-level layers suﬃciently large, variational bound likelihood improves]. popular method supervised fine-tuning backpropagates error) update network parameters. shown perform initialized learning model input data unsupervised pre-training]. alternative supervised method generative model implements supervised RBM (figure models, top layer. for training, network employs-down backfitting algorithm]. algorithm initialized untying network recognition generative weights. first, stochastic bottom pass performed generative weights adjusted good reconstructing layer below. next, iterations alternating sampling respective conditional probabilities top-level supervised RBM concatenated vector latent layer. using contrastive divergence RBM updated fitting posterior distribution. finally, stochastic top-down pass adjusts bottom recognition weights reconstruct activations layer above. work, extend existing DBN training strategy additional supervised training phase discriminative error backpropagation. top-down regularization network parameters proposed. network optimized globally inputs gradually map output layers. retain simple method gradient descent update weights RBMs retain convention generative RBM learning. top-down RBM regularization: Building Block regularize RBM learning targets obtained sampling higherlevel representations. generic cross-entropy regularization. aim construct top-down regularized building block deep networks, combining optimization criteria directly], supervised RBM model (figure). give control individual elements latent vector, manipulate representations point-wise bias \\x0cactivations latent variable]. given training dataset Dtrain regularizer based cross-entropy loss defined penalize difference latent vector target vector —dtrain LRBM +reg (dtrain  log  —dtrain log zjk —zjk ) update rule cross-entropy-regularized RBM modified: ?wij hxi hxi)   ) merger latent target activations update parameters. here, inﬂuences regulated parameter  activationes match.  parameter update original contrastive divergence learning algorithm. Building block. principle regularizing latent activations combine signals bottom top-down. this forms building block optimizing DBN top-down regularization. basic building block three-layer structure consisting consecutive layers: previous current  layers. layers connected sets weight parameters previous layers respectively. for current layer bottom representations sampled previous layer weighted connections with exp ) terms subscripts sampled representation zdest,src refer destination (dest) source (src) layers respectively. meanwhile, sampling layer weights drives top-down representations exp  )). ) objective learn RBM parameters map previous layer current latent layer maximizing likelihood previous layer top-down samples layer target representations. loss function network layers broken: LDBN +topdown,rbm +topdown) cross-entropy regularization loss function layer —dtrain,rbm +topdown  log  —dtrain log ) This results gradient descent hzl hzl ) bottom top-down merged representation bottom top-down signals (see Figure), weighted hyperparameter bias source signal adjusted selecting additionally, alternating Gibbs sampling, contrastive divergence updates, performed unbiased bottom samples Equation symmetric decoder exp  )). previous layer! Intermediate layer! ) Next layer! -step!  merged! bottom! top-down! figure basic building block learns bottom latent representation regularized topdown signals. bottom top-down latent activations sampled respectively. merged modified activations parameter updates. reconstructions independently driven input signals form Gibbs sampling Markov chain. globally-optimized Deep Belief Networks forward-backward Learning strategy. dbn, RBMs stacked bottom greedy layer-wise manner, layer modeling posterior distribution previous layer. similarly, regularized building blocks construct regularized DBN (figure). network, illustrated Figure), comprises total rbms. network trained forward backward strategy (figure)). integrates top-down regularization contrastive divergence learning, alternating Gibbs sampling layers (figure)). input! layer layer layer output! ) top-down regularized deep belief network. forward pass!  Backward pass! merged! ) Forward backward passes top-down regularization. -step! ) Alternating Gibbs sampling chains contrastive divergence learning. figure Constructing top-down regularized deep belief network (dbn). all restricted Boltzmann machines (rbm) make network concurrently optimized. ) building blocks connected layer-wise. both bottom top-down activations training network. ) Activations top-down regularization obtained sampling merging forward pass backward pass. ) From activations forward pass, reconstructions obtained performing alternating Gibbs sampling previous layer. forward pass, input features, layer sampled bottom, based representation previous layer (equation). top-level vector activated softmax function. upon reaching output layer, backward pass begins. activations combined output labels produce  yck) merged activations (equation), parameter updates, role activating lower layer top-down exp  this repeated layer reached computed. ) top-down sampling encourages class-based invariance bottom representations. however, sampling top-down, output vector source result activation pattern class. this undesirable, bottom layers, representations heavily inﬂuenced bottom data. merging top-down representations bottom ones, representations encode instance-based variations class-based variations. layer, typically set final RBM learns map class labels Backward activation class-based invariant representation obtained regularize all backward activations point onwards based merged representation instanceand class-based representations. three-phase Learning procedure. after greedy learning models) top-down regularized forward-backward learning executed. eventual goal network give prediction). this suggest network adopt three-phase strategy \\x0cfor training, parameters learned phase initializes next, follows: phase unsupervised greedy. network constructed greedily learning unsupervised RBM top existing network. enhance representations, regularizations, sparsity], applied. stacking process repeated rbms, layer added network.  phase supervised regularized. this phase begins connecting final layer, activated softmax activation function classification problem. using one-hot coded output vector target activations setting RBM learned associative memory update hzl hzl ) This final rbm, toger RBMs learned Phase form initialization top-down regularized forward-backward learning algorithm. this phase fine-tune network generative learning, binds layers toger aligning parameters network outputs.  phase supervised discriminative. finally, supervised error backpropagation algorithm improve class discrimination representations. backpropagation passes. forward pass, layer activated bottom obtain class predictions. classification error computed based groundtruth backward pass performs gradient descent parameters backpropagating errors layers top-down. from Phase Phase form parameter update rule based gradient descent change. only top-down signals account. essentially, phases performing variant contrastive divergence algorithm. meanwhile, Phase Phase inputs phases change, optimization function modified performing regularization completely discriminative. Empirical Evaluation work, proposed deep learning strategy top-down regularization method evaluated analyzed MNIST handwritten digit dataset] caltech-101 object recognition dataset].  MNIST Handwritten Digit Recognition MNIST dataset images handwritten digits. task recognize digit pixel image. dataset split, 000 images training, 000 test images. many methods dataset perform evaluation classification performances, specifically DBNN]. basic version dataset, neir preprocessing enhancements, evaluation. fivelayer DBN setup topography evaluated]. number units layer, layer, 784, 500, 500, 2000, order. five architectural setups tested:     Stacked RBMs-down learning (original DBN reported]), Stacked \\x0crbms forward-backward learning backpropagation, Stacked sparse RBMs] forward-backward learning backpropagation, Stacked sparse RBMs] backpropagation, forward-backward learning random weights. phases evaluation procedure Hinton. ] initially, 000 training, 000 validation images train network retraining full training set. phase sets, 000, 000 images initial training validation sets. after model selection, network retrained training set, 000 images. simplify parameterization forward-backward learning phase top-down modulation parameter layers controlled single parameter function: —?  —?   —?  )  top-down inﬂuence layer dependent relative position network. function assigns layers nearer input stronger inﬂuences input, layers output biased output. this distancebased modulation inﬂuences enables gradual mapping input output layers. our performance obtained setting error rate% test set. figure shows wrongly classified test examples setting. when initialized conventional RBMs fine-tuned forward-backward learning error backpropagation, score%. comparison, conventional DBN obtained error rate%. directly optimizing network random weights produced error%, fairly decent, network optimized globally scratch. for setup, intermediate results training phase reported Table overall, results achieved competitive methods complexity rely neir convolution image distortions normalization. variant dbn, focused learning nonlinear transformations feature space nearest neighbor classification], error rate%. deep convex net], utilized complex convex-optimized modules building blocks perform fine-tuning global network level, score%. time writing, performing model dataset gave error rate% heavy architecture committee deep convolutional neural nets elastic distortions image normalization]. from Table observe learning phases helped improve performance networks. forward-backward algorithm outperforms-down learning original dbn. using sparse RBMs] backpropagation, furr improve recognition performances. forward-backward learning effective bridge phases, improvement% setup phase method standalone algorithm, demonstrating potential learning randomly initialized weights. table Results MNIST phases training process. setup Learning algorithm* Classifica \\x0ction error rate Phase Phase Phase Phase Phase Deep belief network (reported]) rbms-down% proposed top-down regularized deep belief network rbms forward-backward% sparse RBMs forward-backward% sparse RBMs %  random weights forward-backward % *phase runs error backpropagation algorithm employed. % figure wrongly classified test examples MNIST dataset.  caltech-101 Object Recognition caltech-101 dataset] popular datasets object recognition evaluation. 144 images belonging 101 object categories background class. images resized retaining original aspect ratios, longer spatial dimension 300 pixels. sift descriptors] extracted densely sampled patches pixel intervals. SIFT descriptors -normalized constraining descriptor vector sum maximum one, resulting quasi-binary feature. additionally, SIFT descriptors spatial neighborhood concatenated form macrofeature]. DBN setup learn dictionary map local macrofeatures mid-level representation. two layers RBMs stacked model macrofeatures. both RBMs regularized population lifetime sparseness training]. first single rbm, 1024 latent variables, trained macrofeature. set 200, 000 randomly selected macrofeatures training layer. resulting representations RBM concatenated spatial neighborhood  RBM modeled spatially aggregated representation higher-level representation. anor set 200, 000 randomly selected spatially aggregated representations training rbm. higher-level RBM representation image label. for experimental trial, set training examples class (totaling 3060) randomly selected supervised learning. forward-backward learning algorithm regularize learning finetuning network. finally, error backpropagation performed furr optimize dictionary. from representations, max-pooling spatial regions defined spatial pyramid employed] obtain single vector representing image. employ advanced pooling schemes]. linear SVM classifier trained, train-test split previous supervised learning phase. table shows average class-wise classification accuracy, averaged 102 classes experimental trials. results demonstrate consistent improvement moving Phase phase final accuracy obtained%. this outperforms existing dictionary learning methods based single image descriptors% improvement previous state--art results]. comparison, existing reported dictionary learning methods encode sift-based local descriptors included Table Table Classification accuracy caltech-101. method Training phase Accuracy Proposed top-down regularized DBN Phase Unsupervised stacking Phase top-down regularization Phase Error backpropagation% Sparse coding max-pooling] Extended HMAX] Convolutional RBM] Unsupervised supervised RBM] Gated Convolutional RBM% Conclusion proposed notion deep learning gradually transitioning fully unsupervised strongly discriminative. this achieved introduction intermediate phase unsupervised supervised learning phases. this notion implemented incorporating top-down information DBNs regularization. method easily integrated intermediate learning phase based simple building blocks. performed complement greedy layer-wise unsupervised learning discriminative optimization error backpropagation. empirical evaluation show method leads competitive results handwritten digit recognition object recognition datasets.',\n",
       " 'PP5050': 'illustrate performance models neural data.” Probability distributions spike words form fundamental building blocks neural code. accurate estimates distributions diﬃcult obtain context modern experimental techniques, make record simultaneous spiking activity hundreds neurons. diﬃculties, computational statistical, arise fundamentally exponential scaling population size) number words \\x0cpopulation capable expressing. strategy combating combinatorial explosion introduce parametric model seeks make trade-offs ﬂexibility, computational expense], mamatical completeness] order applicable large-scale neural recordings. variety parametric models proposed literature, including 2ndorder maxent Ising model], reliable interaction model], restricted Boltzmann machine], deep learning], mixture Bernoulli model], dichotomized Gaussian model]. however, number parameters model chosen parametric family increase number neurons, increase exponentially number words. thus, size population increases, parametric model rapidly loses ﬂexibility describing full spike distribution. contrast, nonparametric models ﬂexibility grow amount data]. naive nonparametric model, histogram spike words, oretically preserves representational power computational simplicity. practice, empirical histogram extremely slow converge, high dimensional data primarily interested independent Bernoulli model neurons cascaded logistic model time Figure) Binary representation neural population activity. single spike word red. ) Hierarchical Dirichlet process prior universal binary model (ubm) spike words. word drawn probability  drawn Dirichlet parameters base distribution spike words parameter , Graphical models base measures spike words: independent Bernoulli model cascaded logistic model. base measure distribution spike word    . cases, expect data empirical histogram converge. naive histogram model fails smooth space words: unobserved words accounted model. propose framework combines parsimony parametric models ﬂexibility nonparametric models. model spike word distribution Dirichlet process centered parametric base measure. appropriately chosen base measure smooths observations, Dirichlet process data depart systematically base measure. models universal sense converge distribution supported)dimensional simplex. inﬂuence base measure diminishes increasing sample size, model ultimately converges empirical distribution function. choice base measure inﬂuences small-sample behavior computational tractability universal models, crucial neural applications. base measures exploit priori knowledge neural data remaining computationally tractable large populations: independent Bernoulli spiking model, cas \\x0ccaded logistic model]. Bernoulli cascaded logistic models show performance base measure universal model alone. apply models simulated neural data examples. universal binary model Consider (random) binary spike word length denotes number distinct neurons (and time bins; fig. ). words, index,   }. universal binary model hierarchical probabilistic model bottom level (fig. ), drawn multinomial (categorical) distribution probability observing word vector (spike word distribution). top level, model dirichlet process] discrete base measure hence, cat(?   ?   ) concentration parameter, base measure, discrete probability distribution spike words, parameterized(?— hyper-prior. choose discrete probability measure positive measure,   }, denote ). thus, Dirichlet process probability mass spike words, (finite dimensional) Dirichlet distribution,  dir    ) absence data, parametric base measure controls nonparametric model[?—?]  ) refore, loosely ?centered?  start good parametric models neural populations, extend nonparametric model base measure]. scheme, base measure quickly learns basic structure data Dirichlet extension takes account deviations data predicted parametric component. call extension universal binary model (ubm) base measure  marginal distribution collection words UBM obtained integrating form Polya. dirichletmultinomial) distribution—?, (?)  ) number observations word This leads simple formula sampling predictive distribution words   ) thus, sampling proceeds Chinese restaurant process (crp): set word probability proportional probability proportional draw word (which turn increases probability word draw). note  predictive distribution converges histogram estimate nnk  converges base measure itself. jensenshannon divergence predictive distribution quantify performance experiments.  Model fitting Given data, fit UBM maximum posteriori (map) inference coordinate ascent. marginal log-likelihood, log —?, log log log (?) log  ) Derivatives respect  are,     (?) ) ) denotes digamma function. note summation terms vanish observations), words observed dataset. note limit   converges derivative logarithm base respect hand, limit  derivative 1measure reﬂecting fact number observations ignored: likelihood effectively reﬂects single draw base distribution probability likelihood defined base measure convex log-convex UBM likelihood guaranteed convex. hence, optimize coordinate ascent procedure alternates optimizing   hyper-prior When modeling large populations neurons, number parameters base measure grows over-fitting concern. UBM relies base measure provide smoothing words, critical properly regularize estimate technically, mode    distribution symmetric probability mass concentrated corners simplex. place hyper-prior(?— regularization. regularization, correspond Gaussian double exponential priors, respectively. regularization, loss function optimization?kpp typical multi-neuron recording, connectivity \\x0cknown sparse lower order], assume connectivity sparse. prior promotes sparsity. base measures scalability UBM hinges scalability base measure. describe computationally eﬃcient base measures.  Independent Bernoulli model independent Bernoulli model assumes (statistically) independent spiking neurons. baseline model simplicity]. bernoulli base measure takes form,    —?) pxi)   distribution full support spike words long non-zero. Bernoulli model capture higher-order correlation structure spike word distribution parameters, inference fast memoryeﬃcient.   cascaded logistic model introduce rich dependence structure neurons, assume joint firing probability neuron factors cascaded structure (see fig.         parametric form conditional distribution   bilistic model spike words.  ) probaa natural choice conditional logistic-bernoulli linear model widely model binary observations].  logistic wij  wij parameters. combination factorization likelihoods give rise cascaded logistic (bernoulli) model2 written,    —?)   exp (2xi)   wij) cascaded logistic model Ising model (second order maxent model) number parameters) parametric form. ising model written as3   —?) exp Jij upper triangular matrix parameters) normalizer. however, unlike cascaded logistic model, diﬃcult evaluate likelihood Ising model, computationally tractable normalizer (partition function). hence, fitting Ising model typically challenging. conditional independently fit logistic regression Example cascaded logistic model orem sparse Ising Bernoulli sparse Ising 100 cascaded logistic cascaded logistic Bernoulli Equivalent Ising model dense Ising dense Ising 100 Figure Tight relation cascaded logistic model Ising model. ) cascaded logistic model depicted graphical model conditioning (incoming arrow) node (see orem). parameters nodes interaction terms, wij shown arrows nodes. ) Parameter matrix Ising model equivalent). ) scatter plot simulated Ising models fit cascaded logistic (blue tone) independent Bernoulli (red tone) models. point word spike word space. -axis probability word actual Ising model-axis shows estimated probability fit model. ising model parameters sparsely connect generated randomly. diagonal terms (jii drawn standard normal. % off-diagonal (jij terms set rest drawn normal standard deviation models fit maximum likelihood 107 samples. ) histogram JensenShannon) divergence 100 random pairs sparse Ising model fit models. ) Same) Ising models generated dense connectivity. diagonal terms Ising model parameters constant. off-diagonal terms drawn standard normal distribution. convex optimization), cascaded logistic model estimation computationally tractable large number neurons]. differences, remarkably, Ising model cascaded logistic models overlap substantially. neurons, Ising model cascaded logistic model equivalent. larger populations, orem describes intersection models. orem (pentadiagonal Ising model cascaded logistic model). ising model Jij, cascaded logistic model. moreover, parameter transformation bijective. mapping models parameters    exp log exp    exp exp log log exp exp  exp exp log exp exp symmetric proof found supplemental material. logistic autoregressive network. ], chapter. note}, incorporated diagonal ) Figure 3rd order maxent distribution experiment. ) Convergence jensen-shannon) divergence fit model true model. error bar represents SEM repeats. ) Histogram number spikes word. ) Scatter plots log-likelihood ratio log(pemp)) log(pmodel)) model (column), sample sizes 1000 100000 (rows). note scale difference-axes. error line represents standard deviation repeats. shaded area represents frequentist% confidence interval histogram estimator assuming amount data. number bottom divergence. unlike Ising model, order neurons plays role formulation cascaded logistic model. permutation pentadiagonal matrix necessarily pentadiagonal, poses potential challenge application equivalency. however, cuthill-mckee algorithm heuristic find permutation lowest bandwidth., closest pentadiagonal]. orem generalized sparse, structured cascaded logistic models. orem (intersection cascaded logistic model Ising model). cascaded logistic model interactions neurons Ising model. example, cascaded logistic sparse cascade Ising model (fig.  remark cascaded logistic model written exponential family form, cascaded logistic correspond simple family maximum entropy models general. orems show subset Ising models equivalent cascaded logistic models. however, cascaded logistic models generally provide good approximations Ising model. demonstrate drawing random Ising models (both sparse dense pairwise coupling), fitting cascaded logistic model (fig. ). Ising models widely accepted effective models neural populations, cascaded logistic model presents computationally tractable alternative. simulations compare parametric models (independent Bernoulli cascaded logistic model) nonparametric models (two universal binary models centered parametric models, naive histogram estimator) simulated data neurons. find MAP solution parameter \\x0cestimate model. regularization fit cascaded logistic model ubm. regularizer selected scanning grid cross-validation likelihood started decreasing% training data. fig. simulate maximum entropy (maxent) distribution order interaction. number samples increases, JensenShannon) divergence estimated model true maxent model decreases exponentially nonparametric models. -divergence provide MATLAB code convert back subset Ising models subset cascaded logistic models (see online supplemental material). 1011 Figure Synchrony histogram model. word number total spikes neuron identity probability. Bernoulli cascaded logistic models provide good approximation case saturate, terms divergence. format fig.  figure Ising model nearest neighbor interaction. format fig.  note cascaded logistic UBM cascaded logistic base measure perform identically, convergence saturate \\x0cexpected orem). parametric models saturates actual distribution lie parametric family. cascaded logistic model UBM centered show performance small sample regime, eventually nonparametric models catch cascaded logistic model. scatter plot (fig. ) displays log-likelihood ratio log(ptrue log(pmodel quantify accuracy predictive distribution. significant deviations base measure model observed fig. , UBM adapts account deviations. fig. draw samples distribution higher-order dependences; Each word number total spikes assigned probability. example, words neurons spiking (and spiking, neurons) occur high probability histogram total spikes (fig. ). neir Bernoulli model cascaded logistic model capture structure accurately, plateau convergence plots (fig. ). case, nonparameteric models behave similarly: UBMs converge histogram. addition, data model class assumed base measure, UBM good base measure (fig. ). toger, results suggest UBM.0016.0349 Figure Various models fit population ten retinal ganglion neurons? response naturalistic movie]. words consisted, binarized responses.  105 samples reserved testing. ) divergence estimated model, histogram constructed test data. ising model included, trace closely cascaded logistic model. ) Histogram number spikes word. ) log-likelihood ratio scatter plot models trained 105 randomized observations. ) concentration parameter function sample size. supplements base measure model ﬂexibly observed firing patterns, performs histogram worst case. neural data apply UBMs simultaneously recorded population retinal gangilion cells, compare Ising model. fig. evaluate convergence model. models?cascaded logistic, ubm, Ising model?initially perform similarly, however, data provided, UBM predicts probabilities better. panel confirm cascaded logistic UBM fit. decrease shown panel cascaded logistic UBM confident data actual cascaded logistic model obtain data. conclusion proposed universal binary models (ubms), nonparametric framework extends parametric models neural recordings. ubms ﬂexibly trade smoothing base measure ?histogram-like? behavior. dirichlet process incorporate deviations base measure supported data, base measure buttresses nonparametric approach desirable properties parametric models, fast convergence interpretability. unlike reliable interaction model], aims provide features heuristic manner, UBM well-defined probabilistic model. main source smoothing base measure, ubm ability extrapolate limited repeatedly observed words. however, UBM capable adjusting probabilities frequent words focus fitting regularities small probability events. proposed cascaded logistic model powerful, computationally tractable, base measure. showed, oretically empirically, cascaded logistic model effective, scalable alternative Ising model, limited smaller populations. ubm model class potential reveal complex structure large-scale recordings limitations priori parametric assumptions. acknowledgments Segev Ganmor retinal data. work supported Sloan Research fellowship, McKnight scholar award, NSF CAREER Award iis-1150186). illustrate performance models neural data.” Probability distributions spike words form fundamental building blocks neural code. accurate estimates distributions diﬃcult obtain context modern experimental techniques, make record simultaneous spiking activity hundreds neurons. diﬃculties, computational statistical, arise fundamentally exponential scaling population size) number words \\x0cpopulation capable expressing. one strategy combating combinatorial explosion introduce parametric model seeks make trade-offs ﬂexibility, computational expense], mamatical completeness] order applicable large-scale neural recordings. variety parametric models proposed literature, including 2ndorder maxent Ising model], reliable interaction model], restricted Boltzmann machine], deep learning], mixture Bernoulli model], dichotomized Gaussian model]. however, number parameters model chosen parametric family increase number neurons, increase exponentially number words. thus, size population increases, parametric model rapidly loses ﬂexibility describing full spike distribution. contrast, nonparametric models ﬂexibility grow amount data]. naive nonparametric model, histogram spike words, oretically preserves representational power computational simplicity. yet practice, empirical histogram extremely slow converge, high dimensional data primarily interested independent Bernoulli model neurons cascaded logistic model time Figure) Binary representation neural population activity. single spike word red. ) Hierarchical Dirichlet process prior universal binary model (ubm) spike words. each word drawn probability  drawn Dirichlet parameters base distribution spike words parameter , Graphical models base measures spike words: independent Bernoulli model cascaded logistic model. base measure distribution spike word    . cases, expect data empirical histogram converge. perhaps naive histogram model fails smooth space words: unobserved words accounted model. propose framework combines parsimony parametric models ﬂexibility nonparametric models. model spike word distribution Dirichlet process centered parametric base measure. appropriately chosen base measure smooths observations, Dirichlet process data depart systematically base measure. models universal sense converge distribution supported)dimensional simplex. inﬂuence base measure diminishes increasing sample size, model ultimately converges empirical distribution function. choice base measure inﬂuences small-sample behavior computational tractability universal models, crucial neural applications. base measures exploit priori knowledge neural data remaining computationally tractable large populations: independent Bernoulli spiking model, cas \\x0ccaded logistic model]. both Bernoulli cascaded logistic models show performance base measure universal model alone. apply models simulated neural data examples. Universal binary model Consider (random) binary spike word length denotes number distinct neurons (and time bins; fig. ). words, index,   }. universal binary model hierarchical probabilistic model bottom level (fig. ), drawn multinomial (categorical) distribution probability observing word vector (spike word distribution). top level, model Dirichlet process] discrete base measure hence, cat(?   ?   ) concentration parameter, base measure, discrete probability distribution spike words, parameterized(?— hyper-prior. choose discrete probability measure positive measure,   }, denote ). thus, Dirichlet process probability mass spike words, (finite dimensional) Dirichlet distribution,  dir    ) absence data, parametric base measure controls nonparametric model[?—?]  ) refore, loosely ?centered?  start good parametric models neural populations, extend nonparametric model base measure]. under scheme, base measure quickly learns basic structure data Dirichlet extension takes account deviations data predicted parametric component. call extension universal binary model (ubm) base measure  marginal distribution collection words UBM obtained integrating form Polya. dirichletmultinomial) distribution—?, (?)  ) number observations word This leads simple formula sampling predictive distribution words   ) thus, sampling proceeds Chinese restaurant process (crp): set word probability proportional probability proportional draw word (which turn increases probability word draw). note  predictive distribution converges histogram estimate nnk  converges base measure itself. JensenShannon divergence predictive distribution quantify performance experiments.  Model fitting Given data, fit UBM maximum posteriori (map) inference coordinate ascent. marginal log-likelihood, log —?, log log log (?) log  ) Derivatives respect  are,     (?) ) ) denotes digamma function. note summation terms vanish observations), words observed dataset. Note limit   converges derivative logarithm base respect hand, limit  derivative 1measure reﬂecting fact number observations ignored: likelihood effectively reﬂects single draw base distribution probability even likelihood defined base measure convex log-convex UBM likelihood guaranteed convex. hence, optimize coordinate ascent procedure alternates optimizing   hyper-prior When modeling large populations neurons, number parameters base measure grows over-fitting concern. since UBM relies base measure provide smoothing words, critical properly regularize estimate technically, mode    distribution symmetric probability mass concentrated corners simplex. place hyper-prior(?— regularization. regularization, correspond Gaussian double exponential priors, respectively. with regularization, loss function optimization?kpp typical multi-neuron recording, connectivity \\x0cknown sparse lower order], assume connectivity sparse. prior promotes sparsity. Base measures scalability UBM hinges scalability base measure. describe computationally eﬃcient base measures.  Independent Bernoulli model independent Bernoulli model assumes (statistically) independent spiking neurons. baseline model simplicity]. Bernoulli base measure takes form,    —?) pxi)   distribution full support spike words long non-zero. although Bernoulli model capture higher-order correlation structure spike word distribution parameters, inference fast memoryeﬃcient.   cascaded logistic model introduce rich dependence structure neurons, assume joint firing probability neuron factors cascaded structure (see fig.         along parametric form conditional distribution   bilistic model spike words.  ) probaa natural choice conditional logistic-bernoulli linear model widely model binary observations].  logistic wij  wij parameters. combination factorization likelihoods give rise cascaded logistic (bernoulli) model2 written,    —?)   exp (2xi)   wij) cascaded logistic model Ising model (second order maxent model) number parameters) parametric form. Ising model written as3   —?) exp Jij upper triangular matrix parameters) normalizer. however, unlike cascaded logistic model, diﬃcult evaluate likelihood Ising model, computationally tractable normalizer (partition function). hence, fitting Ising model typically challenging. since conditional independently fit logistic regression Example cascaded logistic model orem sparse Ising Bernoulli sparse Ising 100 cascaded logistic cascaded logistic Bernoulli Equivalent Ising model dense Ising dense Ising 100 Figure Tight relation cascaded logistic model Ising model. ) cascaded logistic model depicted graphical model conditioning (incoming arrow) node (see orem). parameters nodes interaction terms, wij shown arrows nodes. ) Parameter matrix Ising model equivalent). ) scatter plot simulated Ising models fit cascaded logistic (blue tone) independent Bernoulli (red tone) models. each point word spike word space. -axis probability word actual Ising model-axis shows estimated probability fit model. Ising model parameters sparsely connect generated randomly. diagonal terms (jii drawn standard normal. % off-diagonal (jij terms set rest drawn normal standard deviation both models fit maximum likelihood 107 samples. ) histogram JensenShannon) divergence 100 random pairs sparse Ising model fit models. ) Same) Ising models generated dense connectivity. diagonal terms Ising model parameters constant. off-diagonal terms drawn standard normal distribution. convex optimization), cascaded logistic model estimation computationally tractable large number neurons]. despite differences, remarkably, Ising model cascaded logistic models overlap substantially. neurons, Ising model cascaded logistic model equivalent. for larger populations, orem describes intersection models. orem (pentadiagonal Ising model cascaded logistic model). Ising model Jij, cascaded logistic model. moreover, parameter transformation bijective. mapping models parameters    exp log exp    exp exp log log exp exp  exp exp log exp exp symmetric proof found supplemental material. also logistic autoregressive network. see], chapter. note}, incorporated diagonal ) Figure 3rd order maxent distribution experiment. ) Convergence jensen-shannon) divergence fit model true model. error bar represents SEM repeats. ) Histogram number spikes word. ) Scatter plots log-likelihood ratio log(pemp)) log(pmodel)) model (column), sample sizes 1000 100000 (rows). note scale difference-axes. error line represents standard deviation repeats. shaded area represents frequentist% confidence interval histogram estimator assuming amount data. number bottom divergence. unlike Ising model, order neurons plays role formulation cascaded logistic model. since permutation pentadiagonal matrix necessarily pentadiagonal, poses potential challenge application equivalency. however, cuthill-mckee algorithm heuristic find permutation lowest bandwidth., closest pentadiagonal]. this orem generalized sparse, structured cascaded logistic models. orem (intersection cascaded logistic model Ising model). cascaded logistic model interactions neurons Ising model. for example, cascaded logistic sparse cascade Ising model (fig.  remark cascaded logistic model written exponential family form, cascaded logistic correspond simple family maximum entropy models general. orems show subset Ising models equivalent cascaded logistic models. however, cascaded logistic models generally provide good approximations Ising model. demonstrate drawing random Ising models (both sparse dense pairwise coupling), fitting cascaded logistic model (fig. ). since Ising models widely accepted effective models neural populations, cascaded logistic model presents computationally tractable alternative. Simulations compare parametric models (independent Bernoulli cascaded logistic model) nonparametric models (two universal binary models centered parametric models, naive histogram estimator) simulated data neurons. find MAP solution parameter \\x0cestimate model. regularization fit cascaded logistic model ubm. regularizer selected scanning grid cross-validation likelihood started decreasing% training data. fig. simulate maximum entropy (maxent) distribution order interaction. number samples increases, JensenShannon) divergence estimated model true maxent model decreases exponentially nonparametric models. -divergence provide MATLAB code convert back subset Ising models subset cascaded logistic models (see online supplemental material). 1011 Figure Synchrony histogram model. each word number total spikes neuron identity probability. both Bernoulli cascaded logistic models provide good approximation case saturate, terms divergence. same format fig.  Figure Ising model nearest neighbor interaction. same format fig.  note cascaded logistic UBM cascaded logistic base measure perform identically, convergence saturate \\x0cexpected orem). parametric models saturates actual distribution lie parametric family. cascaded logistic model UBM centered show performance small sample regime, eventually nonparametric models catch cascaded logistic model. scatter plot (fig. ) displays log-likelihood ratio log(ptrue log(pmodel quantify accuracy predictive distribution. where significant deviations base measure model observed fig. , UBM adapts account deviations. fig. draw samples distribution higher-order dependences; Each word number total spikes assigned probability. for example, words neurons spiking (and spiking, neurons) occur high probability histogram total spikes (fig. ). neir Bernoulli model cascaded logistic model capture structure accurately, plateau convergence plots (fig. ). case, nonparameteric models behave similarly: UBMs converge histogram. addition, data model class assumed base measure, UBM good base measure (fig. ). toger, results suggest UBM.0016.0349 Figure Various models fit population ten retinal ganglion neurons? response naturalistic movie]. words consisted, binarized responses.  105 samples reserved testing. ) divergence estimated model, histogram constructed test data. ising model included, trace closely cascaded logistic model. ) Histogram number spikes word. ) log-likelihood ratio scatter plot models trained 105 randomized observations. ) concentration parameter function sample size. supplements base measure model ﬂexibly observed firing patterns, performs histogram worst case. neural data apply UBMs simultaneously recorded population retinal gangilion cells, compare Ising model. fig. evaluate convergence model. three models?cascaded logistic, ubm, Ising model?initially perform similarly, however, data provided, UBM predicts probabilities better. panel confirm cascaded logistic UBM fit. decrease shown panel cascaded logistic UBM confident data actual cascaded logistic model obtain data. Conclusion proposed universal binary models (ubms), nonparametric framework extends parametric models neural recordings. ubms ﬂexibly trade smoothing base measure ?histogram-like? behavior. Dirichlet process incorporate deviations base measure supported data, base measure buttresses nonparametric approach desirable properties parametric models, fast convergence interpretability. unlike reliable interaction model], aims provide features heuristic manner, UBM well-defined probabilistic model. since main source smoothing base measure, ubm ability extrapolate limited repeatedly observed words. however, UBM capable adjusting probabilities frequent words focus fitting regularities small probability events. proposed cascaded logistic model powerful, computationally tractable, base measure. showed, oretically empirically, cascaded logistic model effective, scalable alternative Ising model, limited smaller populations. UBM model class potential reveal complex structure large-scale recordings limitations priori parametric assumptions. acknowledgments Segev Ganmor retinal data. this work supported Sloan Research fellowship, McKnight scholar award, NSF CAREER Award iis-1150186).',\n",
       " 'PP5084': 'understanding dependencies multivariate data central problem analysis financial time series, underpinning common tasks portfolio construction calculation value-atrisk. classical methods estimate dependencies terms covariance matrix (possibly time varying) induced data]. however, general approach copula functions model dependencies]. copulas popular separate estimation marginal distributions estimation dependence structure, completely determined copula. standard copula methods estimate dependencies inaccurate actual dependencies strongly inﬂuenced covariates. example, dependencies vary time affected observations time series. standard copula methods handle conditional dependencies. address limitation, propose probabilistic framework estimate conditional copulas. specifically assume parametric copulas \\x0cparameters unknown non-linear functions arbitrary conditioning variables. latent functions approximated Gaussian processes]. gps previously model conditional copulas] work applies copulas single parameter. extend work accommodate copulas multiple parameters. important improvement richer set copulas including student asymmetric copulas. demonstrate method choosing conditioning variables time evaluating ability estimate time-varying dependencies Symmetrized Joe Clayton Copula student Copula Gaussian Copula Figure left, Gaussian copula density . middle, student copula density   right, symmetrized Joe Clayton copula density  . copula model asymmetric main diagonal unit square. currency equity time series. method achieves consistently superior predictive performance compared static copula models dynamic copula methods. include models parameters change time. regime switching models] methods proposing garch-style updates copula parameters]. Copulas Conditional Copulas \\x0ccopulas provide powerful framework construction multivariate probabilistic models separating modeling univariate marginal distributions modeling dependencies variables]. focus bivariate copulas higher dimensional copulas typically constructed bivariate copulas building blocks]. sklar orem] states one-dimensional random variables, continuous marginal cumulative distribution functions (cdfs) express joint cdf unique copula ) marginally uniformly distributed cdf probability distribution unit square, , uniform marginals. figure shows plots copula densities parametric copula models: gaussian, student symmetrized Joe Clayton (sjc) copulas. copula models learnt step process]. first, marginals learnt fitting univariate models. second, data mapped unit square. probability integral transform fit transformed data.  Conditional Copulas When access covariate vector estimate conditional version copula model.  ) here, two-step estimation process estimate). estimation marginals implemented standard methods univariate conditional distribution estimation. however, estimation constrained uniform marginal distributions; problem considered recently]. propose general Bayesian non-parametric framework estimation conditional copulas based GPs alternating expectation propagation) algorithm eﬃcient approximate inference. gaussian Process Conditional Copulas Let sample drawn assume parametric copula model Cpar),   )] parameters   functions conditioning variable )], arbitrary real function function maps real line set valid configurations example, Cpar student copula. case, correlation degrees freedom student copula, ?). choose (?) ?(?)   standard Gaussian cdf (?) exp(?) satisfy constraint sets respectively. parametric form Cpar mapping functions   learn latent functions    perform Bayesian non-parametric analysis placing priors functions computing posterior distribution observed data.     prior distribution    function) covariance matrix generated squared exponential covariance function.  cov exp  diag ) vector inverse length-scales amplitude noise parameters. posterior distribution       cpar   ) cpar density parametric copula model normalization constant called model evidence. denoted make predictions conditional distribution standard prediction formula    cpar?  ?    ?          df1   dfk )     ?    ?              cov (cov?  )],   cov?  unfortunately) computed analytically, approximate expectation propagation].  Alternating Algorithm Approximate Bayesian Inference joint distribution    written product factors    (f1i   fki       fji (f1i fki cpar [f1i [fki ]]. approximates factor withan approximate Gaussian factor integrate one.  (f1i fki exp (fji  ? vji  parameters calculated. factors Gaussian form approximated.  gaussian, product, normalization constant, approximates multivariate Gaussian distribution  exact posterior) factorizes   predictive distribution) approximated integrating     respect    results factorized Gaussian distribution  approximates  finally) approximated monte-carlo sampling averaging cpar?  ?  ? samples. iteratively updates convergence computing /? minimizing kullback-leibler divergence involves updating marginal moments     match. however, compute moments analytically due complicated form solution numerical methods compute-dimensional integrals. however, typically exponentially large computational cost prohibitive perform additional approximation computing marginal moments fji respect loss generality, assume compute expectation f1i respect make approximation: f1i (f1i   fki (f1i   ,fki df1i   dfki  f1i (f1i    (f1i    df1i           fki (f1i  means f1i fki respect constant approximates width integrand maximum dimensions f1i practice moments normalized moment ignored. hand side) onedimensional integral easily computed numerical techniques. approximation similar approximating integral product maximum integrand estimate width. however, maximizing (f1i fki respect f2i fki maximizing easier task Gaussian maximizer vector. practice, (f1i   fki ﬂat compared maximizer approximates maximizer (f1i   fki (f1i   fki factorizes    implementation decouples sub-routines alternate subroutine approximates posterior distribution input means generated sub-routines. sub-routine finds Gaussian approximation set one-dimensional factors; factor data point.  sub-routine factor (f1i   fki {f1i   fki {fji fixed current estimated sub-routines. iteratively alternate sub-routines, running convergence-running one. convergence achieved quickly; run sub-routine times. sub-routines implemented parallel update scheme]. speed related computations, generalized FITC approximation]: Each covariance matrix approximated K0i diag kinn0 [kin0 [kinn0 Kin0 covariance matrix generated evaluating) pseudoinputs, Kinn0 matrix covariances training points pseudo-inputs. cost(knn20 time call subroutine, optimize kernel hyper-parameters pseudo-inputs maximizing approximation model evidence]. related Work model proposed extension conditional copula model]. case bivariate data copula based parameter models identical. extended approximate inference model accommodate copulas multiple parameters; previously computationally infeasible due requiring numerical calculation multidimensional integrals loop inference. demonstrated model produce excellent predictive results financial time series conditioning copula time.  Dynamic Copula Models] dynamic copula model proposed based two-state hidden Markov model (hmm }) assumes data generating process regimes low/high correlation. time copula density student parameters values hidden state maximum likelihood estimation copula parameters transition probabilities performed algorithm. ]. timevarying correlation (tvc) model based student copula]. correlation parameter1 student copula assumed satisfy   ?)?    empirical correlation previous observations  satisfy          number degrees freedom assumed constant. previous formula GARCH equation correlation variance. estimation  easily performed maximum likelihood. ] dynamic copula based SJC copula (dsjcc) introduced. method, parameters  sjc copula assumed depend time ?   ) ?   ) P10 ?[?] logistic function  copula sample time constants avoid numerical instabilities. formulae GARCH equation correlations, additional logistic function constrain parameter values. estimation performed maximum likelihood. prior work allowing copula parameters depend arbitrary conditioning variables rar time alone. also, models eir assume Markov independence garch-like updates copula parameters. assumptions empirically proven effective estimation univariate variances, consistent performance gains proposed method suggest assumptions applicable estimation dependencies.  Dynamic Covariance Models direct extension GARCH equations multiple time series, vec, proposed]. ) multivariate time series assumed satisfy) )). vec, models dynamics ) equation form vech(? )) vech   vech(?   vech operation stacks lower triangular part matrix column vector. vec model large number parameters commonly model bekk, model] assumes dynamics ) ATk   BkT    This model parameters restricted versions models proposed avoid over-fitting (see. section]). alternative solution over-fitting due over-parameterization Bayesian approach] Bayesian inference performed dynamic bekk, model. bayesian approaches include non-parametric generalized Wishart process]. works ) modeled generalized Wishart process.  ) lui uid (?) distributed independent gps. experiments evaluate proposed Gaussian process conditional copula models (gpcc) one-step-ahead prediction task syntic data financial time series. time conditioning variable parametric copula families; Gaussian (gpcc), student (gpcc) symmetrized Joe Clayton (gpcc-sjc). parameters copulas presented Table transformations model figure shows plots densities parametric copula models. code data publicly http://jmhl.org. parameterization paper related sin?  copula Gaussian student SJC Parameters correlation, correlation, degrees freedom, upper depen dence, lower dependence, transformation? )] ? )] 106 ? ? )] \\x0csyntic parameter function  cos?/125)  cos?/125)  cos?/250))  cos?/125))  cos?/125)) Table Copula parameters, modeling formulae parameter functions generate syntic data.  standard Gaussian cumulative density function gps. variants GPCC compared dynamic copula methods constant copula models. dynamic methods include HMM based model, TVC DSJCC introduced Section constant copula models gaussian, student SJC copulas parameter values change time (const, const constsjc). perform one-step-ahead rolling-window prediction task bivariate time series )}. model trained data points predictive log-likelihood data point recorded, 1000. repeated, shifting training test windows forward data point. methods compared average predictive log-likelihood; performance measure copula estimation copulas probability distributions.  Syntic Data generated syntic datasets length 5001 copula models (gaussian, student sjc) parameters vary periodic functions time, Table table reports average predictive log-likelihood method syntic time series. results performing method syntic time series shown bold. results method underlined differences respect performing method statistically significant paired test . gpcc gpcc-sjc obtain results student SJC time series respectively. however, HMM performing method Gaussian time series. technique successfully captures regimes low/high correlation peaks troughs sinusoid maps time correlation  proposed methods gpcc,sjc] ﬂexible eﬃcient HMM problem. however, HMM performs significantly worse student SJC time series periods copula parameter functions captured state model. figure shows gpcc successfully tracks ) ) student time series. plots display (red) confidence bands (orange quantiles) predictive distribution ) ) ground truth values (blue). finally, Table shows static copula methods const,sjc] outperformed dynamic techniques gpcc,sjc], dsjcc, TVC hmm.  Foreign Exchange Time Series evaluated method daily logarithmic returns currencies shown Table (all priced respect. dollar). date range data-1990-2013; total 6011 observations. evaluated methods bivariate time series, pairing currency pair Swiss franc (chf). chf safe haven currency, meaning investors ﬂock times uncertainty]. expect correlations CHF currencies large variability time response financial conditions. process data asymmetric)-garch) process non-parametric innovations] estimate univariate marginal cdfs time points. train GARCH model 2016 data points predict cdf data point; subsequent cdfs predicted shifting training window data point rolling-window methodology. cdf estimates transform raw logarithmic returns pseudosample underlying copula Section note method predicting univariate cdfs produce pseudo-samples copula. perform rolling-window predictive likelihood experiment transformed data. results shown Table technique gpcc, gpcc. dynamic copula methods gpcc,sjc], hmm, TVC outperform static methods const,sjc] analyzed series. dynamic method DSJCC occasionally performed poorly; worse static methods experiments. student Time Series student Time series, Mean gpcc Ground truth Mean gpcc Ground truth 200 400 600 800 1000 200 400 600 800 Method gpcc gpcc gpcc-sjc HMM TVC DSJCC const CONST const-sjc Gaussian.3347.3397.3355.3555.3277.3329.3129.3178.3002 Student.3879.4656.4132.4422.4273.4096.3201.4218.3812 SJC.2513.2610.2771.2547.2534.2612.2339.2499.2502 1000 \\x0cfigure Predictions made gpcc ) ) Table avg. test log-likelihood syntic time series sampled student copula. method time series. currency Name Swiss Franc Australian Dollar Canadian Dollar Japanese Yen Norwegian Krone Swedish Krona Euro New Zeland Dollar British Pound Table currencies. method gpcc gpcc gpcc-sjc HMM TVC DSJCC const CONST const-sjc CAD.0562.0589.0469.0478.0524.0259.0398.0463.0425 JPY.1221.1201.1064.1009.1038.0891.0771.0898.0852 NOK.4106.4161.3941.4069.3930.3994.3413.3765.3536 SEK.4132.4192.3905.3955.3878.3937.3426.3760.3544 EUR.8842.8995.8287.8700.7855.8335.6803.7732.7113 GBP.2487.2514.2404.2374.2301.2320.2085.2231.2165 NZD.1045.1079.0921.0926.0974.0560.0745.0875.0796 Table avg. test log-likelihood method currency data. eur?chf gpcc eur?chf gpcc, Mean gpcc eur?chf gpcc-sjc, Mean gpcc Mean gpcc-sjc Mean gpcc-sjc 100 120 140 AUD.1260.1319.1168.1164.1181.0798.0925.1078.1000 Code CHF AUD CAD JPY NOK SEK EUR NZD GBP Oct Mar Aug Jan Jun Nov Apr Oct Mar Aug Oct Mar Aug Jan Jun Nov Apr Oct Mar Aug Oct Mar Aug Jan Jun Nov Apr Oct Mar Aug \\x0cfigure Left middle, predictions made gpcc ) ) time series EURCHF trained data-2006 0908-2010. significant reduction ) onset 2008-2012 global recession. right, predictions made gpcc-sjc ) ) trained time-series data. predictions ) erratic ). proposed method gpcc capture time parameters student copula. left middle plots Figure show predictions ) ) generated gpcct. left plot, observe reduction ) onset 2008-2012 global recession indicating return series prone outliers. plot ) (middle) shows large time. particular, observe large drops dependence level eur-usd chf-usd fall 2008 onset global recession) summer 2010 (corresponding worsening European sovereign debt crisis). comparison, include predictions ) ) made gpcc-sjc plot Figure case, prediction ) similar made gpcc ), prediction ) noisier erratic. suggests gpcc-sjc robust gpcc. All copula densities Figure large values proximity points. positive correlation. however, student copula copulas high values proximity points. negative correlation. plot left Figure shows ) takes low values end time period, increasing robustness gpcc negatively correlated outliers.  Equity Time Series furr comparison, evaluated method logarithmic returns equity pairs, date range processed)-garch) model discussed previously. equities chosen include pairs high correlation. rbs barc) low correlation. axp). results shown Table technique gpcc, gpcc. rbs?barc gpcc Mean gpcc Method gpcc gpcc gpcc-sjc HMM TVC DSJCC const CONST const-sjc Apr Sep Aug Jan Jul HON.1247.1289.1210.1260.1251.0935.1162.1239.1175 AXP.1133.1187.1095.1119.1119.0750.1027.1091.1046 CNW CSX.1450.1499.1399.1458.1459.1196.1288.1408.1307 EIX.2072.2059.1935.2040.2011.1721.1962.2007.1891 \\x0chpq IBM.1536.1591.1462.1511.1511.1163.1325.1481.1373 BARC HSBC.2424.2486.2342.2486.2449.2188.2307.2426.2268 RBS BARC.3401.3501.3234.3414.3336.3051.2979.3301.2992 RBS HSBC.1860.1882.1753.1818.1823.1582.1663.1775.1639 Dec Jun Nov Apr Figure Prediction ) Table Average test log-likelihood method pair rbs-barc. stocks. figure shows predictions ) generated gpcc. observe low values 2010 suggesting Gaussian copula bad fit data. indeed, gpcc performs significantly worse gpcc equity pair. conclusions Future Work proposed inference scheme fit conditional copula model multivariate data copula multiple parameters. copula parameters modeled unknown nonlinear functions arbitrary conditioning variables. evaluated framework estimating timevarying copula parameters bivariate financial time series. method consistently outperforms static copula models dynamic copula models. initial investigation focused bivariate copulas. higher dimensional copulas typically constructed bivariate copulas building blocks]. framework applied constructions empirical predictive performance gains transfer setting. evaluating effectiveness approach compared models multivariate covariance profitable area empirical research. extend analysis presented including additional conditioning variables time. example, including prediction univariate volatility conditioning variable copula parameters change response changing volatility. pose inference challenges dimension increases, create richer models. acknowledgements David?opez-paz Andrew Gordon Wilson interesting discussions. jos Miguel hern?andez-lobato acknowledges support Infosys labs, Infosys limited. daniel HernandezLobato acknowledges support Spanish direcci General investigaci, project ALLS (tin2010-21575-c02). Understanding dependencies multivariate data central problem analysis financial time series, underpinning common tasks portfolio construction calculation value-atrisk. classical methods estimate dependencies terms covariance matrix (possibly time varying) induced data]. however, general approach copula functions model dependencies]. copulas popular separate estimation marginal distributions estimation dependence structure, completely determined copula. standard copula methods estimate dependencies inaccurate actual dependencies strongly inﬂuenced covariates. for example, dependencies vary time affected observations time series. standard copula methods handle conditional dependencies. address limitation, propose probabilistic framework estimate conditional copulas. specifically assume parametric copulas \\x0cparameters unknown non-linear functions arbitrary conditioning variables. latent functions approximated Gaussian processes]. gps previously model conditional copulas] work applies copulas single parameter. extend work accommodate copulas multiple parameters. this important improvement richer set copulas including student asymmetric copulas. demonstrate method choosing conditioning variables time evaluating ability estimate time-varying dependencies Symmetrized Joe Clayton Copula student Copula Gaussian Copula Figure left, Gaussian copula density . middle, student copula density   right, symmetrized Joe Clayton copula density  . copula model asymmetric main diagonal unit square. currency equity time series. our method achieves consistently superior predictive performance compared static copula models dynamic copula methods. include models parameters change time. regime switching models] methods proposing garch-style updates copula parameters]. Copulas Conditional Copulas \\x0ccopulas provide powerful framework construction multivariate probabilistic models separating modeling univariate marginal distributions modeling dependencies variables]. focus bivariate copulas higher dimensional copulas typically constructed bivariate copulas building blocks]. sklar orem] states one-dimensional random variables, continuous marginal cumulative distribution functions (cdfs) express joint cdf unique copula since) marginally uniformly distributed cdf probability distribution unit square, , uniform marginals. figure shows plots copula densities parametric copula models: gaussian, student symmetrized Joe Clayton (sjc) copulas. copula models learnt step process]. first, marginals learnt fitting univariate models. second, data mapped unit square. probability integral transform fit transformed data.  Conditional Copulas When access covariate vector estimate conditional version copula model.  ) here, two-step estimation process estimate). estimation marginals implemented standard methods univariate conditional distribution estimation. however, estimation constrained uniform marginal distributions; problem considered recently]. propose general Bayesian non-parametric framework estimation conditional copulas based GPs alternating expectation propagation) algorithm eﬃcient approximate inference. Gaussian Process Conditional Copulas Let sample drawn assume parametric copula model Cpar),   )] parameters   functions conditioning variable let)], arbitrary real function function maps real line set valid configurations for example, Cpar student copula. case, correlation degrees freedom student copula, ?). one choose (?) ?(?)   standard Gaussian cdf (?) exp(?) satisfy constraint sets respectively. once parametric form Cpar mapping functions   learn latent functions    perform Bayesian non-parametric analysis placing priors functions computing posterior distribution observed data. let    prior distribution    function) covariance matrix generated squared exponential covariance function.  cov exp  diag ) vector inverse length-scales amplitude noise parameters. posterior distribution       cpar   ) cpar density parametric copula model normalization constant called model evidence. given denoted make predictions conditional distribution standard prediction formula    cpar?  ?    ?          df1   dfk )     ?    ?              cov (cov?  )],   cov?  unfortunately) computed analytically, approximate expectation propagation].  Alternating Algorithm Approximate Bayesian Inference joint distribution    written product factors    (f1i   fki       fji (f1i fki cpar [f1i [fki ]]. approximates factor withan approximate Gaussian factor integrate one.  (f1i fki exp (fji  ? vji  parameters calculated. factors Gaussian form approximated. since gaussian, product, normalization constant, approximates multivariate Gaussian distribution  exact posterior) factorizes   predictive distribution) approximated integrating     respect    this results factorized Gaussian distribution  approximates  finally) approximated monte-carlo sampling averaging cpar?  ?  ? samples. iteratively updates convergence computing /? minimizing kullback-leibler divergence this involves updating marginal moments    and match. however, compute moments analytically due complicated form solution numerical methods compute-dimensional integrals. however, typically exponentially large computational cost prohibitive instead perform additional approximation computing marginal moments fji respect without loss generality, assume compute expectation f1i respect make approximation: f1i (f1i   fki (f1i   ,fki df1i   dfki  f1i (f1i    (f1i    df1i           fki (f1i  means f1i fki respect constant approximates width integrand maximum dimensions f1i practice moments normalized moment ignored. hand side) onedimensional integral easily computed numerical techniques. approximation similar approximating integral product maximum integrand estimate width. however, maximizing (f1i fki respect f2i fki maximizing this easier task Gaussian maximizer vector. practice, (f1i   fki ﬂat compared maximizer approximates maximizer (f1i   fki (f1i   fki since factorizes    implementation decouples sub-routines alternate subroutine approximates posterior distribution input means generated sub-routines. each sub-routine finds Gaussian approximation set one-dimensional factors; factor data point.  sub-routine factor (f1i   fki {f1i   fki {fji fixed current estimated sub-routines. iteratively alternate sub-routines, running convergence-running one. convergence achieved quickly; run sub-routine times. sub-routines implemented parallel update scheme]. speed related computations, generalized FITC approximation]: Each covariance matrix approximated K0i diag Kinn0 [kin0 [kinn0 Kin0 covariance matrix generated evaluating) pseudoinputs, Kinn0 matrix covariances training points pseudo-inputs. cost(knn20 each time call subroutine, optimize kernel hyper-parameters pseudo-inputs maximizing approximation model evidence]. Related Work model proposed extension conditional copula model]. case bivariate data copula based parameter models identical. extended approximate inference model accommodate copulas multiple parameters; previously computationally infeasible due requiring numerical calculation multidimensional integrals loop inference. demonstrated model produce excellent predictive results financial time series conditioning copula time.  Dynamic Copula Models] dynamic copula model proposed based two-state hidden Markov model (hmm }) assumes data generating process regimes low/high correlation. time copula density student parameters values hidden state maximum likelihood estimation copula parameters transition probabilities performed algorithm. ]. timevarying correlation (tvc) model based student copula]. correlation parameter1 student copula assumed satisfy   ?)?    empirical correlation previous observations  satisfy          number degrees freedom assumed constant. previous formula GARCH equation correlation variance. estimation  easily performed maximum likelihood. ] dynamic copula based SJC copula (dsjcc) introduced. method, parameters  SJC copula assumed depend time ?   ) ?   ) P10 ?[?] logistic function  copula sample time constants avoid numerical instabilities. formulae GARCH equation correlations, additional logistic function constrain parameter values. estimation performed maximum likelihood. prior work allowing copula parameters depend arbitrary conditioning variables rar time alone. also, models eir assume Markov independence garch-like updates copula parameters. assumptions empirically proven effective estimation univariate variances, consistent performance gains proposed method suggest assumptions applicable estimation dependencies.  Dynamic Covariance Models direct extension GARCH equations multiple time series, vec, proposed]. let) multivariate time series assumed satisfy) )). vec, models dynamics ) equation form vech(? )) vech   vech(?   vech operation stacks lower triangular part matrix column vector. VEC model large number parameters commonly model bekk, model] assumes dynamics ) ATk   BkT    This model parameters restricted versions models proposed avoid over-fitting (see. section]). alternative solution over-fitting due over-parameterization Bayesian approach] Bayesian inference performed dynamic bekk, model. Bayesian approaches include non-parametric generalized Wishart process]. works ) modeled generalized Wishart process.  ) Lui uid (?) distributed independent gps. Experiments evaluate proposed Gaussian process conditional copula models (gpcc) one-step-ahead prediction task syntic data financial time series. time conditioning variable parametric copula families; Gaussian (gpcc), student (gpcc) symmetrized Joe Clayton (gpcc-sjc). parameters copulas presented Table transformations model figure shows plots densities parametric copula models. code data publicly http://jmhl.org. parameterization paper related sin?  Copula Gaussian student SJC Parameters correlation, correlation, degrees freedom, upper depen dence, lower dependence, Transformation? )] ? )] 106 ? ? )] \\x0csyntic parameter function  cos?/125)  cos?/125)  cos?/250))  cos?/125))  cos?/125)) Table Copula parameters, modeling formulae parameter functions generate syntic data.  standard Gaussian cumulative density function gps. variants GPCC compared dynamic copula methods constant copula models. dynamic methods include HMM based model, TVC DSJCC introduced Section constant copula models gaussian, student SJC copulas parameter values change time (const, const constsjc). perform one-step-ahead rolling-window prediction task bivariate time series )}. each model trained data points predictive log-likelihood data point recorded, 1000. this repeated, shifting training test windows forward data point. methods compared average predictive log-likelihood; performance measure copula estimation copulas probability distributions.  Syntic Data generated syntic datasets length 5001 copula models (gaussian, student sjc) parameters vary periodic functions time, Table table reports average predictive log-likelihood method syntic time series. results performing method syntic time series shown bold. results method underlined differences respect performing method statistically significant paired test . gpcc gpcc-sjc obtain results student SJC time series respectively. however, HMM performing method Gaussian time series. this technique successfully captures regimes low/high correlation peaks troughs sinusoid maps time correlation  proposed methods gpcc,sjc] ﬂexible eﬃcient HMM problem. however, HMM performs significantly worse student SJC time series periods copula parameter functions captured state model. figure shows gpcc successfully tracks ) ) student time series. plots display (red) confidence bands (orange quantiles) predictive distribution ) ) ground truth values (blue). finally, Table shows static copula methods const,sjc] outperformed dynamic techniques gpcc,sjc], dsjcc, TVC hmm.  Foreign Exchange Time Series evaluated method daily logarithmic returns currencies shown Table (all priced respect. dollar). date range data-1990-2013; total 6011 observations. evaluated methods bivariate time series, pairing currency pair Swiss franc (chf). chf safe haven currency, meaning investors ﬂock times uncertainty]. consequently expect correlations CHF currencies large variability time response financial conditions. process data asymmetric)-garch) process non-parametric innovations] estimate univariate marginal cdfs time points. train GARCH model 2016 data points predict cdf data point; subsequent cdfs predicted shifting training window data point rolling-window methodology. cdf estimates transform raw logarithmic returns pseudosample underlying copula Section note method predicting univariate cdfs produce pseudo-samples copula. perform rolling-window predictive likelihood experiment transformed data. results shown Table technique gpcc, gpcc. dynamic copula methods gpcc,sjc], hmm, TVC outperform static methods const,sjc] analyzed series. dynamic method DSJCC occasionally performed poorly; worse static methods experiments. student Time Series student Time series, Mean gpcc Ground truth Mean gpcc Ground truth 200 400 600 800 1000 200 400 600 800 Method gpcc gpcc gpcc-sjc HMM TVC DSJCC const CONST const-sjc Gaussian.3347.3397.3355.3555.3277.3329.3129.3178.3002 Student.3879.4656.4132.4422.4273.4096.3201.4218.3812 SJC.2513.2610.2771.2547.2534.2612.2339.2499.2502 1000 \\x0cfigure Predictions made gpcc ) ) Table avg. test log-likelihood syntic time series sampled student copula. method time series. currency Name Swiss Franc Australian Dollar Canadian Dollar Japanese Yen Norwegian Krone Swedish Krona Euro New Zeland Dollar British Pound Table currencies. method gpcc gpcc gpcc-sjc HMM TVC DSJCC const CONST const-sjc CAD.0562.0589.0469.0478.0524.0259.0398.0463.0425 JPY.1221.1201.1064.1009.1038.0891.0771.0898.0852 NOK.4106.4161.3941.4069.3930.3994.3413.3765.3536 SEK.4132.4192.3905.3955.3878.3937.3426.3760.3544 EUR.8842.8995.8287.8700.7855.8335.6803.7732.7113 GBP.2487.2514.2404.2374.2301.2320.2085.2231.2165 NZD.1045.1079.0921.0926.0974.0560.0745.0875.0796 Table avg. test log-likelihood method currency data. eur?chf gpcc eur?chf gpcc, Mean gpcc eur?chf gpcc-sjc, Mean gpcc Mean gpcc-sjc Mean gpcc-sjc 100 120 140 AUD.1260.1319.1168.1164.1181.0798.0925.1078.1000 Code CHF AUD CAD JPY NOK SEK EUR NZD GBP Oct Mar Aug Jan Jun Nov Apr Oct Mar Aug Oct Mar Aug Jan Jun Nov Apr Oct Mar Aug Oct Mar Aug Jan Jun Nov Apr Oct Mar Aug \\x0cfigure Left middle, predictions made gpcc ) ) time series EURCHF trained data-2006 0908-2010. significant reduction ) onset 2008-2012 global recession. right, predictions made gpcc-sjc ) ) trained time-series data. predictions ) erratic ). proposed method gpcc capture time parameters student copula. left middle plots Figure show predictions ) ) generated gpcct. left plot, observe reduction ) onset 2008-2012 global recession indicating return series prone outliers. plot ) (middle) shows large time. particular, observe large drops dependence level eur-usd chf-usd fall 2008 onset global recession) summer 2010 (corresponding worsening European sovereign debt crisis). for comparison, include predictions ) ) made gpcc-sjc plot Figure case, prediction ) similar made gpcc ), prediction ) noisier erratic. this suggests gpcc-sjc robust gpcc. All copula densities Figure large values proximity points. positive correlation. however, student copula copulas high values proximity points. negative correlation. plot left Figure shows ) takes low values end time period, increasing robustness gpcc negatively correlated outliers.  Equity Time Series furr comparison, evaluated method logarithmic returns equity pairs, date range processed)-garch) model discussed previously. equities chosen include pairs high correlation. rbs barc) low correlation. axp). results shown Table technique gpcc, gpcc. rbs?barc gpcc Mean gpcc Method gpcc gpcc gpcc-sjc HMM TVC DSJCC const CONST const-sjc Apr Sep Aug Jan Jul HON.1247.1289.1210.1260.1251.0935.1162.1239.1175 AXP.1133.1187.1095.1119.1119.0750.1027.1091.1046 CNW CSX.1450.1499.1399.1458.1459.1196.1288.1408.1307 EIX.2072.2059.1935.2040.2011.1721.1962.2007.1891 \\x0chpq IBM.1536.1591.1462.1511.1511.1163.1325.1481.1373 BARC HSBC.2424.2486.2342.2486.2449.2188.2307.2426.2268 RBS BARC.3401.3501.3234.3414.3336.3051.2979.3301.2992 RBS HSBC.1860.1882.1753.1818.1823.1582.1663.1775.1639 Dec Jun Nov Apr Figure Prediction ) Table Average test log-likelihood method pair rbs-barc. stocks. figure shows predictions ) generated gpcc. observe low values 2010 suggesting Gaussian copula bad fit data. indeed, gpcc performs significantly worse gpcc equity pair. Conclusions Future Work proposed inference scheme fit conditional copula model multivariate data copula multiple parameters. copula parameters modeled unknown nonlinear functions arbitrary conditioning variables. evaluated framework estimating timevarying copula parameters bivariate financial time series. our method consistently outperforms static copula models dynamic copula models. initial investigation focused bivariate copulas. higher dimensional copulas typically constructed bivariate copulas building blocks]. our framework applied constructions empirical predictive performance gains transfer setting. evaluating effectiveness approach compared models multivariate covariance profitable area empirical research. one extend analysis presented including additional conditioning variables time. for example, including prediction univariate volatility conditioning variable copula parameters change response changing volatility. this pose inference challenges dimension increases, create richer models. acknowledgements David?opez-paz Andrew Gordon Wilson interesting discussions. jos Miguel hern?andez-lobato acknowledges support Infosys labs, Infosys limited. daniel HernandezLobato acknowledges support Spanish direcci General investigaci, project ALLS (tin2010-21575-c02).',\n",
       " 'PP5104': 'variable selection core inferential problem multitude statistical analyses. confronted large number (potentially) predictive variables, goal select small subset variables construct parsimonious model. variable selection relevant linear observation models form   , )  matrix features predictors, unknown pdimensional regression parameter, noise vector. high-dimensional settings ordinary squares generally inappropriate. assuming  sparse., support set(?  ? cardinality), mainstay algorithm settings Lasso]: lasso: argmin?    ) For choice variable selection properties Lasso analyzed quan? approximates true support(?   careful tifying estimated support(?) analyses focus recovering signed support   ?  ?   . oretical developments decade shed light support recovery properties Lasso highlighted practical diﬃculties columns correlated. developments led conditions support recovery, mutual incoherence irrepresentable condition]. recent years, modifications standard Lasso proposed improve support recovery properties]. paper focus class ?preconditioned lasso? algorithms, pre-multiply suitable matrices yield  prior running lasso. thus, general strategy methods argmin ——?——   preconditioned lasso:    )  Although class algorithms compares favorably Lasso practice, oretical understanding present fairly poor. huang Jojic], example, empirical evaluations, Jia Rohe] Paul. ] asymptotic consistency assumptions. important are, consistency results provide insight relative performance Preconditioned Lasso variants finite data sets. paper provide oretical basis making comparisons. focus paper problems form. ), note core ideas applied algorithms right-multiply and matrices]). instances  discover wher Preconditioned Lasso algorithm. ) improves degrades signed support recovery relative standard Lasso. ). major roadblock one-one comparison auxiliary penalty param? trade penalty quadratic objective. . ). eters, correct choice penalty parameter essential signed support recovery: small, algorithm behaves ordinary squares; large, estimated support empty. unfortunately, simplest cases, pre-multiplying data matrices relative geometry penalty contours elliptical objective contours nontrivial way. suppose wanted compare Lasso Preconditioned Lasso choosing . ). fair comparison, resulting mapfor . ) suitable, matching ping capture change relative geometry induced preconditioning (?, diﬃcult oretically characterize mapping. furi.  rmore, comparative framework built independently choosing meinshausen?uhlmann], example, demonstrate ?ideal? penalty parameters seemingly reasonable oracle estimator lead consistent support recovery lasso. preconditioned Lasso literature problem commonly sidestepped \\x0ceir resorting asymptotic comparisons], empirically comparing regularization paths], modelselection techniques aim choose ?good? matching penalty parameters]. deem approaches unsatisfactory?asymptotic empirical analyses provide limited insight, model selection strategies add layer complexity lead unfair comparisons. view approaches place unnecessary emphasis choices penalty parameter. paper propose alternative strategy compares Lasso Preconditioned Lasso comparing data-dependent upper lower penalty parameter bounds. specifically, give bounds lasso. ) guaranteed recover signed support iff  consequently, signed support recovery possible.   induce Preconditioned Lasso. ) data   bounds comparison Lasso Preconditioned Lasso instance   advantage approach proceeds suitably comparing bounds  upper lower bounds easy compute, general mapping specific penalty parameters readily derived. demonstrate effectiveness framework, analyze Preconditioned Lasso algorithms]. framework make contributions) confirm intuitions advantages disadvantages algorithms proposed) show svd-based construction matrices algorithm] bounds deterministically) show context framework, svd-based construction thought limit point Gaussian construction. paper organized follows. section discuss recent instances. ). outline comparative framework Section highlight consequences] general matrices Section detailed comparisons made generative model section introduce model based block-wise SVD analyze] specific instances generative model. finally, show terms signed support recovery, generative model thought limit point Gaussian construction. section concludes final thoughts. proofs lemmas orems supplementary material. Preconditioned Lasso Algorithms Our interest lies class Preconditioned Lasso algorithms summarized. ). extensions related algorithms] follow readily. section focus recent Preconditioned Lasso examples instantiate matrices appropriately. detailed derivations found supplementary material. reference, denote algorithm author initials. huang Jojic). huang Jojic proposed Correlation Sifting], which, presented preconditioning algorithm, rewritten one. SVD algorithm parameter set smallest left singular vectors amounts setting ) Paul. ] (pbht). earlier instance preconditioning idea put forward Paul. ]. algorithm parameter column indices largest absolute correlation largest). define largest left singular vectors this, PBHT expressed setting ) Jia Rohe). jia Rohe] propose preconditioning method amounts whitening matrix full rank, defines2¿ )      both PBHT estimate basis-dimensional subspace project and however, methods differ substantially assumptions, estimators differ also. empirical results] suggest respective assumptions variety situations. contrast, reweights column space directions requires extra parameter estimated. comparative Framework section propose comparative approach Preconditioned Lasso algorithms derive upper lower bounds avoids choosing penalty parameters   satisfy bounds.  signed support recovery guaranteed iff  compare estimators comparing resulting bounds.  Conditions signed support recovery Before proceeding, make definitions motivated Wainwright]. suppose support set  (?  — simplify notation, assume, off-support set,  denote column submatrix consisting columns indexed set Define variables: For   sgn?  )    sgn? ¿ ) choice smallest singular vectors considered matrices sharply decaying spectrum. note Jia Rohe] square, directly inverted. full rank, pseudo-inverse used.             ) Signed support recovery ) Signed support recovery figure Empirical evaluation penalty parameter bounds Lemma 500 syntic Lasso problems 300, 1000) computed Lemma ran Lasso penalty parameters Figure) Figure), factor,   . figures show empirical probability signed support recovery function factor expected, probabilities change sharply traditional Lasso. ), results (for example) Wainwright] connect settings instances  certify wher Lasso recover signed support. invert results and, instances  derive bounds signed support recovery guaranteed bounds satisfied. specifically, prove Lemma supplementary material. lemma suppose¿ invertible sgn?      lasso unique solution recovers signed support., (?)    maxc) min   denotes indicator function max, denotes hinge function. hand¿ invertible, signed support general recovered. lemma recapitulates well-worn intuitions Lasso diﬃculty recovering signed support. instance, assuming symmetric distribution  small., irrepresentable condition fails hold), tend large. extreme cases signed support recovery impossible. figure empirically validates bounds Lemma estimating probabilities signed support recovery range penalty parameters syntic Lasso problems.  Comparisons paper propose compare preconditioning algorithm traditional Lasso comparing penalty parameter bounds produced Lemma highlighted. precondition?   purpose applying ing framework runs Lasso modified variables Lemma transformations induce noise vector  ?          )   note provided conditions Lemma hold   define updated variables   bounds penalty parameter derived. order comparison scale-invariant, compare algorithms ratios resulting penalty parameter bounds. , deem Preconditioned Lasso algorithm    intuitively, upper bound  effective traditional Lasso  . disproportionately larger relative principle \\x0callows easier tuning     encounter special case case define    signed support recovery preconditioned problem easy.        general impossible. finally, match intuition, define   considered. however, find ratio particuor functions larly intuitive measure. general Comparisons begin comparisons consequences Lemma pbht. order highlight utility proposed framework, focus section special cases framework applied general matrices see, PBHT potential improve signed support recovery relative traditional lasso, provided matrices suitably estimated. notation comparisons: write random variable stochastically ?    ). minimal basis column dominates space submatrix define span .   finally, minimal basis orthogonal complement span consequences. recall Section column basis estimated orem: orem suppose conditions Lemma met fixed instance   span span preconditioning conditions continue hold,  ) stochasticity sides due independent noise vectors hand, invertible, general recover signed support. ¿ brieﬂy sketch proof orem span span plugging definition   derive  )   ) span span easy  notice  unchanged, conditions Lemma hold original Lasso problem¿ invertible sgn?  ), continue hold preconditioned problem. suppose conditions set Lemma met. additional work show?    max  ) min?      independent   note result showing    span span common case span span performance Lasso depends misaligned are. extreme cases¿ singular signed support recovery general possible. consequences pbht. recall Section PBHT column basis estimated orem. orem suppose conditions Lemma met fixed instance   span span preconditioning PBHT conditions continue hold, ) stochasticity sides due independent noise vectors hand, span span PBHT recover signed support. before, sketch proof build intuition. PBHT set does, danger¿ singular. hand, complicates form induced noise vector plugging. ), find   ? however, noise complicated form, derivations supplementary material show span span  )    Orthogonal data Gaussian data Lasso 1000 2000 3000 4000        5000) Empirical validation orems ) Evaluation Gaussian ensembles. figure Experimental evaluations. figure) shows empirical. penalty parameter bounds ratios estimated 1000 variable selection problems. problem consists Gaussians  100, 300, blue curve shows.  estimated original data (lasso). projected data span span dim dim(span variable (see legend), estimated resulting    predicted orems     . updated bounds ratio figure) blue curve shows scale factorp   predicted orem problems constructed. )  ). red curve plots factor estimated Gaussian construction. ) 100, 2000, 200, orem averaged problem instances error bars standard deviation. orem factor approximately , span span    unchanged, conditions Lemma continue hold preconditioned problem hold original Lasso problem. previous equalities established, remainder proof identical orem fact   identical depends crucially fact span span general values differ PBHT sets not. hand, span span distribution depends misaligned are. extreme case span span show?     (?   signed support recovery possible. results remarks. oretical analyses show PBHT lead improved signed support recovery relative Lasso finite datasets. underline findings, empirically validate orems Figure), plot estimated. penalty parameter bounds ratios Lasso Preconditioned Lasso subspaces orems focussed specific settings ors. general, gains PBHT Lasso depend decoy signals suppressed true signal due preserved. furr comparison PBHT analyze subspaces span estimated context assumptions made]. final note concerns dimension subspace span PBHT proposed implicit goal finding basis span requires estimating— adds anor layer complexity algorithms. orems suggest underestimating detrimental signed support recovery overestimating. overestimating trade milder improvement span span poor behavior span span model-based Comparisons previous section Lemma conjunction assumptions make statements pbht. course, quality estimated depends specific instances  hinders general analysis. similarly, direct application Lemma yields bounds exhibit strong dependence. crystallize prototypical examples specializing generative model. section brieﬂy present model show resulting penalty parameter bounds Generative model discussed Section preconditioning algorithms phrased truncating reweighting column subspaces]. suggests natural generative model formulated terms SVD submatrices assume fixedspectrum matrices dimension   respectively. assume paper top left ?diagonal? entries positive remainder zero. furrmore, orthonormal bases dimension     respectively. assume bases chosen uniformly random Stiefel manifold. loss generality, suppose,   }. lasso problem   , ) ensure column norms controlled, compute spectra normal?   arbitrary positive elements diagonal. specifically, izing spectra     ——?    .  ——? ) verify supplementary material assumptions squared column norms expectation (provided orthonormal bases chosen uniformly random). intuition. note matrix decomposed block-wise SVD) orthonormal bases model. ) minor restriction model, set develop intuition, temporarily set  scaling equals columns diﬃculty Lasso lies correctly selecting columns highly correlated columns  Piecewise constant spectra For notational clarity focus special case model. begin, develop notation. previous sections denote basis column space continue notation, columns accordingly,   denote columns diagonal elements identified column indices. , diagonal entries   indexed , }; diagonal entries indexed , }. diagonal entries column set diagonal entries ,   set diagonal entries , ). construct spectrum matrices piecewise constant diagonals.              )     Consequences. Recall¿ orem. orem assume Lasso problem generated generative model. )  ),           ). conditions Lemma hold preconditioning. moreover,     ) words, deterministically scales ratio penalty parameter bounds. proof idea follows. easy¿ invertible. furrmore, show      sgn?  thus, assumptions, preconditions Lemma satisfied original Lasso problem. plugging definitions. ) find SVD column basis. ), diagonal elements determined substituting definitions   preconditioning   )      thus, conditions Lemma hold  continue hold preconditioning. furrmore, notice?       applying   claimed. orem ratio    lemma ratio larger iff  ). indeed,  )  coincides standard lasso.  Extension Gaussian ensembles construction. ) orthonormal matrix column basis sight restrictive. however, show supplementary material, construct Lasso problems Gaussian basis lead penalty parameter bounds ratios converge distribution Lasso problem. ). fixed  generate independent problems: One. ),     ) standard Gaussian ensemble. note constructed low rank generative model bears resemblance Gaussian models considered Paul. . )) Jia Rohe] (proposition). note problem. ) observations noise variance . ) observations noise variance . increased variance matrix expected column length columns length fixed  penalty parameter bounds ratio induced problem.  induced.  result. orem   fixed. conditions Lemma hold  large hold   furrmore,   ) stochasticity left due due thus, respect bounds ratio construction. ) thought limiting construction Gaussian Lasso problems. ) large such. ) proxy restrictive generative models. indeed, experiment Figure) shows, orem predict scaling factor penalty parameter bounds    good accuracy Gaussian ensembles. ratios., conclusions This paper proposes framework comparing Preconditioned Lasso algorithms standard Lasso skirts diﬃculty choosing penalty parameters. eliminating parameter consideration, finite data comparisons greatly simplified, avoiding model selection strategies. demonstrate framework usefulness, applied number Preconditioned Lasso algorithms process confirmed intuitions revealed fragilities mitigation strategies. additionally, presented svd-based generative model Lasso problems thought limit point restrictive Gaussian model. work step comprehensive ory evaluating comparing lasso-style algorithms strategy extended comparing penalized likelihood methods finite datasets. Variable selection core inferential problem multitude statistical analyses. confronted large number (potentially) predictive variables, goal select small subset variables construct parsimonious model. variable selection relevant linear observation models form   , )  matrix features predictors, unknown pdimensional regression parameter, noise vector. high-dimensional settings ordinary squares generally inappropriate. assuming  sparse., support set(?  ? cardinality), mainstay algorithm settings Lasso]: lasso: argmin?    ) For choice variable selection properties Lasso analyzed quan? approximates true support(?   more careful tifying estimated support(?) analyses focus recovering signed support   ?  ?   . oretical developments decade shed light support recovery properties Lasso highlighted practical diﬃculties columns correlated. developments led conditions support recovery, mutual incoherence irrepresentable condition]. recent years, modifications standard Lasso proposed improve support recovery properties]. paper focus class ?preconditioned lasso? algorithms, pre-multiply suitable matrices yield  prior running lasso. thus, general strategy methods argmin ——?——   preconditioned lasso:    )  Although class algorithms compares favorably Lasso practice, oretical understanding present fairly poor. huang Jojic], example, empirical evaluations, Jia Rohe] Paul. ] asymptotic consistency assumptions. important are, consistency results provide insight relative performance Preconditioned Lasso variants finite data sets. paper provide oretical basis making comparisons. although focus paper problems form. ), note core ideas applied algorithms right-multiply and matrices]). for instances  discover wher Preconditioned Lasso algorithm. ) improves degrades signed support recovery relative standard Lasso. ). major roadblock one-one comparison auxiliary penalty param? trade penalty quadratic objective. . ). eters, correct choice penalty parameter essential signed support recovery: small, algorithm behaves ordinary squares; large, estimated support empty. unfortunately, simplest cases, pre-multiplying data matrices relative geometry penalty contours elliptical objective contours nontrivial way. suppose wanted compare Lasso Preconditioned Lasso choosing . ). for fair comparison, resulting mapfor . ) suitable, matching ping capture change relative geometry induced preconditioning (?, diﬃcult oretically characterize mapping. furi.  rmore, comparative framework built independently choosing meinshausen?uhlmann], example, demonstrate ?ideal? penalty parameters seemingly reasonable oracle estimator lead consistent support recovery lasso. Preconditioned Lasso literature problem commonly sidestepped \\x0ceir resorting asymptotic comparisons], empirically comparing regularization paths], modelselection techniques aim choose ?good? matching penalty parameters]. deem approaches unsatisfactory?asymptotic empirical analyses provide limited insight, model selection strategies add layer complexity lead unfair comparisons. view approaches place unnecessary emphasis choices penalty parameter. paper propose alternative strategy compares Lasso Preconditioned Lasso comparing data-dependent upper lower penalty parameter bounds. specifically, give bounds Lasso. ) guaranteed recover signed support iff  consequently, signed support recovery possible.   induce Preconditioned Lasso. ) data   bounds comparison Lasso Preconditioned Lasso instance   advantage approach proceeds suitably comparing bounds  upper lower bounds easy compute, general mapping specific penalty parameters readily derived. demonstrate effectiveness framework, analyze Preconditioned Lasso algorithms]. using framework make contributions) confirm intuitions advantages disadvantages algorithms proposed) show svd-based construction matrices algorithm] bounds deterministically) show context framework, svd-based construction thought limit point Gaussian construction. paper organized follows. Section discuss recent instances. ). outline comparative framework Section highlight consequences] general matrices Section more detailed comparisons made generative model Section introduce model based block-wise SVD analyze] specific instances generative model. finally, show terms signed support recovery, generative model thought limit point Gaussian construction. section concludes final thoughts. proofs lemmas orems supplementary material. Preconditioned Lasso Algorithms Our interest lies class Preconditioned Lasso algorithms summarized. ). extensions related algorithms] follow readily. section focus recent Preconditioned Lasso examples instantiate matrices appropriately. detailed derivations found supplementary material. for reference, denote algorithm author initials. huang Jojic). huang Jojic proposed Correlation Sifting], which, presented preconditioning algorithm, rewritten one. let SVD given algorithm parameter set smallest left singular vectors amounts setting ) Paul. ] (pbht). earlier instance preconditioning idea put forward Paul. ]. for algorithm parameter column indices largest absolute correlation largest). define largest left singular vectors with this, PBHT expressed setting ) Jia Rohe). jia Rohe] propose preconditioning method amounts whitening matrix full rank, defines2¿ )      Both PBHT estimate basis-dimensional subspace project and however, methods differ substantially assumptions, estimators differ also. empirical results] suggest respective assumptions variety situations. contrast, reweights column space directions requires extra parameter estimated. Comparative Framework section propose comparative approach Preconditioned Lasso algorithms derive upper lower bounds avoids choosing penalty parameters   satisfy bounds.  signed support recovery guaranteed iff  compare estimators comparing resulting bounds.  Conditions signed support recovery Before proceeding, make definitions motivated Wainwright]. suppose support set  (?  — simplify notation, assume, off-support set,  Denote column submatrix consisting columns indexed set Define variables: For   sgn?  )    sgn? ¿ ) choice smallest singular vectors considered matrices sharply decaying spectrum. note Jia Rohe] square, directly inverted. full rank, pseudo-inverse used.             ) Signed support recovery ) Signed support recovery figure Empirical evaluation penalty parameter bounds Lemma for 500 syntic Lasso problems 300, 1000) computed Lemma ran Lasso penalty parameters Figure) Figure), factor,   . figures show empirical probability signed support recovery function factor expected, probabilities change sharply for traditional Lasso. ), results (for example) Wainwright] connect settings instances  certify wher Lasso recover signed support. invert results and, instances  derive bounds signed support recovery guaranteed bounds satisfied. specifically, prove Lemma supplementary material. lemma suppose¿ invertible sgn?      Lasso unique solution recovers signed support., (?)    maxc) min   denotes indicator function max, denotes hinge function. hand¿ invertible, signed support general recovered. lemma recapitulates well-worn intuitions Lasso diﬃculty recovering signed support. for instance, assuming symmetric distribution  small., irrepresentable condition fails hold), tend large. extreme cases signed support recovery impossible. figure empirically validates bounds Lemma estimating probabilities signed support recovery range penalty parameters syntic Lasso problems.  Comparisons paper propose compare preconditioning algorithm traditional Lasso comparing penalty parameter bounds produced Lemma highlighted. precondition?   for purpose applying ing framework runs Lasso modified variables Lemma transformations induce noise vector  ?          )   Note Provided conditions Lemma hold   define updated variables   bounds penalty parameter derived. order comparison scale-invariant, compare algorithms ratios resulting penalty parameter bounds. that, deem Preconditioned Lasso algorithm    intuitively, upper bound  effective traditional Lasso  . disproportionately larger relative principle \\x0callows easier tuning     encounter special case case define    signed support recovery preconditioned problem easy.        general impossible. finally, match intuition, define   considered. however, find ratio particuor functions larly intuitive measure. General Comparisons begin comparisons consequences Lemma pbht. order highlight utility proposed framework, focus section special cases framework applied general matrices see, PBHT potential improve signed support recovery relative traditional lasso, provided matrices suitably estimated. notation comparisons: write random variable stochastically ?    ). minimal basis column dominates space submatrix define span .   finally, minimal basis orthogonal complement span Consequences. recall Section column basis estimated orem: orem suppose conditions Lemma met fixed instance   span span preconditioning conditions continue hold,  ) stochasticity sides due independent noise vectors hand, invertible, general recover signed support. ¿ brieﬂy sketch proof orem span span plugging definition   derive  )   ) span span easy  notice  unchanged, conditions Lemma hold original Lasso problem¿ invertible sgn?  ), continue hold preconditioned problem. suppose conditions set Lemma met. with additional work show?    max  ) min?      independent   note result showing    span span common case span span performance Lasso depends misaligned are. extreme cases¿ singular signed support recovery general possible. Consequences pbht. recall Section PBHT column basis estimated orem. orem suppose conditions Lemma met fixed instance   span span preconditioning PBHT conditions continue hold, ) stochasticity sides due independent noise vectors hand, span span PBHT recover signed support. before, sketch proof build intuition. because PBHT set does, danger¿ singular. hand, complicates form induced noise vector plugging. ), find   ? however, noise complicated form, derivations supplementary material show span span  )    Orthogonal data Gaussian data Lasso 1000 2000 3000 4000        5000) Empirical validation orems ) Evaluation Gaussian ensembles. figure Experimental evaluations. figure) shows empirical. penalty parameter bounds ratios estimated 1000 variable selection problems. each problem consists Gaussians  100, 300, blue curve shows.  estimated original data (lasso). projected data span span dim dim(span variable (see legend), estimated resulting    predicted orems     . updated bounds ratio Figure) blue curve shows scale factorp   predicted orem problems constructed. )  ). red curve plots factor estimated Gaussian construction. ) 100, 2000, 200, orem averaged problem instances error bars standard deviation. orem factor approximately , span span  because  unchanged, conditions Lemma continue hold preconditioned problem hold original Lasso problem. with previous equalities established, remainder proof identical orem fact   identical depends crucially fact span span general values differ PBHT sets not. hand, span span distribution depends misaligned are. extreme case span span show?     because(?   signed support recovery possible. results remarks. our oretical analyses show PBHT lead improved signed support recovery relative Lasso finite datasets. underline findings, empirically validate orems Figure), plot estimated. penalty parameter bounds ratios Lasso Preconditioned Lasso subspaces our orems focussed specific settings ors. general, gains PBHT Lasso depend decoy signals suppressed true signal due preserved. furr comparison PBHT analyze subspaces span estimated context assumptions made]. final note concerns dimension subspace span both PBHT proposed implicit goal finding basis span this requires estimating— adds anor layer complexity algorithms. orems suggest underestimating detrimental signed support recovery overestimating. overestimating trade milder improvement span span poor behavior span span model-based Comparisons previous section Lemma conjunction assumptions make statements pbht. course, quality estimated depends specific instances  hinders general analysis. similarly, direct application Lemma yields bounds exhibit strong dependence. crystallize prototypical examples specializing generative model. section brieﬂy present model show resulting penalty parameter bounds Generative model discussed Section preconditioning algorithms phrased truncating reweighting column subspaces]. this suggests natural generative model formulated terms SVD submatrices assume fixedspectrum matrices dimension   respectively. assume paper top left ?diagonal? entries positive remainder zero. furrmore, orthonormal bases dimension     respectively. assume bases chosen uniformly random Stiefel manifold. loss generality, suppose,   }. Lasso problem   , ) ensure column norms controlled, compute spectra normal?   arbitrary positive elements diagonal. specifically, izing spectra     ——?    .  ——? ) verify supplementary material assumptions squared column norms expectation (provided orthonormal bases chosen uniformly random). intuition. note matrix decomposed block-wise SVD) orthonormal bases our model. ) minor restriction model, set develop intuition, temporarily set  scaling equals columns diﬃculty Lasso lies correctly selecting columns highly correlated columns  Piecewise constant spectra For notational clarity focus special case model. begin, develop notation. previous sections denote basis column space continue notation, columns accordingly,   denote columns diagonal elements identified column indices. that, diagonal entries   indexed , }; diagonal entries indexed , }. each diagonal entries column set diagonal entries ,   set diagonal entries , ). construct spectrum matrices piecewise constant diagonals. for             )     Consequences. Recall¿ orem. orem assume Lasso problem generated generative model. )  ),           ). conditions Lemma hold preconditioning. moreover,     ) words, deterministically scales ratio penalty parameter bounds. proof idea follows. easy¿ invertible. furrmore, show      sgn?  thus, assumptions, preconditions Lemma satisfied original Lasso problem. plugging definitions. ) find SVD column basis. ), diagonal elements determined substituting definitions   preconditioning   )      thus, conditions Lemma hold  continue hold preconditioning. furrmore, notice?       applying   claimed. according orem ratio    Lemma ratio larger iff  ). indeed,  )  coincides standard lasso.  Extension Gaussian ensembles construction. ) orthonormal matrix column basis sight restrictive. however, show supplementary material, construct Lasso problems Gaussian basis lead penalty parameter bounds ratios converge distribution Lasso problem. ). for fixed  generate independent problems: One. ),     ) standard Gaussian ensemble. note constructed low rank generative model bears resemblance Gaussian models considered Paul. . )) Jia Rohe] (proposition). note problem. ) observations noise variance . ) observations noise variance . increased variance matrix expected column length columns length fixed  let penalty parameter bounds ratio induced problem.  induced.  result. orem let  fixed. conditions Lemma hold  large hold   furrmore,   ) stochasticity left due due thus, respect bounds ratio construction. ) thought limiting construction Gaussian Lasso problems. ) large such. ) proxy restrictive generative models. indeed, experiment Figure) shows, orem predict scaling factor penalty parameter bounds    good accuracy Gaussian ensembles. ratios., Conclusions This paper proposes framework comparing Preconditioned Lasso algorithms standard Lasso skirts diﬃculty choosing penalty parameters. eliminating parameter consideration, finite data comparisons greatly simplified, avoiding model selection strategies. demonstrate framework usefulness, applied number Preconditioned Lasso algorithms process confirmed intuitions revealed fragilities mitigation strategies. additionally, presented svd-based generative model Lasso problems thought limit point restrictive Gaussian model. work step comprehensive ory evaluating comparing lasso-style algorithms strategy extended comparing penalized likelihood methods finite datasets.',\n",
       " 'PP5132': 'situation samples drawn. distribution unknown covariance compute top eigenvector incremental fashion algorithm maintains estimate top eigenvector) space, incrementally adjusts estimate data point arrives. classical schemes due Krasulina (1969) Oja (1983). give finite-sample convergence rates both. introduction Principal component analysis (pca) popular form dimensionality reduction projects data set top eigenvector) covariance matrix. default method computing eigenvectors space data prohibitive practice. refore interest study incremental schemes data point time, updating estimates desired eigenvectors point. computing eigenvector, methods) space. case top eigenvector, problem long studied, elegant solutions obtained Krasulina] Oja]. methods closely related. time estimate top eigenvector. data point, update estimate follows: XnT XnT  (krasulina kvn XnT kvn XnT (oja) Here ?learning rate? typically proportional. suppose points   drawn. distribution covariance matrix original papers proved estimators converge surely top eigenvector (call mild conditions:    denote top eigenvalues  ekxn suitable (for instance, works). incremental estimators convergence established; see, instance]. paper, analyze rate convergence Krasulina Oja estimators. treated common framework, stochastic approximation algorithms maximizing Rayleigh quotient maximum function achieved  nonzero multiple reof). gradient ) kvk2) Since EXn XnT krasulina method stochastic gradient descent. oja procedure closely related: pointed], identical second-order terms. recently, lot work rates convergence stochastic gradient descent (for instance]), typically limited convex cost functions. results apply non-convex Rayleigh quotient, end, system convergence. analysis focuses buildup finale. measure quality solution time potential function   kvn unit norm. quantity lies range], interested rate approaches zero. result, brief), conditions similar above, stronger. particular, require proportional kxn bounded.  algorithm analyze procedure.  set starting time. set clock time  initialization. initialize Vno uniformly random unit sphere  time   ) Receive data point, ) Update step. perform eir Krasulina Oja update. step similar learning rate form stochastic gradient descent implementations]. adopted initial sequence updates highly noisy: phase moves wildly, shown make progress. behaved step size smaller, \\x0cwhen larger suitable setting start time simply fast-forward analysis moment.  Initialization One initialization set Vno data point arrives, average data points. enough, fail dramatically situations. example. suppose values    coordinate directions small constant. suppose furr distribution single positive number  covariance diag,   ),      )). assume chosen   ); notation, top eigenvalues   ), target vector    orthogonal remain forever. Krasulina Oja updates properties     span vno initialized random data point, probability assigned converge multiple rar likewise, initialized average  data points, constant probability orthogonal remain always. setting Vno random unit vector avoids problem. however, doubtless cases, instance data intrinsic dimension initializer possible.  setting learning rate order sense rates convergence expect, let return random vector values. oja update XnT ignore normalization interested progress potential function correspond coordinate directions, update coordinate    recall initialize Vno random vector unit sphere. simplicity, let suppose initial all-ones vector (again, don worry normalization). iteration coordinate updated probability    exp    . likewise     expectation, time  n2c   (?   kvn )n2c (this rough, made precise obtaining concentration bounds this, achieve) rate  )). refore, assume stating final results, analysis terms general interesting practical question, answer, empirically set prior knowledge eigenvalue gap.   nested sample spaces For denote sigma-field outcomes including start showing time (vno Xno      initially close instance, initial Vno picked uniformly random surface unit sphere expect  . means initial rate decrease small,  term. deal this, divide analysis epochs: takes  , finally drops. martingale large deviation bounds bound length epoch, argue regress. particular, establish sequence times (with high probability)   ) analysis epoch martingale arguments, time, assumes remains bounded above. combining requires careful specification sample space step.  denote sample space realizations (vno xno xno   probability distribution sequences.  define nested sequence spaces  ?0no ?0no     -measurable, probability  consists exclusively realizations   satisfy constraints) including time  build martingale arguments restricting attention computing conditional expectations quantities time Main result make assumptions) . covariance constant kxn  eigenvalues      satisfy step sizes form. conditions, rate convergence Krasulina update. orem. absolute constants holds. pick  set step sizes )), set starting time  /?). nested sequence subsets sample space  ?0no ?0no     have      ? kvn  denotes expectation restricted bound form). result holds Oja update absolute constants. remark small modification final step proof yields rate identical definition details proof, Appendix.  Related work extensive line work analyzing PCA statistical perspective, convergence estimators characterized conditions, including generative models data] assumptions covariance matrix spectrum, eigenvalue spacing]. works provide finite-sample guarantees, apply batch case and computationally intensive, rar eﬃcient incremental algorithm. incremental algorithms, work Warmuth Kuzmin] describes analyzes worst-case online pca, experts-setting algorithm super-quadratic per-iteration cost. eﬃcient general-purpose incremental PCA algorithms lacked finitesample analyses]. recent attempts remedy situation relaxing nonconvexity inherent problem] making generative assumptions]. present paper directly analyzes oldest incremental PCA algorithms mild assumptions. outline proof sketch proof orem; details relegated appendix. recall sigma-field outcomes including time, (vno Xno    additional piece notation: denote/kuk, unit vector direction  thus, instance, Rayleigh quotient written) vbt Expected per-step change potential bound expected improvement step Krasulina Oja algorithms. orem. write  (krasulina (oja) -measurable random variable properties:  (vbn          orem Lemmas  appendix. characterization estimators identical, simplicity henceforth deal krasulina estimator. subsequent results hold oja method, constants.  large deviation bound orem  nonstochastic quantity positive expected value. thus, expectation, modulo small additive term, decreases monotonically. however, amount decrease nth time step arbitrarily small close thus, show eventually bounded. exists time    recall algorithm specification advance \\x0cclock skip pre phase. this, expect? initial estimate vno random unit vector  and, roughly speaking ) suﬃciently large, subsequently increase bit, much. section, establish bound. orem. suppose initial estimate Vno chosen uniformly random surface unit sphere assume step sizes form, constant      .  prove this, start simple recurrence moment-generating function lemma. filtration random variables sequences nonnegative constants which:     takes values interval length [etyn exp )). relation shows define supermartingale based etyn derive large deviation bound lemma. assume conditions Lemma  integer   [etym exp   )  order apply sequence calculate moment-generating function starting lemma. suppose vector picked uniformly random surface unit sphere  define (v12   putting pieces toger yields orem.  Intermediate epochs improvement that, suitable    define series epochs successively doubles, finally drops. this, intermediate goals         , intention that: For      course, hold probability.  denote sample space realizations (vno xno xno  probability distribution sequences. show that, choice )}, constraints) met excluding small portion specific realization   good satisfies). call set   (?)     }.  For technical reasons, realizations good time. specifically, define   (?)     }.  crucially -measurable. note talk expectations distribution restricted subsets particular, restriction, )  expectations respect function  define ?).  Here main result section. orem. assume  pick select schedule    satisfies conditions 8ed       (20c2/?).    step proving orem bounding moment-generating function terms lemma. suppose suppose )).  32t) exp  exp 4n2 result bound terms shift sample spaces easily handled observation. lemma.  nondecreasing  repeated application Lemmas yields following. lemma. suppose conditions) hold.  tc2 32t enj exp     Now bounds moment-generating functions intermediate apply martingale deviation bounds, Lemma, obtain following, orem ensues. lemma. assume conditions) hold. pick set (20c2/?).   pnj    final epoch Recall definition intermediate goals). final epoch period point . consequence Lemmas  captures rate decreases phase. lemma.     solving recurrence relation, piecing toger epochs, convergence result orem. note Lemma closely resembles recurrence relation squared distance optimum stochastic gradient \\x0cdescent (sgd) strongly convex function].  incremental PCA algorithms study convergence rates form SGD scenario. experiments When performing PCA practice massive large/growing dataset, incremental method Krasulina Oja remains practically viable, quadratic-time -memory algorithms increasingly impractical. arora. ] complete discussion empirical necessity incremental PCA algorithms, including version oja method shown extremely competitive practice. eﬃciency benefits types algorithms understood, focus effect learning rate performance oja algorithm (results krasulina extremely similar). cmu PIE faces], consisting 11554 images size , prototypical dataset variance captured pcs, shown fig.  set expect orem discussion introduction varying constant learning rate) inﬂuence rate convergence. particular, low, halving expected halve exponent slope log-log convergence graph (ref. remark thm. ). occurs practice, illustrated fig.  dotted line figure convergence rate, drawn guide. pie Dataset Covariance Spectrum Oja Subspace Rule Dependence 5000 4500 4000 Reconstruction Error Eigenvalue 3500 3000 2500 2000.666.444.296 1500 1000 500 Component Number Iteration Number Figures open problems Several fundamental questions remain unanswered. first, convergence rates incremental schemes depend multiplier learning rate low, convergence slower). high, constant rate convergence large. simple practical scheme setting second, incrementally estimating top eigenvectors, methods extend easily case]; estimate time matrix columns correspond eigenvectors, invariant VnT maintained. oja algorithm, instance, data point arrives, update performed XnT orth step orthonormalizes columns, instance gram-schmidt. interesting characterize rate convergence scheme. finally, analysis applies modified procedure starting time artificially set large constant. unnecessary practice, extend analysis case acknowledgments authors grateful National Science Foundation support grant iis-1162581. situation samples drawn. distribution unknown covariance compute top eigenvector incremental fashion algorithm maintains estimate top eigenvector) space, incrementally adjusts estimate data point arrives. two classical schemes due Krasulina (1969) Oja (1983). give finite-sample convergence rates both. Introduction Principal component analysis (pca) popular form dimensionality reduction projects data set top eigenvector) covariance matrix. default method computing eigenvectors space data prohibitive practice. refore interest study incremental schemes data point time, updating estimates desired eigenvectors point. for computing eigenvector, methods) space. for case top eigenvector, problem long studied, elegant solutions obtained Krasulina] Oja]. methods closely related. time estimate top eigenvector. upon data point, update estimate follows: XnT XnT  (krasulina kvn XnT kvn XnT (oja) Here ?learning rate? typically proportional. suppose points   drawn. distribution covariance matrix original papers proved estimators converge surely top eigenvector (call mild conditions:    denote top eigenvalues  ekxn suitable (for instance, works). incremental estimators convergence established; see, instance]. paper, analyze rate convergence Krasulina Oja estimators. treated common framework, stochastic approximation algorithms maximizing Rayleigh quotient maximum function achieved  nonzero multiple reof). gradient ) kvk2) Since EXn XnT krasulina method stochastic gradient descent. Oja procedure closely related: pointed], identical second-order terms. recently, lot work rates convergence stochastic gradient descent (for instance]), typically limited convex cost functions. results apply non-convex Rayleigh quotient, end, system convergence. most analysis focuses buildup finale. measure quality solution time potential function   kvn unit norm. this quantity lies range], interested rate approaches zero. result, brief), conditions similar above, stronger. particular, require proportional kxn bounded.  algorithm analyze procedure.  set starting time. set clock time  initialization. initialize Vno uniformly random unit sphere  for time   ) Receive data point, ) Update step. perform eir Krasulina Oja update. step similar learning rate form stochastic gradient descent implementations]. adopted initial sequence updates highly noisy: phase moves wildly, shown make progress. behaved step size smaller, \\x0cwhen larger suitable setting start time simply fast-forward analysis moment.  Initialization One initialization set Vno data point arrives, average data points. this enough, fail dramatically situations. here example. suppose values    coordinate directions small constant. suppose furr distribution single positive number  covariance diag,   ),      )). assume chosen   ); notation, top eigenvalues   ), target vector    orthogonal remain forever. this Krasulina Oja updates properties     span Vno initialized random data point, probability assigned converge multiple rar likewise, initialized average  data points, constant probability orthogonal remain always. setting Vno random unit vector avoids problem. however, doubtless cases, instance data intrinsic dimension initializer possible.  setting learning rate order sense rates convergence expect, let return random vector values. Oja update XnT ignore normalization interested progress potential function since correspond coordinate directions, update coordinate    Recall initialize Vno random vector unit sphere. for simplicity, let suppose initial all-ones vector (again, don worry normalization). iteration coordinate updated probability    exp    . likewise     expectation, time  n2c   (?   kvn )n2c (this rough, made precise obtaining concentration bounds from this, achieve) rate  )). refore, assume stating final results, analysis terms general interesting practical question, answer, empirically set prior knowledge eigenvalue gap.   nested sample spaces For denote sigma-field outcomes including start showing time (vno Xno      initially close for instance, initial Vno picked uniformly random surface unit sphere expect  . this means initial rate decrease small,  term. deal this, divide analysis epochs: takes  , finally drops. martingale large deviation bounds bound length epoch, argue regress. particular, establish sequence times (with high probability)   ) analysis epoch martingale arguments, time, assumes remains bounded above. combining requires careful specification sample space step. let denote sample space realizations (vno xno xno   probability distribution sequences. for define nested sequence spaces  ?0no ?0no     -measurable, probability  consists exclusively realizations   satisfy constraints) including time  build martingale arguments restricting attention computing conditional expectations quantities time Main result make assumptions) . covariance constant kxn  eigenvalues      satisfy step sizes form. under conditions, rate convergence Krasulina update. orem. absolute constants holds. pick  set step sizes )), set starting time  /?). nested sequence subsets sample space  ?0no ?0no     have      ? kvn  denotes expectation restricted since bound form). result holds Oja update absolute constants. remark small modification final step proof yields rate identical definition details proof, Appendix.  Related work extensive line work analyzing PCA statistical perspective, convergence estimators characterized conditions, including generative models data] assumptions covariance matrix spectrum, eigenvalue spacing]. such works provide finite-sample guarantees, apply batch case and computationally intensive, rar eﬃcient incremental algorithm. among incremental algorithms, work Warmuth Kuzmin] describes analyzes worst-case online pca, experts-setting algorithm super-quadratic per-iteration cost. more eﬃcient general-purpose incremental PCA algorithms lacked finitesample analyses]. recent attempts remedy situation relaxing nonconvexity inherent problem] making generative assumptions]. present paper directly analyzes oldest incremental PCA algorithms mild assumptions. Outline proof sketch proof orem; details relegated appendix. recall sigma-field outcomes including time, (vno Xno    additional piece notation: denote/kuk, unit vector direction  thus, instance, Rayleigh quotient written) vbt Expected per-step change potential bound expected improvement step Krasulina Oja algorithms. orem. for write  (krasulina (oja) -measurable random variable properties:  (vbn          orem Lemmas  appendix. its characterization estimators identical, simplicity henceforth deal krasulina estimator. all subsequent results hold oja method, constants.  large deviation bound orem  nonstochastic quantity positive expected value. thus, expectation, modulo small additive term, decreases monotonically. however, amount decrease nth time step arbitrarily small close thus, show eventually bounded. exists time    recall algorithm specification advance \\x0cclock skip pre phase. given this, expect? initial estimate vno random unit vector  and, roughly speaking ) suﬃciently large, subsequently increase bit, much. section, establish bound. orem. suppose initial estimate Vno chosen uniformly random surface unit sphere assume step sizes form, constant      .  prove this, start simple recurrence moment-generating function lemma. consider filtration random variables sequences nonnegative constants which:     each takes values interval length [etyn exp )). this relation shows define supermartingale based etyn derive large deviation bound lemma. assume conditions Lemma  integer   [etym exp   )  order apply sequence calculate moment-generating function starting lemma. suppose vector picked uniformly random surface unit sphere  define (v12   Putting pieces toger yields orem.  Intermediate epochs improvement that, suitable    define series epochs successively doubles, finally drops. this, intermediate goals         , intention that: For      course, hold probability. let denote sample space realizations (vno xno xno  probability distribution sequences. show that, choice )}, constraints) met excluding small portion specific realization   good satisfies). call set   (?)     }.  For technical reasons, realizations good time. specifically, define   (?)     }.  crucially -measurable. also note talk expectations distribution restricted subsets particular, restriction, )  expectations respect function  define ?).  Here main result section. orem. assume  pick select schedule    satisfies conditions 8ed       (20c2/?).    step proving orem bounding moment-generating function terms lemma. suppose suppose )).  32t) exp  exp 4n2 result bound terms shift sample spaces easily handled observation. lemma.  nondecreasing  repeated application Lemmas yields following. lemma. suppose conditions) hold.  tc2 32t enj exp     Now bounds moment-generating functions intermediate apply martingale deviation bounds, Lemma, obtain following, orem ensues. lemma. assume conditions) hold. pick set (20c2/?).   pnj    final epoch Recall definition intermediate goals). final epoch period point . consequence Lemmas  captures rate decreases phase. lemma. for    solving recurrence relation, piecing toger epochs, convergence result orem. note Lemma closely resembles recurrence relation squared distance optimum stochastic gradient \\x0cdescent (sgd) strongly convex function].  incremental PCA algorithms study convergence rates form SGD scenario. Experiments When performing PCA practice massive large/growing dataset, incremental method Krasulina Oja remains practically viable, quadratic-time -memory algorithms increasingly impractical. arora. ] complete discussion empirical necessity incremental PCA algorithms, including version oja method shown extremely competitive practice. since eﬃciency benefits types algorithms understood, focus effect learning rate performance oja algorithm (results krasulina extremely similar). CMU PIE faces], consisting 11554 images size , prototypical dataset variance captured pcs, shown fig.  set expect orem discussion introduction varying constant learning rate) inﬂuence rate convergence. particular, low, halving expected halve exponent slope log-log convergence graph (ref. remark thm. ). this occurs practice, illustrated fig.  dotted line figure convergence rate, drawn guide. PIE Dataset Covariance Spectrum Oja Subspace Rule Dependence 5000 4500 4000 Reconstruction Error Eigenvalue 3500 3000 2500 2000.666.444.296 1500 1000 500 Component Number Iteration Number Figures Open problems Several fundamental questions remain unanswered. first, convergence rates incremental schemes depend multiplier learning rate low, convergence slower). high, constant rate convergence large. simple practical scheme setting second, incrementally estimating top eigenvectors, both methods extend easily case]; estimate time matrix columns correspond eigenvectors, invariant VnT maintained. oja algorithm, instance, data point arrives, update performed XnT orth step orthonormalizes columns, instance gram-schmidt. interesting characterize rate convergence scheme. finally, analysis applies modified procedure starting time artificially set large constant. this unnecessary practice, extend analysis case acknowledgments authors grateful National Science Foundation support grant iis-1162581.',\n",
       " 'PP5246': 'standard optimization criterion infinite horizon Markov decision process (mdp) expected sum (discounted) costs., finding policy minimizes function initial state system). applications, prefer minimize measure risk addition standard optimization criterion. cases, criterion incorporates penalty variability (due stochastic nature system) induced policy. risk-sensitive MDPs], objective minimize risk-sensitive criterion expected exponential utility], variance-related measure], percentile performance]. issue construct criteria manner conceptually meaningful mamatically tractable open question. losses (returns) distributed, typical Markowitz mean-variance optimization], relies moments loss (return) distribution, dominated risk management years. numerous alternatives mean-variance optimization emerged literature, clear leader alternative \\x0crisk-sensitive objective functions. value-risk (var) conditional valueat-risk (cvar) promising alternatives quantify losses encountered tail loss distribution, thus, received high status risk management. (continuous) loss distributions, var? measures risk maximum loss incurred. confidence level cvar? measures expected loss loss greater equal var?  VaR popular risk measure, cvar computational advantages VaR boosted development CVaR optimization techniques. provide exact definitions risk measures brieﬂy discuss var shortcomings Section cvar minimization developed Rockafellar Uryasev] numerical effectiveness demonstrated portfolio optimization option hedging problems. work extended objective functions consist combinations expected loss cvar, minimization expected loss subject constraint cvar. objective function  part work completed Yinlam chow internship Adobe research. mohammad Ghavamzadeh Adobe research, leave absence INRIA Lille Team sequel. study paper, proposed algorithms easily extended cvar-related objective functions. boda Filar?auerle Ott, extended results] MDPs (sequential decision-making). proposed dynamic programming) optimize cvar, approach limited small problems, showed finite infinite horizon mdps, exists deterministic historydependent optimal policy CVaR optimization (see Section details). work risk-sensitive sequential decision-making context MDPs (when model known) work reinforcement learning) framework. risk-sensitive, mention work Borkar] considered expected exponential utility Tamar. ] Prashanth Ghavamzadeh] variance-related risk measures. cvar optimization rar subject. morimura. ] estimate return distribution exploring cvar-based risksensitive policy. algorithm scale large problems. petrik Subramanian] propose method based stochastic dual optimize CVaR large-scale mdps. however, method limited linearly controllable problems. borkar Jain] finitehorizon MDP CVaR constraint sketch stochastic approximation algorithm solve. finally, Tamar. ] recently proposed policy gradient algorithm CVaR optimization. paper, develop policy gradient) actorcritic) algorithms mean-cvar optimization mdps. derive formula computing gradient risk-sensitive objective function. propose methods estimate gradient incrementally system trajectories (update time-step. update observing \\x0cone trajectories). gradient estimations devise algorithms update policy parameters descent direction. ordinary differential equations (ode) approach, establish asymptotic convergence algorithms locally risk-sensitive optimal policies. finally, demonstrate usefulness algorithms optimal stopping problem. comparison], develop algorithm CVaR optimization stochastic shortest path problems considers continuous loss distributions, biased estimator var, incremental, comprehensive convergence proof, study mean-cvar optimization, discrete continuous loss distributions, devise (several) algorithms (trajectory-based incremental helps reducing variance algorithms), establish convergence proof algorithms.     , preliminaries problems agent interaction environment modeled mdp. mdp tuple, state action spaces,  max Cmax bounded cost random variable expectation denoted, transition probability distribution; (?) initial state distribution. simplicity, assume system single initial state results paper easily extended case system initial state. rule agent selects actions state. stationary policy ) probability distribution actions, conditioned current state. policy gradient actor-critic methods, define class parameterized stochastic policies ;       estimate gradient performance measure. policy parameters observed system trajectories, improve policy adjusting parameters direction gradient. setting policy represented -dimensional parameter vector policy dependent functions written function place ,  interchangeably paper. denote ??     ??? ??  ) ?-discounted visiting distribution state state-action pair, policy respectively. bounded-mean random variable—] cumulative distribution function ., loss investment strategy ?). define value-risk confidence level  , var? ) min)   minimum attained non-decreasing right-continuous continuous strictly increasing, var? ) unique satisfying) orwise, VaR equation solution range solutions. VaR popular risk measure, suffers unstable diﬃcult work numerically \\x0cnormally distributed, case loss distributions tend exhibit fat tails empirical discreteness. moreover, VaR coherent risk measure] importantly quantify losses suffered ?-tail distribution]. alternative measure addresses var shortcomings conditional value-atrisk, cvar? ), ?-tail distribution probability atom var? ), cvar? ) unique defined cvar? ) var? ) rockafellar Uryasev] showed cvar? ) min , min      ?? )+ max, represents positive part note function (?, finite convex (hence continuous). cvar Optimization MDPs For policy define loss state (state-action pair)) sum (discounted) costs encountered Pagent starts state (state-action pairp))  policy., )     expected random variables action-value functions policy., ) ) , , goal standard discounted formulation find optimal policy argmin?   For CVaR optimization mdps, optimization problem: For confidence level  , loss tolerance  min  subject cvar?    ) orem], optimization problem) equivalent? defined)) min  ?,? subject     ) solve), employ Lagrangian relaxation procedure] convert unconstrained problem: max min(?,         ?,? ) lagrange multiplier. goal find saddle point(?, ., point (??    satisfies(?,  (??     (??        achieved descending (?, ascending gradients(?, .   (?,          (?,                 (?,        ) assume exists policy ?(?—?  cvar?    (feasibility assumption). discussed Section?auerle Ott, showed exists deterministic history-dependent optimal policy CVaR optimization. important point policy depend complete history, current time step current state system accumulated discounted cost  following, present policy gradient) algorithm (sec. actor-critic) algorithms (sec. optimize). algorithm updates parameters observing trajectories, algorithms incremental update parameters time-step. notation) means right-most term member sub-gradient set (?, ?). trajectory-based Policy Gradient Algorithm section, present policy gradient algorithm solve optimization problem). unit observation algorithm system trajectory generated current policy. iteration, algorithm generates trajectories current policy, estimate gradients eqs. , estimates update parameters     trajectory generated policy terminal state system. visits terminal state, enters recurring sink state time step, incurring cost  time index referred stopping time mdp. transition stochastic, non-deterministic quantity. assume policy proper.,     furr means probability MDP exits transient states hits (and stays finite time simplicity, assume agent incurs cost terminal state. analogous results general case non-zero terminal cost derived identical arguments. loss probability defined(?)   (?)    respectively. easily shown log (?)  log  ?). algorithm pseudo-code proposed policy gradient algorithm. appears inside parenses right-hand-side update equations estimates gradients(?, .  (estimates eqs. ) (see Appendix]).  operator projects vector   closest point compact convex set   max Cmax  projection operators ?? ?? , ?max respectively. projection operators ensure convergence algorithm. step-size schedules satisfy standard conditions stochasticapproximation algorithms, ensure VaR parameter update ison fastest time-scale) policy parameter update intermediate time-scale) Lagrange multiplier update slowest time-scale) (see Appendix] conditions step-size schedules). results timescale stochastic approximation algorithm. prove policy gradient algorithm converges (local) saddle point risk-sensitive objective function(?, (see Appendix]). Algorithm trajectory-based Policy Gradient Algorithm CVaR Optimization input: parameterized policy ?(?—?  confidence level loss tolerance initialization: policy parameter var parameter lagrangian parameter   generate trajectories starting current policy end     update  )     update  ) log  log      update )       end return parameters incremental actor-critic Algorithms mentioned Section unit observation policy gradient algorithm (algorithm system trajectory. result high variance gradient estimates, length trajectories long. address issue, section, propose actor-critic algorithms linear approximation quantities gradient estimates update parameters incrementally (after state-action transition). present actor-critic algorithms optimizing risk-sensitive measure). algorithms based gradient estimates Sections. algorithm (spsa-based) fully incremental updates parameters time-step, updates time-step updates  end trajectory, semi trajectory-based. algorithm pseudo-code algorithms. projection operators   defined Section ensure convergence algorithms. step-size schedules satisfy standard conditions forstochastic approximation algorithms, ensures critic update fastest time-scale ) policy VaR parameter updates interme4 \\x0cdiate time-scale, ?-update) faster ?-update ) finally Lagrange multiplier update slowest time-scale) (see Appendix] conditions step-size schedules). results time-scale stochastic approximation algorithms. prove actor-critic algorithms converge (local) saddle point risk-sensitive objective function(?, (see Appendix]).  Gradient. policy Parameters gradient objective function. policy parameters ) rewritten (?,          ) Given original MDP parameter define augmented MDP ?       }, ,   , , orwise, , orwise terminal state original MDP part state policy reaches terminal state steps    define class parameterized stochastic policies , ,       augmented mdp. thus, total (discounted) loss trajectory written           ) From), clear quantity parensis) function policy  .,  ?). thus, easy show state augmented MDP equality. result policy gradient orem]) (?,        log ,  ),  ) ??? discounted visiting distribution (defined Section action-value show log   function policy augmented MDP ??     unbiased estimate (?,  unbiased estimator (see]). temporal-difference) error actor-critic algorithms, critic linear approximation function ,  ), feature vector ?(?) belongs low-dimensional space  \\x0cgradient. lagrangian Parameter rewrite gradient objective function. lagrangian parameters ) (?,            )      ?). ) Similar Section) fact quantity parensis) note  function policy state augmented MDP dependence   definition cost function derive expression   turn give expression (?, ?). written   lemma gradient  . lagrangian parameter    )+     ) proof. Appendix]. ??) ) unbiased From Lemma), easy   estimate (?, ?). issue estimator fixed  system trajectory, end  ??)  affect incremental nature actor-critic algorithm. address issue, propose approach estimate gradients.   sec.  free). anor important issue estimator unbiased samples generated distribution ???  ?). follow policy, ??)  estimate (?, ?). note issue discounted actorcritic algorithms (likelihood ratio based) estimate gradient unbiased samples generated ??? simply follow policy. reason convergence analysis knowledge) (likelihood ratio based) discounted actor-critic algorithms sub-gradient. var Parameter  rewrite sub-gradient objective function. var parameter .  (?,            probability) written definition augmented MDP reach terminal state,  part state., (see Section). thus, rewrite) \\x0c?? (?,         ) From), easy     unbiased estimate sub-gradient(?, .  issue (unbiased) estimator applied end system trajectory., reach terminal state thus, prevents fully incremental algorithm. fact, estimator semi trajectory-based actor-critic algorithm. approach estimate sub-gradient incrementally simultaneous perturbation stochastic approximation (spsa) method]. idea SPSA estimate subgradient(?)   (?, values        positive perturbation (see] detailed description  order SPSA estimate sub-gradient incrementally, note (?,    )        ?).   ) Similar Sections) fact quantity parensis)   function policy state augmented MDP critic linear approximation function., ,  ), actor-critic algorithms (see Section Algorithm), SPSA estimate sub-gradient form(?)         ?.  Alternative Approach Compute Gradients section, present alternative compute gradients.   estimate gradient.  (more) incremental fashion (compared method Section), cost linear function approximators Note discounted actor-critic algorithm convergence proof] based spsa. spsa-based gradient estimate proposed] widely settings, involving highdimensional parameter. spsa estimate two-sided. implemented single-sided, values function   refer readers] details SPSA] application learning risk-sensitive mdps. (instead Algorithm). approach, define augmented MDP slightly Section. difference definition cost function, defined (note, replaced removed)   orwise, xht terminal state original MDP easy term   appearing gradients eqs.  \\x0cvalue function pol??) icy state augmented mdp. result, Gradient.  easy gradient. gradient function original mdp,   times gradient function augmented mdp,   initial states MDPs (with abuse notation, function mdps). thus, linear approximators, , functions original augmented mdps, (?, estimated log    -errors mdps. gradient.  similar case easy gradient.    function augmented mdp,  thus, estimated incrementally (?,      ). sub-gradient.  this sub-gradient.  times gradient.  function augmented mdp,   thus, estimated incrementally  )??   spsa   algorithm Appendix] pseudo-code resulting algorithm. algorithm actor-critic Algorithms CVaR Optimization input: Parameterized policy ?(?—?  function feature vector ?(?) (both augmented confidence level loss tolerance mdp), initialization: policy parameters var parameter lagrangian parameter function weight vector) spsa-based algorithm:    (with draw action   Observe cost Observe state   note       error) Critic update )     ) update  ) update   log   ?? ) update )      (reach terminal state), set end) Semi trajectory-based algorithm:    (with state Draw action   observe cost   Update eqs. , Update eqs. ; Update  update  )  ?? set end end return policy function parameters Experimental Results optimal stopping problem state time step consists cost time), \\x0cstopping time. agent (buyer) decide eir accept present cost wait. accepts system reaches terminal state cost received, orwise, receives cost state. .   constants). moreover, discounted factor  , account increase buyer affordability. problem details Appendix]. note change cost reward minimization maximization, American option pricing problem, standard testbed evaluate risk-sensitive algorithms]). state space continuous, finding exact solution infeasible, thus, requires approximation sampling techniques. compare performance risk-sensitive policy gradient Algorithm-cvar) actor-critic Algorithms-cvar-spsa-cvarsemi-traj) risk-neutral counterparts) (see Appendix] details experiments). figure shows distribution discounted cumulative cost  policy learned algorithms. results risk-sensitive algorithms yield higher expected loss, variance, compared risk-neutral methods. precisely, loss distributions risksensitive algorithms lower righttail risk-neutral counterparts. table summarizes performance algorithms. numbers reiterate concluded Figure mean?cvar Mean Probability Probability Reward mean?cvar mean?cvar SPSA Mean 100 Reward Figure Loss distributions policies learned risk-sensitive risk-neutral policy gradient actor critic algorithms. left figures correspond methods, figures correspond algorithms. cases, loss tolerance equals . -cvar-cvar-spsa-cvar-semi-traj. ? ?  cvar?  122 Table Performance comparison policies learned risk-sensitive risk-neutral algorithms. conclusions Future Work proposed policy gradient actor critic) algorithms CVaR optimization mdps. provided proofs convergence]) locally risk-sensitive optimal policies proposed algorithms. furr, \\x0coptimal stopping problem, observed algorithms resulted policies loss distributions lower right-tail compared risk-neutral counterparts. extremely important risk averse decision-maker, righttail catastrophic losses. future work includes: Providing convergence proofs algorithms samples generated policy discounted visiting distribution, Using importance sampling methods] improve gradient estimates right-tail loss distribution (worst-case events observed low probability) CVaR objective function, Evaluating algorithms challenging problems. acknowledgement authors Professor Marco Pavone Lucas Janson comments helped technical details proofs algorithms. standard optimization criterion infinite horizon Markov decision process (mdp) expected sum (discounted) costs., finding policy minimizes function initial state system). however applications, prefer minimize measure risk addition standard optimization criterion. cases, criterion incorporates penalty variability (due stochastic nature system) induced policy. risk-sensitive MDPs], objective minimize risk-sensitive criterion expected exponential utility], variance-related measure], percentile performance]. issue construct criteria manner conceptually meaningful mamatically tractable open question. although losses (returns) distributed, typical Markowitz mean-variance optimization], relies moments loss (return) distribution, dominated risk management years. numerous alternatives mean-variance optimization emerged literature, clear leader alternative \\x0crisk-sensitive objective functions. value-risk (var) conditional valueat-risk (cvar) promising alternatives quantify losses encountered tail loss distribution, thus, received high status risk management. for (continuous) loss distributions, var? measures risk maximum loss incurred. confidence level cvar? measures expected loss loss greater equal var?  although VaR popular risk measure, cvar computational advantages VaR boosted development CVaR optimization techniques. provide exact definitions risk measures brieﬂy discuss var shortcomings Section cvar minimization developed Rockafellar Uryasev] numerical effectiveness demonstrated portfolio optimization option hedging problems. work extended objective functions consist combinations expected loss cvar, minimization expected loss subject constraint cvar. this objective function  part work completed Yinlam chow internship Adobe research. mohammad Ghavamzadeh Adobe research, leave absence INRIA Lille Team sequel. study paper, proposed algorithms easily extended cvar-related objective functions. boda Filar?auerle Ott, extended results] MDPs (sequential decision-making). while proposed dynamic programming) optimize cvar, approach limited small problems, showed finite infinite horizon mdps, exists deterministic historydependent optimal policy CVaR optimization (see Section details). most work risk-sensitive sequential decision-making context MDPs (when model known) work reinforcement learning) framework. risk-sensitive, mention work Borkar] considered expected exponential utility Tamar. ] Prashanth Ghavamzadeh] variance-related risk measures. cvar optimization rar subject. morimura. ] estimate return distribution exploring cvar-based risksensitive policy. algorithm scale large problems. petrik Subramanian] propose method based stochastic dual optimize CVaR large-scale mdps. however, method limited linearly controllable problems. borkar Jain] finitehorizon MDP CVaR constraint sketch stochastic approximation algorithm solve. finally, Tamar. ] recently proposed policy gradient algorithm CVaR optimization. paper, develop policy gradient) actorcritic) algorithms mean-cvar optimization mdps. derive formula computing gradient risk-sensitive objective function. propose methods estimate gradient incrementally system trajectories (update time-step. update observing \\x0cone trajectories). gradient estimations devise algorithms update policy parameters descent direction. using ordinary differential equations (ode) approach, establish asymptotic convergence algorithms locally risk-sensitive optimal policies. finally, demonstrate usefulness algorithms optimal stopping problem. comparison], develop algorithm CVaR optimization stochastic shortest path problems considers continuous loss distributions, biased estimator var, incremental, comprehensive convergence proof, study mean-cvar optimization, discrete continuous loss distributions, devise (several) algorithms (trajectory-based incremental helps reducing variance algorithms), establish convergence proof algorithms.     , Preliminaries problems agent interaction environment modeled mdp. MDP tuple, state action spaces,  max Cmax bounded cost random variable expectation denoted, transition probability distribution; (?) initial state distribution. for simplicity, assume system single initial state all results paper easily extended case system initial state. rule agent selects actions state. stationary policy ) probability distribution actions, conditioned current state. policy gradient actor-critic methods, define class parameterized stochastic policies ;       estimate gradient performance measure. policy parameters observed system trajectories, improve policy adjusting parameters direction gradient. since setting policy represented -dimensional parameter vector policy dependent functions written function place ,  interchangeably paper. denote ??     ??? ??  ) ?-discounted visiting distribution state state-action pair, policy respectively. let bounded-mean random variable—] cumulative distribution function ., loss investment strategy ?). define value-risk confidence level  , var? ) min)   here minimum attained non-decreasing right-continuous when continuous strictly increasing, var? ) unique satisfying) orwise, VaR equation solution range solutions. although VaR popular risk measure, suffers unstable diﬃcult work numerically \\x0cnormally distributed, case loss distributions tend exhibit fat tails empirical discreteness. moreover, VaR coherent risk measure] importantly quantify losses suffered ?-tail distribution]. alternative measure addresses var shortcomings conditional value-atrisk, cvar? ), ?-tail distribution probability atom var? ), cvar? ) unique defined cvar? ) var? ) rockafellar Uryasev] showed cvar? ) min , min      ?? )+ max, represents positive part note function (?, finite convex (hence continuous). CVaR Optimization MDPs For policy define loss state (state-action pair)) sum (discounted) costs encountered Pagent starts state (state-action pairp))  policy., )     expected random variables action-value functions policy., ) ) , , goal standard discounted formulation find optimal policy argmin?   For CVaR optimization mdps, optimization problem: For confidence level  , loss tolerance  min  subject cvar?    ) orem], optimization problem) equivalent? defined)) min  ?,? subject     ) solve), employ Lagrangian relaxation procedure] convert unconstrained problem: max min(?,         ?,? ) Lagrange multiplier. goal find saddle point(?, ., point (??    satisfies(?,  (??     (??        this achieved descending (?, ascending gradients(?, .   (?,          (?,                 (?,        ) assume exists policy ?(?—?  cvar?    (feasibility assumption). discussed Section?auerle Ott, showed exists deterministic history-dependent optimal policy CVaR optimization. important point policy depend complete history, current time step current state system accumulated discounted cost  following, present policy gradient) algorithm (sec. actor-critic) algorithms (sec. optimize). while algorithm updates parameters observing trajectories, algorithms incremental update parameters time-step. notation) means right-most term member sub-gradient set (?, ?). trajectory-based Policy Gradient Algorithm section, present policy gradient algorithm solve optimization problem). unit observation algorithm system trajectory generated current policy. iteration, algorithm generates trajectories current policy, estimate gradients eqs. , estimates update parameters let    trajectory generated policy terminal state system. after visits terminal state, enters recurring sink state time step, incurring cost  time index referred stopping time mdp. since transition stochastic, non-deterministic quantity. here assume policy proper.,     this furr means probability MDP exits transient states hits (and stays finite time for simplicity, assume agent incurs cost terminal state. analogous results general case non-zero terminal cost derived identical arguments. loss probability defined(?)   (?)    respectively. easily shown log (?)  log  ?). algorithm pseudo-code proposed policy gradient algorithm. what appears inside parenses right-hand-side update equations estimates gradients(?, .  (estimates eqs. ) (see Appendix]).  operator projects vector   closest point compact convex set   max Cmax  projection operators ?? ?? , ?max respectively. projection operators ensure convergence algorithm. step-size schedules satisfy standard conditions stochasticapproximation algorithms, ensure VaR parameter update ison fastest time-scale) policy parameter update intermediate time-scale) Lagrange multiplier update slowest time-scale) (see Appendix] conditions step-size schedules). this results timescale stochastic approximation algorithm. prove policy gradient algorithm converges (local) saddle point risk-sensitive objective function(?, (see Appendix]). Algorithm trajectory-based Policy Gradient Algorithm CVaR Optimization input: parameterized policy ?(?—?  confidence level loss tolerance initialization: policy parameter VaR parameter Lagrangian parameter   Generate trajectories starting current policy end     update  )     update  ) log  log      update )       end return parameters Incremental actor-critic Algorithms mentioned Section unit observation policy gradient algorithm (algorithm system trajectory. this result high variance gradient estimates, length trajectories long. address issue, section, propose actor-critic algorithms linear approximation quantities gradient estimates update parameters incrementally (after state-action transition). present actor-critic algorithms optimizing risk-sensitive measure). algorithms based gradient estimates Sections. while algorithm (spsa-based) fully incremental updates parameters time-step, updates time-step updates  end trajectory, semi trajectory-based. algorithm pseudo-code algorithms. projection operators   defined Section ensure convergence algorithms. step-size schedules satisfy standard conditions forstochastic approximation algorithms, ensures critic update fastest time-scale ) policy VaR parameter updates interme4 \\x0cdiate time-scale, ?-update) faster ?-update ) finally Lagrange multiplier update slowest time-scale) (see Appendix] conditions step-size schedules). this results time-scale stochastic approximation algorithms. prove actor-critic algorithms converge (local) saddle point risk-sensitive objective function(?, (see Appendix]).  Gradient. Policy Parameters gradient objective function. policy parameters ) rewritten (?,          ) Given original MDP parameter define augmented MDP ?       }, ,   , , orwise, , orwise terminal state original MDP part state policy reaches terminal state steps    define class parameterized stochastic policies , ,       augmented mdp. thus, total (discounted) loss trajectory written           ) From), clear quantity parensis) function policy  .,  ?). thus, easy show state augmented MDP equality. result policy gradient orem]) (?,        log ,  ),  ) ??? discounted visiting distribution (defined Section action-value show log   function policy augmented MDP ??     unbiased estimate (?,  unbiased estimator (see]). temporal-difference) error actor-critic algorithms, critic linear approximation function ,  ), feature vector ?(?) belongs low-dimensional space  \\x0cgradient. Lagrangian Parameter rewrite gradient objective function. Lagrangian parameters ) (?,            )      ?). ) Similar Section) fact quantity parensis) note  function policy state augmented MDP dependence   definition cost function derive expression   turn give expression (?, ?). written   Lemma gradient  . Lagrangian parameter    )+     ) proof. see Appendix]. ??) ) unbiased From Lemma), easy   estimate (?, ?). issue estimator fixed  system trajectory, end  ??)  this affect incremental nature actor-critic algorithm. address issue, propose approach estimate gradients.   sec.  free). anor important issue estimator unbiased samples generated distribution ???  ?). follow policy, ??)  estimate (?, ?). note issue discounted actorcritic algorithms (likelihood ratio based) estimate gradient unbiased samples generated ??? simply follow policy. this reason convergence analysis knowledge) (likelihood ratio based) discounted actor-critic algorithms sub-gradient. VaR Parameter  rewrite sub-gradient objective function. VaR parameter .  (?,            probability) written from definition augmented MDP reach terminal state,  part state., (see Section). thus, rewrite) \\x0c?? (?,         ) From), easy     unbiased estimate sub-gradient(?, .  issue (unbiased) estimator applied end system trajectory., reach terminal state thus, prevents fully incremental algorithm. fact, estimator semi trajectory-based actor-critic algorithm. one approach estimate sub-gradient incrementally simultaneous perturbation stochastic approximation (spsa) method]. idea SPSA estimate subgradient(?)   (?, values        positive perturbation (see] detailed description  order SPSA estimate sub-gradient incrementally, note (?,    )        ?).   ) Similar Sections) fact quantity parensis) since  function policy state augmented MDP critic linear approximation function., ,  ), actor-critic algorithms (see Section Algorithm), SPSA estimate sub-gradient form(?)         ?.  Alternative Approach Compute Gradients section, present alternative compute gradients.   this estimate gradient.  (more) incremental fashion (compared method Section), cost linear function approximators Note discounted actor-critic algorithm convergence proof] based spsa. spsa-based gradient estimate proposed] widely settings, involving highdimensional parameter. SPSA estimate two-sided. implemented single-sided, values function   refer readers] details SPSA] application learning risk-sensitive mdps. (instead Algorithm). approach, define augmented MDP slightly Section. difference definition cost function, defined (note, replaced removed)   orwise, xht terminal state original MDP easy term   appearing gradients eqs.  \\x0cvalue function pol??) icy state augmented mdp. result, Gradient.  easy gradient. gradient function original mdp,   times gradient function augmented mdp,   initial states MDPs (with abuse notation, function mdps). thus, linear approximators, , functions original augmented mdps, (?, estimated log    -errors mdps. gradient.  Similar case easy gradient.    function augmented mdp,  thus, estimated incrementally (?,      ). sub-gradient.  This sub-gradient.  times gradient.  function augmented mdp,   thus, estimated incrementally  )??   SPSA   algorithm Appendix] pseudo-code resulting algorithm. algorithm actor-critic Algorithms CVaR Optimization input: Parameterized policy ?(?—?  function feature vector ?(?) (both augmented confidence level loss tolerance mdp), initialization: policy parameters VaR parameter Lagrangian parameter function weight vector) spsa-based algorithm:    (with Draw action   Observe cost Observe state   note       error) Critic update )     ) update  ) update   log   ?? ) update )      (reach terminal state), set end) Semi trajectory-based algorithm:    (with state Draw action   observe cost   Update eqs. , Update eqs. ; Update  update  )  ?? set end end return policy function parameters Experimental Results optimal stopping problem state time step consists cost time), \\x0cstopping time. agent (buyer) decide eir accept present cost wait. accepts system reaches terminal state cost received, orwise, receives cost state. .   constants). moreover, discounted factor  , account increase buyer affordability. problem details Appendix]. note change cost reward minimization maximization, American option pricing problem, standard testbed evaluate risk-sensitive algorithms]). since state space continuous, finding exact solution infeasible, thus, requires approximation sampling techniques. compare performance risk-sensitive policy gradient Algorithm-cvar) actor-critic Algorithms-cvar-spsa-cvarsemi-traj) risk-neutral counterparts) (see Appendix] details experiments). figure shows distribution discounted cumulative cost  policy learned algorithms. results risk-sensitive algorithms yield higher expected loss, variance, compared risk-neutral methods. more precisely, loss distributions risksensitive algorithms lower righttail risk-neutral counterparts. table summarizes performance algorithms. numbers reiterate concluded Figure mean?cvar Mean Probability Probability Reward mean?cvar mean?cvar SPSA Mean 100 Reward Figure Loss distributions policies learned risk-sensitive risk-neutral policy gradient actor critic algorithms. left figures correspond methods, figures correspond algorithms. cases, loss tolerance equals . -cvar-cvar-spsa-cvar-semi-traj. ? ?  cvar?  122 Table Performance comparison policies learned risk-sensitive risk-neutral algorithms. Conclusions Future Work proposed policy gradient actor critic) algorithms CVaR optimization mdps. provided proofs convergence]) locally risk-sensitive optimal policies proposed algorithms. furr, \\x0coptimal stopping problem, observed algorithms resulted policies loss distributions lower right-tail compared risk-neutral counterparts. this extremely important risk averse decision-maker, righttail catastrophic losses. future work includes: Providing convergence proofs algorithms samples generated policy discounted visiting distribution, Using importance sampling methods] improve gradient estimates right-tail loss distribution (worst-case events observed low probability) CVaR objective function, Evaluating algorithms challenging problems. acknowledgement authors Professor Marco Pavone Lucas Janson comments helped technical details proofs algorithms.',\n",
       " 'PP5249': 'importance sampling essential component off-policy model-free reinforcement learning algorithms. however, effective variant, weighted importance sampling, carry easily function approximation and, this, utilized existing off-policy learning algorithms. paper, steps bridging gap. first, show weighted importance sampling viewed special case weighting error individual training samples, weighting oretical empirical benefits similar weighted importance sampling. second, show benefits extend weighted-importance-sampling version offpolicy lstd( show empirically wis-lstd( algorithm result rapid reliable convergence conventional off-policy lstd 2010, Bertsekas 2009). importance sampling weighted importance sampling Importance sampling (kahn Marshall 1953, Rubinstein 1981, Koller Friedman 2009) wellknown Monte Carlo technique estimating expectation distribution samples distribution. data samples generated. sample distribution interested estimating expected samples distribution importance sampling achieved simply averaging samples weighted ratio likelihoods called importance-sampling ratio. , estimated:  ) This unbiased estimate samples averages unbiased ) unfortunately, importance sampling estimate unnecessarily high variance. happen, case samples (under distributions) importance-sampling ratios vary greatly sample sample. easy case samples similar distributions, importance sampling average high variance, estimates high variance. fact, furr bounds importance-sampling ratios infinite variance (andrad?ottir. 1995, Robert Casella 2004). important variation importance sampling lower variance weighted importance sampling (rubinstein 1981, Koller Friedman 2009). weighted importance sampling (wis) estimate weighted average samples importance sampling ratios weights:   This estimate biased, consistent (asymptotically correct) typically lower variance ordinary importance-sampling (ois) estimate, acknowledged authors (hesterberg 1988, Casella Robert 1998, precup, Sutton Singh 2000, Shelton 2001, Liu 2001, Koller Friedman 2009). example, problematic case sketched (near constant widely varying variance WIS estimate related variance note samples bounded, WIS estimate bounded variance, estimate bounded highest absolute matter large ratios (precup, Sutton Dasgupta 2001). WIS successful importance sampling technique, extended parametric function approximation. problematic applications off-policy reinforcement learning, function approximation viewed essential large-scale applications sequential decision problems large state action spaces. important subproblem approximation function? expected sum future discounted rewards function state?for designated target policy differ select actions. existing methods off-policy value-function approximation eir OIS (maei Sutton 2010, 2010, Sutton. 2014, Geist Scherrer 2014, Dann. 2014) WIS limited tabular non-parametric case (precup. 2000, Shelton \\x0c2001). extend WIS parametric function approximation important, clear noted Precup. 2001). importance sampling linear function approximation section, step bridging gap WIS off-policy learning function approximation. general supervised learning setting linear function approximation, develop analyze importance-sampling methods. show methods oretical properties similar OIS wis. fully-representable case, methods equivalent OIS estimate WIS estimate. key idea OIS WIS leastsquares solutions empirical objectives. ois estimate least-squares solution empirical mean-squared objective samples importance weighted arg min  similarly, WIS estimate least-squares solution empirical mean-squared objective individual errors importance weighted arg min  solve similar empirical objectives general supervised learning setting linear function approximation derive methods. correlated random variables takes values finite set estimate conditional expectation target distribution however, samples generated. joint sample distribution lxy (?) conditional probabilities differ conditional target distribution. input mapped feature vector goal estimate expectation linear function features  )  estimating expectation diﬃcult target joint distribution input-output pairs gxy sample joint distribution lxy generally, discrepancy joint distribution arise sources: difference marginal distribution inputs, difference conditional distribution outputs, problems discrepancy arise covariate shift problems (shimodaira 2000). problems conditional expectation outputs assumed unchanged target sample distributions. off-policy learning problems, discrepancy conditional probabilities important. off-policy learning methods correct discrepancy target sample conditional distributions outputs (hachiya. 2009, Maei Sutton 2010, 2010, Maei 2011, Geist Scherrer 2014, Dann. 2014). paper, focus correcting discrepancy conditional distributions. problem estimating) linear function features \\x0cusing samples generated formulated minimization squared error (mse) solution follows:       arg min  ) similar empirical mean-squared objectives defined), empirical objectives defined approximate solution. case importance weighting applied output samples, case importance weighting applied error,      (?)   (?)   importance-sampling ratios defined minimize objectives equating derivatives empirical objectives zero. provided relevant matrix inverses exist, resulting solutions are,        ois estimator  wis estimator. call leastsquares method similar wis introduced covariate shift problems hachiya, Sugiyama Ueda (2012). superficially similar, method importancesampling ratios correct discrepancy marginal distributions inputs, wis corrects discrepancy conditional expectations outputs. fullyrepresentable case, unlike wis, method Hachiya. ordinary Monte Carlo estimator importance sampling. analysis least-squares importance-sampling methods least-squares importance-sampling methods properties similar OIS WIS methods. orems prove represented linear function features, ois unbiased estimator  wis biased estimator, similar WIS estimator. solution linearly representable, least-squares methods generally unbiased. orem show least-squares estimators consistent   finally, demonstrate least-squares methods generalizations OIS WIS showing, orem fully representable case (when features form orthonormal basis) ois equivalent OIS wis equivalent wis. orem linear function features)  ), ois   unbiased estimator, lxy orem linear function features)  ), wis    general biased estimator, lxy  consistent estimator MSE solution  ). orem ois estimator  consistent estimator MSE solution  ). orem wis estimator  ) input orem features form orthonormal basis, ois estimate equivalent OIS estimate outputs ) input orem features form orthonormal basis, wis estimate equivalent WIS estimate outputs proofs orem appendix. wis estimate interesting least-squares estimates, generalizes WIS parametric function approximation time extends advantages. off-policy lstd( WIS sequential decision problems, off-policy learning methods based important sampling suffer high-variance issues discussed supervised case. address this, extend idea wis off-policy reinforcement learning construct off-policy wislstd( algorithm. explain problem setting. learning agent interacts environment step state environment agent observes feature vector  agent takes action based behavior policy typically function state features. environment agent scalar (reward) signal transitions state process continues, generating trajectory states, actions rewards. goal estimate values states target policy defined expected returns sum future discounted rewards:    , state-dependent degree discounting arrival Sutton. 2014). assume rewards discounting chosen ) well-defined finite. primary objective estimate linear function features: )  ), parameter vector estimated. before, correct difference sample distribution resulting behavior policy target distribution induced target policy. partial trajectory  time step time consisting sequence  probability trajectory occurring starts target policy generally differ probability behavior policy. importancesampling ratio defined ratio probabilities. importance-sampling ratio written terms product action-selection probabilities needing model environment (sutton Barto 1998):    shorthand  incorporate common technique reinforcement learning) updates estimated bootstrapping, fully partially, previously constructed state-value estimates. bootstrapping potentially reduces variance updates compared full returns makes algorithms applicable non-episodic tasks. paper assume bootstrapping parameter, depend state Sutton Barto 1998, Maei Sutton 2010).   following, notational shorthands Sutton. (2014), construct empirical loss sum pairs squared corrected uncorrected errors, number steps lookahead, weighted function intervening discounting bootstrapping. Gtk   undiscounted return truncated ahead steps. imagine constructing empirical loss time leaving observing uncorrected error G10 weight equal probability terminating terminate, continue form corrected error G10 bootstrapping estimate weight error parameter vector differ continuing time step, obtain uncorrected error G20 corrected error G20 respective weights reach horizon data, time bootstrap fully generating final corrected return error Gt0 weight     general form uncorrected error (?) gtk general form corrected error (?, Gtk  errors squared, weighted weights, summed form empirical loss. off-policy case, weight squares errors importance-sampling ratio hence, empirical loss time data time written (?, cki Ckt   (?)  (?)   (?, ckt  This loss differs lstd( methods importance weighting applied individual errors). now, ready state least-squares problem. noted Geist Scherrer (2014), lstd( methods derived solving least-squares problems estimates step matched multi-step returns starting steps bootstrapping solution itself. proposed method, called wis-lstd( computes time solution least-squares problem:  arg min (?,    solution, derivative objective zero:    errors defined  (?, cki next, separate terms   (?, Ckt      (?, ?=?  (?) (?, involve not:    defined  Cki (?)   Ckt Gtk  Cki Ckt  refore, solution found follows      show wis special case algorithm defined). orem shows wis generalizes wis, algorithm generalizes WIS well. orem termination, algorithm defined) equivalent wis method sense       defined) equals  defined), Gtk (proved appendix). our challenge find equivalent eﬃcient online algorithm method. solution) computed incrementally form. sample arrives time com, computational complexity puted solution grows time. preferable solution time computed incrementally based estimates time requiring constant computational complexity time step. immediately obvious eﬃcient update exists. instance, method achieves full Monte Carlo (weighted) importance-sampling estimation, means target policy deviates behavior policy previously made updates unmade updates made trajectory impossible target policy. sutton. (2014) show derive eﬃcient updates cases provisional parameters track provisional updates unmade deviation occurs. following, show provisional parameters achieve equivalent eﬃcient update). write recursively (derivations Appendix Ckt   Ckt recursions, write updates incrementally. vector updated incrementally Ckt Ckt Ckt Gtk eligibility trace provisional vector defined follows: Ckt Ckt Ckt Gtk Ckt Ckt  Ckt) Ckt) ) Ckt provisional matrix defined Ckt Ckt gtk matrix updated incrementally ) parameter vector updated:  ) Equations) comprise wis-lstd( per-step computational complexity number features. computational cost method increase time. present unsure wher implementation. orem off-policy lstd( method defined) equivalent off-policy lstd( method defined) sense compute time proof. result immediately derivation. easy-policy case method equivalent-policy lstd( (boyan 1999) noting term updates) zero-policy case importance-sampling ratios recently Dann. (2014) proposed anor least-squares based off-policy method called recursive lstdto( unlike algorithm, algorithm specialize WIS fully representable case, closely related wis. adaptive per-decision Importance Weighting (apdiw) method Hachiya. (2009) superficially similar wis-lstd( important differences. apdiw one-step method fully bootstraps wis-lstd( covers full spectrum multi-step backups including onestep backup Monte Carlo update. fully representable case, APDIW equivalent WIS estimate, wis-lstd) does. moreover, APDIW find consistent estimation off-policy target WIS algorithms. experimental results compared performance proposed wis-lstd( method conventional offpolicy lstd( (2010) random-walk tasks off-policy policy evaluation. random-walk tasks consist Markov chain non-terminal terminal states. imagined laid horizontally, terminal states left ends chain. non-terminal state, actions available: left, leads state left right, leads state right. reward transitions rightmost transition terminal state. initial state set state middle chain. behavior policy chooses action uniformly randomly, target policy chooses action probability. termination function set non-terminal states terminal states. tasks based Markov chain experiments. tasks differ non-terminal states mapped features. terminal states mapped vector elements. non-terminal state, features normalized norm feature vector one. task, feature representation tabular, feature vectors standard representation, feature corresponded basis vectors. state. task, feature vectors binary representations state indices. non-terminal states, feature vector blog2 components. vectors states left)¿   )¿ normalized unit vectors. features heavily underrepresented states, due fact states represented features. tested algorithms values constant steps steps.025. matrix inverted methods initialized, regularization parameter varied powers powers chosen steps. performance measured empirical squared error (mse) estimated initial state true target policy projected space spanned features. error measured end 200 episodes 100 independent runs. figure shows results tasks terms empirical convergence rate, optimum performance parameter sensitivity. curve shows MSE toger standard errors. row shows results tabular task row shows results function approximation task. column shows learning curves , task) second. shows cases wis-lstd( learned faster gave lower error period learning. column shows performance respect optimized -axis plotted reverse log scale, higher values spread lower values. tasks, wis-lstd( outperformed conventional lstd( values parameter setting (best wis-lstd( outperformed lstd( order Tabular task?-policy lstd( MSE MSE MSE   wis-lstd( episodes regularization parameter func. approx. task?-policy lstd( MSE MSE MSE   wis-lstd( episodes regularization parameter figure Empirical comparison wis-lstd( conventional offpolicy lstd( random-walk tasks. empirical Mean Squared Error shown initial state end episode, averaged 100 independent runs (and 200 episodes column). magnitude. column shows performance respect regularization parameter representative values wide range wis-lstd( \\x0coutperformed conventional lstd( order magnitude. methods performed similarly large large values essentially prevent learning long period time. function approximation task smaller values chosen, close led stable estimates, smaller introduced high variance methods. tasks, better-performing regions -shaped depressions) wider wis-lstd( conclusion Although importance sampling essential off-policy learning key part modern reinforcement learning algorithms, effective form?wis?has neglected diﬃculty combining parametric function approximation. paper, begun overcome diﬃculties. first, shown WIS estimate viewed solution empirical objective squared errors individual samples weighted importance-sampling ratios. second, introduced method general supervised learning called wis extending error-weighted empirical objective linear function approximation shown method similar properties WIS estimate. finally, introduced off-policy LSTD algorithm wis-lstd( extends benefits WIS reinforcement learning. empirical results show wis-lstd( outperform off-policy lstd( tabular function approximation tasks shows robustness terms parameters. interesting direction future work extend ideas off-policy linear-complexity methods. acknowledgement This work supported grants Alberta Innovates Technology futures, National Science Engineering Research council, Alberta Innovates Centre Machine learning. Importance sampling essential component off-policy model-free reinforcement learning algorithms. however, effective variant, weighted importance sampling, carry easily function approximation and, this, utilized existing off-policy learning algorithms. paper, steps bridging gap. first, show weighted importance sampling viewed special case weighting error individual training samples, weighting oretical empirical benefits similar weighted importance sampling. second, show benefits extend weighted-importance-sampling version offpolicy lstd( show empirically wis-lstd( algorithm result rapid reliable convergence conventional off-policy lstd 2010, Bertsekas 2009). importance sampling weighted importance sampling Importance sampling (kahn Marshall 1953, Rubinstein 1981, Koller Friedman 2009) wellknown Monte Carlo technique estimating expectation distribution samples distribution. consider data samples generated. sample distribution interested estimating expected samples distribution importance sampling achieved simply averaging samples weighted ratio likelihoods called importance-sampling ratio. that, estimated:  ) This unbiased estimate samples averages unbiased ) unfortunately, importance sampling estimate unnecessarily high variance. happen, case samples (under distributions) importance-sampling ratios vary greatly sample sample. this easy case samples similar distributions, importance sampling average high variance, estimates high variance. fact, furr bounds importance-sampling ratios infinite variance (andrad?ottir. 1995, Robert Casella 2004). important variation importance sampling lower variance weighted importance sampling (rubinstein 1981, Koller Friedman 2009). weighted importance sampling (wis) estimate weighted average samples importance sampling ratios weights:   This estimate biased, consistent (asymptotically correct) typically lower variance ordinary importance-sampling (ois) estimate, acknowledged authors (hesterberg 1988, Casella Robert 1998, precup, Sutton Singh 2000, Shelton 2001, Liu 2001, Koller Friedman 2009). for example, problematic case sketched (near constant widely varying variance WIS estimate related variance note samples bounded, WIS estimate bounded variance, estimate bounded highest absolute matter large ratios (precup, Sutton Dasgupta 2001). although WIS successful importance sampling technique, extended parametric function approximation. this problematic applications off-policy reinforcement learning, function approximation viewed essential large-scale applications sequential decision problems large state action spaces. here important subproblem approximation function? expected sum future discounted rewards function state?for designated target policy differ select actions. existing methods off-policy value-function approximation eir OIS (maei Sutton 2010, 2010, Sutton. 2014, Geist Scherrer 2014, Dann. 2014) WIS limited tabular non-parametric case (precup. 2000, Shelton \\x0c2001). how extend WIS parametric function approximation important, clear noted Precup. 2001). Importance sampling linear function approximation section, step bridging gap WIS off-policy learning function approximation. general supervised learning setting linear function approximation, develop analyze importance-sampling methods. show methods oretical properties similar OIS wis. fully-representable case, methods equivalent OIS estimate WIS estimate. key idea OIS WIS leastsquares solutions empirical objectives. OIS estimate least-squares solution empirical mean-squared objective samples importance weighted arg min  similarly, WIS estimate least-squares solution empirical mean-squared objective individual errors importance weighted arg min  solve similar empirical objectives general supervised learning setting linear function approximation derive methods. consider correlated random variables takes values finite set estimate conditional expectation target distribution however, samples generated. joint sample distribution lxy (?) conditional probabilities differ conditional target distribution. each input mapped feature vector goal estimate expectation linear function features  )  estimating expectation diﬃcult target joint distribution input-output pairs gxy sample joint distribution lxy generally, discrepancy joint distribution arise sources: difference marginal distribution inputs, difference conditional distribution outputs, problems discrepancy arise covariate shift problems (shimodaira 2000). problems conditional expectation outputs assumed unchanged target sample distributions. off-policy learning problems, discrepancy conditional probabilities important. most off-policy learning methods correct discrepancy target sample conditional distributions outputs (hachiya. 2009, Maei Sutton 2010, 2010, Maei 2011, Geist Scherrer 2014, Dann. 2014). paper, focus correcting discrepancy conditional distributions. problem estimating) linear function features \\x0cusing samples generated formulated minimization squared error (mse) solution follows:       arg min  ) similar empirical mean-squared objectives defined), empirical objectives defined approximate solution. case importance weighting applied output samples, case importance weighting applied error,      (?)   (?)   importance-sampling ratios defined minimize objectives equating derivatives empirical objectives zero. provided relevant matrix inverses exist, resulting solutions are,        ois estimator  wis estimator. call leastsquares method similar wis introduced covariate shift problems hachiya, Sugiyama Ueda (2012). although superficially similar, method importancesampling ratios correct discrepancy marginal distributions inputs, wis corrects discrepancy conditional expectations outputs. for fullyrepresentable case, unlike wis, method Hachiya. ordinary Monte Carlo estimator importance sampling. Analysis least-squares importance-sampling methods least-squares importance-sampling methods properties similar OIS WIS methods. orems prove represented linear function features, ois unbiased estimator  wis biased estimator, similar WIS estimator. solution linearly representable, least-squares methods generally unbiased. orem show least-squares estimators consistent   finally, demonstrate least-squares methods generalizations OIS WIS showing, orem fully representable case (when features form orthonormal basis) ois equivalent OIS wis equivalent wis. orem linear function features)  ), ois   unbiased estimator, lxy orem even linear function features)  ), wis    general biased estimator, lxy  consistent estimator MSE solution  ). orem ois estimator  consistent estimator MSE solution  ). orem wis estimator  ) input orem features form orthonormal basis, ois estimate equivalent OIS estimate outputs ) input orem features form orthonormal basis, wis estimate equivalent WIS estimate outputs proofs orem appendix. wis estimate interesting least-squares estimates, generalizes WIS parametric function approximation time extends advantages. off-policy lstd( WIS sequential decision problems, off-policy learning methods based important sampling suffer high-variance issues discussed supervised case. address this, extend idea wis off-policy reinforcement learning construct off-policy wislstd( algorithm. explain problem setting. consider learning agent interacts environment step state environment agent observes feature vector  agent takes action based behavior policy typically function state features. environment agent scalar (reward) signal transitions state this process continues, generating trajectory states, actions rewards. goal estimate values states target policy defined expected returns sum future discounted rewards:    , state-dependent degree discounting arrival Sutton. 2014). assume rewards discounting chosen ) well-defined finite. our primary objective estimate linear function features: )  ), parameter vector estimated. before, correct difference sample distribution resulting behavior policy target distribution induced target policy. consider partial trajectory  time step time consisting sequence  probability trajectory occurring starts target policy generally differ probability behavior policy. importancesampling ratio defined ratio probabilities. this importance-sampling ratio written terms product action-selection probabilities needing model environment (sutton Barto 1998):    shorthand  incorporate common technique reinforcement learning) updates estimated bootstrapping, fully partially, previously constructed state-value estimates. bootstrapping potentially reduces variance updates compared full returns makes algorithms applicable non-episodic tasks. paper assume bootstrapping parameter, depend state Sutton Barto 1998, Maei Sutton 2010).   following, notational shorthands following Sutton. (2014), construct empirical loss sum pairs squared corrected uncorrected errors, number steps lookahead, weighted function intervening discounting bootstrapping. let Gtk   undiscounted return truncated ahead steps. imagine constructing empirical loss time after leaving observing uncorrected error G10 weight equal probability terminating terminate, continue form corrected error G10 bootstrapping estimate weight error parameter vector differ continuing time step, obtain uncorrected error G20 corrected error G20 respective weights this reach horizon data, time bootstrap fully generating final corrected return error Gt0 weight     general form uncorrected error (?) Gtk general form corrected error (?, Gtk  all errors squared, weighted weights, summed form empirical loss. off-policy case, weight squares errors importance-sampling ratio hence, empirical loss time data time written (?, Cki Ckt   (?)  (?)   (?, Ckt  This loss differs lstd( methods importance weighting applied individual errors). now, ready state least-squares problem. noted Geist Scherrer (2014), lstd( methods derived solving least-squares problems estimates step matched multi-step returns starting steps bootstrapping solution itself. our proposed method, called wis-lstd( computes time solution least-squares problem:  arg min (?,    solution, derivative objective zero:    errors defined  (?, Cki next, separate terms   (?, Ckt      (?, ?=?  (?) (?, involve not:    defined  Cki (?)   Ckt Gtk  Cki Ckt  refore, solution found follows      show wis special case algorithm defined). orem shows wis generalizes wis, algorithm generalizes WIS well. orem termination, algorithm defined) equivalent wis method sense       defined) equals  defined), Gtk (proved appendix). Our challenge find equivalent eﬃcient online algorithm method. solution) computed incrementally form. when sample arrives time com, computational complexity puted solution grows time. preferable solution time computed incrementally based estimates time requiring constant computational complexity time step. immediately obvious eﬃcient update exists. for instance, method achieves full Monte Carlo (weighted) importance-sampling estimation, means target policy deviates behavior policy previously made updates unmade updates made trajectory impossible target policy. sutton. (2014) show derive eﬃcient updates cases provisional parameters track provisional updates unmade deviation occurs. following, show provisional parameters achieve equivalent eﬃcient update). write recursively (derivations Appendix Ckt   Ckt using recursions, write updates incrementally. vector updated incrementally Ckt Ckt Ckt Gtk eligibility trace provisional vector defined follows: Ckt Ckt Ckt Gtk Ckt Ckt  Ckt) Ckt) ) Ckt provisional matrix defined Ckt Ckt gtk matrix updated incrementally ) parameter vector updated:  ) Equations) comprise wis-lstd( its per-step computational complexity number features. computational cost method increase time. present unsure wher implementation. orem off-policy lstd( method defined) equivalent off-policy lstd( method defined) sense compute time proof. result immediately derivation. easy-policy case method equivalent-policy lstd( (boyan 1999) noting term updates) zero-policy case importance-sampling ratios recently Dann. (2014) proposed anor least-squares based off-policy method called recursive lstdto( unlike algorithm, algorithm specialize WIS fully representable case, closely related wis. Adaptive per-decision Importance Weighting (apdiw) method Hachiya. (2009) superficially similar wis-lstd( important differences. apdiw one-step method fully bootstraps wis-lstd( covers full spectrum multi-step backups including onestep backup Monte Carlo update. fully representable case, APDIW equivalent WIS estimate, wis-lstd) does. moreover, APDIW find consistent estimation off-policy target WIS algorithms. Experimental results compared performance proposed wis-lstd( method conventional offpolicy lstd( (2010) random-walk tasks off-policy policy evaluation. random-walk tasks consist Markov chain non-terminal terminal states. imagined laid horizontally, terminal states left ends chain. from non-terminal state, actions available: left, leads state left right, leads state right. reward transitions rightmost transition terminal state. initial state set state middle chain. behavior policy chooses action uniformly randomly, target policy chooses action probability. termination function set non-terminal states terminal states. tasks based Markov chain experiments. tasks differ non-terminal states mapped features. terminal states mapped vector elements. for non-terminal state, features normalized norm feature vector one. for task, feature representation tabular, feature vectors standard representation, feature corresponded basis vectors. state. for task, feature vectors binary representations state indices. non-terminal states, feature vector blog2 components. vectors states left)¿   )¿ normalized unit vectors. features heavily underrepresented states, due fact states represented features. tested algorithms values constant steps steps.025. matrix inverted methods initialized, regularization parameter varied powers powers chosen steps. performance measured empirical squared error (mse) estimated initial state true target policy projected space spanned features. this error measured end 200 episodes 100 independent runs. figure shows results tasks terms empirical convergence rate, optimum performance parameter sensitivity. each curve shows MSE toger standard errors. row shows results tabular task row shows results function approximation task. column shows learning curves , task) second. shows cases wis-lstd( learned faster gave lower error period learning. column shows performance respect optimized -axis plotted reverse log scale, higher values spread lower values. tasks, wis-lstd( outperformed conventional lstd( values for parameter setting (best wis-lstd( outperformed lstd( order Tabular task?-policy lstd( MSE MSE MSE   wis-lstd( episodes regularization parameter func. approx. task?-policy lstd( MSE MSE MSE   wis-lstd( episodes regularization parameter figure Empirical comparison wis-lstd( conventional offpolicy lstd( random-walk tasks. empirical Mean Squared Error shown initial state end episode, averaged 100 independent runs (and 200 episodes column). magnitude. column shows performance respect regularization parameter representative values for wide range wis-lstd( \\x0coutperformed conventional lstd( order magnitude. both methods performed similarly large large values essentially prevent learning long period time. function approximation task smaller values chosen, close led stable estimates, smaller introduced high variance methods. tasks, better-performing regions -shaped depressions) wider wis-lstd( Conclusion Although importance sampling essential off-policy learning key part modern reinforcement learning algorithms, effective form?wis?has neglected diﬃculty combining parametric function approximation. paper, begun overcome diﬃculties. first, shown WIS estimate viewed solution empirical objective squared errors individual samples weighted importance-sampling ratios. second, introduced method general supervised learning called wis extending error-weighted empirical objective linear function approximation shown method similar properties WIS estimate. finally, introduced off-policy LSTD algorithm wis-lstd( extends benefits WIS reinforcement learning. our empirical results show wis-lstd( outperform off-policy lstd( tabular function approximation tasks shows robustness terms parameters. interesting direction future work extend ideas off-policy linear-complexity methods. acknowledgement This work supported grants Alberta Innovates Technology futures, National Science Engineering Research council, Alberta Innovates Centre Machine learning.',\n",
       " 'PP5420': 'recent advances convolutional neural nets] dramatically improved state--art image classification. magnitude results, doubted] resulting features spatial specificity localization; all, image classification rely context cues overly large pooling regions job done. coarse localization, doubts alleviated record breaking results extending features detection PASCAL]. now, questions loom finer scale. modern convnets excel classification detection find precise correspondences object parts? large receptive fields correspondence effectively pooled away, making task suited hand-engineered features? paper, provide evidence convnet features perform conventional ones, regime point-point correspondence, show considerable performance improvement settings, including category-level keypoint prediction.  \\x0crelated work Image alignment Image alignment key step computer vision tasks, including face verification, motion analysis, stereo matching, object recognition. alignment results correspondence images removing intraclass variability canonicalizing pose. alignment methods exist supervision spectrum requiring manually labeled fiducial points landmarks, requiring class labels, fully unsupervised joint alignment clustering models. congealing] unsupervised joint alignment method based entropy objective. deep congealing] builds idea replacing hand-engineered features unsupervised feature learning multiple resolutions. inspired optical ﬂow, SIFT ﬂow] matches densely sampled SIFT features correspondence applied motion prediction motion transfer. section apply SIFT ﬂow deep features aligning instances class. keypoint localization Semantic parts carry important information object recognition, object detection, pose estimation. particular, fine-grained categorization, subject recent works, depends strongly part localization]. large pose appearance variation examples make part localization generic object categories challenging task. existing works part localization keypoint prediction focus eir facial landmark localization] human pose estimation. human pose estimation approached tree structured methods model spatial relationships parts], poselets] intermediate step localize human keypoints]. tree structured models poselets struggle applied generic objects large articulated deformations wide shape variance. deep learning Convolutional neural networks gained recent attention due success image classification]. convnets trained backpropagation initially succesful digit recognition] OCR]. feature representations learned large data sets found generalize image classification tasks] object detection]. recently, Toshev. ] trained cascade regression-based convnets human pose estimation Jain. ] combine weak spatial model deep learning methods. work trains multiple small, independent convnets patches binary bodypart detection. contrast, employ powerful pretained ImageNet model shares mid-elvel feature representations parts Section recent works attempted analyze explain overwhelming success. zeiler Fergus] provide heuristic visualizations suggesting coarse localization ability. szegedy. ] show counterintuitive properties convnet representation, suggest individual feature channels semantically meaningful bases feature space. concurrent work] compares convnet features SIFT standard descriptor matching task. work illuminates extends comparison providing visual analysis moving single instance matching intraclass correspondence keypoint prediction.  Preliminaries perform experiments network architecture identical1 popularized Krizhevsky. ] trained classification million images ILSVRC 2012 challenge dataset]. experiments implemented caffe], network publicly caffe reference model. activations layer features, referred convn, pooln, fcn nth convolutional, pooling, fully connected layer, respectively. term receptive field, abbreviated, refer set input pixels path-connected unit convnet. feature visualization section Figures provide visual investigation effective pooling regions convnet features. table Convnet receptive field sizes strides, input size 227 227. figure perform nonparametric reconlayer size stride struction images features spirit conv1  HOGgles]. rar paired dictionary conv2  learning, however, simply replace patches conv3  averages top nearest neighbors conv4 131 131 convnet feature space. , conv5 163 163 compute features layer, repool5 195 195 sulting grid feature vectors. associate feature vector patch original image center receptive field size equal receptive field stride. (note strides receptive fields smaller receptive fields Ours reverses order response normalization pooling layers. conv4 conv5 uniform neighbors neighbor neighbors neighbor conv3 Figure Even large receptive fields, convnet features carry local information finer scale. upper left: input image, replaced patches averages nearest neighbor patches, computed convnet features centered patches. yellow square illustrates input patch, black squares show rfs layers shown. right: Notice features retrieve reasonable matches centers receptive fields, rfs extend large regions source image. ?uniform? column, show expected convnet features discarded spatial information rfs, choosing input patches uniformly random conv3sized neighborhoods. (best viewed electronically.) mselves, overlap. refer Table specific numbers.) replace patch average nearest \\x0cneighbor patches database features densely computed images PASCAL VOC 2011. database million patches layer. features matched cosine similarity. feature rfs cover large regions source images, specific resemblance resulting images shows information spread uniformly regions. notable features., tires bicycle facial features cat) replaced locations. note replacement appears semantic visually specific layer deepens: eyes nose cat replaced differently colored shaped eyes noses, fur replaced animal furs, diversity increasing layer number. figure featurecentric rar image-centric view feature locality. column, pick random seed feature vector (computed PASCAL image), find nearest neighbor features, cosine similarity. averaging centers, average entire receptive fields neighbors. resulting images show similar features tend respond similar colors specifically centers receptive fields. conv4 conv5 500 nbrs nbrs nbrs conv3 Figure Similar convnet features tend similar receptive field centers. starting randomly selected seed patch occupying conv3, find nearest neighbor features computed database natural images, average toger receptive fields. contrast image expanded averaging. (note layer computed stride, upper bound quality alignment witnessed here.) intraclass alignment conjecture category learning implicitly aligns instances pooling discriminative mid-level representation. true, features post-hoc alignment similar fashion conventional features. test this, convnet features task aligning instances class. approach diﬃcult task style SIFT ﬂow]: retrieve neighbors coarse similarity measure, compute dense correspondences impose MRF smoothness prior finally images warped alignment. nearest neighbors computed fc7 features. specifically testing quality alignment, nearest neighbors convnet conventional features, compute types features locations, grid convnet centers response single image. alignment determined solving MRF formulated grid feature locations. point grid) feature vector source image point) feature vector target image point. each feature grid location source image, vector) giving displacement feature target image. energy function kfs)  ) )k22 edges-neighborhood graph regularization parameter. optimization performed belief propagation, techniques suggested]. message passing performed eﬃciently squared Euclidean distance transform]. (unlike regularization originally SIFT ﬂow], formulation maintains rotational invariance.) Based performance section, conv4 convnet feature, SIFT descriptor radius conventional feature. validation experiments, set   conv4 SIFT features (which similar scale). alignment field warp target source bivariate spline interpolation (implemented SciPy]). figure examples alignment quality seed images, SIFT convnet features. show warped nearest neighbors keypoints transferred neighbors. quantitatively assess alignment measuring accuracy predicted keypoints. obtain good predictions, warp nearest neighbors target image, order smallest greatest deformation energy found method outperform ordering data term). predicted keypoints median points (coordinate-wise) top aligned keypoints ordering. assess correctness PCK]. ground truth keypoint correctly predicted prediction lies Euclidean distance times maximum bounding nearest neighbors SIFT ﬂow conv4 ﬂow SIFT ﬂow conv4 ﬂow target image Figure Convnet features bring instances class good alignment average) traditional features. target image (left column), show warped versions nearest neighbor images aligned conv4 ﬂow (first row), warped versions aligned SIFT ﬂow] (second row). keypoints warped images shown copied target image. cat shows case convnet features perform better, bicycle shows case SIFT features perform better. (note instance warped square bounding box alignment. viewed color.) table Keypoint transfer accuracy convnet ﬂow, SIFT ﬂow, simple copying nearest neighbors. accuracy (pck) shown category  (see text) means shown stricter values .025. average, convnet ﬂow performs SIFT ﬂow, performs bit stricter tolerances. aero bike bird conv4 ﬂow SIFT ﬂow transfer \\x0cboat bttl bus car conv4 ﬂow SIFT ﬂow transfer cat chair cow  table dog  horse mbike prsn plant sheep sofa train .025 box width height, picking  ]. compute accuracy type keypoint, report average keypoint types. penalize predicted keypoints visible target image. results Table show category results , results .025. indeed, convnet learned features capable SIFT alignment, expected size receptive fields. keypoint classification section, specifically address ability convnet features understand semantic information scale parts. initial test, task keypoint classification: image coordinates keypoint image, train classifier label keypoint? table Keypoint classification accuracies, percent, twenty categories PASCAL 2011 val, trained SIFT convnet features. sift convnet scores bolded category. aero SIFT (radius) 160 conv (layer) bike bird boat bttl bus car cat \\x0cchair cow table dog horse mbike prsn) cat left eye plant sheep sofa train) Figure Cross validation scores cat keypoint classification function SVM parameter), plot accuracy convnet features) plot SIFT features sizes.  experiments Table ) cat nose Figure Convnet features show fine localization ability, stride cases SIFT features perform well. plot histogram locations maximum responses classifer pixel rectangle ground truth keypoint. task keypoint data] twenty classes PASCAL VOC 2011]. extract features keypoint SIFT] column convnet layer center lies closest keypoint. (note SIFT features precisely result approximation.) trained one-all linear SVMs train set SIFT radii convolutional layer activations features general, found pooling normalization layers lower performance). set SVM parameter experiments based five-fold cross validation training set (see Figure). table resulting accuracies val set. find features convnet layers consistently perform SIFT task, highest performance coming layers conv4 conv5. note specifically testing convnet features trained classification; net expected achieve higher performance trained task. finally, study precise location understanding classifiers computing responses single-pixel stride ground truth keypoint locations. keypoints (cat left eye nose), histogram locations maximum responses pixel pixel rectangle keypoint, shown Figure include maximum responses lie boundary rectangle. SIFT classifiers sensitive precise locations keypoints, cases convnet capable localization finer strides receptive field sizes. observation motivates final experiments detection-based localization performance. keypoint prediction large receptive field sizes, convnets work handengineered feature SIFT alignment slightly SIFT keypoint classification. keypoint prediction natural followup test. section keypoint annotations PASCAL VOC 2011, assume ground truth bounding box. inspired part], train sliding window part detectors predict keypoint locations independently. -cnn] OverFeat] demonstrated effectiveness deep convolutional networks generic object detection task. however, neir investigated application CNNs keypoint prediction-cnn starts bottom region proposal], overlook signal small parts. overfeat, hand, combines convnets trained classification regression runs multi-scale sliding window fashion. rescale bounding box 500 500 compute conv5 (with stride pixels). cell conv5 256-dimensional descriptor. concatenate conv5 descriptors local region cells, giving receptive field size 195 195 feature dimension 2304. keypoint, train linear SVM hard negative mining. ten closest features ground truth keypoint positive examples, features rfs keypoint negative examples. train dense SIFT descriptors comparison. compute SIFT grid stride bin size VLFeat]. sift, features bin size ground truth keypoint positives, samples times bin size negatives. augment SVM detectors spherical Gaussian prior candidate locations constructed nearest neighbor matching. gaussian location keypoint nearest neighbor training set found cosine similarity pool5 features, fixed standard deviation pixels.  output score local detector keypoint prior score. combine yield final score??    , tradeoff parameter. experiments, set  cross validation. test time, predict keypoint location highest scoring candidate feature locations. evaluate predicted keypoints measure PCK introduced Section taking . predicted keypoint defined correct distance ground truth keypoint  max, height width bounding box. results conv5 SIFT prior shown Table table, local part detectors trained conv5 feature outperform SIFT large margin prior information helpful cases. knowledge, keypoint prediction results reported dataset. show results categories Figure set consists rescaled bounding box images ground truth keypoint annotations predicted keypoints SIFT conv5 features, color corresponds keypoint. figure shows, conv5 outperforms sift, managing satisfactory outputs challenge task. small offset noticed keypoints eyes noses, due limited stride scanning windows. final regression finer stride mitigate issue. conclusion Through visualization, alignment, keypoint prediction, studied ability intermediate features implicitly learned state--art convnet classifier understand specific, local correspondence. large receptive fields weak label training, found cases convnet features (and considerably useful) conventional extracting local visual information. acknowledgements This work supported part darpa MSEE SMISC programs, NSF awards iis-1427425, iis-1212798, iis-1116411, support toyota. but works cited Section keypoint localization. table Keypoint prediction results PASCAL VOC 2011. numbers give average accuracy keypoint prediction criterion Section PCK . sift sift+prior conv5 conv5+prior aero Groundtruth bike bird boat bttl sift+prior bus car cat chair cow table dog horse Groundtruth conv5+prior mbike prsn plant sheep sift+prior sofa \\x0ctrain conv5+prior Figure Examples keypoint prediction classes PASCAL dataset: aeroplane, cat, cow, potted plant, horse. keypoint color. column ground truth annotation, column prediction result sift+prior column conv5+prior. (best viewed color). Recent advances convolutional neural nets] dramatically improved state--art image classification. despite magnitude results, doubted] resulting features spatial specificity localization; all, image classification rely context cues overly large pooling regions job done. for coarse localization, doubts alleviated record breaking results extending features detection PASCAL]. now, questions loom finer scale. are modern convnets excel classification detection find precise correspondences object parts? large receptive fields correspondence effectively pooled away, making task suited hand-engineered features? paper, provide evidence convnet features perform conventional ones, regime point-point correspondence, show considerable performance improvement settings, including category-level keypoint prediction.  \\x0crelated work Image alignment Image alignment key step computer vision tasks, including face verification, motion analysis, stereo matching, object recognition. alignment results correspondence images removing intraclass variability canonicalizing pose. alignment methods exist supervision spectrum requiring manually labeled fiducial points landmarks, requiring class labels, fully unsupervised joint alignment clustering models. congealing] unsupervised joint alignment method based entropy objective. deep congealing] builds idea replacing hand-engineered features unsupervised feature learning multiple resolutions. inspired optical ﬂow, SIFT ﬂow] matches densely sampled SIFT features correspondence applied motion prediction motion transfer. Section apply SIFT ﬂow deep features aligning instances class. Keypoint localization Semantic parts carry important information object recognition, object detection, pose estimation. particular, fine-grained categorization, subject recent works, depends strongly part localization]. large pose appearance variation examples make part localization generic object categories challenging task. most existing works part localization keypoint prediction focus eir facial landmark localization] human pose estimation. human pose estimation approached tree structured methods model spatial relationships parts], poselets] intermediate step localize human keypoints]. tree structured models poselets struggle applied generic objects large articulated deformations wide shape variance. deep learning Convolutional neural networks gained recent attention due success image classification]. convnets trained backpropagation initially succesful digit recognition] OCR]. feature representations learned large data sets found generalize image classification tasks] object detection]. recently, Toshev. ] trained cascade regression-based convnets human pose estimation Jain. ] combine weak spatial model deep learning methods. work trains multiple small, independent convnets patches binary bodypart detection. contrast, employ powerful pretained ImageNet model shares mid-elvel feature representations parts Section several recent works attempted analyze explain overwhelming success. zeiler Fergus] provide heuristic visualizations suggesting coarse localization ability. szegedy. ] show counterintuitive properties convnet representation, suggest individual feature channels semantically meaningful bases feature space. concurrent work] compares convnet features SIFT standard descriptor matching task. this work illuminates extends comparison providing visual analysis moving single instance matching intraclass correspondence keypoint prediction.  Preliminaries perform experiments network architecture identical1 popularized Krizhevsky. ] trained classification million images ILSVRC 2012 challenge dataset]. all experiments implemented caffe], network publicly caffe reference model. activations layer features, referred convn, pooln, fcn nth convolutional, pooling, fully connected layer, respectively. term receptive field, abbreviated, refer set input pixels path-connected unit convnet. Feature visualization section Figures provide visual investigation effective pooling regions convnet features. table Convnet receptive field sizes strides, input size 227 227. Figure perform nonparametric reconlayer size stride struction images features spirit conv1  HOGgles]. rar paired dictionary conv2  learning, however, simply replace patches conv3  averages top nearest neighbors conv4 131 131 convnet feature space. , conv5 163 163 compute features layer, repool5 195 195 sulting grid feature vectors. associate feature vector patch original image center receptive field size equal receptive field stride. (note strides receptive fields smaller receptive fields Ours reverses order response normalization pooling layers. conv4 conv5 uniform neighbors neighbor neighbors neighbor conv3 Figure Even large receptive fields, convnet features carry local information finer scale. upper left: input image, replaced patches averages nearest neighbor patches, computed convnet features centered patches. yellow square illustrates input patch, black squares show rfs layers shown. right: Notice features retrieve reasonable matches centers receptive fields, rfs extend large regions source image. ?uniform? column, show expected convnet features discarded spatial information rfs, choosing input patches uniformly random conv3sized neighborhoods. (best viewed electronically.) mselves, overlap. refer Table specific numbers.) replace patch average nearest \\x0cneighbor patches database features densely computed images PASCAL VOC 2011. our database million patches layer. features matched cosine similarity. even feature rfs cover large regions source images, specific resemblance resulting images shows information spread uniformly regions. notable features., tires bicycle facial features cat) replaced locations. also note replacement appears semantic visually specific layer deepens: eyes nose cat replaced differently colored shaped eyes noses, fur replaced animal furs, diversity increasing layer number. figure featurecentric rar image-centric view feature locality. for column, pick random seed feature vector (computed PASCAL image), find nearest neighbor features, cosine similarity. instead averaging centers, average entire receptive fields neighbors. resulting images show similar features tend respond similar colors specifically centers receptive fields. conv4 conv5 500 nbrs nbrs nbrs conv3 Figure Similar convnet features tend similar receptive field centers. starting randomly selected seed patch occupying conv3, find nearest neighbor features computed database natural images, average toger receptive fields. contrast image expanded averaging. (note layer computed stride, upper bound quality alignment witnessed here.) Intraclass alignment conjecture category learning implicitly aligns instances pooling discriminative mid-level representation. true, features post-hoc alignment similar fashion conventional features. test this, convnet features task aligning instances class. approach diﬃcult task style SIFT ﬂow]: retrieve neighbors coarse similarity measure, compute dense correspondences impose MRF smoothness prior finally images warped alignment. nearest neighbors computed fc7 features. since specifically testing quality alignment, nearest neighbors convnet conventional features, compute types features locations, grid convnet centers response single image. alignment determined solving MRF formulated grid feature locations. let point grid) feature vector source image point) feature vector target image point. for \\x0ceach feature grid location source image, vector) giving displacement feature target image. energy function kfs)  ) )k22 edges-neighborhood graph regularization parameter. optimization performed belief propagation, techniques suggested]. message passing performed eﬃciently squared Euclidean distance transform]. (unlike regularization originally SIFT ﬂow], formulation maintains rotational invariance.) Based performance section, conv4 convnet feature, SIFT descriptor radius conventional feature. from validation experiments, set   conv4 SIFT features (which similar scale). given alignment field warp target source bivariate spline interpolation (implemented SciPy]). figure examples alignment quality seed images, SIFT convnet features. show warped nearest neighbors keypoints transferred neighbors. quantitatively assess alignment measuring accuracy predicted keypoints. obtain good predictions, warp nearest neighbors target image, order smallest greatest deformation energy found method outperform ordering data term). predicted keypoints median points (coordinate-wise) top aligned keypoints ordering. assess correctness PCK]. ground truth keypoint correctly predicted prediction lies Euclidean distance times maximum bounding nearest neighbors SIFT ﬂow conv4 ﬂow SIFT ﬂow conv4 ﬂow target image Figure Convnet features bring instances class good alignment average) traditional features. for target image (left column), show warped versions nearest neighbor images aligned conv4 ﬂow (first row), warped versions aligned SIFT ﬂow] (second row). keypoints warped images shown copied target image. cat shows case convnet features perform better, bicycle shows case SIFT features perform better. (note instance warped square bounding box alignment. best viewed color.) table Keypoint transfer accuracy convnet ﬂow, SIFT ﬂow, simple copying nearest neighbors. accuracy (pck) shown category  (see text) means shown stricter values .025. average, convnet ﬂow performs SIFT ﬂow, performs bit stricter tolerances. aero bike bird conv4 ﬂow SIFT ﬂow transfer \\x0cboat bttl bus car conv4 ﬂow SIFT ﬂow transfer cat chair cow  table dog  horse mbike prsn plant sheep sofa train .025 box width height, picking  ]. compute accuracy type keypoint, report average keypoint types. penalize predicted keypoints visible target image. results Table show category results , results .025. indeed, convnet learned features capable SIFT alignment, expected size receptive fields. Keypoint classification section, specifically address ability convnet features understand semantic information scale parts. initial test, task keypoint classification: image coordinates keypoint image, train classifier label keypoint? Table Keypoint classification accuracies, percent, twenty categories PASCAL 2011 val, trained SIFT convnet features. SIFT convnet scores bolded category. aero SIFT (radius) 160 conv (layer) bike bird boat bttl bus car cat \\x0cchair cow table dog horse mbike prsn) cat left eye plant sheep sofa train) Figure Cross validation scores cat keypoint classification function SVM parameter), plot accuracy convnet features) plot SIFT features sizes.  experiments Table ) cat nose Figure Convnet features show fine localization ability, stride cases SIFT features perform well. each plot histogram locations maximum responses classifer pixel rectangle ground truth keypoint. for task keypoint data] twenty classes PASCAL VOC 2011]. extract features keypoint SIFT] column convnet layer center lies closest keypoint. (note SIFT features precisely result approximation.) trained one-all linear SVMs train set SIFT radii convolutional layer activations features general, found pooling normalization layers lower performance). set SVM parameter experiments based five-fold cross validation training set (see Figure). table resulting accuracies val set. find features convnet layers consistently perform SIFT task, highest performance coming layers conv4 conv5. note specifically testing convnet features trained classification; net expected achieve higher performance trained task. finally, study precise location understanding classifiers computing responses single-pixel stride ground truth keypoint locations. for keypoints (cat left eye nose), histogram locations maximum responses pixel pixel rectangle keypoint, shown Figure include maximum responses lie boundary rectangle. while SIFT classifiers sensitive precise locations keypoints, cases convnet capable localization finer strides receptive field sizes. this observation motivates final experiments detection-based localization performance. Keypoint prediction large receptive field sizes, convnets work handengineered feature SIFT alignment slightly SIFT keypoint classification. keypoint prediction natural followup test. Section keypoint annotations PASCAL VOC 2011, assume ground truth bounding box. inspired part], train sliding window part detectors predict keypoint locations independently. -cnn] OverFeat] demonstrated effectiveness deep convolutional networks generic object detection task. however, neir investigated application CNNs keypoint prediction-cnn starts bottom region proposal], overlook signal small parts. overfeat, hand, combines convnets trained classification regression runs multi-scale sliding window fashion. rescale bounding box 500 500 compute conv5 (with stride pixels). each cell conv5 256-dimensional descriptor. concatenate conv5 descriptors local region cells, giving receptive field size 195 195 feature dimension 2304. for keypoint, train linear SVM hard negative mining. ten closest features ground truth keypoint positive examples, features rfs keypoint negative examples. train dense SIFT descriptors comparison. compute SIFT grid stride bin size VLFeat]. for sift, features bin size ground truth keypoint positives, samples times bin size negatives. augment SVM detectors spherical Gaussian prior candidate locations constructed nearest neighbor matching. Gaussian location keypoint nearest neighbor training set found cosine similarity pool5 features, fixed standard deviation pixels. let output score local detector keypoint prior score. combine yield final score??    , tradeoff parameter. experiments, set  cross validation. test time, predict keypoint location highest scoring candidate feature locations. evaluate predicted keypoints measure PCK introduced Section taking . predicted keypoint defined correct distance ground truth keypoint  max, height width bounding box. results conv5 SIFT prior shown Table from table, local part detectors trained conv5 feature outperform SIFT large margin prior information helpful cases. knowledge, keypoint prediction results reported dataset. show results categories Figure each set consists rescaled bounding box images ground truth keypoint annotations predicted keypoints SIFT conv5 features, color corresponds keypoint. figure shows, conv5 outperforms sift, managing satisfactory outputs challenge task. small offset noticed keypoints eyes noses, due limited stride scanning windows. final regression finer stride mitigate issue. Conclusion Through visualization, alignment, keypoint prediction, studied ability intermediate features implicitly learned state--art convnet classifier understand specific, local correspondence. despite large receptive fields weak label training, found cases convnet features (and considerably useful) conventional extracting local visual information. acknowledgements This work supported part darpa MSEE SMISC programs, NSF awards iis-1427425, iis-1212798, iis-1116411, support toyota. But works cited Section keypoint localization. Table Keypoint prediction results PASCAL VOC 2011. numbers give average accuracy keypoint prediction criterion Section PCK . SIFT sift+prior conv5 conv5+prior aero Groundtruth bike bird boat bttl sift+prior bus car cat chair cow table dog horse Groundtruth conv5+prior mbike prsn plant sheep sift+prior sofa \\x0ctrain conv5+prior Figure Examples keypoint prediction classes PASCAL dataset: aeroplane, cat, cow, potted plant, horse. each keypoint color. column ground truth annotation, column prediction result sift+prior column conv5+prior. (best viewed color).',\n",
       " 'PP5431': 'with advent online crowdsourcing services Amazon Mechanical turk, crowdsourcing appealing collect labels large-scale data. approach virtues terms scalability availability, labels collected crowd low quality crowdsourcing workers non-experts unreliable. remedy, crowdsourcing services resort labeling redundancy, collecting multiple labels workers item. strategy raises fundamental problem crowdsourcing: infer true labels noisy redundant worker labels? labeling tasks categories, Dawid Skene] propose maximum likelihood approach based expectation-maximization) algorithm. assume worker confusion matrix entry represents probability randomly chosen item class labeled class worker. true labels worker confusion matrices jointly estimated maximizing likelihood observed worker labels, unobserved true labels treated latent variables. -based approach empirical success], oretical guarantee performance. recent oretical study] shows global optimal solutions dawid-skene estimator achieve minimax rates convergence simplified scenario, labeling task binary worker single parameter represent labeling accuracy (referred ?one-coin model? follows). however, likelihood function non-convex, guarantee operational algorithm trapped local optimum. alternative approaches developed aim circumvent oretical deficiencies algorithm, context one-coin model]. unfor1 tunately, eir fail achieve optimal rates depend restrictive assumptions hard justify practice. propose computationally eﬃcient provably optimal algorithm simultaneously estimate true labels worker confusion matrices multi-class labeling problems. approach two-stage procedure, compute initial estimate worker confusion matrices spectral method, stage turn algorithm. mild conditions, show two-stage procedure achieves minimax rates convergence logarithmic factor, iteration. particular,  ), provide bounds number workers number items method correctly estimate labels items probability  establish lower bound demonstrate optimality approach. furr, provide upper lower bounds estimating confusion matrix worker show algorithm achieves optimal accuracy. work optimal algorithm crowdsourcing sheds light understanding general method moments. empirical studies show spectral method initialization algorithm, outperforms random initialization]. work concrete oretically justify observations. starting root consistent estimator obtained spectral method, newton-raphson step leads asymptotically optimal estimator]. however, obtaining root consistent estimator performing newton-raphson step demanding computationally. contrast, initialization doesn root consistent, small portion data suﬃces initialize. moreover, performing iteration computationally attractive numerically robust newton-raphson step high-dimensional problems. related Work Many methods proposed address problem estimating true labels crowdsourcing]. methods, based generative model proposed Dawid \\x0cskene]. particular, Ghosh. ] propose method based Singular Value Decomposition (svd) addresses binary labeling problems one-coin model. analysis] assumes labeling matrix full, worker labels items. relax assumption, Dalvi. ] propose anor svd-based algorithm explicitly considers sparsity labeling matrix algorithm design oretical analysis. karger. propose iterative algorithm binary labeling problems one-coin model] extend multi-class labeling tasks converting-class problem binary problems]. line work assumes tasks assigned workers random regular graph, imposing specific constraints number workers number items. section compare oretical results existing approaches]. methods, incorporate Bayesian inference dawid-skene estimator assuming prior confusion matrices. zhou. ] propose minimax entropy principle crowdsourcing leads exponential family model parameterized worker ability item diﬃculty. items diﬃculty, exponential family model reduces generative model suggested Dawid Skene]. method initializing algorithm crowdsourcing inspired recent work spectral methods estimate latent variable models]. basic idea line work compute third-order empirical moments data estimate parameters computing orthogonal decomposition tensor derived moments. special symmetric structure moments, tensor factorization computed eﬃciently robust tensor power method]. problem approach estimation error poor dependence condition number second-order moment matrix empirically performs worse multiple random initializations. method, contrast, requires rough initialization moment moments; show estimation error depend condition number (see orem)). problem Setup Throughout paper] denotes integer set, ) denotes largest singular matrix suppose workers, items classes. true  algorithm Estimating confusion matrices input: integer observed labels zij  ] ].   ]. output: confusion matrix estimates) Partition workers disjoint non-empty group compute group aggregated labels Zgj. ). ) For, )}, compute order moments  . ), compute   tensor decomposition:  (such svd. ) Compute whitening matrix) Compute eigenvalue-eigenvector pairs vbh whitened tensor robust tensor power method]. compute    vbh  ) For set column coordinate greatest component, set diagonal entry . ). ) Compute label item ] assumed sampled probability distribution ]} positive values satisfying denote vector zij label worker assigns item assigned label write zij represents canonical basis vector entry entries worker label item. probability worker labels randomly chosen item. item labeled worker write zij goal estimate true labels ]} observed labels {zij ], ]}. order obtain estimator, make assumptions process generating observed labels. work Dawid Skene], assume probability worker labels item class class independent chosen item, constant ]. denote constant probability ?ilc  [?il1 ?il2   ?ilk matrix    called confusion matrix worker estimating true labels, estimate confusion matrix worker.  our Algorithm section, present algorithm estimate confusion matrices true labels. algorithm consists stages. stage, compute initial estimate confusion matrices method moments. stage, perform standard algorithm taking result Stage initialization.  Stage Estimating Confusion Matrices Partitioning workers disjoint non-empty groups outline stage following: spectral method estimate averaged confusion matrices groups, utilize intermediate estimate obtain confusion matrix individual worker. particular, , ], calculate averaged labeling group Zgj zij  Denoting aggregated confusion matrix columns(zgj —g1g step estimate    estimate distribution true labels diag    proposition shows solve moments {zgj proposition (anandkumar linearly. independent }. , permutation}. define Zaj Zbj Zaj[zcj zbj[zaj Zbj Zbj]). assume vectors   [zcj zaj[zbj zaj[zaj Zbj [zaj zbj zcj    finite samples, expectations Proposition approximated empirical moments. particular, computed averaging indices   for permutation, )}, compute baj bbj Zcj zbj Zcj Zaj Zaj) Zbj Zaj Zbj Zbj zaj baj bbj) baj bbj zcj ) statement Proposition suggests recover columns diagonal implemented tensor facentries operating moments torization method Algorithm particular, tensor factorization algorithm returns set vectors   }, estimates column (for diagonal entry (for important note tensor factorization algorithm doesn provide one-one correspondence recovered column true columns thus,   represents arbitrary permutation true columns. discover index correspondence, examine greatest component. assume group, probability assigning correct label greater probability assigning  specific incorrect label. assumption made precise section. consequence, corresponds column coordinate expected greater coordinates. thus, set column vector  coordinate greatest component multiple vectors, randomly select vector, randomly select  scalar set diagonal entry  note iterating respectively. , )}, obtain copies estimating matrix average accuracy. step, estimate individual confusion matrix proposition shows recover moments {zij ] proof. proposition  ,  } remaining group index. [zij Zaj empirical approximation Proposition suggests plug estimator compute[zij Zaj matrices obtained step. concretely, calculate    normalize zij Zaj)   normalization operator rescales matrix columns, making column sums one. procedure Stage summarized Algorithm  Stage algorithm stage devoted refining initial estimate provided Stage joint likelihood true label observed labels zij function confusion matrices written(? (?iyj(zij  assuming uniform prior maximize marginal log-likelihood function ‘(?) log( (? )). refine initial estimate Stage maximizing objective function, implemented Expectation Maximization) algorithm. algorithm takes values ?ilc provided output Stage initialization, executes Estep-step round. -step Calculate expected log-likelihood function, respect conditional distribution current estimate  (zij qbjl log (?ilc(?) ? [log(?  qbjl exp \\x0cexp ?ilc(zij log ?il0(zij log ], ]. -step Find estimate maximizes function(? bjl(zij ], ], ].  bilc bjl(zij ec0) practice, alternatively execute updates), iteration convergence. update increases objective function ‘(?). ‘(?) concave, update doesn guarantee converging global maximum. converge distinct local stationary points initializations. neverless, prove section, guaranteed algorithm output statistically optimal estimates true labels worker confusion matrices initialized Algorithm convergence Analysis state main oretical results, introduce notation assumptions. wmin min ?min min smallest portion true labels extreme sparsity level workers. assumption assumes wmin ?min strictly positive, class worker contributes dataset. assumption assumes confusion matrices groups, nonsingular. consequence, define matrices Sab tensors Tabc , Sab  Tabc   positive scalar (sab  assumption assumes group, average probability assigning correct label higher average probability assigning incorrect label. make statement rigorous, define quantity min min min {?gll ?glc } ] } indicating smallest gap diagonal entries non-diagonal entries confusion matrix column. assumption requires strictly positive. note assumption group-based, assume accuracy individual worker. finally, introduce quantity measures average ability ofp workers identifying distinct labels. two discrete distributions DKL) log)) represent-divergence column confusion matrix represents discrete distribution, define quantity: min0 DKL ?il0  quantity lower bounds averaged-divergence columns. strictly positive, means pair labels distinguished subset workers. assumption, assume strictly positive. orems characterize performance algorithm. split convergence analysis parts. orem characterizes performance Algorithm providing suﬃcient conditions achieving arbitrarily accurate initialization. provide proof orem long version paper]. orem scalar scalar satisfying min ?min36 wmin number items satisfies log)/?) =?  ?min min confusion matrices returned Algorithm bounded    ], probability  here,  denotes element-wise -norm matrix. orem characterizes error rate Stage states suﬃciently accurate initialization taken, updates) refine estimates optimal accuracy. long version paper] proof. orem assume positive scalar ?ilc  , ]  initialized manner For scalar confusion matrices    min  ) number workers number items satisfy log/?) log/?) log/?) log) =? ?min wmin obtained iterating) (for round), probability ) Letting ybj arg maxl? ] qbjl ybj holds ].   k22 log(2mk/?) holds, ] ]. orem assumption confusion matrix entries lower bounded restrictive. datasets violating assumption, enforce positive confusion matrix entries adding random noise: Given observed label zij replace random label, ..., probability?. modified model, entry confusion matrix lower bounded orem holds. random noise makes constant smaller original value, change minor small dataset Bird RTE TREC Dog Web classes items 108 800,033 807,665 workers 164 762 177 worker labels,212,000,385,354,567 Table Summary datasets real data experiment. consequence convergence analysis, error rate orem equal constant defined orem combine statements orems. shows choose number \\x0cworkers number items =?  min ?min min, lower bounded problem-specific constant logarithmic terms, high probability, predictor perfectly accurate, estimator bounded  k22 /(? )). show optimality convergence rate, present minimax lower bounds. again] proof. orem universal constants that) For {?ilc number items number workers , ]  ){?ilc inf ) For worker-item pair, pair indices inf  min    part) orem number workers), orwise predictor make mistakes. lower bound matches suﬃcient condition number workers (see. )). part), estimate )) mean-squared error. verifies optimality estimator bil worth noting constraint number items (see. )) improvable. real datasets optimality important worth contrasting convergence rate existing algorithms. ghosh. ] proposed consistent estimators binary one-coin model. attain error rate algorithms require scaling/? algorithm requires scaling log/?). karger. ] proposed algorithms binary multi-class problems. algorithm assumes workers assigned random regular graph. moreover, analysis assumes limit number items infinity, number workers times number items. algorithm longer requires assumptions. compare algorithm majority voting estimator, true label simply estimated majority vote workers. gao Zhou] showed spammers experts, majority voting estimator random guess. cone guarantee good performance. trast, algorithm requires ) aggregated-divergence, small number experts suﬃcient ensure large enough. ] Dalvi. experiments section, report results empirical studies comparing algorithm propose Section (referred opt) variety existing methods based generative model Dawid skene. specifically, compare Dawid Skene estimator opt: 1st iteration opt: 50th iteration: 1st itera tion: 50th iteration Label prediction error Label prediction error opt: 1st iteration opt: 50th iteration: 1st iteration: 50th iteration opt: 1st iteration opt: 50th iteration: 1st iteration: 50th iteration Label prediction error Threshold Threshold Threshold) RTE) Dog) Web Figure Comparing opt thresholding parameter label prediction error plotted 1st update convergence. bird RTE TREC Dog Web opt Majority Voting KOS ghosh-svd  eigenratio  table Error rate (%) predicting true labels real data. initialized majority voting (referred), pure majority voting estimator, multi-class labeling algorithm proposed Karger. ] (referred kos), svd-based algorithm proposed Ghosh. ] (referred ghost-svd) ?eigenvalues ratio? algorithm proposed Dalvi. ] (referred eigenratio). evaluation made real datasets. compare crowdsourcing algorithms binary tasks multi-class tasks. binary tasks include labeling bird species] (bird dataset), recognizing textual entailment] (rte dataset) assessing quality documents TREC 2011 crowdsourcing track] (trec dataset). multi-class tasks include labeling breed dogs ImageNet] (dog dataset) judging relevance web search results] (web dataset). statistics datasets summarized Table ghost-svd algorithm EigenRatio algorithm work binary tasks, evaluated bird, RTE TREC datasets.  opt methods, iterate steps convergence. entries confusion matrix positive, find helpful incorporate prior knowledge initialization stage opt algorithm. particular, estimating confusion matrix entries. ), add extra checking step normalization, examining matrix components greater equal small threshold components smaller reset default choice thresholding parameter  later, compare opt algorithm respect choices important note modification doesn change oretical result, thresholding needed case initialization error bounded orem table summarizes performance method.  opt algorithms consistently outperform methods predicting true label items. kos algorithm, ghost-svd algorithm EigenRatio algorithm yield poorer performance, due fact rely idealized assumptions met real data. figure compare opt algorithm respect thresholding parameters   plot results datasets (ret, dog, web), performance equal slightly opt. plot shows performance opt algorithm stable convergence. iterate, error rates sensitive choice proper choice makes opt outperform. result suggests proper initialization combined iterate good purposes prediction. practice, choice obtained cross validation. With advent online crowdsourcing services Amazon Mechanical turk, crowdsourcing appealing collect labels large-scale data. although approach virtues terms scalability availability, labels collected crowd low quality crowdsourcing workers non-experts unreliable. remedy, crowdsourcing services resort labeling redundancy, collecting multiple labels workers item. such strategy raises fundamental problem crowdsourcing: infer true labels noisy redundant worker labels? for labeling tasks categories, Dawid Skene] propose maximum likelihood approach based expectation-maximization) algorithm. assume worker confusion matrix entry represents probability randomly chosen item class labeled class worker. true labels worker confusion matrices jointly estimated maximizing likelihood observed worker labels, unobserved true labels treated latent variables. although-based approach empirical success], oretical guarantee performance. recent oretical study] shows global optimal solutions dawid-skene estimator achieve minimax rates convergence simplified scenario, labeling task binary worker single parameter represent labeling accuracy (referred ?one-coin model? follows). however, likelihood function non-convex, guarantee operational algorithm trapped local optimum. several alternative approaches developed aim circumvent oretical deficiencies algorithm, context one-coin model]. unfor1 tunately, eir fail achieve optimal rates depend restrictive assumptions hard justify practice. propose computationally eﬃcient provably optimal algorithm simultaneously estimate true labels worker confusion matrices multi-class labeling problems. our approach two-stage procedure, compute initial estimate worker confusion matrices spectral method, stage turn algorithm. under mild conditions, show two-stage procedure achieves minimax rates convergence logarithmic factor, iteration. particular,  ), provide bounds number workers number items method correctly estimate labels items probability  establish lower bound demonstrate optimality approach. furr, provide upper lower bounds estimating confusion matrix worker show algorithm achieves optimal accuracy. this work optimal algorithm crowdsourcing sheds light understanding general method moments. empirical studies show spectral method initialization algorithm, outperforms random initialization]. this work concrete oretically justify observations. starting root consistent estimator obtained spectral method, newton-raphson step leads asymptotically optimal estimator]. however, obtaining root consistent estimator performing newton-raphson step demanding computationally. contrast, initialization doesn root consistent, small portion data suﬃces initialize. moreover, performing iteration computationally attractive numerically robust newton-raphson step high-dimensional problems. Related Work Many methods proposed address problem estimating true labels crowdsourcing]. methods, based generative model proposed Dawid \\x0cskene]. particular, Ghosh. ] propose method based Singular Value Decomposition (svd) addresses binary labeling problems one-coin model. analysis] assumes labeling matrix full, worker labels items. relax assumption, Dalvi. ] propose anor svd-based algorithm explicitly considers sparsity labeling matrix algorithm design oretical analysis. karger. propose iterative algorithm binary labeling problems one-coin model] extend multi-class labeling tasks converting-class problem binary problems]. this line work assumes tasks assigned workers random regular graph, imposing specific constraints number workers number items. Section compare oretical results existing approaches]. methods, incorporate Bayesian inference dawid-skene estimator assuming prior confusion matrices. zhou. ] propose minimax entropy principle crowdsourcing leads exponential family model parameterized worker ability item diﬃculty. when items diﬃculty, exponential family model reduces generative model suggested Dawid Skene]. our method initializing algorithm crowdsourcing inspired recent work spectral methods estimate latent variable models]. basic idea line work compute third-order empirical moments data estimate parameters computing orthogonal decomposition tensor derived moments. given special symmetric structure moments, tensor factorization computed eﬃciently robust tensor power method]. problem approach estimation error poor dependence condition number second-order moment matrix empirically performs worse multiple random initializations. our method, contrast, requires rough initialization moment moments; show estimation error depend condition number (see orem)). Problem Setup Throughout paper] denotes integer set, ) denotes largest singular matrix suppose workers, items classes. true  algorithm Estimating confusion matrices input: integer observed labels zij  ] ].   ]. output: confusion matrix estimates) Partition workers disjoint non-empty group compute group aggregated labels Zgj. ). ) For, )}, compute order moments  . ), compute   tensor decomposition:  (such svd. ) Compute whitening matrix) Compute eigenvalue-eigenvector pairs vbh whitened tensor robust tensor power method]. compute    vbh  ) For set column coordinate greatest component, set diagonal entry . ). ) Compute label item ] assumed sampled probability distribution ]} positive values satisfying denote vector zij label worker assigns item when assigned label write zij represents canonical basis vector entry entries worker label item. let probability worker labels randomly chosen item. item labeled worker write zij our goal estimate true labels ]} observed labels {zij ], ]}. order obtain estimator, make assumptions process generating observed labels. following work Dawid Skene], assume probability worker labels item class class independent chosen item, constant ]. let denote constant probability ?ilc let [?il1 ?il2   ?ilk matrix    called confusion matrix worker besides estimating true labels, estimate confusion matrix worker.  Our Algorithm section, present algorithm estimate confusion matrices true labels. our algorithm consists stages. stage, compute initial estimate confusion matrices method moments. stage, perform standard algorithm taking result Stage initialization.  Stage Estimating Confusion Matrices Partitioning workers disjoint non-empty groups outline stage following: spectral method estimate averaged confusion matrices groups, utilize intermediate estimate obtain confusion matrix individual worker. particular, , ], calculate averaged labeling group Zgj zij  Denoting aggregated confusion matrix columns(zgj —g1g step estimate    estimate distribution true labels diag    proposition shows solve moments {zgj proposition (anandkumar linearly. independent }. let, permutation}. define Zaj Zbj Zaj[zcj zbj[zaj Zbj Zbj]). assume vectors   [zcj zaj[zbj Zaj[zaj Zbj [zaj zbj zcj    since finite samples, expectations Proposition approximated empirical moments. particular, computed averaging indices   For permutation, )}, compute baj bbj Zcj zbj Zcj Zaj Zaj) Zbj Zaj Zbj Zbj zaj baj bbj) baj bbj zcj ) statement Proposition suggests recover columns diagonal this implemented tensor facentries operating moments torization method Algorithm particular, tensor factorization algorithm returns set vectors   }, estimates column (for diagonal entry (for important note tensor factorization algorithm doesn provide one-one correspondence recovered column true columns thus,   represents arbitrary permutation true columns. discover index correspondence, examine greatest component. assume group, probability assigning correct label greater probability assigning  any specific incorrect label. this assumption made precise section. consequence, corresponds column coordinate expected greater coordinates. thus, set column vector  coordinate greatest component multiple vectors, randomly select vector, randomly select  scalar set diagonal entry  note iterating respectively. , )}, obtain copies estimating matrix average accuracy. step, estimate individual confusion matrix proposition shows recover moments {zij see] proof. proposition for ,  } remaining group index. [zij Zaj empirical approximation Proposition suggests plug estimator compute[zij Zaj matrices obtained step. concretely, calculate    normalize zij Zaj)   normalization operator rescales matrix columns, making column sums one. procedure Stage summarized Algorithm  Stage algorithm stage devoted refining initial estimate provided Stage joint likelihood true label observed labels zij function confusion matrices written(? (?iyj(zij  assuming uniform prior maximize marginal log-likelihood function ‘(?) log( (? )). refine initial estimate Stage maximizing objective function, implemented Expectation Maximization) algorithm. algorithm takes values ?ilc provided output Stage initialization, executes Estep-step round. -step Calculate expected log-likelihood function, respect conditional distribution current estimate  (zij qbjl log (?ilc(?) ? [log(?  qbjl exp \\x0cexp ?ilc(zij log ?il0(zij log ], ]. -step Find estimate maximizes function(? bjl(zij ], ], ].  bilc bjl(zij ec0) practice, alternatively execute updates), iteration convergence. each update increases objective function ‘(?). since ‘(?) concave, update doesn guarantee converging global maximum. converge distinct local stationary points initializations. neverless, prove section, guaranteed algorithm output statistically optimal estimates true labels worker confusion matrices initialized Algorithm Convergence Analysis state main oretical results, introduce notation assumptions. let wmin min ?min min smallest portion true labels extreme sparsity level workers. our assumption assumes wmin ?min strictly positive, class worker contributes dataset. our assumption assumes confusion matrices groups, nonsingular. consequence, define matrices Sab tensors Tabc , Sab  Tabc   positive scalar (sab  our assumption assumes group, average probability assigning correct label higher average probability assigning incorrect label. make statement rigorous, define quantity min min min {?gll ?glc } ] } indicating smallest gap diagonal entries non-diagonal entries confusion matrix column. assumption requires strictly positive. note assumption group-based, assume accuracy individual worker. finally, introduce quantity measures average ability ofp workers identifying distinct labels. for \\x0ctwo discrete distributions DKL) log)) represent-divergence since column confusion matrix represents discrete distribution, define quantity: min0 DKL ?il0  quantity lower bounds averaged-divergence columns. strictly positive, means pair labels distinguished subset workers. assumption, assume strictly positive. orems characterize performance algorithm. split convergence analysis parts. orem characterizes performance Algorithm providing suﬃcient conditions achieving arbitrarily accurate initialization. provide proof orem long version paper]. orem for scalar scalar satisfying min ?min36 wmin number items satisfies log)/?) =?  ?min min confusion matrices returned Algorithm bounded    ], probability  here,  denotes element-wise -norm matrix. orem characterizes error rate Stage states suﬃciently accurate initialization taken, updates) refine estimates optimal accuracy. see long version paper] proof. orem assume positive scalar ?ilc  , ]  initialized manner For scalar confusion matrices    min  ) number workers number items satisfy log/?) log/?) log/?) log) =? ?min wmin obtained iterating) (for round), probability ) Letting ybj arg maxl? ] qbjl ybj holds ].   k22 log(2mk/?) holds, ] ]. orem assumption confusion matrix entries lower bounded restrictive. for datasets violating assumption, enforce positive confusion matrix entries adding random noise: Given observed label zij replace random label, ..., probability?. modified model, entry confusion matrix lower bounded orem holds. random noise makes constant smaller original value, change minor small Dataset Bird RTE TREC Dog Web classes items 108 800,033 807,665 workers 164 762 177 worker labels,212,000,385,354,567 Table Summary datasets real data experiment. consequence convergence analysis, error rate orem equal constant defined orem combine statements orems. this shows choose number \\x0cworkers number items =?  min ?min min, lower bounded problem-specific constant logarithmic terms, high probability, predictor perfectly accurate, estimator bounded  k22 /(? )). show optimality convergence rate, present minimax lower bounds. again] proof. orem universal constants that) For {?ilc number items number workers , ]  ){?ilc inf ) For worker-item pair, pair indices inf  min    part) orem number workers), orwise predictor make mistakes. this lower bound matches suﬃcient condition number workers (see. )). part), estimate )) mean-squared error. verifies optimality estimator bil worth noting constraint number items (see. )) improvable. real datasets optimality important worth contrasting convergence rate existing algorithms. ghosh. ] proposed consistent estimators binary one-coin model. attain error rate algorithms require scaling/? algorithm requires scaling log/?). karger. ] proposed algorithms binary multi-class problems. algorithm assumes workers assigned random regular graph. moreover, analysis assumes limit number items infinity, number workers times number items. our algorithm longer requires assumptions. compare algorithm majority voting estimator, true label simply estimated majority vote workers. gao Zhou] showed spammers experts, majority voting estimator random guess. cone guarantee good performance. since trast, algorithm requires ) aggregated-divergence, small number experts suﬃcient ensure large enough. ] Dalvi. Experiments section, report results empirical studies comparing algorithm propose Section (referred opt) variety existing methods based generative model Dawid skene. specifically, compare Dawid Skene estimator opt: 1st iteration opt: 50th iteration: 1st itera tion: 50th iteration Label prediction error Label prediction error opt: 1st iteration opt: 50th iteration: 1st iteration: 50th iteration opt: 1st iteration opt: 50th iteration: 1st iteration: 50th iteration Label prediction error Threshold Threshold Threshold) RTE) Dog) Web Figure Comparing opt thresholding parameter label prediction error plotted 1st update convergence. bird RTE TREC Dog Web opt Majority Voting KOS ghosh-svd  eigenratio  table Error rate (%) predicting true labels real data. initialized majority voting (referred), pure majority voting estimator, multi-class labeling algorithm proposed Karger. ] (referred kos), svd-based algorithm proposed Ghosh. ] (referred ghost-svd) ?eigenvalues ratio? algorithm proposed Dalvi. ] (referred eigenratio). evaluation made real datasets. compare crowdsourcing algorithms binary tasks multi-class tasks. binary tasks include labeling bird species] (bird dataset), recognizing textual entailment] (rte dataset) assessing quality documents TREC 2011 crowdsourcing track] (trec dataset). multi-class tasks include labeling breed dogs ImageNet] (dog dataset) judging relevance web search results] (web dataset). statistics datasets summarized Table since ghost-svd algorithm EigenRatio algorithm work binary tasks, evaluated bird, RTE TREC datasets. for opt methods, iterate steps convergence. since entries confusion matrix positive, find helpful incorporate prior knowledge initialization stage opt algorithm. particular, estimating confusion matrix entries. ), add extra checking step normalization, examining matrix components greater equal small threshold for components smaller reset default choice thresholding parameter  later, compare opt algorithm respect choices important note modification doesn change oretical result, thresholding needed case initialization error bounded orem table summarizes performance method.  opt algorithms consistently outperform methods predicting true label items. KOS algorithm, ghost-svd algorithm EigenRatio algorithm yield poorer performance, due fact rely idealized assumptions met real data. Figure compare opt algorithm respect thresholding parameters   plot results datasets (ret, dog, web), performance equal slightly opt. plot shows performance opt algorithm stable convergence. but iterate, error rates sensitive choice proper choice makes opt outperform. result suggests proper initialization combined iterate good purposes prediction. practice, choice obtained cross validation.',\n",
       " 'PP5465': 'over past decade, progress made developing non-asymptotic bounds estimation error structured parameters based norm regularized regression. estimators form]: ??? argmin(? (?) ) (?) suitable norm(?) suitable loss function  training set, regularization parameter. optimal parameter assumed ?structured,? characterized low norm(?).  estimate optimal structure focus bounding suitable (???   ., norm   function error vector understand state--art non-asymptotic bounds estimation error normregularized regression, aspects) considered) norm(? ) properties design matrix  (iii) loss function(? ) noise model, typically terms ]. literature focused linear model: squared-loss function(?  ?k22  early work estimators focussed norm], led suﬃcient conditions design matrix including restricted-isometry properties (rip) restricted eigenvalue) conditions]. development focussed isotropic Gaussian design matrices, recent work extended analysis norm correlated Gaussian designs] anisotropic sub-gaussian design matrices]. building development] presents unified framework case decomposable norms considers generalized linear models (glms) norms key insights lies restricted set, cone offered]: first, suitably large error vector star, second, restricted error set, loss function satisfy restricted strong convexity (rsc), generalization condition, analysis work out. for isotropic Gaussian design matrices, additional progress made. ] considers constrained estimation formulation atomic norms, gain condition, equivalent condition, Gordons inequality, succinctly represented terms Gaussian width intersection cone error set unit ball/sphere. ] considers related formulations generalized Lasso problems, establish recovery guarantees based Gordons inequality, quantities related Gaussian width. sharper analysis recovery considered], yielding precise characterization phase transition behavior quantities related Gaussian width. ] linear programming estimator-bit compressed sensing setting and, interestingly, concept Gaussian width shows analysis. spite advances, results restricted isotropic Gaussian design matrices. paper, structured estimation problems norm regularization, substantially generalize existing results pertinent aspects: norm, design matrix, loss, noise model. analysis present applies norms. characterize structure error set norms, develop precise relationships error sets regularized constrained versions], establish estimation error bound Section bound depends regularization parameter RSC condition constant section Gaussian sub-gaussian noise develop suitable characterizations terms Gaussian width unit norm ball) }. section characterize RSC condition norm, families design matrices  Gaussian subgaussian, settings family: independent isotropic designs, independent anisotropic designs rows correlated dependent isotropic designs rows isotropic columns correlated implying dependent samples. section show extend analysis generalized linear models (glms) sub-gaussian design matrices norm. analysis techniques simple largely uniform types noise \\x0cdesign matrices. parts analysis geometric, Gaussian widths, measure size suitable sets, tools play key role]. standard covering arguments, sudakov-dudley inequality switch covering numbers Gaussian widths], generic chaining upper bound ?sub-gaussian widths? gaussian widths]. restricted Error Set Recovery Guarantees section, give characterization restricted error set error vector lives, establish clear relationships error sets regularized constrained problems, finally establish upper bounds estimation error. error bound deterministic, quantities involve develop high probability bounds Sections  Restricted Error Set Error Cone belong. start characterization restricted error set lemma For assuming ? (??  ???   belongs set error vector (??    (??   (?? (?)   ) restricted error set convex general norms. interestingly, inequality) triangle inequality, satisfied note restricts set satisfy inequality, yielding restricted error set. particular, direction ., ???   furr, note condition) similar] characterization holds norm, decomposable norms]. while convex set, establish relationship cone constrained problem], (?? cone  (??   (??  ) orem Let ?b2p ?b2p B2p—kuk2 unit ball norm suitable radius.  ??  )  ) denotes Gaussian width set) [supa], isotropic Gaussian random vector. thus, Gaussian width error sets regularized constrained problems closely related. particular??     related observations made special case norm], past work provide explicit characterization terms Gaussian widths. result suggests move error analysis regularized constrained versions estimation problem.  Recovery Guarantees order establish recovery guarantees, start assuming restricted strong convexity (rsc) satisfied loss function cone.,  exists suitable constant (?, (??   (??  (??  ?k22 ) Sections establish precise forms RSC condition wide variety design matrices loss functions. order establish recovery guarantees, focus quantity(?) (??   (?? (??   (??  ) estimated parameter., ??? minimum objective, Since     implies bound   unlike previous results, bound(? established making additional assumptions norm(?). start terms gradient objective result, expresses upper bound   lemma Assume RSC condition satisfied loss(?) parameter  ???   norm(?  (?? (??  (?) sub-gradient norm(?).    ) Note hand side simply norm gradient objective evaluated  special case  gradient objective zero, implying correctly  result insights bound    quantities hand side depend unknown. present anor form result terms quantities norm compatibility constant  supu) kuk2 easier compute bound. orem Assume RSC condition satisfied loss(?) parameter  ???   norm(? with         ) result deterministic, section give precise characterizations satisfy). sections characterize RSC condition constant losses variety design matrices. bounds Regularization Parameter Recall parameter satisfy inequality ? (??  ) hand side inequality issues: depends random variable, depends section, characterize? (?? ))] terms Gaussian width unit norm ball) }, discuss large deviation bounds expectation. ease exposition, present results case squared loss(??  ??  linear model  gaussian sub-gaussian noise. setting(??  ??  analysis extended glms, analysis techniques discussed Section Gaussian designs: \\x0cfirst, Gaussian design xij , independent, elementwise independent Gaussian sub-gaussian noise. orem Let) }. Gaussian design Gaussian subgaussian noise suitable constant? (?? ))]   ) furr, suitable constants probability exp(?    (??      For anisotropic Gaussian design., columns covariance pof result continues hold replaced ?max  ?max (?) denotes operator norm (largest eigenvalue). correlated isotropic design., rows covariance result continues hold replaced ?max  sub-gaussian designs: Recall sub-gaussian variable sub-gaussian norm supp]. now, sub-gaussian design ———xij xij., elementwise independent Gaussian sub-gaussian noise. orem Let) }. subgaussian design Gaussian subgaussian noise suitable constant? (?? ))]   ) interestingly, analysis result involves ?sub-gaussian width? upper bounded constant times Gaussian width, generic chaining]. furr, gaussian-like exponential concentration expectation important classes subgaussian random variables, including bounded random variables, unit vector, Malliavin derivatives surely bounded norm  ]. next, provide mechanism bounding Gaussian width unit norm ball terms Gaussian width suitable cone, obtained shifting translating norm ball. particular, result involves taking point boundary unit norm ball, origin, constructing cone norm ball. construction point boundary, tightest bound obtained taking infimum points boundary. motivation upper bound Gaussian width unit norm ball terms Gaussian width cone considerable advances made recent years upper bounding Gaussian widths cones. lemma Let) unit norm ball)  boundary.   ?(?) (?    diameter measured (?)  cone    ?(?   respect ., cone  intersecting ball radius ?(?).    inf(?))  ??? least Squares models: Restricted Eigenvalue Conditions When loss function squared loss(?   RSC condition) equivalent Restricted Eigenvalue) condition?k22 ?k22  error cone absolute magnitude equivalently  play role condition, loss generality work unit vectors   unit sphere. section, establish conditions variety Gaussian sub-gaussian design matrices, isotropic, anisotropic, dependent rows., samples (rows correlated. results types design matrices types norms, norm, appeared literature]. analysis considers wider variety design matrices establishes RSC conditions  norm. interestingly, Gaussian width) shows bounds, geometric measure size set sub-gaussian design matrices. fact, existing results implicitly width term, form specific chosen norm]. analysis atomic norm) term explicitly, analysis relies gordon inequality], applicable isotropic Gaussian design matrices. proof technique simple, standard covering argument, largely cases considered. unique aspect analysis, proofs, covering numbers Gaussian width sudakov-dudley inequality]. general techniques sharp contrast existing literature conditions, commonly specialized tools Gaussian comparison principles], and specialized analysis geared norm].  Restricted Eigenvalue conditions: Gaussian Designs section, focus case Gaussian design matrices  settings) independent-isotropic, entries elementwise independent) independentanisotropic, rows independent row covariance XiT   (iii) dependent-isotropic, rows isotropic columns correlated XjT   convenience, assume[x2ij noting analysis easily extends general case[x2ij  independent Isotropic Gaussian (iig) designs: IIG setting extensively studied literature]. discussed recent work atomic norms], gordon inequality, conditions IIG setting. goal section two-fold: first, present conditions obtained simple proof technique, show equivalent, constants, condition obtained gordon inequality, arguably heavy-duty technique applicable IIG setting; second, facets present results, apply subsequent-style results give plug estimation error bound). orem Let design matrix  elementwise independent normal., xij ).    probability exp(?  )), inf kxuk2  )   absolute constants. equivalent result obtain directly gordon inequality]: orem Let design matrix elementwise independent normal., xij ).   probability exp(?? )), inf kxuk2  )  [khk2 expected length Gaussian random vector ) interestingly, results equivalent, constants. however, unlike gordon inequality, proof technique generalizes design matrices considered sequel. emphasize additional aspects context analysis, continue hold subsequent results discussed explicitly. first, form result can plugged estimation error bound), simply choose  )) inf kxuk2   high probability. table shows summary recovery bounds Independent Isotropic Gaussian design matrices Gaussian noise. second, result depend fact    kuk2 example, cone intersecting sphere radius give    kuk2 simplicity, .,  straightforward extension yields inf? kxuk2  )  )kuk2 probability exp(?  )), kxuk2 kuk kuk2(akuk2 kuk2]. scale independence fact error bound analysis Section finally, note leading constant consequence choice -net covering proof. constants, choices constants change based choice. independent Anisotropic Gaussian (iag) designs: setting rows design matrix independent, row sampled anisotropic Gaussian distribution.,   setting considered literature] special case norms, sharp results established Gaussian comparison techniques]. show equivalent results obtained simple technique, rely Gaussian comparisons]. orem Let design matrix row wise independent row    probability exp(?    inf kxuk2   ?max (?) )    inf uk2 ?max (?) denotes largest eigenvalue constants.  comparison results] instructive. leading term appears] well simply considered inf sides, result]  uk2 term. term] depends largest entry diagonal log kuk1 terms consequence special case analysis forp norm. contrast, general case simply scaled Gaussian width term ?max (?) ). dependent Isotropic Gaussian (dig) designs: setting rows isotropic gaussians, columns correlated  design matrix interestingly, correlation structure columns make samples dependent, scenario widely studied literature]. show simple technique continues work scenario rar intuitive result.    matrix rows isotropic Gaussian random vectors orem Let correlated  set  columns probability exp(?   inf kxuk2 (?)  ?max (?) )  constants. note assumption[x2ij correlation matrix implying(?) making sample size dependence explicit. intuitively, due sample correlations, samples effectively equivalent(?) ?max (?) samples. max (?)  Restricted Eigenvalue conditions: sub-gaussian Designs section, focus case sub-gaussian design matrices  settings) independent-isotropic, rows independent isotropic) independentanisotropic, rows independent row covariance XiT (iii) dependent-isotropic, rows isotropic columns correlated XjT convenience, assume[x2ij sub-gaussian norm ———xij ]. recent work] considers generalizations conditions subgaussian designs, proof techniques different. independent Isotropic sub-gaussian designs: start setting sub-gaussian design matrix  independent rows row isotropic. orem Let  design matrix rows independent isotropic subgaussian random vectors set  probability exp(?  )), inf kxuk2  )   constants depend sub-gaussian norm ———xij Independent Anisotropic sub-gaussian designs: setting rows design matrix independent, row sampled anisotropic sub-gaussian distribution., ———xij XiT orem Let subgaussian design matrix row wise independent, row XiT     probability exp(?  )),  ) inf kxuk2   ?max (?) )    inf uk2 ?max (?) denotes largest eigenvalue constants depend sub-gaussian norm ———xij Note] establish conditions anisotropic sub-gaussian designs special case norm. contrast, results general terms Gaussian width). dependent Isotropic sub-gaussian designs: setting sub-gaussian? isotropic sub-gaussian rows, columns correlated sign matrix implying dependent samples.    sub-gaussian design matrix isotropic rows correlated \\x0corem Let      probability columns exp(?  )),  (?)  ?max )  ) inf kxuk constants depend sub-gaussian norm ———xij Generalized Linear models: Restricted Strong Convexity section, setting conditional probabilistic distribution exponential family distribution; exp?,  )}, ?(?) logpartition function. generalized linear models negative likelihood conditional distributions loss function(?  ?, ). squares regression logistic regression popular special cases glms.  (??  ?,  plays role noise. hence, analysis Section applied assuming gaussian sub-gaussian. obtain RSC conditions glms, note(??   ??  Table summary values norms values correct upto constants. ) norm norm(? log oi2  max )      log ], orem.  legendre type, derivative ?(?) positive. RSC condition relies non-trivial lower bound quantity, analysis considers suitable compact set min ) bounded zero. compact set, ?(?)  (??   hxi[—hxi [—hxi—  give characterization RSC condition independent isotropic sub-gaussian \\x0cdesign matrices  analysis suitably generalized design matrices considered Section techniques. before, denote    kuk2 furr, assume??  constant assuming subgaussian entries ———xij hxi hxi sub-gaussian random variables sub-gaussian norm. {—hxi—  exp {—hxi   exp result present terms constants   , suitably chosen orem Let  design matrix independent isotropic sub-gaussian rows. set   ),  ?  (cw2?  ??)? suitable constants probability exp  c44  inf(??    )        min ), constants depend sub-gaussian norm ———xij form result closely related result condition inf kxuk2 Section. note RSC analysis GLMs considered] specific norms, analysis applies set  norm. furr, similar argument structure Section, analysis GLMs extended anisotropic dependent design matrices. conclusions paper presents general set results tools characterizing nonasymptotic estimation error norm regularized regression problems. analysis holds norm, includes existing literature focused structured sparsity related mes special cases. work viewed direct generalization results], presented related results decomposable norms. analysis illustrates important role Gaussian widths, geometric measure size suitable sets, play results. furr, error sets regularized constrained versions problems shown closely related]. forward, interesting explore similar generalizations semi-parametric non-parametric settings. acknowledgements: anonymous reviewers helpful comments suggestions related work. sergey bobkov, Snigdhansu chatterjee, Pradeep Ravikumar discussions related paper. research supported NSF grants iis-1447566, iis-1422557, ccf-1451986, cns-1314560, iis-0953274, iis-1029711, NASA grant nnx12aq39a. Over past decade, progress made developing non-asymptotic bounds estimation error structured parameters based norm regularized regression. such estimators form]: ??? argmin(? (?) ) (?) suitable norm(?) suitable loss function  training set, regularization parameter. optimal parameter assumed ?structured,? characterized low norm(?). since estimate optimal structure focus bounding suitable (???   ., norm   function error vector understand state--art non-asymptotic bounds estimation error normregularized regression, aspects) considered) norm(? ) properties design matrix  (iii) loss function(? ) noise model, typically terms ]. most literature focused linear model: squared-loss function(?  ?k22  early work estimators focussed norm], led suﬃcient conditions design matrix including restricted-isometry properties (rip) restricted eigenvalue) conditions]. while development focussed isotropic Gaussian design matrices, recent work extended analysis norm correlated Gaussian designs] anisotropic sub-gaussian design matrices]. building development] presents unified framework case decomposable norms considers generalized linear models (glms) norms two key insights lies restricted set, cone offered]: first, suitably large error vector star, second, restricted error set, loss function satisfy restricted strong convexity (rsc), generalization condition, analysis work out. For isotropic Gaussian design matrices, additional progress made. ] considers constrained estimation formulation atomic norms, gain condition, equivalent condition, Gordons inequality, succinctly represented terms Gaussian width intersection cone error set unit ball/sphere. ] considers related formulations generalized Lasso problems, establish recovery guarantees based Gordons inequality, quantities related Gaussian width. sharper analysis recovery considered], yielding precise characterization phase transition behavior quantities related Gaussian width. ] linear programming estimator-bit compressed sensing setting and, interestingly, concept Gaussian width shows analysis. spite advances, results restricted isotropic Gaussian design matrices. paper, structured estimation problems norm regularization, substantially generalize existing results pertinent aspects: norm, design matrix, loss, noise model. analysis present applies norms. characterize structure error set norms, develop precise relationships error sets regularized constrained versions], establish estimation error bound Section bound depends regularization parameter RSC condition constant Section Gaussian sub-gaussian noise develop suitable characterizations terms Gaussian width unit norm ball) }. Section characterize RSC condition norm, families design matrices  Gaussian subgaussian, settings family: independent isotropic designs, independent anisotropic designs rows correlated dependent isotropic designs rows isotropic columns correlated implying dependent samples. Section show extend analysis generalized linear models (glms) sub-gaussian design matrices norm. our analysis techniques simple largely uniform types noise \\x0cdesign matrices. parts analysis geometric, Gaussian widths, measure size suitable sets, tools play key role]. standard covering arguments, sudakov-dudley inequality switch covering numbers Gaussian widths], generic chaining upper bound ?sub-gaussian widths? Gaussian widths]. Restricted Error Set Recovery Guarantees section, give characterization restricted error set error vector lives, establish clear relationships error sets regularized constrained problems, finally establish upper bounds estimation error. error bound deterministic, quantities involve develop high probability bounds Sections  Restricted Error Set Error Cone belong. start characterization restricted error set lemma For assuming ? (??  ???   belongs set error vector (??    (??   (?? (?)   ) restricted error set convex general norms. interestingly, inequality) triangle inequality, satisfied note restricts set satisfy inequality, yielding restricted error set. particular, direction ., ???   furr, note condition) similar] characterization holds norm, decomposable norms]. While convex set, establish relationship cone constrained problem], (?? cone  (??   (??  ) orem Let ?b2p ?b2p B2p—kuk2 unit ball norm suitable radius.  ??  )  ) denotes Gaussian width set) [supa], isotropic Gaussian random vector. thus, Gaussian width error sets regularized constrained problems closely related. particular??     related observations made special case norm], past work provide explicit characterization terms Gaussian widths. result suggests move error analysis regularized constrained versions estimation problem.  Recovery Guarantees order establish recovery guarantees, start assuming restricted strong convexity (rsc) satisfied loss function cone.,  exists suitable constant (?, (??   (??  (??  ?k22 ) Sections establish precise forms RSC condition wide variety design matrices loss functions. order establish recovery guarantees, focus quantity(?) (??   (?? (??   (??  ) estimated parameter., ??? minimum objective, Since     implies bound   unlike previous results, bound(? established making additional assumptions norm(?). start terms gradient objective result, expresses upper bound   lemma Assume RSC condition satisfied loss(?) parameter with ???   norm(?  (?? (??  (?) sub-gradient norm(?).    ) Note hand side simply norm gradient objective evaluated  for special case  gradient objective zero, implying correctly  while result insights bound    quantities hand side depend unknown. present anor form result terms quantities norm compatibility constant  supu) kuk2 easier compute bound. orem Assume RSC condition satisfied loss(?) parameter  ???   norm(? With         ) result deterministic, Section give precise characterizations satisfy). Sections characterize RSC condition constant losses variety design matrices. Bounds Regularization Parameter Recall parameter satisfy inequality ? (??  ) hand side inequality issues: depends random variable, depends section, characterize? (?? ))] terms Gaussian width unit norm ball) }, discuss large deviation bounds expectation. for ease exposition, present results case squared loss(??  ??  linear model  Gaussian sub-gaussian noise. for setting(??  ??  analysis extended glms, analysis techniques discussed Section Gaussian designs: \\x0cfirst, Gaussian design xij , independent, elementwise independent Gaussian sub-gaussian noise. orem Let) }. Gaussian design Gaussian subgaussian noise suitable constant? (?? ))]   ) furr, suitable constants probability exp(?    (??      For anisotropic Gaussian design., columns covariance pof result continues hold replaced ?max  ?max (?) denotes operator norm (largest eigenvalue). for correlated isotropic design., rows covariance result continues hold replaced ?max  sub-gaussian designs: Recall sub-gaussian variable sub-gaussian norm supp]. now, sub-gaussian design ———xij xij., elementwise independent Gaussian sub-gaussian noise. orem Let) }. subgaussian design Gaussian subgaussian noise suitable constant? (?? ))]   ) interestingly, analysis result involves ?sub-gaussian width? upper bounded constant times Gaussian width, generic chaining]. furr, gaussian-like exponential concentration expectation important classes subgaussian random variables, including bounded random variables, unit vector, Malliavin derivatives surely bounded norm  ]. next, provide mechanism bounding Gaussian width unit norm ball terms Gaussian width suitable cone, obtained shifting translating norm ball. particular, result involves taking point boundary unit norm ball, origin, constructing cone norm ball. since construction point boundary, tightest bound obtained taking infimum points boundary. motivation upper bound Gaussian width unit norm ball terms Gaussian width cone considerable advances made recent years upper bounding Gaussian widths cones. lemma Let) unit norm ball)  boundary. for  ?(?) (?    diameter measured let(?)  cone    ?(?   respect ., cone  intersecting ball radius ?(?).    inf(?))  ??? Least Squares models: Restricted Eigenvalue Conditions When loss function squared loss(?   RSC condition) equivalent Restricted Eigenvalue) condition?k22 ?k22  error cone since absolute magnitude equivalently  play role condition, loss generality work unit vectors   unit sphere. section, establish conditions variety Gaussian sub-gaussian design matrices, isotropic, anisotropic, dependent rows., samples (rows correlated. results types design matrices types norms, norm, appeared literature]. our analysis considers wider variety design matrices establishes RSC conditions  norm. interestingly, Gaussian width) shows bounds, geometric measure size set sub-gaussian design matrices. fact, existing results implicitly width term, form specific chosen norm]. analysis atomic norm) term explicitly, analysis relies gordon inequality], applicable isotropic Gaussian design matrices. proof technique simple, standard covering argument, largely cases considered. unique aspect analysis, proofs, covering numbers Gaussian width sudakov-dudley inequality]. our general techniques sharp contrast existing literature conditions, commonly specialized tools Gaussian comparison principles], and specialized analysis geared norm].  Restricted Eigenvalue conditions: Gaussian Designs section, focus case Gaussian design matrices  settings) independent-isotropic, entries elementwise independent) independentanisotropic, rows independent row covariance XiT   (iii) dependent-isotropic, rows isotropic columns correlated XjT   for convenience, assume[x2ij noting analysis easily extends general case[x2ij  independent Isotropic Gaussian (iig) designs: IIG setting extensively studied literature]. discussed recent work atomic norms], gordon inequality, conditions IIG setting. our goal section two-fold: first, present conditions obtained simple proof technique, show equivalent, constants, condition obtained gordon inequality, arguably heavy-duty technique applicable IIG setting; second, facets present results, apply subsequent-style results give plug estimation error bound). orem Let design matrix  elementwise independent normal., xij ).    probability exp(?  )), inf kxuk2  )   absolute constants. equivalent result obtain directly gordon inequality]: orem Let design matrix elementwise independent normal., xij ).   probability exp(?? )), inf kxuk2  )  [khk2 expected length Gaussian random vector ) interestingly, results equivalent, constants. however, unlike gordon inequality, proof technique generalizes design matrices considered sequel. emphasize additional aspects context analysis, continue hold subsequent results discussed explicitly. first, form result can plugged estimation error bound), simply choose  )) inf kxuk2   high probability. table shows summary recovery bounds Independent Isotropic Gaussian design matrices Gaussian noise. second, result depend fact    kuk2 for example, cone intersecting sphere radius give    kuk2 for simplicity, .,  straightforward extension yields inf? kxuk2  )  )kuk2 probability exp(?  )), kxuk2 kuk kuk2(akuk2 kuk2]. such scale independence fact error bound analysis Section finally, note leading constant consequence choice -net covering proof. one constants, choices constants change based choice. independent Anisotropic Gaussian (iag) designs: setting rows design matrix independent, row sampled anisotropic Gaussian distribution.,   setting considered literature] special case norms, sharp results established Gaussian comparison techniques]. show equivalent results obtained simple technique, rely Gaussian comparisons]. orem Let design matrix row wise independent row    probability exp(?    inf kxuk2   ?max (?) )    inf uk2 ?max (?) denotes largest eigenvalue constants.  comparison results] instructive. leading term appears] well simply considered inf sides, result]  uk2 term. term] depends largest entry diagonal log kuk1 terms consequence special case analysis forp norm. contrast, general case simply scaled Gaussian width term ?max (?) ). dependent Isotropic Gaussian (dig) designs: setting rows isotropic gaussians, columns correlated  design matrix interestingly, correlation structure columns make samples dependent, scenario widely studied literature]. show simple technique continues work scenario rar intuitive result.    matrix rows isotropic Gaussian random vectors orem Let correlated  set  columns probability exp(?   inf kxuk2 (?)  ?max (?) )  constants. note assumption[x2ij correlation matrix implying(?) making sample size dependence explicit. intuitively, due sample correlations, samples effectively equivalent(?) ?max (?) samples. max (?)  Restricted Eigenvalue conditions: sub-gaussian Designs section, focus case sub-gaussian design matrices  settings) independent-isotropic, rows independent isotropic) independentanisotropic, rows independent row covariance XiT (iii) dependent-isotropic, rows isotropic columns correlated XjT for convenience, assume[x2ij sub-gaussian norm ———xij ]. recent work] considers generalizations conditions subgaussian designs, proof techniques different. independent Isotropic sub-gaussian designs: start setting sub-gaussian design matrix  independent rows row isotropic. orem Let  design matrix rows independent isotropic subgaussian random vectors set  probability exp(?  )), inf kxuk2  )   constants depend sub-gaussian norm ———xij Independent Anisotropic sub-gaussian designs: setting rows design matrix independent, row sampled anisotropic sub-gaussian distribution., ———xij XiT orem Let subgaussian design matrix row wise independent, row XiT     probability exp(?  )),  ) inf kxuk2   ?max (?) )    inf uk2 ?max (?) denotes largest eigenvalue constants depend sub-gaussian norm ———xij Note] establish conditions anisotropic sub-gaussian designs special case norm. contrast, results general terms Gaussian width). dependent Isotropic sub-gaussian designs: setting sub-gaussian? isotropic sub-gaussian rows, columns correlated sign matrix implying dependent samples.    sub-gaussian design matrix isotropic rows correlated \\x0corem Let      probability columns exp(?  )),  (?)  ?max )  ) inf kxuk constants depend sub-gaussian norm ———xij Generalized Linear models: Restricted Strong Convexity section, setting conditional probabilistic distribution exponential family distribution; exp?,  )}, ?(?) logpartition function. generalized linear models negative likelihood conditional distributions loss function(?  ?, ). least squares regression logistic regression popular special cases glms. since (??  ?,  plays role noise. hence, analysis Section applied assuming Gaussian sub-gaussian. obtain RSC conditions glms, note(??   ??  Table summary values norms values correct upto constants. ) norm norm(? log oi2  max )      log ], orem. since Legendre type, derivative ?(?) positive. since RSC condition relies non-trivial lower bound quantity, analysis considers suitable compact set min ) bounded zero. outside compact set, ?(?)  (??   hxi[—hxi [—hxi—  give characterization RSC condition independent isotropic sub-gaussian \\x0cdesign matrices  analysis suitably generalized design matrices considered Section techniques. before, denote    kuk2 furr, assume??  constant assuming subgaussian entries ———xij hxi hxi sub-gaussian random variables sub-gaussian norm. let {—hxi—  exp {—hxi   exp result present terms constants   , suitably chosen orem Let  design matrix independent isotropic sub-gaussian rows. set   ),  ?  (cw2?  ??)? suitable constants probability exp  c44  inf(??    )        min ), constants depend sub-gaussian norm ———xij form result closely related result condition inf kxuk2 Section. note RSC analysis GLMs considered] specific norms, analysis applies set  norm. furr, similar argument structure Section, analysis GLMs extended anisotropic dependent design matrices. Conclusions paper presents general set results tools characterizing nonasymptotic estimation error norm regularized regression problems. analysis holds norm, includes existing literature focused structured sparsity related mes special cases. work viewed direct generalization results], presented related results decomposable norms. our analysis illustrates important role Gaussian widths, geometric measure size suitable sets, play results. furr, error sets regularized constrained versions problems shown closely related]. going forward, interesting explore similar generalizations semi-parametric non-parametric settings. acknowledgements: anonymous reviewers helpful comments suggestions related work. Sergey bobkov, Snigdhansu chatterjee, Pradeep Ravikumar discussions related paper. research supported NSF grants iis-1447566, iis-1422557, ccf-1451986, cns-1314560, iis-0953274, iis-1029711, NASA grant nnx12aq39a.',\n",
       " 'PP5497': 'multivariate Gaussian (normal) distribution ubiquitous statistical applications machine learning, signal processing, computational biology, ors. usually, distributed random vectors denoted (?,   mean,  covariance matrix. set realizations applications require estimating eir covariance inverse called precision matrix. estimating inverse covariance matrix applications] represents underlying graph Gaussian Markov Random Field (gmrf). samples vector covariance matrix approximated standard maximum likelihood estimator (mle), leads  mle=?       ) called empirical covariance matrix. specifically, mle, estimated solving optimization problem min) min log(det), ) authors contributed equally work. eran Treister grateful Azrieli Foundation award Azrieli fellowship. equation) standard MLE estimator. however, unbiased MLE estimation preferred, replaces denominator.  obtained applying log probability density function Normal distribution. however, number samples lower dimension vectors) rank deficient invertible, true assumed positive definite, full-rank. still, estimate matrix adding furr assumptions. well-known]  random scalar variables entries conditionally independent. refore, work adopt notion estimating inverse covariance, assuming sparse. (note cases dense.) purpose, follow], minimize) sparsity-promoting prior: min) min) ?kak1 ) here) MLE functional defined), kak1  —aij regularization parameter balances sparsity solution fidelity data. sparsity assumption corresponds small number statistical dependencies variables. problem) called Covariance Selection], non-smooth convex. methods recently developed solving)?see] references rein. current state--art methods], involve ?proximal newton? approach], quadratic approximation applied smooth part), leaving non-smooth term intact, order obtain Newton descent direction. obtain this, gradient Hessian) needed)  ) kronecker product. gradient) shows main diﬃculty solving problem inverse sparse matrix dense expensive compute. advantage proximal Newton approach problem low overhead: calculating), Hessian cost]. work aim solving large scale instances), large variables fit memory. problem sizes required fmri] gene expression analysis] applications, example. large values introduce limitations) preclude storing full matrix), vectors assumed fit memory. ) While sparse matrix) fits memory, dense inverse not. limitation, methods mentioned solve), require computing full gradient), dense symmetric matrix. applies blocking strategies], target dense covariance matrix rar inverse, dual formulation). exception proximal Newton approach], made suitable large-scale matrices treating Newton direction problem blocks. paper, introduce iterative block-coordinate Descent] method solving largescale instances). treat problem blocks defined subsets columns block sub-problem solved quadratic approximation, resulting descent direction corresponds variables block. sub-problem time, fully store gradient Hessian block. contrast] applies blocking approach full Newton problem, results sparse descent direction. , columns calculated gradient Hessian problem iteration solving full Newton problem. refore, method requires calculations], computationally expensive task algorithms. furrmore, blocking strategy eﬃcient linesearch procedure] requires computing determinant sparse matrix. method linear order convergence, converges iterations] experiments. note asymptotic convergence] quadratic exact Newton direction found iteration, costly large-scale problems.  newton Method Covariance Selection proximal Newton approach mentioned earlier iterative, iteration smooth part objective) approximated order Taylor expansion iterate) newton direction solution penalized quadratic minimization problem, min ) min(?  )  ) inverse iterate. note gradient Hessian) featured terms), respectively, term) constant ignored. problem) corresponds well-known LASSO problem], popular machine learning signal/image processing applications]. methods] apply lasso-solvers treating Newton direction minimization). direction computed, added) employing linesearch procedure suﬃciently reduce objective) \\x0censuring positive definiteness. end, updated iterate)  parameter obtained armijo rule]. , choose initial step size define  smallest satisfies constraint) condition)  ) (??  )  ) ) parameters chosen respectively.  Restricting Updates Active Sets additional significant idea] restrict minimization) iteration ?active set? variables rest zeros. active set matrix defined active, Aij    ) This set definition sub-gradient). particular) approaches ) solution active approaches, Aij noted], restricting) variables Active) reduces computational complexity: matrix Hessian (third) term) calculated) operations) —active hence, method solving LASSO problem utilized solve) effectively saving computations restricting solution Active) experiments verified restricting minimization) Active) significantly increase number iterations needed convergence. block-coordinate-descent Inverse Covariance (bcd) Estimation Section describe contribution. solve problem), apply iterative blockcoordinate-descent approach]. iteration, divide column set, ..., blocks. iterate blocks, turn minimize) restricted ?active? variables block, determined). matrix entries remain fixed update. matrix updated block-minimization. choose blocks sets columns portion gradient) corresponds blocks computed solutions linear systems. matrix symmetric, rows updated simultaneously. figure shows BCD iteration blocks columns chosen sequential order. practice, sets columns non-contiguous vary BCD iterations. elaborate partition Figure Example BCD iteration. blocks treated successively. columns, advantages block-partitioning. partitioning matrix small blocks enables method solve) high dimensions millions variables), requiring) additional memory, number blocks (that addition memory needed storing iterated solution) itself).  Block Coordinate Descent Iteration Assume set columns, ..., divided blocks set indices corresponds columns rows block. mentioned before, bcd algorithm traverse blocks update iterated solution matrix block block. ) denote updated matrix treating block iteration iterate) defined block treated) treat block), adopt ideas earlier: quadratic approximation solve block, restricting updated entries active set. simplicity) notation section, denote updated matrix treating block iteration update block change entries rows/columns first, form minimize quadratic approximation problem), restricted rows/columns min ) similarly), non-zero (?) quadratic approximation) entries rows/columns addition, non-zeros restricted active) defined). , restrict minimization) active)  ,   activeij) elements set entire treatment block. calculate set, check condition) columns rows define active set, calculate gradient) block calculate columns main computational task algorithm. achieve that, solve linear systems,   solution canonical vectors right-handsides ., linear systems achieved ways. direct methods applied Cholesky factorization, requires operations. large dimensions, iterative methods Conjugate Gradients) preferred, cost iteration proportional number non-zeros sparse matrix. Section Appendix details computational cost part algorithm.  Treating block-subproblem newton Method Newton direction block, solve LASSO problem), solvers]. choose polak-ribiere non-linear Conjugate Gradients (nlcg) method] which, toger diagonal preconditioner, solve problem]. describe NLCG algorithm Apendix. method, calculate objective) gradient eﬃciently. calculation objective) simpler full version), compute objective) blocks rows/columns considered. denoting gradient calculate matrices entries non-zero rows/columns matrices symmetric, hence, columns necessary. idea applies term objective) well. iteration NLCG method, main \\x0ccomputational task involves calculating calculated obtaining), columns that, reuse columns denote WIj result columns notice WIj product WIj computed eﬃciently sparse. computing WIj anor expensive part algorithm, exploit restriction Active set. , compute entries). this, follow idea] rows columns) represented). columns ?neighborhood? defined , activeij) ) size set determine amount additional columns need, refore small possible. achieve that, define blocks clustering methods]. metis], methods instead. aim methods partition indices matrix columns/rows disjoint subsets small size, non-zero entries diagonal blocks matrix correspond subset. notation, aim size small block size small enough. note compute WNj store  numbers WNj however, situations matrix dense columns, resulting sets size). computing WNj sets memory limitations. treat case separately?see Section Appendix details. discussion computational cost part?see Section appendix.  Optimizing Solution Newton Direction line-search Assume newton direction obtained solving problem). seek update) iterated matrix   obtained linesearch procedure similarly Equation). general Newton direction matrix ), procedure requires calculating determinant matrix. ], solving linear systems decreasing sizes  however, direction special block structure, obtain significantly cheaper linesearch procedure compared], assuming blocks small. first, trace terms involved objective) calculated respect entries columns rows account symmetry). log det term, however, special care, eventually reduced calculating determinant  matrix, cheaper block size decreases. introduce partitioning matrix blocks, set indices }. assume loss generality rows columns permuted columns/rows indices first, a11 A12  =?  a21 A22   ) partitioning blocks. sub-matrix A11 corresponds elements rows Schur complement], invertible matrix columns block-partitioning above, holds: log det) log det(a22 log det(a11 a12 A21 ) addition, symmetric matrix applies: a22 A11 a12 A21 ) partitioning write): Using notation  log det log det log det)               A22 (note replaced ease notation.)    finally, positive definiteness condition+? involved linesearch) equivalent ). iterations assuming remains positive definite linesearch guarantee iterated solution matrix update. requires initialization algorithm) positive definite. set small, matrices) small  —), easily compute objective apply Armijo rule)  calculating matrices) expensive, however, show Appendix, obtained previously computed matrices WIj WNj mentioned earlier. refore, computing) achieved time complexity. algorithm: bcd ,?) ... Calculate clusters elements based) ) denote) Compute WIj solve linear systems) Define ActiveIj), define set). ) Compute WNj solve linear systems Find Newton direction solving LASSO problem). ) Update solution: end  linesearch. ) denote) end Algorithm Block Coordinate Descent Inverse Covariance Estimation Convergence Analysis section, elaborate convergence bcd algorithm global optimum). base analysis]. ], general block-coordinate-descent approach analyzed solve minimization problems form) composed sum smooth function (?) separable convex function(? case log det) kak1 respectively. setup fits functional] treats problem domain, minimization) constrained symmetric positive definite matrices domain. overcome limitation, authors] extended analysis] treat specific constrained problem). particular] block-coordinate-descent methods step subset variables updated. gauss-seidel condition ensure variables updated steps    set variables, fixed number. similarly], treating block columns bcd algorithm equivalent updating elements active set ActiveIj), update elements ActiveIj). refore), set j2t,   activeij), j2t ActiveIj), step index corresponds block iteration bcdic. , Lemma], shown setting elements active set block satisfies optimality condition step. refore, algorithm update elements ActiveIj). now, fixed blocks coordinates Algorithm clustering applied), gauss-seidel condition) satisfied blocks. clustering applied, block-partitioning change activation clustering method. refore, condition) satisfied  maximum number blocks obtained activations clustering algorithm. completeness, include Appendix lemmas] proof orem) orem algorithm sequence converges global optimum). numerical Results section demonstrate eﬃciency bcd method, compare methods small large scales. small-scale problems include QUIC], BIGQUIC-ista], state--art methods scale. large-scale problems, compare method big-quic feasible method scale. methods, original code provided authors? implemented parallelized (except QUIC partially parallelized). code bcd MATLAB based routines. All experiments run machine Intel Xeon-2650.0ghz processors cores 64gb ram, Windows. stopping criterion bcd, rule]: kgrads) grads (?) minimal norm subgradient, defined Equation) Appendix.  choose, results entries) digits accurate compared true solution  ], approximate WIj WNj, stop residual drops respectively. stopping NLCG (algorithm nlcg (see details end Section). note large-scale test problems, bcd optimal block size requires memory big-quic.  Syntic Experiments syntic experiments compare performance methods. first, random matrix], generated non-zeros row, well-conditioned. generate matrices sizes varying,000 160,000, generate 200 samples 200). values chosen solution approximately 10n non-zeros. bcd run block sizes, 128, 256, 256 random tests Table respectively. problem i2d version chain], represented stencil applied square lattice.  chosen non-zeros. tests, bcd run block size 1024. table summarizes results test case. results show small-scale problems-ista fastest method bcd. however, size,000 higher, bcd fastest. run QUIC-ista problems larger,000 memory limitations. time gap-ista bcd big-quic smallscales reduced programs receive matrix input  Gene Expression Analysis Experiments For large-scale real-world experiments, gene expression datasets Gene Expression Omnibus (http://www.ncbi.nlm.nih.gov/geo/).  test,   bcd big-quic QUIC-ista random random 10k random 20k random 40k random 80k random 160k 5002 7082 10002,138 118,794 237,898 475,406 950,950,901,404,248,000,503,488,996,000,164 139,708 311,932 423,696 891,268,852,198,553,698,002,338,684,306) 265s) 729s,102s,296s,235s) 130,636s) 777,947s) 673s,671s,764s,584s,530s) 203,370s,220,213s) 114s) 823s) 491s) table Results random syntic experiments.   denote number non-zeros true estimated inverse covariance matrices, respectively. run, timings reported seconds number iterations parenses. ?*? means algorithm ran memory. tests reported]. data preprocessed unit variance variable., diag). table shows datasets numbers variables) samples) each. particular, datasets variables samples). size problems, ran bcd big-quic test cases. tests Table chosen solution matrix 10n non-zeros. fourth test, choose high  low number samples solutions smaller dense. bcd run block size 256 tests Table found datasets challenging syntic experiments above. still, algorithms bcd big-quic manage estimate inverse covariance matrix reasonable time. syntic case, bcd outperforms big-quic test cases. bcd requires smaller number iterations converge, translates shorter timings. moreover, average time bcd iteration faster big-quic. code Description GSE1898 GSE20194 GSE17951 GSE14322 Liver cancer Breast cancer Prostate cancer Liver cancer   bcd big-quic, 794, 283, 675 104, 702 182 278 154 293,845 197,953 558,929,973,476 788) 452,621,314,079,810,229) 127,199s) table Gene expression results.  denotes number non-zeros estimated covariance matrix. run, timings reported seconds number iterations parenses. conclusions work introduced block-coordinate Descent method solving sparse inverse covariance problem. method low memory footprint, refore attractive solving large-scale instances problem. solves problem iterating updating matrix block block, block chosen subset columns respective rows. block sub-problem, proximal Newton method applied, requiring solution LASSO problem find descent direction. update limited subset columns rows, store gradient Hessian block, enjoy eﬃcient linesearch procedure. numerical results show medium-large scale experiments algorithm faster state--art methods, problem hard. acknowledgement: authors prof. Irad Yavneh valuable comments guidance work. research leading results received funding European union Seventh Framework Programme (fp7/2007-2013) grant agreement 623212 Multiscale inversion. multivariate Gaussian (normal) distribution ubiquitous statistical applications machine learning, signal processing, computational biology, ors. usually, distributed random vectors denoted (?,   mean,  covariance matrix. given set realizations applications require estimating eir covariance inverse called precision matrix. estimating inverse covariance matrix applications] represents underlying graph Gaussian Markov Random Field (gmrf). given samples vector covariance matrix approximated standard maximum likelihood estimator (mle), leads  mle=?       ) called empirical covariance matrix. specifically, mle, estimated solving optimization problem min) min log(det), ) authors contributed equally work. eran Treister grateful Azrieli Foundation award Azrieli fellowship. Equation) standard MLE estimator. however, unbiased MLE estimation preferred, replaces denominator.  obtained applying log probability density function Normal distribution. however, number samples lower dimension vectors) rank deficient invertible, true assumed positive definite, full-rank. still, estimate matrix adding furr assumptions. well-known]  random scalar variables entries conditionally independent. refore, work adopt notion estimating inverse covariance, assuming sparse. (note cases dense.) for purpose, follow], minimize) sparsity-promoting prior: min) min) ?kak1 ) here) MLE functional defined), kak1  —aij regularization parameter balances sparsity solution fidelity data. sparsity assumption corresponds small number statistical dependencies variables. problem) called Covariance Selection], non-smooth convex. many methods recently developed solving)?see] references rein. current state--art methods], involve ?proximal newton? approach], quadratic approximation applied smooth part), leaving non-smooth term intact, order obtain Newton descent direction. obtain this, gradient Hessian) needed)  ) Kronecker product. gradient) shows main diﬃculty solving problem inverse sparse matrix dense expensive compute. advantage proximal Newton approach problem low overhead: calculating), Hessian cost]. this work aim solving large scale instances), large variables fit memory. such problem sizes required fmri] gene expression analysis] applications, example. large values introduce limitations) preclude storing full matrix), vectors assumed fit memory. ) While sparse matrix) fits memory, dense inverse not. because limitation, methods mentioned solve), require computing full gradient), dense symmetric matrix. applies blocking strategies], target dense covariance matrix rar inverse, dual formulation). one exception proximal Newton approach], made suitable large-scale matrices treating Newton direction problem blocks. paper, introduce iterative block-coordinate Descent] method solving largescale instances). treat problem blocks defined subsets columns each block sub-problem solved quadratic approximation, resulting descent direction corresponds variables block. since sub-problem time, fully store gradient Hessian block. contrast] applies blocking approach full Newton problem, results sparse descent direction. , columns calculated gradient Hessian problem iteration solving full Newton problem. refore, method requires calculations], computationally expensive task algorithms. furrmore, blocking strategy eﬃcient linesearch procedure] requires computing determinant sparse matrix. although method linear order convergence, converges iterations] experiments. note asymptotic convergence] quadratic exact Newton direction found iteration, costly large-scale problems.  newton Method Covariance Selection proximal Newton approach mentioned earlier iterative, iteration smooth part objective) approximated order Taylor expansion iterate) Newton direction solution penalized quadratic minimization problem, min ) min(?  )  ) inverse iterate. note gradient Hessian) featured terms), respectively, term) constant ignored. problem) corresponds well-known LASSO problem], popular machine learning signal/image processing applications]. methods] apply lasso-solvers treating Newton direction minimization). once direction computed, added) employing linesearch procedure suﬃciently reduce objective) \\x0censuring positive definiteness. end, updated iterate)  parameter obtained armijo rule]. that, choose initial step size define  smallest satisfies constraint) condition)  ) (??  )  ) ) parameters chosen respectively.  Restricting Updates Active Sets additional significant idea] restrict minimization) iteration ?active set? variables rest zeros. active set matrix defined active, Aij    ) This set definition sub-gradient). particular) approaches ) solution active approaches, Aij noted], restricting) variables Active) reduces computational complexity: matrix Hessian (third) term) calculated) operations) —active hence, method solving LASSO problem utilized solve) effectively saving computations restricting solution Active) our experiments verified restricting minimization) Active) significantly increase number iterations needed convergence. block-coordinate-descent Inverse Covariance (bcd) Estimation Section describe contribution. solve problem), apply iterative blockcoordinate-descent approach]. iteration, divide column set, ..., blocks. iterate blocks, turn minimize) restricted ?active? variables block, determined). matrix entries remain fixed update. matrix updated block-minimization. choose blocks sets columns portion gradient) corresponds blocks computed solutions linear systems. because matrix symmetric, rows updated simultaneously. figure shows BCD iteration blocks columns chosen sequential order. practice, sets columns non-contiguous vary BCD iterations. elaborate partition Figure Example BCD iteration. blocks treated successively. columns, advantages block-partitioning. partitioning matrix small blocks enables method solve) high dimensions millions variables), requiring) additional memory, number blocks (that addition memory needed storing iterated solution) itself).  Block Coordinate Descent Iteration Assume set columns, ..., divided blocks set indices corresponds columns rows block. mentioned before, bcd algorithm traverse blocks update iterated solution matrix block block. ) denote updated matrix treating block iteration iterate) defined block treated) treat block), adopt ideas earlier: quadratic approximation solve block, restricting updated entries active set. for simplicity) notation section, denote updated matrix treating block iteration update block change entries rows/columns first, form minimize quadratic approximation problem), restricted rows/columns min ) similarly), non-zero (?) quadratic approximation) entries rows/columns addition, non-zeros restricted active) defined). that, restrict minimization) active)  ,   ActiveIj) elements set entire treatment block. calculate set, check condition) columns rows define active set, calculate gradient) block calculate columns main computational task algorithm. achieve that, solve linear systems,   solution canonical vectors right-handsides ., linear systems achieved ways. direct methods applied Cholesky factorization, requires operations. for large dimensions, iterative methods Conjugate Gradients) preferred, cost iteration proportional number non-zeros sparse matrix. see Section Appendix details computational cost part algorithm.  Treating block-subproblem newton Method Newton direction block, solve LASSO problem), solvers]. choose polak-ribiere non-linear Conjugate Gradients (nlcg) method] which, toger diagonal preconditioner, solve problem]. describe NLCG algorithm Apendix. method, calculate objective) gradient eﬃciently. calculation objective) simpler full version), compute objective) blocks rows/columns considered. denoting gradient calculate matrices entries non-zero rows/columns matrices symmetric, hence, columns necessary. this idea applies term objective) well. iteration NLCG method, main \\x0ccomputational task involves calculating calculated obtaining), columns for that, reuse columns denote WIj since result columns notice WIj product WIj computed eﬃciently sparse. computing WIj anor expensive part algorithm, exploit restriction Active set. that, compute entries). for this, follow idea] rows columns) represented). besides columns ?neighborhood? defined , activeij) ) size set determine amount additional columns need, refore small possible. achieve that, define blocks clustering methods]. METIS], methods instead. aim methods partition indices matrix columns/rows disjoint subsets small size, non-zero entries diagonal blocks matrix correspond subset. notation, aim size small block size small enough. note compute WNj store  numbers WNj however, situations matrix dense columns, resulting sets size). computing WNj sets memory limitations. treat case separately?see Section Appendix details. for discussion computational cost part?see Section appendix.  Optimizing Solution Newton Direction line-search Assume Newton direction obtained solving problem). now seek update) iterated matrix   obtained linesearch procedure similarly Equation). for general Newton direction matrix ), procedure requires calculating determinant matrix. ], solving linear systems decreasing sizes  however, direction special block structure, obtain significantly cheaper linesearch procedure compared], assuming blocks small. first, trace terms involved objective) calculated respect entries columns rows account symmetry). log det term, however, special care, eventually reduced calculating determinant  matrix, cheaper block size decreases. let introduce partitioning matrix blocks, set indices }. assume loss generality rows columns permuted columns/rows indices first, a11 A12  =?  a21 A22   ) partitioning blocks. sub-matrix A11 corresponds elements rows according Schur complement], invertible matrix columns block-partitioning above, holds: log det) log det(a22 log det(a11 a12 A21 ) addition, symmetric matrix applies: a22 A11 a12 A21 ) partitioning write): Using notation  log det log det log det)               A22 (note replaced ease notation.)    finally, positive definiteness condition+? involved linesearch) equivalent ). throughout iterations assuming remains positive definite linesearch guarantee iterated solution matrix update. this requires initialization algorithm) positive definite. set small, matrices) small  —), easily compute objective apply Armijo rule)  calculating matrices) expensive, however, show Appendix, obtained previously computed matrices WIj WNj mentioned earlier. refore, computing) achieved time complexity. algorithm: bcd ,?) ... Calculate clusters elements based) ) denote) Compute WIj solve linear systems) Define ActiveIj), define set). ) Compute WNj solve linear systems Find Newton direction solving LASSO problem). ) Update solution: end  linesearch. ) denote) end Algorithm Block Coordinate Descent Inverse Covariance Estimation Convergence Analysis section, elaborate convergence bcd algorithm global optimum). base analysis]. ], general block-coordinate-descent approach analyzed solve minimization problems form) composed sum smooth function (?) separable convex function(? case log det) kak1 respectively. although setup fits functional] treats problem domain, minimization) constrained symmetric positive definite matrices domain. overcome limitation, authors] extended analysis] treat specific constrained problem). particular] block-coordinate-descent methods step subset variables updated. gauss-seidel condition ensure variables updated steps    set variables, fixed number. similarly], treating block columns bcd algorithm equivalent updating elements active set ActiveIj), update elements ActiveIj). refore), set j2t,   ActiveIj), j2t ActiveIj), step index corresponds block iteration bcdic. , Lemma], shown setting elements active set block satisfies optimality condition step. refore, algorithm update elements ActiveIj). now, fixed blocks coordinates Algorithm clustering applied), gauss-seidel condition) satisfied blocks. when clustering applied, block-partitioning change activation clustering method. refore, condition) satisfied  maximum number blocks obtained activations clustering algorithm. for completeness, include Appendix lemmas] proof orem) orem Algorithm sequence converges global optimum). Numerical Results section demonstrate eﬃciency bcd method, compare methods small large scales. for small-scale problems include QUIC], BIGQUIC-ista], state--art methods scale. for large-scale problems, compare method big-quic feasible method scale. for methods, original code provided authors? implemented parallelized (except QUIC partially parallelized). our code bcd MATLAB based routines. All experiments run machine Intel Xeon-2650.0ghz processors cores 64gb ram, Windows. stopping criterion bcd, rule]: kgrads) grads (?) minimal norm subgradient, defined Equation) Appendix. for choose, results entries) digits accurate compared true solution  ], approximate WIj WNj, stop residual drops respectively. for stopping NLCG (algorithm nlcg (see details end Section). note large-scale test problems, bcd optimal block size requires memory big-quic.  Syntic Experiments syntic experiments compare performance methods. first, random matrix], generated non-zeros row, well-conditioned. generate matrices sizes varying,000 160,000, generate 200 samples 200). values chosen solution approximately 10n non-zeros. bcd run block sizes, 128, 256, 256 random tests Table respectively. problem i2d version chain], represented stencil applied square lattice.  chosen non-zeros. for tests, bcd run block size 1024. table summarizes results test case. results show small-scale problems-ista fastest method bcd. however, size,000 higher, bcd fastest. run QUIC-ista problems larger,000 memory limitations. time gap-ista bcd big-quic smallscales reduced programs receive matrix input  Gene Expression Analysis Experiments For large-scale real-world experiments, gene expression datasets Gene Expression Omnibus (http://www.ncbi.nlm.nih.gov/geo/).  test,   bcd big-quic QUIC-ista random random 10k random 20k random 40k random 80k random 160k 5002 7082 10002,138 118,794 237,898 475,406 950,950,901,404,248,000,503,488,996,000,164 139,708 311,932 423,696 891,268,852,198,553,698,002,338,684,306) 265s) 729s,102s,296s,235s) 130,636s) 777,947s) 673s,671s,764s,584s,530s) 203,370s,220,213s) 114s) 823s) 491s) table Results random syntic experiments.   denote number non-zeros true estimated inverse covariance matrices, respectively. for run, timings reported seconds number iterations parenses. ?*? means algorithm ran memory. tests reported]. data preprocessed unit variance variable., diag). table shows datasets numbers variables) samples) each. particular, datasets variables samples). because size problems, ran bcd big-quic test cases. for tests Table chosen solution matrix 10n non-zeros. for fourth test, choose high  low number samples solutions smaller dense. bcd run block size 256 tests Table found datasets challenging syntic experiments above. still, algorithms bcd big-quic manage estimate inverse covariance matrix reasonable time. syntic case, bcd outperforms big-quic test cases. bcd requires smaller number iterations converge, translates shorter timings. moreover, average time bcd iteration faster big-quic. code Description GSE1898 GSE20194 GSE17951 GSE14322 Liver cancer Breast cancer Prostate cancer Liver cancer   bcd big-quic, 794, 283, 675 104, 702 182 278 154 293,845 197,953 558,929,973,476 788) 452,621,314,079,810,229) 127,199s) table Gene expression results.  denotes number non-zeros estimated covariance matrix. for run, timings reported seconds number iterations parenses. Conclusions work introduced block-coordinate Descent method solving sparse inverse covariance problem. our method low memory footprint, refore attractive solving large-scale instances problem. solves problem iterating updating matrix block block, block chosen subset columns respective rows. for block sub-problem, proximal Newton method applied, requiring solution LASSO problem find descent direction. because update limited subset columns rows, store gradient Hessian block, enjoy eﬃcient linesearch procedure. numerical results show medium-large scale experiments algorithm faster state--art methods, problem hard. acknowledgement: authors prof. Irad Yavneh valuable comments guidance work. research leading results received funding European union Seventh Framework Programme (fp7/2007-2013) grant agreement 623212 Multiscale inversion.',\n",
       " 'PP5521': 'structure motion (sfm) task jointly reconstructing scenes camera poses set images. keypoints features extracted image provide correspondences pairs images, making estimate relative camera pose. rise association graph images connected edge share suﬃcient number keypoints, edge labeled estimated matching sets keypoints. starting putative image image associations, typically socalled bundle adjustment procedure simultaneously solve global camera pose parameters scene locations, incrementally minimizing sum squares-projection error. popularity, large scale bundle adjustment methods limitations. particular, highly nonlinear nature \\x0cjective function, stuck bad local minima. refore, starting good initial matching., informative image association graph) critical. papers studied behavior detail], conclude starts numerical optimization incorrect ?seed? ., subgraph image associations), downstream optimization recover. similar challenges arise commonly fields, ranging machine learning] computational biology. instance, novo genome assembly problem computational biology]. goal reconstruct original DNA sequence fragments reference genome. genome repeated structures, alignment problem hard. general, reconstruction algorithms start maximally overlapping sequences proceed selecting fragment similar criterion. procedure runs type issues]. model reasons globally pairwise information provide robust metric association. eﬃcacy global reasoning largely depend richness representation encoding pu1 tative pairwise information. choice representation specific underlying application, paper, make presentation concrete possible, restrict describing evaluating global association algorithm context structure motion problem. large scale structure motion, authors, recentely identified situations setting good image association graph diﬃcult, refore direct application bundle adjustment yields highly unsatisfactory results. example, scene large number duplicate structures (fig. ). preprocessing step standard pipeline match visual features set associations accordingly. key underlying assumption all) approaches observe single instance structure. assumption problematic scenes numerous architectural components recur) ring patterns, windows, Figure HOUSE sequence. ) Representative images. ) Folded bricks. reconstruction traditional SfM pipeline]. figure) views necessarily represent physical structure.  all) points image occluded image. typical SfM methods work initialized image associations, type solver use. example, resulting reconstruction folded (figure)). cases], errors ranging phantom walls severely superimposed structures yielding nonsensical reconstructions. related work. issue variously literature SfM disambiguation problem data/image association problem structure motion. strategies proposed mitigate impose additional conditions], breaks presence large coherent sets incorrectly matched pairs. creative solution recent work metadata alongside images. ?geotags? gis data shown effective deriving initialization \\x0cfor bundle adjustment post-processing step stitch toger components reconstruction. ], authors suggest image timestamps impose natural association images, valuable images acquired single camera temporal sequence diﬃcult deploy orwise. separate metadata approach, controlled scenes occlusion, missing correspondences yield important local cues infer potentially incorrect image pairs]. recently] formalized intuition incorrect feature correspondences result anomalous structures-called visibility graph features. measure local track quality (from local clustering), reason associations erroneous. works number points large, authors] acknowledge datasets shown fig. much. contrast approaches, number recent algorithms association disambiguation) problem argue global geometric reasoning. ], authors number point correspondences measure certainty, globally optimized find maximum-weight set consistent pairwise associations. authors] seek consistency epipopolar geometry constraints triplets] expands larger consistent cliques. procedure] takes account loops associations concurrently minimal spanning tree image image matches. summary, bulk prior work suggests locally based statistics chained transformations run problems inconsistencies global nature. however, objectives global, approximate inference robust coherent noise face presence duplicate structures]. paper. idea reasoning globally association consistency triples higher order loops extreme, implies deriving likelihood specific image image association conditioned associations. maximum likelihood expression fac2 tor easily explicit enumeration quickly intractable. approach make group structure image image relationships explicit. operate association graph derived image pairs key distinguishing feature. association relationships denoted terms ?certificate, transformation justifies relationship. transformation denote pose parameters derived correspondences matching (between features) itself. options long transformation group action set. , carry intuition consistency larger cliques images desired existing works rewrite ideas invariance properties functions defined group. example, transformation matching, edge graph permutation., member symmetric group, special form Laplacian graph, derived representation ory group consideration, encodes symmetries functions group. key contribution paper show global inference desired existing works falls nicely diffusion process laplacian. show promising results demonstrating diﬃcult datasets large repetitive patterns, results simple decomposition procedure are, fact, competitive obtained sophisticated optimization schemes with/without metadata. finally, note proposed algorithm eir standalone derive meaningful inputs bundle adjustment procedure pre-conditioner approaches (especially, incorporate timestamps and GPS data).   synchronization Consider collection images   object scene viewpoints possibly conditions, assume keypoint detector detected landmarks (keypoints) {xi1 xi2 xin images landmark matching problem consists finding pairs landmarks xip xjp images correspond physical feature. critical component classical computer vision tasks, including structure motion. assuming images landmarks, matching permutation,  xip ) initial guess  matchings provided local image features, SIFT descriptors. however, matchings individually prone error, presence occlusion repetitive structures. major clue correcting errors constraint matchings consistent tells xip corresponds xjq tells xjq corresponds xkr permutation assign xip xkr mamatically, reﬂection fact define product permutations usual ,    ))    permutations,   form group. group called symmetric group order denoted group oretic notation, consistency conditions require relative matchings satisfy equivalent condition associate base permutation, pair. thus, problem finding consistent set reduces finding base permutations    problems general form, (finite continuous) group estimate matrix (gji group elements obeying consistency relations, called synchronization problems. starting seminal work Singer. ] synchronization rotation group aligning images cryo, synchronization Euclidean group], recently synchronization matching landmarks], problems form recently generated considerable interest.  Vector Diffusion Maps context synchronizing dimensional rotations cryo, \\x0csinger] proposed elegant formalism, called Vector Diffusion maps, conceives syn3 chronization diffusing base rotation image neighbors. however, unlike ordinary diffusion, diffuses observed Oji relative rotation Oji (oji observations perfectly synchronized, matter path       resulting rotation   oi2 Oi1 same. however practical cases, majority) Oji incorrect, paths vertex anor contribute rotations averaged out. natural choice loss describes extent imputed base rotations (playing role permutation case) satisfy Oji observations     wij oji k2frob wij  Oji kfrob wij edge weight descibes confidence rotation Oji crucial observation loss rewritten form       ?w21 O21   ?wm1 Om1    =?  ...    ?w1m O1m ?w2m O2m    matrix symmetric.  wij note wij wji Oij Oji Oji furrmore, analogous spectral graph ory, (see]) functional)  describing ?smoothness? function defined vertices graph respect graph topology written terms usual graph Laplacian  consequence (constraining unit norm excluding constant functions), function minimizing eigenvector (second) smallest eigenvalue. analogously, synchronizing rotations, steady state diffusion system) minimal, computed forming lowest eigenvalue eigenvectors identifying), denote block. resulting consistent array  imputed relative rotations minimizes loss). permutation Diffusion Its elegance notwithstanding, vector diffusion formalism previous section ill suited present purposes improving SfM pipeline reasons) synchronizing finite group, harder synchronizing continuous group rotations) rar actual synchronized array matchings, critical SfM estimate association graph captures extent images related oneanor. main contribution present paper show \\x0cthat problems natural solutions formalism group representations. key observation (already brieﬂy mentioned]) critical step rewriting loss) terms Laplacian) depend special properties rotation group fact) rotation matrices unitary fact, orthogonal) follow rotation anor, matrices simply multiply. general, group complex valued function  ? ? satisfies    called representation representation unitary    denotes Hermitian conjugate (conjugate transpose) thus, proposition. proposition compact group identity  ? ? unitary representation array possibly noisy unsynchronized group elements, (gji positive confidence weights (wji synchronization loss (assuming gii    wji   (gji Frob    written form       ?w21 (g21     =?    ?w1m (g1m ?w2m (g2m  ... ... ?wm1 (gm1   ) synchronize putative matchings images, instantiate proposition approriate unitary representation symmetric group. obvious choice-called defining representation, elements familiar permutation matrices ) ?def (?) (?)   orwise, loss function   wji  k2frob  squared Frobenius norm expression simply counts number mismatches observed noisy permutations inferred permutations furrmore, results previous section, letting  )) Pbji  notational simplicity) written form     ?w21 Pb21   ?wm1 Pbm1    =?  ...    ?w1m P1m ?w2m P2m   refore, similarly rotation case, synchronization solved forming?def lowest eigenvectors extracting block. care synchronized, priori guaranteed \\x0cthat resulting block valid permutation matrix. refore, analogously procedure], block) multiplied)? linear assignment procedure find estimated permutation matrix  resulting algorithm call Synchronization Permutation diffusion.   uncertain matches diffusion distance obvious limitation framework, far, assumes keypoint image single counterpart image. assumption satisfied realistic scenarios due occlusion, repetitive structures, noisy detections. algorithms, including], deal problem simply setting Pij entry Laplacian matrix) equal weighted sum permutations. example, landmarks number present images, landmarks  not, effective Pij matrix block, rescaled factor. consequence approach block matrix derived eigendecomposition correspond distribution base permutations. principle, amounts replacing single observed matching distribution tji matchings, concomitantly replacing distribution (?). however, set landmarks   occluded tji agnostic respect assignment landmarks, refore invariant labels assigned defining permutation maps     subgroup permutations permute   mselves leave   fixed, set permutations form?      called ?coset, denoted    occluded constant ...uksk ., choice ?). occlusion, invariances spontaneously matrix formed eigenvectors, related set landmarks hidden uncertain, invariances important clue viewpoint image from. aﬃnity score based information valuable synchronized matchings mselves. invariance structure read easily-called autocorrelation function (?) (??) (?). )  particular, coset ...uksk  , fall ...uksk coset, (?)   (?) maximum attain. however)? reveals weighted sum pbi (?)   (?) ?(?) ), rar full function compute) directly. recent years emergence number applications Fourier transforms symmetric group, which, function defined(?) (?) ??(?     special-called irreducible, representations indexed integer partitions. due space restrictions, leave details construction literature, see]. suﬃce) Fourier component expressed direct sum Fourier components pbi (?) ) ??? unitary matrix effectively basis transform. properties Fourier transform cross-correlation functions(?) (?)?  consequently, assuming) normal?  (??) (? (?) (?)  ized ensure) pbi (?) pbi (?)? ))? )? (?)  (?)  ??? ??? easily computable matrix captures essentially coset invariance structure encoded inferred distribution compute aﬃnity score remains compare coset invariance structures, example, computing  (?)   omitting multiplicative constants arising inverse Fourier transform, correlation orem, finds equivalent )? )?  call Permutation Diffusion Aﬃnity (pda). remarkably, PDA closely related notion diffusion similarity derived] rotations, different, differential geometric tools. experiments show PDA surprisingly informative actual distance image viewpoints physical space, and, easy compute, greatly improve performance SfM pipeline. experiments experiments Permutation Diffusion Maps infer image association matrix datasets literature. geometric ambiguities due large duplicate structures evident datasets% matches], sophisticated SfM pipelines run diﬃculties. approach precede entire SfM engine simple preprocessing step. preprocessing step generates good image association information, existing SfM pipeline mature software linear algebra toolboxes vision libraries integrated toger, provide good reconstructions. primary interest sfm, illustrate utility pdm, present experimental results scene summarization set images]. additional experiments project website http://pages.wisc.edu/?pachauri/pdm/. structure Motion (sfm). pdm generate image match matrix fed state--art SfM pipeline reconstruction]. baseline, provide images Bundle Adjustment procedure visual features matching built heuristic outlier removal module. papers similar set comparisons]. dataset, SIFT detect characterize \\x0clandmarks]. compute putative pairwise matchings solving linear independent assignments] based SIFT features. image Match matrix: Permutation matrix representation putative matchings here, relative large, order 1000. ideally total number distinct keypoints scene directly observable. experiments, maximum keypoints detected complete dataset estimate Eigenvector based procedure computes weighted aﬃnity matrix. specialized methods extract binary image matrix (such optimizes criteria), simple thresholding procedure. reconstruction: binary match matrix input SfM library]. note provide library image association hyposes, leaving modules unchanged. (potentially) good image association information, SfM modules sample landmarks densely perform bundle adjustment, leaving unchanged. baseline reconstruction performed SfM pipeline intervention. house sequence instances similar houses, Figure diffusion process accumulates evidence eventually strongly connected images data association matrix, Figure). warm colors correspond high aﬃnity pairs images. binary match matrix obtained applying threshold weighted matrix, Figure). matrix define image matching feature tracks. means features matched images connected match matrix. sfm pipeline image matches hyposes explain images ?connected?. resulting reconstruction correctly houses, Figure). sfm pipeline allowed track features automatically outlier removal heuristic, resulted folded reconstruction, Figure). specialized heuristics better, time stamps, suggested]. however, experimental results] ors, strongly suggest datasets remain challenging. ) Figure House sequence) Weighted image association matrix. ) Binary image match matrix. ) PDM dense reconstruction. cup dataset multiple images 180 degree symmetric cup sides, Figure). pdm reveals strongly connected component diagonal dataset, shown warm colors Figure). global reasoning space permutations substantially mitigates coherent errors. binary match matrix obtained thresholding weighted matrix, Figure). evident reconstructions, baseline method reconstruct ?half cup?. due structural ambiguity, concludes cup handles, Figure). pdm reconstruction perfect reconstruction ?full cup? handle expected, Figure). oat dataset instances red oat box, left) Figure) Representative images CUP dataset. ) Weighted data association matrix. ) Binary data association matrix. ) Figure CUP dataset. ) PDM dense reconstruction. ) Baseline dense reconstruction. wheat things, anor right, Figure). pdm weighted match matrix binary match matrix successfully discover strongly connected components, Figure). baseline method confused oat boxes one, reconstructed single box, Figure). moreover, structural ambiguity splits wheat thins pieces. hand, PDM nice reconstruction oat boxes entire wheat things middle, Figure). experiments (with videos), found project website. ) Figure) Representative images OAT dataset. ) Weighted data association matrix. ) Binary data association matrix. ) Figure OAT dataset. ) PDM dense reconstruction. ) Baseline dense reconstruction. conclusions Permutation diffusion maps significantly improve quality correspondences found image association problems, large number initial visual feature matches erroneous. experiments variety challenging datasets literature give strong evidence supporting hyposis deploying proposed formulation, preconditioner, significantly mitigate problems encountered performing structure motion scenes repetitive structures. proposed model easily generalize applications, computer vision, involving multi-matching problems. acknowledgments This work supported part nsf?1320344, nsf?1320755, funds University Wisconsin Graduate school. charles Dyer Zhang discussions suggestions. Structure motion (sfm) task jointly reconstructing scenes camera poses set images. keypoints features extracted image provide correspondences pairs images, making estimate relative camera pose. this rise association graph images connected edge share suﬃcient number keypoints, edge labeled estimated matching sets keypoints. starting putative image image associations, typically socalled bundle adjustment procedure simultaneously solve global camera pose parameters scene locations, incrementally minimizing sum squares-projection error. despite popularity, large scale bundle adjustment methods limitations. particular, highly nonlinear nature \\x0cjective function, stuck bad local minima. refore, starting good initial matching., informative image association graph) critical. several papers studied behavior detail], conclude starts numerical optimization incorrect ?seed? ., subgraph image associations), downstream optimization recover. similar challenges arise commonly fields, ranging machine learning] computational biology. for instance, novo genome assembly problem computational biology]. goal reconstruct original DNA sequence fragments reference genome. because genome repeated structures, alignment problem hard. general, reconstruction algorithms start maximally overlapping sequences proceed selecting fragment similar criterion. this procedure runs type issues]. model reasons globally pairwise information provide robust metric association. eﬃcacy global reasoning largely depend richness representation encoding pu1 tative pairwise information. choice representation specific underlying application, paper, make presentation concrete possible, restrict describing evaluating global association algorithm context structure motion problem. large scale structure motion, authors, recentely identified situations setting good image association graph diﬃcult, refore direct application bundle adjustment yields highly unsatisfactory results. for example, scene large number duplicate structures (fig. ). preprocessing step standard pipeline match visual features set associations accordingly. key underlying assumption all) approaches observe single instance structure. this assumption problematic scenes numerous architectural components recur) ring patterns, windows, Figure HOUSE sequence. ) Representative images. ) Folded bricks. reconstruction traditional SfM pipeline]. Figure) views necessarily represent physical structure. some all) points image occluded image. typical SfM methods work initialized image associations, type solver use. example, resulting reconstruction folded (figure)). cases], errors ranging phantom walls severely superimposed structures yielding nonsensical reconstructions. related work. issue variously literature SfM disambiguation problem data/image association problem structure motion. some strategies proposed mitigate impose additional conditions], breaks presence large coherent sets incorrectly matched pairs. one creative solution recent work metadata alongside images. ?geotags? GIS data shown effective deriving initialization \\x0cfor bundle adjustment post-processing step stitch toger components reconstruction. ], authors suggest image timestamps impose natural association images, valuable images acquired single camera temporal sequence diﬃcult deploy orwise. separate metadata approach, controlled scenes occlusion, missing correspondences yield important local cues infer potentially incorrect image pairs]. very recently] formalized intuition incorrect feature correspondences result anomalous structures-called visibility graph features. measure local track quality (from local clustering), reason associations erroneous. this works number points large, authors] acknowledge datasets shown fig. much. contrast approaches, number recent algorithms association disambiguation) problem argue global geometric reasoning. ], authors number point correspondences measure certainty, globally optimized find maximum-weight set consistent pairwise associations. authors] seek consistency epipopolar geometry constraints triplets] expands larger consistent cliques. procedure] takes account loops associations concurrently minimal spanning tree image image matches. summary, bulk prior work suggests locally based statistics chained transformations run problems inconsistencies global nature. however, objectives global, approximate inference robust coherent noise face presence duplicate structures]. this paper. idea reasoning globally association consistency triples higher order loops extreme, implies deriving likelihood specific image image association conditioned associations. maximum likelihood expression fac2 tor easily explicit enumeration quickly intractable. our approach make group structure image image relationships explicit. operate association graph derived image pairs key distinguishing feature. association relationships denoted terms ?certificate, transformation justifies relationship. transformation denote pose parameters derived correspondences matching (between features) itself. options long transformation group action set. , carry intuition consistency larger cliques images desired existing works rewrite ideas invariance properties functions defined group. example, transformation matching, edge graph permutation., member symmetric group, special form Laplacian graph, derived representation ory group consideration, encodes symmetries functions group. key contribution paper show global inference desired existing works falls nicely diffusion process laplacian. show promising results demonstrating diﬃcult datasets large repetitive patterns, results simple decomposition procedure are, fact, competitive obtained sophisticated optimization schemes with/without metadata. finally, note proposed algorithm eir standalone derive meaningful inputs bundle adjustment procedure pre-conditioner approaches (especially, incorporate timestamps and GPS data).   Synchronization Consider collection images   object scene viewpoints possibly conditions, assume keypoint detector detected landmarks (keypoints) {xi1 xi2 xin given images landmark matching problem consists finding pairs landmarks xip xjp images correspond physical feature. this critical component classical computer vision tasks, including structure motion. assuming images landmarks, matching permutation,  xip ) initial guess  matchings provided local image features, SIFT descriptors. however, matchings individually prone error, presence occlusion repetitive structures. major clue correcting errors constraint matchings consistent tells xip corresponds xjq tells xjq corresponds xkr permutation assign xip xkr mamatically, reﬂection fact define product permutations usual ,    ))    permutations,   form group. this group called symmetric group order denoted group oretic notation, consistency conditions require relative matchings satisfy equivalent condition associate base permutation, pair. thus, problem finding consistent set reduces finding base permutations    problems general form, (finite continuous) group estimate matrix (gji group elements obeying consistency relations, called synchronization problems. starting seminal work Singer. ] synchronization rotation group aligning images cryo, synchronization Euclidean group], recently synchronization matching landmarks], problems form recently generated considerable interest.  Vector Diffusion Maps context synchronizing dimensional rotations cryo, \\x0csinger] proposed elegant formalism, called Vector Diffusion maps, conceives syn3 chronization diffusing base rotation image neighbors. however, unlike ordinary diffusion, diffuses observed Oji relative rotation Oji (oji observations perfectly synchronized, matter path       resulting rotation   oi2 Oi1 same. however practical cases, majority) Oji incorrect, paths vertex anor contribute rotations averaged out. natural choice loss describes extent imputed base rotations (playing role permutation case) satisfy Oji observations     wij oji k2frob wij  Oji kfrob wij edge weight descibes confidence rotation Oji crucial observation loss rewritten form       ?w21 O21   ?wm1 Om1    =?  ...    ?w1m O1m ?w2m O2m    matrix symmetric.  wij note wij wji Oij Oji Oji furrmore, analogous spectral graph ory, (see]) functional)  describing ?smoothness? function defined vertices graph respect graph topology written terms usual graph Laplacian  consequence (constraining unit norm excluding constant functions), function minimizing eigenvector (second) smallest eigenvalue. analogously, synchronizing rotations, steady state diffusion system) minimal, computed forming lowest eigenvalue eigenvectors identifying), denote block. resulting consistent array  imputed relative rotations minimizes loss). Permutation Diffusion Its elegance notwithstanding, vector diffusion formalism previous section ill suited present purposes improving SfM pipeline reasons) synchronizing finite group, harder synchronizing continuous group rotations) rar actual synchronized array matchings, critical SfM estimate association graph captures extent images related oneanor. main contribution present paper show \\x0cthat problems natural solutions formalism group representations. our key observation (already brieﬂy mentioned]) critical step rewriting loss) terms Laplacian) depend special properties rotation group fact) rotation matrices unitary fact, orthogonal) follow rotation anor, matrices simply multiply. general, group complex valued function  ? ? satisfies    called representation representation unitary    denotes Hermitian conjugate (conjugate transpose) thus, proposition. proposition let compact group identity  ? ? unitary representation array possibly noisy unsynchronized group elements, (gji positive confidence weights (wji synchronization loss (assuming gii    wji   (gji Frob    written form       ?w21 (g21     =?    ?w1m (g1m ?w2m (g2m  ... ... ?wm1 (gm1   ) synchronize putative matchings images, instantiate proposition approriate unitary representation symmetric group. obvious choice-called defining representation, elements familiar permutation matrices ) ?def (?) (?)   orwise, loss function   wji  k2frob  squared Frobenius norm expression simply counts number mismatches observed noisy permutations inferred permutations furrmore, results previous section, letting  )) Pbji  notational simplicity) written form     ?w21 Pb21   ?wm1 Pbm1    =?  ...    ?w1m P1m ?w2m P2m   refore, similarly rotation case, synchronization solved forming?def lowest eigenvectors extracting block. here care synchronized, priori guaranteed \\x0cthat resulting block valid permutation matrix. refore, analogously procedure], block) multiplied)? linear assignment procedure find estimated permutation matrix  resulting algorithm call Synchronization Permutation diffusion.   Uncertain matches diffusion distance obvious limitation framework, far, assumes keypoint image single counterpart image. this assumption satisfied realistic scenarios due occlusion, repetitive structures, noisy detections. most algorithms, including], deal problem simply setting Pij entry Laplacian matrix) equal weighted sum permutations. for example, landmarks number present images, landmarks  not, effective Pij matrix block, rescaled factor. consequence approach block matrix derived eigendecomposition correspond distribution base permutations. principle, amounts replacing single observed matching distribution tji matchings, concomitantly replacing distribution (?). however, set landmarks   occluded tji agnostic respect assignment landmarks, refore invariant labels assigned defining permutation maps     subgroup permutations permute   mselves leave   fixed, set permutations form?      called ?coset, denoted    occluded constant ...uksk ., choice ?). whenever occlusion, invariances spontaneously matrix formed eigenvectors, related set landmarks hidden uncertain, invariances important clue viewpoint image from. aﬃnity score based information valuable synchronized matchings mselves. invariance structure read easily-called autocorrelation function (?) (??) (?). )  particular, coset ...uksk  , fall ...uksk coset, (?)   (?) maximum attain. however)? reveals weighted sum pbi (?)   (?) ?(?) ), rar full function compute) directly. recent years emergence number applications Fourier transforms symmetric group, which, function defined(?) (?) ??(?     special-called irreducible, representations indexed integer partitions. due space restrictions, leave details construction literature, see]. suﬃce) Fourier component expressed direct sum Fourier components pbi (?) ) ??? unitary matrix effectively basis transform. one properties Fourier transform cross-correlation functions(?) (?)?  consequently, assuming) normal?  (??) (? (?) (?)  ized ensure) pbi (?) pbi (?)? ))? )? (?)  (?)  ??? ??? easily computable matrix captures essentially coset invariance structure encoded inferred distribution compute aﬃnity score remains compare coset invariance structures, example, computing  (?)   omitting multiplicative constants arising inverse Fourier transform, correlation orem, finds equivalent )? )?  call Permutation Diffusion Aﬃnity (pda). remarkably, PDA closely related notion diffusion similarity derived] rotations, different, differential geometric tools. our experiments show PDA surprisingly informative actual distance image viewpoints physical space, and, easy compute, greatly improve performance SfM pipeline. Experiments experiments Permutation Diffusion Maps infer image association matrix datasets literature. geometric ambiguities due large duplicate structures evident datasets% matches], sophisticated SfM pipelines run diﬃculties. our approach precede entire SfM engine simple preprocessing step. preprocessing step generates good image association information, existing SfM pipeline mature software linear algebra toolboxes vision libraries integrated toger, provide good reconstructions. while primary interest sfm, illustrate utility pdm, present experimental results scene summarization set images]. additional experiments project website http://pages.wisc.edu/?pachauri/pdm/. Structure Motion (sfm). PDM generate image match matrix fed state--art SfM pipeline reconstruction]. baseline, provide images Bundle Adjustment procedure visual features matching built heuristic outlier removal module. several papers similar set comparisons]. for dataset, SIFT detect characterize \\x0clandmarks]. compute putative pairwise matchings solving linear independent assignments] based SIFT features. image Match matrix: Permutation matrix representation putative matchings here, relative large, order 1000. ideally total number distinct keypoints scene directly observable. experiments, maximum keypoints detected complete dataset estimate Eigenvector based procedure computes weighted aﬃnity matrix. while specialized methods extract binary image matrix (such optimizes criteria), simple thresholding procedure. reconstruction: binary match matrix input SfM library]. note provide library image association hyposes, leaving modules unchanged. with (potentially) good image association information, SfM modules sample landmarks densely perform bundle adjustment, leaving unchanged. baseline reconstruction performed SfM pipeline intervention. HOUSE sequence instances similar houses, Figure diffusion process accumulates evidence eventually strongly connected images data association matrix, Figure). warm colors correspond high aﬃnity pairs images. binary match matrix obtained applying threshold weighted matrix, Figure). matrix define image matching feature tracks. this means features matched images connected match matrix. SfM pipeline image matches hyposes explain images ?connected?. resulting reconstruction correctly houses, Figure). SfM pipeline allowed track features automatically outlier removal heuristic, resulted folded reconstruction, Figure). one specialized heuristics better, time stamps, suggested]. however, experimental results] ors, strongly suggest datasets remain challenging. ) Figure House sequence) Weighted image association matrix. ) Binary image match matrix. ) PDM dense reconstruction. CUP dataset multiple images 180 degree symmetric cup sides, Figure). pdm reveals strongly connected component diagonal dataset, shown warm colors Figure). our global reasoning space permutations substantially mitigates coherent errors. binary match matrix obtained thresholding weighted matrix, Figure). evident reconstructions, baseline method reconstruct ?half cup?. due structural ambiguity, concludes cup handles, Figure). pdm reconstruction perfect reconstruction ?full cup? handle expected, Figure). OAT dataset instances red oat box, left) Figure) Representative images CUP dataset. ) Weighted data association matrix. ) Binary data association matrix. ) Figure CUP dataset. ) PDM dense reconstruction. ) Baseline dense reconstruction. wheat things, anor right, Figure). PDM weighted match matrix binary match matrix successfully discover strongly connected components, Figure). baseline method confused oat boxes one, reconstructed single box, Figure). moreover, structural ambiguity splits wheat thins pieces. hand, PDM nice reconstruction oat boxes entire wheat things middle, Figure). several experiments (with videos), found project website. ) Figure) Representative images OAT dataset. ) Weighted data association matrix. ) Binary data association matrix. ) Figure OAT dataset. ) PDM dense reconstruction. ) Baseline dense reconstruction. Conclusions Permutation diffusion maps significantly improve quality correspondences found image association problems, large number initial visual feature matches erroneous. our experiments variety challenging datasets literature give strong evidence supporting hyposis deploying proposed formulation, preconditioner, significantly mitigate problems encountered performing structure motion scenes repetitive structures. proposed model easily generalize applications, computer vision, involving multi-matching problems. acknowledgments This work supported part nsf?1320344, nsf?1320755, funds University Wisconsin Graduate school. Charles Dyer Zhang discussions suggestions.',\n",
       " 'PP5548': 'convolutional neural networks (cnns) trained backpropagation recently shown perform image classification tasks millions training images thousands categories]. feature representation learned networks achieves state--art performance classification task network trained, visual recognition tasks, example: classification caltech-101], caltech-256] CaltechUCSD birds dataset]; scene recognition sun-397 database]; detection PASCAL VOC dataset]. capability generalize datasets makes supervised CNN training attractive approach generic visual feature learning. downside supervised training expensive labeling, amount required labeled samples grows quickly larger model gets. large performance increase achieved methods based work Krizhevsky. ] was, example, due massive efforts manually annotating mil \\x0clions images. reason, unsupervised learning underperforming remains appealing paradigm, make raw unlabeled images videos. furrmore, vision tasks classification wher training based object class labels advantageous. example, unsupervised feature learning beneficial image restoration] recent results show outperforms supervised feature learning descriptor matching]. work combine power discriminative objective major advantage unsupervised feature learning: cheap data acquisition. introduce training procedure convolutional neural networks require labeled data. rar relies automatically generated surrogate task. task created taking idea data augmentation commonly supervised learning extreme. starting trivial surrogate classes consisting random image patch each, augment data applying random set transformations patch. train CNN classify surrogate classes. refer method exemplar training convolutional neural networks (exemplar-cnn). feature representation learned exemplar-cnn, construction, discriminative invariant typical transformations. confirm oretically empirically, showing approach matches outperforms previous unsupervised feature learning methods standard image classification benchmarks stl, cifar, caltech-101.  Related Work Our approach related large body work unsupervised learning invariant features training convolutional neural networks. convolutional training commonly supervised unsupervised methods utilize invariance image statistics translations. lecun. ], Kavukcuoglu. ], Krizhevsky. ]). similar approach current surge successful methods employing convolutional neural networks object recognition rely data augmentation generate additional training samples classification objective. krizhevsky. ], Zeiler Fergus]). share architecture convolutional neural network) approaches, method rely labeled training data. unsupervised learning, studies learning invariant representations exist. denoising autoencoders], example, learn features robust noise reconstruct data randomly perturbed input samples. zou. ] learn invariant features video enforcing temporal slowness constraint feature representation learned linear autoencoder. sohn Lee] Hui] learn features invariant local image transformations. contrast discriminative approach, methods rely directly modeling input distribution typically hard jointly training multiple layers cnn. idea learning features invariant transformations explored supervised training neural networks. research similar early work tangent propagation] (and related double backpropagation]) aims learn invariance small predefined transformations neural network directly penalizing derivative output respect magnitude transformations. contrast, algorithm regularize derivative explicitly. sensitive magnitude applied transformation. work loosely related unlabeled data regularizing supervised algorithms, self-training] entropy regularization]. contrast semi-supervised methods, ExemplarCNN training require labeled data. finally, idea creating auxiliary task order learn good data representation Ahmed. ], Collobert. ]. creating Surrogate Training Data input training procedure set unlabeled images, roughly distribution images aim apply learned features. randomly sample , 32000] patches size pixels images varying positions scales forming initial training set    interested patches objects parts objects, sample regions considerable gradients. define family transformations?   parameterized vectors  set parameter vectors. transformation composition elementary transformations list:    translation: vertical horizontal translation distance patch size; scaling: multiplication patch scale factor; rotation: rotation image angle degrees; contrast multiply projection patch pixel principal components set pixels factor (factors independent principal component pixels patch); contrast raise saturation components HSV color representation) pixels power (same pixels patch), multiply values factor, add; Figure Exemplary patches sampled STL unlabeled dataset augmented transformations obtain surrogate data CNN training. figure Several random transformations applied patches extracted STL unlabeled dataset. original (?seed?) patch top left corner.  color: add hue component HSV color representation) pixels patch pixels patch). numerical parameters elementary transformations, concatenated toger, form single parameter vector initial patch sample , 300] random parameter vectors    apply transformations  patch yields set transformed versions Sxi  subtract pixel resulting dataset. apply preprocessing. exemplary patches  sampled stl unlabeled dataset shown fig.  examples transformed versions patch shown fig.  learning Algorithm Given sets transformed image patches, declare sets class assigning label class Sxi train CNN discriminate surrogate classes. formally, minimize loss function, loss transformed sample (surrogate) true label cnn softmax output layer optimize multinomial negative log likelihood network output, case, log analyze section. log) (?) denotes function computing values output layer CNN input data, ith standard basis vector. note limit infinite number transformations surrogate class, objective function) takes form) , ) intuitively, classification problem serves ensure input samples distinguished. time, enforces invariance transformations. sections provide foundation intuition. present formal analysis objective, separating defined classification problem regularizer enforces invariance (resembling analysis Wager. ]). discuss derived properties classification problem compare common practices unsupervised feature learning.  Formal Analysis denote  random vector transformation parameters) vector activations second-last layer network presented input patch matrix weights network layer) layer activations applying softmax) softmax)) output network. plugging definition softmax activation function softmax) exp exp objective function) loss) takes form ?hei? log exp?  ) With gbi ? average feature representation trans formed versions image patch rewrite. ) ?hei Wgbi log exp(wgbi [log exp?  log exp(wgbi ) sum objective function multinomial logistic regression problem input-target pairs (gbi objective falls back transformation-free instance classification problem ? )]. general, equality hold sum enforces correct classification average representation ? input sample. invariant representation, however, equality achieved. similarly, suppose  small values feature representation? approximately linear respect random variable centered.  [?] gbi ?    ?   sum. ) regularizer enforcing? close average value., feature representation sought approximately invariant transformations  show convexity function log exp(?  jensen inequality, yields (proof supplementary material) [log exp?  log exp(wgbi  ) feature representation perfectly invariant? wgbi inequality) turns equality, meaning regularizer reaches global minimum.  Conceptual Comparison Previous Unsupervised Learning Methods Suppose unsupervisedly learn feature representation recognition task, classification. mapping input images feature representation) satisfy requirements) feature similar images category (invariance) feature suﬃciently images categories (ability discriminate). unsupervised feature learning methods aim learn representation modeling input distribution). based assumption good model) information category distribution). , representation learned, sample reconstructed perfectly, representation expected encode information category sample (ability discriminate). additionally, learned representation invariant variations samples irrelevant classification task., adhere manifold hyposis (see. rifai. ] recent discussion). invariance classically achieved regularization latent representation., enforcing sparsity] robustness noise]. ) directly model input distribution) learns representation discriminates input samples. representation required reconstruct input, unnecessary recognition matching task. leaves degrees freedom model desired variability sample. contrast, discriminative objective. shown analysis (see. )), achieve partial invariance transformations applied surrogate data creation forcing representation? transformed image patch predictive surrogate label assigned original image patch noted approach assumes transformations change identity image content. , example, color transformation force network invariant change expect extracted features perform task relying color information (such differentiating black panrs pumas experiments compare discriminative approach previous unsupervised feature learning methods, report classification results stl], CIFAR10] caltech-101] datasets. moreover, assess inﬂuence augmentation parameters classification performance study invariance properties network.  Experimental Setup datasets test differ number classes CIFAR stl, 101 caltech) number samples class. stl suited unsupervised learning large set 100,000 unlabeled samples. experiments (except dataset transfer experiment supplementary material) extracted surrogate training data unlabeled subset stl. testing cifar, resized images pixels pixels scale depicted objects roughly matches datasets. worked network architectures. ?small? network evaluate inﬂuence components augmentation procedure classification performance. consists convolutional layers filters fully connected layer 128 neurons. layer succeeded softmax layer, serves network output. ?large? network, consisting convolutional layers, 128 256 filters fully connected layer 512 neurons, trained compare method state-art. models convolutional filters connected region input.  maxpooling performed convolutional layers. dropout] applied fully connected layers. trained networks implementation based Caffe]. details training, hyperparameter settings, analysis performance depending network architecture provided supplementary material. code training data http://lmb.informatik.uni-freiburg/resources applied feature representation images arbitrary size convolutionally computing responses network layers top softmax. feature map, applied pooling method commonly respective dataset-quadrant max-pooling, resulting values feature map, standard procedure stl cifar-layer spatial pyramid. max-pooling image quadrants cells grid, resulting values feature map, standard caltech-101]. finally, trained linear support vector machine (svm) pooled features. datasets standard training test protocols. stl SVM trained pre-defined folds training data. report standard deviation achieved fixed test set. cifar report results) training SVM cifar training set (?cifar? ) average random selections 400 training samples class (?cifar(400)?). caltech-101 usual protocol selecting random samples class training samples class testing. repeated times.  Classification Results Table compare exemplar-cnn unsupervised feature learning methods, including current state--art dataset. list state--art supervised learning (which directly comparable). additionally show dimensionality feature vectors Such cases covered eir careful selection applied transformations combining features multiple networks trained sets transformations letting final classifier choose features use. table Classification accuracies datasets percent).  average per-class accuracy2% %.  average per-class accuracy% %. algorithm stl cifar(400) cifar Convolutional-means Network   multi-way local pooling]     slowness videos] Hierarchical Matching Pursuit (hmp   multipath HMP]   view-invariant-means  exemplar-cnn (64c5-64c5-128f   ExemplarCNN (64c5-128c5-256c5-512f   Supervised state art] ] caltech-101 #features 8000  1024  556 1000  5000 6400 ? 256 ? 960] produced method final pooling. small network trained 8000 surrogate classes 150 samples large 16000 classes 100 samples each. features extracted larger network match outperform prior result datasets. fact dimensionality feature vector smaller approaches networks trained stl unlabeled dataset. transfer learning manner applied cifar Caltech 101). increase performance pronounced labeled samples training SVM case datasets full cifar). agreement previous evidence increasing feature vector dimensionality number labeled samples, training SVM dependent quality features]. remarkably, stl achieve accuracy%, large improvement previously reported results.  Detailed Analysis performed additional experiments (using ?small? network) study effect design choices exemplar-cnn training validate invariance properties learned features. experiments sampling ?seed? patches datasets found supplementary.  Number Surrogate Classes varied number surrogate classes 32000. sanity check, classification random filters. results shown fig.  clearly, classification accuracy increases number surrogate classes reaches optimum 8000 surrogate classes change decreased. expected: larger number surrogate classes, draw similar identical samples, hard impossible discriminate. cases detrimental classification performance, collisions dominate set surrogate labels, discriminative loss longer reasonable training network surrogate task longer succeeds. check validity explanation plot fig. classification error validation set (taken surrogate data) computed training network. rapidly grows number surrogate classes increases. observed optimal number surrogate classes increases size network (not shown figure), eventually saturates. demonstrates main limitation approach randomly sample ?seed? patches: scale arbitrarily large amounts unlabeled data. however, fundamental restriction discuss solutions Section  Number Samples Surrogate Class fig. shows classification accuracy number training samples surrogate class varies 300. performance improves samples surrogate class caltech-101 eir measure average accuracy samples (average accuracy) calculate accuracy class average values (average per-class accuracy). differ, classes fewer test samples. researchers average accuracy. classification STL  validation error surrogate data Classification accuracy stl 100 Error validation data Classification accuracy stl 100 250 500 1000 2000 4000 8000 1600032000 1000 classes 2000 classes 4000 classes random filters Number classes (log scale) 100 150 300 Number samples class (log scale) Figure Inﬂuence number surrogate training classes. validation error surrogate data shown red. note-axes curves. figure Classification performance STL numbers samples class. random filters samples class?. saturates 100 samples. amount suﬃcient approximate formal objective. ), furr increasing number samples significantly change optimization problem. hand, number samples small, insuﬃcient data learn desired invariance properties.  Types Transformations varied transformations creating surrogate data analyze inﬂuence final classification performance. set ?seed? patches fixed. result shown fig.  ? corresponds applying random compositions elementary transformations: scaling, rotation, translation, color variation, contrast variation. columns plot show difference classification accuracy discarded types elementary transformations. difference classification accuracy rotation scaling translation color contrast rot col+con stl cifar caltech?101 Removed transformations Figure Inﬂuence removing groups transseveral tendencies observed. first, roformations generation surrogate tation scaling minor impact training data. baseline? value) applying performance, translations, color varitransformations. group bars correations contrast variations significantly sponds removing transformations. important. secondly, results STL10 cifar consistently show spatial invariance color-contrast invariance approximately equal importance classification performance. variations color contrast, neglected, improve performance supervised learning scenario. thirdly, caltech-101 color contrast transformations important compared spatial transformations datasets. surprising, caltech-101 images aligned, dataset bias makes spatial invariance useful.  Invariance Properties Learned Representation final experiment, analyzed extent representation learned network invariant transformations applied training. randomly sampled 500 images stl test set applied range transformations (translation, rotation, contrast, color) image. avoid empty regions image boundaries applying spatial transformations, cropped central pixel sub-patch pixel image. applied measures invariance patches. first, explicit measure invariance, calculated normalized Euclidean distance normalized feature vectors original image patch transformed] (see supplementary material details). downside approach distance extracted features account informative discriminative are. re7 1st layer 2nd layer 3rd layer?quadrant HOG movements training data Rotations degrees Rotations degrees Translation (pixels) color transform Hue change  Hue change  Hue change  Rotation angle (degrees Hue shift Classification accuracy) Classification accuracy) Classification accuracy Distance feature vectors) contrast transform Contrast coeﬃcients) Contrast coeﬃcients) Contrast coeﬃcients) Contrast multiplier Figure Invariance properties feature representation learned exemplar-cnn. ): Normalized Euclidean distance feature vectors original translated image patches. magnitude translation): classification performance transformed image patches. magnitude transformation magnitudes transformations applied creating surrogate data. ): rotation): additive color change): multiplicative contrast change. fore evaluated measure classification performance depending magnitude transformation applied classified patches problem. compute classification accuracy, trained SVM central pixel patches fold stl training set measured classification performance transformed versions 500 samples test set. results experiments shown fig.  due space restrictions show representative plots. experiment empirically confirms exemplar-cnn objective leads learning invariant features. features layer final pooled feature representation compare favorably HOG baseline (fig. )). furrmore, adding stronger transformations surrogate training data leads invariant classification respect transformations (fig. )). however, adding contrast variation deteriorate classification performance (fig. )). reason level contrast feature: example, strong edges image important weak ones. discussion proposed discriminative objective unsupervised feature learning training CNN class labels. core idea generate set surrogate labels data augmentation. features learned network yield large improvement classification accuracy compared features obtained previous unsupervised methods. results strongly discriminative objective superior objectives previously unsupervised feature learning. potential shortcoming proposed method current state scale arbitrarily large datasets. probable reasons) number surrogate classes grows larger, similar, contradicts discriminative objective) surrogate task simple network learn invariance complex variations, viewpoint inter-instance variation. hyposize presented approach learn powerful higher-level features, surrogate data diverse. achieved additional weak supervision, example, means video small number labeled samples. anor obtaining richer surrogate training data time avoiding similar surrogate classes (unsupervised) merging similar surrogate classes. interesting directions future work. acknowledgements acknowledge funding ERC Starting Grant VideoLearn (279401); work partly supported BrainLinksBrainTools Cluster Excellence funded German Research Foundation (dfg, grant number EXC 1086). Convolutional neural networks (cnns) trained backpropagation recently shown perform image classification tasks millions training images thousands categories]. feature representation learned networks achieves state--art performance classification task network trained, visual recognition tasks, example: classification caltech-101], caltech-256] CaltechUCSD birds dataset]; scene recognition sun-397 database]; detection PASCAL VOC dataset]. this capability generalize datasets makes supervised CNN training attractive approach generic visual feature learning. downside supervised training expensive labeling, amount required labeled samples grows quickly larger model gets. large performance increase achieved methods based work Krizhevsky. ] was, example, due massive efforts manually annotating mil \\x0clions images. for reason, unsupervised learning underperforming remains appealing paradigm, make raw unlabeled images videos. furrmore, vision tasks classification wher training based object class labels advantageous. for example, unsupervised feature learning beneficial image restoration] recent results show outperforms supervised feature learning descriptor matching]. work combine power discriminative objective major advantage unsupervised feature learning: cheap data acquisition. introduce training procedure convolutional neural networks require labeled data. rar relies automatically generated surrogate task. task created taking idea data augmentation commonly supervised learning extreme. starting trivial surrogate classes consisting random image patch each, augment data applying random set transformations patch. train CNN classify surrogate classes. refer method exemplar training convolutional neural networks (exemplar-cnn). feature representation learned exemplar-cnn, construction, discriminative invariant typical transformations. confirm oretically empirically, showing approach matches outperforms previous unsupervised feature learning methods standard image classification benchmarks stl, cifar, caltech-101.  Related Work Our approach related large body work unsupervised learning invariant features training convolutional neural networks. convolutional training commonly supervised unsupervised methods utilize invariance image statistics translations. lecun. ], Kavukcuoglu. ], Krizhevsky. ]). similar approach current surge successful methods employing convolutional neural networks object recognition rely data augmentation generate additional training samples classification objective. krizhevsky. ], Zeiler Fergus]). while share architecture convolutional neural network) approaches, method rely labeled training data. unsupervised learning, studies learning invariant representations exist. denoising autoencoders], example, learn features robust noise reconstruct data randomly perturbed input samples. zou. ] learn invariant features video enforcing temporal slowness constraint feature representation learned linear autoencoder. sohn Lee] Hui] learn features invariant local image transformations. contrast discriminative approach, methods rely directly modeling input distribution typically hard jointly training multiple layers cnn. idea learning features invariant transformations explored supervised training neural networks. research similar early work tangent propagation] (and related double backpropagation]) aims learn invariance small predefined transformations neural network directly penalizing derivative output respect magnitude transformations. contrast, algorithm regularize derivative explicitly. thus sensitive magnitude applied transformation. this work loosely related unlabeled data regularizing supervised algorithms, self-training] entropy regularization]. contrast semi-supervised methods, ExemplarCNN training require labeled data. finally, idea creating auxiliary task order learn good data representation Ahmed. ], Collobert. ]. Creating Surrogate Training Data input training procedure set unlabeled images, roughly distribution images aim apply learned features. randomly sample , 32000] patches size pixels images varying positions scales forming initial training set    interested patches objects parts objects, sample regions considerable gradients. define family transformations?   parameterized vectors  set parameter vectors. each transformation composition elementary transformations list:    translation: vertical horizontal translation distance patch size; scaling: multiplication patch scale factor; rotation: rotation image angle degrees; contrast multiply projection patch pixel principal components set pixels factor (factors independent principal component pixels patch); contrast raise saturation components HSV color representation) pixels power (same pixels patch), multiply values factor, add; Figure Exemplary patches sampled STL unlabeled dataset augmented transformations obtain surrogate data CNN training. figure Several random transformations applied patches extracted STL unlabeled dataset. original (?seed?) patch top left corner.  color: add hue component HSV color representation) pixels patch pixels patch). all numerical parameters elementary transformations, concatenated toger, form single parameter vector for initial patch sample , 300] random parameter vectors    apply transformations  patch this yields set transformed versions Sxi  afterwards subtract pixel resulting dataset. apply preprocessing. exemplary patches  sampled stl unlabeled dataset shown fig.  examples transformed versions patch shown fig.  Learning Algorithm Given sets transformed image patches, declare sets class assigning label class Sxi train CNN discriminate surrogate classes. formally, minimize loss function, loss transformed sample (surrogate) true label CNN softmax output layer optimize multinomial negative log likelihood network output, case, log analyze section. log) (?) denotes function computing values output layer CNN input data, ith standard basis vector. note limit infinite number transformations surrogate class, objective function) takes form) , ) intuitively, classification problem serves ensure input samples distinguished. time, enforces invariance transformations. sections provide foundation intuition. present formal analysis objective, separating defined classification problem regularizer enforces invariance (resembling analysis Wager. ]). discuss derived properties classification problem compare common practices unsupervised feature learning.  Formal Analysis denote  random vector transformation parameters) vector activations second-last layer network presented input patch matrix weights network layer) layer activations applying softmax) softmax)) output network. plugging definition softmax activation function softmax) exp exp objective function) loss) takes form ?hei? log exp?  ) With gbi ? average feature representation trans formed versions image patch rewrite. ) ?hei Wgbi log exp(wgbi [log exp?  log exp(wgbi ) sum objective function multinomial logistic regression problem input-target pairs (gbi this objective falls back transformation-free instance classification problem ? )]. general, equality hold sum enforces correct classification average representation ? input sample. for invariant representation, however, equality achieved. similarly, suppose  small values feature representation? approximately linear respect random variable centered.  [?] gbi ?    ?   sum. ) regularizer enforcing? close average value., feature representation sought approximately invariant transformations  show convexity function log exp(?  jensen inequality, yields (proof supplementary material) [log exp?  log exp(wgbi  ) feature representation perfectly invariant? Wgbi inequality) turns equality, meaning regularizer reaches global minimum.  Conceptual Comparison Previous Unsupervised Learning Methods Suppose unsupervisedly learn feature representation recognition task, classification. mapping input images feature representation) satisfy requirements) feature similar images category (invariance) feature suﬃciently images categories (ability discriminate). most unsupervised feature learning methods aim learn representation modeling input distribution). this based assumption good model) information category distribution). that, representation learned, sample reconstructed perfectly, representation expected encode information category sample (ability discriminate). additionally, learned representation invariant variations samples irrelevant classification task., adhere manifold hyposis (see. rifai. ] recent discussion). invariance classically achieved regularization latent representation., enforcing sparsity] robustness noise]. ) directly model input distribution) learns representation discriminates input samples. representation required reconstruct input, unnecessary recognition matching task. this leaves degrees freedom model desired variability sample. contrast, discriminative objective.  shown analysis (see. )), achieve partial invariance transformations applied surrogate data creation forcing representation? transformed image patch predictive surrogate label assigned original image patch noted approach assumes transformations change identity image content. , example, color transformation force network invariant change expect extracted features perform task relying color information (such differentiating black panrs pumas Experiments compare discriminative approach previous unsupervised feature learning methods, report classification results stl], CIFAR10] caltech-101] datasets. moreover, assess inﬂuence augmentation parameters classification performance study invariance properties network.  Experimental Setup datasets test differ number classes CIFAR stl, 101 caltech) number samples class. stl suited unsupervised learning large set 100,000 unlabeled samples. experiments (except dataset transfer experiment supplementary material) extracted surrogate training data unlabeled subset stl. when testing cifar, resized images pixels pixels scale depicted objects roughly matches datasets. worked network architectures. ?small? network evaluate inﬂuence components augmentation procedure classification performance. consists convolutional layers filters fully connected layer 128 neurons. this layer succeeded softmax layer, serves network output. ?large? network, consisting convolutional layers, 128 256 filters fully connected layer 512 neurons, trained compare method state-art. models convolutional filters connected region input.  maxpooling performed convolutional layers. dropout] applied fully connected layers. trained networks implementation based Caffe]. details training, hyperparameter settings, analysis performance depending network architecture provided supplementary material. our code training data http://lmb.informatik.uni-freiburg/resources applied feature representation images arbitrary size convolutionally computing responses network layers top softmax. feature map, applied pooling method commonly respective dataset-quadrant max-pooling, resulting values feature map, standard procedure stl cifar-layer spatial pyramid. max-pooling image quadrants cells grid, resulting values feature map, standard caltech-101]. finally, trained linear support vector machine (svm) pooled features. datasets standard training test protocols. stl SVM trained pre-defined folds training data. report standard deviation achieved fixed test set. for cifar report results) training SVM cifar training set (?cifar? ) average random selections 400 training samples class (?cifar(400)?). for caltech-101 usual protocol selecting random samples class training samples class testing. this repeated times.  Classification Results Table compare exemplar-cnn unsupervised feature learning methods, including current state--art dataset. list state--art supervised learning (which directly comparable). additionally show dimensionality feature vectors Such cases covered eir careful selection applied transformations combining features multiple networks trained sets transformations letting final classifier choose features use. Table Classification accuracies datasets percent).  average per-class accuracy2% %.  average per-class accuracy% %. algorithm stl cifar(400) cifar Convolutional-means Network   multi-way local pooling]     slowness videos] Hierarchical Matching Pursuit (hmp   multipath HMP]   view-invariant-means  exemplar-cnn (64c5-64c5-128f   ExemplarCNN (64c5-128c5-256c5-512f   Supervised state art] ] caltech-101 #features 8000  1024  556 1000  5000 6400 ? 256 ? 960] produced method final pooling. small network trained 8000 surrogate classes 150 samples large 16000 classes 100 samples each. features extracted larger network match outperform prior result datasets. this fact dimensionality feature vector smaller approaches networks trained stl unlabeled dataset. transfer learning manner applied cifar Caltech 101). increase performance pronounced labeled samples training SVM case datasets full cifar). this agreement previous evidence increasing feature vector dimensionality number labeled samples, training SVM dependent quality features]. remarkably, stl achieve accuracy%, large improvement previously reported results.  Detailed Analysis performed additional experiments (using ?small? network) study effect design choices exemplar-cnn training validate invariance properties learned features. experiments sampling ?seed? patches datasets found supplementary.  Number Surrogate Classes varied number surrogate classes 32000. sanity check, classification random filters. results shown fig.  clearly, classification accuracy increases number surrogate classes reaches optimum 8000 surrogate classes change decreased. this expected: larger number surrogate classes, draw similar identical samples, hard impossible discriminate. few cases detrimental classification performance, collisions dominate set surrogate labels, discriminative loss longer reasonable training network surrogate task longer succeeds. check validity explanation plot fig. classification error validation set (taken surrogate data) computed training network. rapidly grows number surrogate classes increases. observed optimal number surrogate classes increases size network (not shown figure), eventually saturates. this demonstrates main limitation approach randomly sample ?seed? patches: scale arbitrarily large amounts unlabeled data. however, fundamental restriction discuss solutions Section  Number Samples Surrogate Class fig. shows classification accuracy number training samples surrogate class varies 300. performance improves samples surrogate class caltech-101 eir measure average accuracy samples (average accuracy) calculate accuracy class average values (average per-class accuracy). differ, classes fewer test samples. most researchers average accuracy. Classification STL  validation error surrogate data Classification accuracy stl 100 Error validation data Classification accuracy stl 100 250 500 1000 2000 4000 8000 1600032000 1000 classes 2000 classes 4000 classes random filters Number classes (log scale) 100 150 300 Number samples class (log scale) Figure Inﬂuence number surrogate training classes. validation error surrogate data shown red. note-axes curves. figure Classification performance STL numbers samples class. random filters samples class?. saturates 100 samples. this amount suﬃcient approximate formal objective. ), furr increasing number samples significantly change optimization problem. hand, number samples small, insuﬃcient data learn desired invariance properties.  Types Transformations varied transformations creating surrogate data analyze inﬂuence final classification performance. set ?seed? patches fixed. result shown fig.  ? corresponds applying random compositions elementary transformations: scaling, rotation, translation, color variation, contrast variation. different columns plot show difference classification accuracy discarded types elementary transformations. difference classification accuracy rotation scaling translation color contrast rot col+con stl cifar caltech?101 Removed transformations Figure Inﬂuence removing groups transseveral tendencies observed. first, roformations generation surrogate tation scaling minor impact training data. baseline? value) applying performance, translations, color varitransformations. each group bars correations contrast variations significantly sponds removing transformations. important. secondly, results STL10 cifar consistently show spatial invariance color-contrast invariance approximately equal importance classification performance. this variations color contrast, neglected, improve performance supervised learning scenario. thirdly, caltech-101 color contrast transformations important compared spatial transformations datasets. this surprising, caltech-101 images aligned, dataset bias makes spatial invariance useful.  Invariance Properties Learned Representation final experiment, analyzed extent representation learned network invariant transformations applied training. randomly sampled 500 images stl test set applied range transformations (translation, rotation, contrast, color) image. avoid empty regions image boundaries applying spatial transformations, cropped central pixel sub-patch pixel image. applied measures invariance patches. first, explicit measure invariance, calculated normalized Euclidean distance normalized feature vectors original image patch transformed] (see supplementary material details). downside approach distance extracted features account informative discriminative are. re7 1st layer 2nd layer 3rd layer?quadrant HOG movements training data Rotations degrees Rotations degrees Translation (pixels) color transform Hue change  Hue change  Hue change  Rotation angle (degrees Hue shift Classification accuracy) Classification accuracy) Classification accuracy Distance feature vectors) contrast transform Contrast coeﬃcients) Contrast coeﬃcients) Contrast coeﬃcients) Contrast multiplier Figure Invariance properties feature representation learned exemplar-cnn. ): Normalized Euclidean distance feature vectors original translated image patches. magnitude translation): classification performance transformed image patches. magnitude transformation magnitudes transformations applied creating surrogate data. ): rotation): additive color change): multiplicative contrast change. fore evaluated measure classification performance depending magnitude transformation applied classified patches problem. compute classification accuracy, trained SVM central pixel patches fold stl training set measured classification performance transformed versions 500 samples test set. results experiments shown fig.  due space restrictions show representative plots. overall experiment empirically confirms exemplar-cnn objective leads learning invariant features. features layer final pooled feature representation compare favorably HOG baseline (fig. )). furrmore, adding stronger transformations surrogate training data leads invariant classification respect transformations (fig. )). however, adding contrast variation deteriorate classification performance (fig. )). one reason level contrast feature: example, strong edges image important weak ones. discussion proposed discriminative objective unsupervised feature learning training CNN class labels. core idea generate set surrogate labels data augmentation. features learned network yield large improvement classification accuracy compared features obtained previous unsupervised methods. results strongly discriminative objective superior objectives previously unsupervised feature learning. one potential shortcoming proposed method current state scale arbitrarily large datasets. two probable reasons) number surrogate classes grows larger, similar, contradicts discriminative objective) surrogate task simple network learn invariance complex variations, viewpoint inter-instance variation. hyposize presented approach learn powerful higher-level features, surrogate data diverse. this achieved additional weak supervision, example, means video small number labeled samples. anor obtaining richer surrogate training data time avoiding similar surrogate classes (unsupervised) merging similar surrogate classes. interesting directions future work. acknowledgements acknowledge funding ERC Starting Grant VideoLearn (279401); work partly supported BrainLinksBrainTools Cluster Excellence funded German Research Foundation (dfg, grant number EXC 1086).',\n",
       " 'PP5559': 'large-scale Bayesian inference remains intractable models, logistic regression, sparse linear models, dynamical systems non-gaussian observations. approximate Bayesian inference requires fast, robust, reliable algorithms. context, algorithms based variational Gaussian) approximations growing popularity, strike favorable balance accuracy, generality, speed, ease use. inference remains problematic models large latent-dimensionality. variants convex], require variational parameters optimized, latent-dimensionality. slows optimization. solution restrict covariance representations naive mean-field] restricted Cholesky], result considerable loss accuracy significant posterior correlations exist. alternative reparameterize covariance obtain number parameters, number data examples]. however, destroys convexity converges slowly]. recent approach called dual variational inference] obtains fast convergence retaining parameterization, applicable models Poisson regression. paper, propose approach called decoupled variational Gaussian inference extends dual variational inference large class models. method relies ory Lagrangian multiplier methods. remaining widely applicable, approach reduces number variational parameters similar] converges similar convergence rates convex methods]. method similar spirit parallel expectation-propagation) provable convergence guarantees likelihoods log-concave. model paper, apply method Bayesian inference Latent Gaussian Models (lgms). choice motivated large amount existing work approximations LGMs], LGMs include popular models, Gaussian processes, Bayesian regression classification, Gaussian Markov random field, probabilistic pca. extensive list models Chapter]. included examples supplementary material. vector observations length LGMs model dependencies components latent Gaussian vector length joint distribution shown below. ), —?,  real-valued matrix size define linear predictors model observation link function exact form function depends type observations. bernoulli-logit distribution binary data]. supplementary material example. usually, exponential family distribution used, choices (such-distribution]). parameter set includes, parameters link function assumed known. suppress notation, simplicity. bayesian inference, compute expectations respect posterior distribution) shown below. anor important task computation marginal likelihood) maximized estimate parameters example, empirical Bayes]. ) —?, —?,  For non-gaussian likelihoods, tasks intractable. applications practice demand good approximations scale favorably Inference Lower Bound Maximization variational Gaussian) inference], assume posterior Gaussian). posterior covariance form set variational parameters, chosen maximize variational lower bound log marginal likelihood shown. ). lower bound, multiply divide) apply jensen inequality concavity log. ) log) log) ) log) simplified lower bound shown. ). detailed derivation found eqs. )? ] (and supplementary material). below, provide summary components.  log) max)]        term-divergence [log) log)], jointly concave). term sums data examples, term denoted expectation log respect wtn Gaussian distribution   wtn variances  wtn Vwn terms closed form, computed quadrature look tables]. note unlike methods], bound approximate terms. approximations lead loss accuracy. denote lower bound. ) expand. , [log— ?           Here— denotes determinant discuss existing methods pros cons.  Related Work straight-forward approach optimize. ) directly]. practice, direct methods slow memory-intensive large number variables. challis Barber] show log-concave likelihoods original problem. ) jointly concave Cholesky factor This fact, however, result reduction number parameters, propose factorizations restricted form, negatively affects approximation accuracy. ] note optimal form  diag(?  suggests reparameterizing. ) terms parameters,  variable. however, problem concave alternative parameterization]. moreover, shown], convergence exceedingly slow. coordinate-ascent approach] dual variational inference] speed convergence, limited class models. range deterministic inference approximations exist well. local variational method convex log-concave potentials solved large scales], applies \\x0conly models super-gaussian likelihoods. bound maximizes provably tight. , making accurate. expectation propagation] general accurate approximations mentioned here. however, based saddle-point rar optimization problem, standard algorithm converge numerically unstable. alternatives, variational Gaussian approximation stands compromise accuracy good algorithmic properties. decoupled Variational Gaussian Inference Lagrangian simplify form objective function decoupling divergence term terms including words, separate prior distribution likelihoods. introducing real-valued auxiliary-variables constraints hold:   (equivalent) optimization problem, max log— ?         subject constraints c1n) wtn c2n wtn Vwn For log-concave likelihoods, function) concave unlike original function (see. )) concave respect Cholesky diﬃculty lies nonlinear constraints c2n). establish problem rise convenient parameterization, affect maximum. significance reformulation lies lagrangian, shown below. ,  wtn wtn Vwn here, Lagrangian multipliers constraints c1n). show maximum. ) parameterized terms multipliers, reparameterization unique. orem states result relationships maximum. ). proof supplementary material. orem. holds maxima. ): stationary point . ) stationary point. ). stationary point exist unique  that,  diag(??     ) ?  depend gradient function satisfy conditions, 5hn? ? ? ?   wtn ? wtn ? denotes gradient) respect   ?  local maximizer. ), set?       strict maximizer. ).  likelihoods log-concave, global maximum?  obtained maximizing. ) global maximizer. ). part establishes parameterization?  (??  uniqueness, part shows conditions (??  satisfy. form] Gaussian Processes fixed-point iteration employed search  part shows parameterization obtained maxima Lagrangian rar minima saddle-points. final part considers case concave shows global maximum obtained maximizing lagrangian. note concavity lower bound required part parts true irrespective concavity. detailed proof orem supplementary material. note conditions. ) restrict values?  take. values valid range gradients unlike formulation] constrain variables, similar method]. algorithm makes problem infeasible values range. ranges variables vary depending likelihood however, show. ) strictly positive log-concave likelihoods. equality obtained. ), equality simply change variables equality obtained. ]. final inequality obtained convex logconcave likelihoods (5xx) denotes Hessian)).  ? ? ? 52hn? ) Optimization Algorithms Decoupled Variational Gaussian Inference orem suggests optimal solution obtained maximizing) Lagrangian maximization diﬃcult reasons. first, constraints c2n) non-linear function) concave. note easy apply augmented Lagrangian method first-order methods (see Chapter]) application require storage instead, method based linearization constraints avoid explicit computation storage first, show) concave, maximize minimizing sequence convex problems. solve convex problem dual-variational method].  Linearly Constrained Lagrangian (lcl) Method derive algorithm based linearly constrained Lagrangian (lcl) method]. lcl approach involves linearization non-linear constraints effective method large-scale optimization. packages MINOS]. variants method globally \\x0cconvergent robust], variant Chapter]. final algorithm: See Algorithm start  iteration minimize dual:  fnk?  min log diag(? — ?,?  ?. functions fnk? obtained follows: here, fnk?  max    obtained previous iteration. ) Algorithm Linearly constrained Lagrangian (lcl) method approximation Initialize           repeat For compute predictive variances? linear regression. )) parallel, compute? maximizes. ). find (?,  gradients?   ?   ?  convergence end constraint set box constraints global minimum. ) exists. show examples section. eﬃcient gradient computation: advantage approach gradient iteration computed eﬃciently, large gradient computation decoupled terms. term computed computing fnk? parallel, term involves prediction linear model. gradients respect (derived supplementary material?   ?  ?  ? ? maximizers. ?  computed follows,: diag(?  ,:  ? wtn wtn diag(?  ,:          ) quantities? computed parallel sometimes, closed form shown section), compute numerically optimizing two-dimensional functions. problems two-dimensional, Newton method easily implemented obtain fast convergence.  interpreted predictive means variances pseudo terms? linear model. compare. .  rasmussen book]. gradient computation expressed Bayesian prediction linear model existing implementation. example, binary multi-class classification, reuse eﬃcient implementation regression. general, Bayesian inference conjuate model compute gradient non-conjugate model. method avoids forming work linear projections eﬃciently computed vector-matrix-vector products. ?decoupling? nature algorithm clear. non-linear \\x0cputations depending data, parallel compute?  completely decoupled linear computations  summarized Algorithm). derivation: derive algorithm, linearize constraints. multiplier point iteration, linearize constraints c2n?2nk) c2n 5c2n      wtn Vwn (wtn Vwn wtn) Since linearized constraint?2nk) close original constraint c2n), penalize difference two. c2n) ?2nk wtn Vwn   wtn Vwn ) key point term independent allowing obtain closed-form solution  crucial extension non-concave case section.  subproblem defined linearized constraints penalization term: max)   wtn.     wtn Vwn This concave problem linear constraints optimized dual variational inference]. detailed derivation supplementary material. convergence: When LCL algorithm converges, quadratic convergence rates]. however, converge. globally convergent methods exist. ]) explore paper. below, present simple approach improves convergence log-concave likelihoods. augmented Lagrangian Methods log-concave likelihoods: When likelihood log-concave, lower bound local minimum, making optimization diﬃcult function). scenarios, algorithm converge starting values. convergence approach improved cases. simply add augmented Lagrangian term c2nk linearly constrained Lagrangian defined. ), shown]. here iteration subproblem: gaug)    subject constraints. ). sequence eir set constant increased slowly ensure convergence local maximum. details setting sequence affect convergence found Chapter]. fact algorithm converge. set examining primal function function respect deviations constraints. turns set larger largest eigenvalues Hessian primal function good discussion found Chapter]. fact linearized constraint?2nk) depend addition term affects computation fnk?  modify algorithm simply changing computation optimization function: max      clear augmented Lagrangian term ?convexify? non-convex function leading improved convergence. computation fnk? (?, functions obtained solving optimization problem shown. ). cases, compute functions closed form. example, shown supplementary material, compute   closed form Poisson likelihood shown below. show range fnk? finite. ?    log expression Laplace likelihood derived supplementary material. closed-form expression fnk?  Newton method optimization. facilitate convergence, warm-start optimization. concave, converges iterations, parallelize significant speed obtained. significant engineering effort required parallelization experiments paper. issue remains open evaluation range fnk? finite. now, simply set range gradients function shown. (also paragraph section). clear wher assure convergence optimization. prediction: Given  compute predictions equations similar regression. details rasmussen book]. results demonstrate advantages approach binary classification problem. model binary data bernoulli-logit likelihoods. function computed reasonable accuracy piecewise bound] pieces. apply model subproblem USPS digit data]. here, task classify. total 1540 data examples feature dimensionality 256. compare convergence, show results data sizes subsampling randomly examples. set squared-exponential kernel entry defined exp[?   feature. show results log(?) log) corresponds diﬃcult case approximations converge slowly (due ill-conditioning kernel]. conclusions hold parameter settings well. compare algorithm approach Opper Archambeau] Challis Barber]. refer ?opper? ?cholesky?, respectively. call approach ?decoupled?. all methods-bfgs method optimization (implemented minfunc Mark schmidt), Newton method expensive large algorithms stopped subsequent lower bound.  methods randomly initialized. results sensitive initialization. compare convergence terms lower bound. prediction errors show similar trend, refore present results summarized Figure plot shows negative lower bound time seconds increasing data sizes 200, 500, 1000 1500. Opper cholesky, show markers iteration. decoupled, show markers completion subproblem. result subproblem here, visible marker obtained subproblem onwards. data size increases, Decoupled converges faster methods, showing clear advantage methods large dimensionality. discussion Future Work paper, proposed decoupled inference method approximate Bayesian inference. obtain eﬃcient reparameterization Lagrangian lower bound. showed parameterization unique, log-concave likelihood functions, maximum lower bound obtained maximizing lagrangian. concave likelihood function, method recovers global maximum. proposed linearly constrained Lagrangian method maximize lagrangian. algorithm desired property reduces gradient computation linear model computation, parallelizing non-linear computations data examples. proposed algorithm capable attaining convergence rates similar convex methods. unlike methods mean-field approximation, method preserves posterior correlations generalizing stochastic variational inference (svi) methods] nonconjugate models. existing SVI methods rely mean-field approximations widely applied conjugate models. method, stochastically include constraints maximize lagrangian. amounts low-rank approximation covariance matrix construct unbiased estimate gradient. focused latent Gaussian models simplicity. easy extend approach non-gaussian latent models. sparse Bayesian linear model] Bayesian nonnegative matrix factorization]. similar decoupling method applied general latent variable models. note choice proper posterior distribution required eﬃcient parameterization posterior. sparse posterior covariance approximation decoupled formulation. idea Hinge type loss approximate likelihood terms. dualization similar shown give sparse posterior covariance. 200 500 Cholesky Opper Decoupled 545 \\x0cnegative Lower Bound Negative Lower Bound 1500 535 525 Cholesky Opper Decoupled 1480 1460 1440 1420 1400 1380 1360 Time seconds Time seconds 1000 1500 Cholesky Opper Decoupled 2840 Negative Lower Bound Negative Lower Bound 4280 2820 2800 2780 Cholesky Opper Decoupled 4270 4260 4250 4240 2760 100 150 4230 200 Time seconds 100 150 200 300 400 500 Time seconds Figure Convergence results classification usps-3vs5 data set. plot shows negative lower bound time seconds data sizes 200, 500, 1000 1500. Opper cholesky, show \\x0cmarkers iteration. decoupled, show markers completion subproblem. result subproblem here, visible marker obtained subproblem. data size increases, Decoupled converges faster, showing clear advantage methods large dimensionality. weakness paper lack strong experiments showing decoupled method converge fast rate. implementation decoupled method requires good engineering effort scale big data. future, plan eﬃcient implementation method demonstrate enables variational inference scale large data. acknowledgments This work supported School Computer Science Communication epfl. specifically Matthias grossglauser, Rudiger urbanke, Jame Larus providing support funding work. personally Volkan cevher, Quoc trandinh, Matthias Seeger EPFL early discussions work Marc Desgroseilliers EPFL checkin proofs. reviewers valuable feedback. experiments paper extensive promised due time space constraints, add experiments arxiv version paper. large-scale Bayesian inference remains intractable models, logistic regression, sparse linear models, dynamical systems non-gaussian observations. approximate Bayesian inference requires fast, robust, reliable algorithms. context, algorithms based variational Gaussian) approximations growing popularity, strike favorable balance accuracy, generality, speed, ease use. inference remains problematic models large latent-dimensionality. while variants convex], require variational parameters optimized, latent-dimensionality. this slows optimization. one solution restrict covariance representations naive mean-field] restricted Cholesky], result considerable loss accuracy significant posterior correlations exist. alternative reparameterize covariance obtain number parameters, number data examples]. however, destroys convexity converges slowly]. recent approach called dual variational inference] obtains fast convergence retaining parameterization, applicable models Poisson regression. paper, propose approach called decoupled variational Gaussian inference extends dual variational inference large class models. our method relies ory Lagrangian multiplier methods. while remaining widely applicable, approach reduces number variational parameters similar] converges similar convergence rates convex methods]. our method similar spirit parallel expectation-propagation) provable convergence guarantees likelihoods log-concave. Model paper, apply method Bayesian inference Latent Gaussian Models (lgms). this choice motivated large amount existing work approximations LGMs], LGMs include popular models, Gaussian processes, Bayesian regression classification, Gaussian Markov random field, probabilistic pca. extensive list models Chapter]. included examples supplementary material. given vector observations length LGMs model dependencies components latent Gaussian vector length joint distribution shown below. ), —?,  real-valued matrix size define linear predictors each model observation link function exact form function depends type observations. bernoulli-logit distribution binary data]. see supplementary material example. usually, exponential family distribution used, choices (such-distribution]). parameter set includes, parameters link function assumed known. suppress notation, simplicity. Bayesian inference, compute expectations respect posterior distribution) shown below. anor important task computation marginal likelihood) maximized estimate parameters example, empirical Bayes]. ) —?, —?,  For non-gaussian likelihoods, tasks intractable. applications practice demand good approximations scale favorably Inference Lower Bound Maximization variational Gaussian) inference], assume posterior Gaussian). posterior covariance form set variational parameters, chosen maximize variational lower bound log marginal likelihood shown. ). lower bound, multiply divide) apply jensen inequality concavity log. ) log) log) ) log) simplified lower bound shown. ). detailed derivation found eqs. )? ] (and supplementary material). below, provide summary components.  log) max)]        term-divergence [log) log)], jointly concave). term sums data examples, term denoted expectation log respect since wtn Gaussian distribution   wtn variances  wtn Vwn terms closed form, computed quadrature look tables]. note unlike methods], bound approximate terms. such approximations lead loss accuracy. denote lower bound. ) expand. , [log— ?           Here— denotes determinant discuss existing methods pros cons.  Related Work straight-forward approach optimize. ) directly]. practice, direct methods slow memory-intensive large number variables. challis Barber] show log-concave likelihoods original problem. ) jointly concave Cholesky factor This fact, however, result reduction number parameters, propose factorizations restricted form, negatively affects approximation accuracy. ] note optimal form  diag(?  suggests reparameterizing. ) terms parameters,  variable. however, problem concave alternative parameterization]. moreover, shown], convergence exceedingly slow. coordinate-ascent approach] dual variational inference] speed convergence, limited class models. range deterministic inference approximations exist well. local variational method convex log-concave potentials solved large scales], applies \\x0conly models super-gaussian likelihoods. bound maximizes provably tight. , making accurate. expectation propagation] general accurate approximations mentioned here. however, based saddle-point rar optimization problem, standard algorithm converge numerically unstable. among alternatives, variational Gaussian approximation stands compromise accuracy good algorithmic properties. Decoupled Variational Gaussian Inference Lagrangian simplify form objective function decoupling divergence term terms including words, separate prior distribution likelihoods. introducing real-valued auxiliary-variables constraints hold:   this (equivalent) optimization problem, max log— ?         subject constraints c1n) wtn c2n wtn Vwn For log-concave likelihoods, function) concave unlike original function (see. )) concave respect Cholesky diﬃculty lies nonlinear constraints c2n). establish problem rise convenient parameterization, affect maximum. significance reformulation lies lagrangian, shown below. ,  wtn wtn Vwn here, Lagrangian multipliers constraints c1n). show maximum. ) parameterized terms multipliers, reparameterization unique. orem states result relationships maximum. ). proof supplementary material. orem. holds maxima. ): stationary point . ) stationary point. ). for stationary point exist unique  that,  diag(??     ) ?  depend gradient function satisfy conditions, 5hn? ? ? ?   wtn ? wtn ? denotes gradient) respect   when?  local maximizer. ), set?       strict maximizer. ).  when likelihoods log-concave, global maximum?  obtained maximizing. ) global maximizer. ). part establishes parameterization?  (??  uniqueness, part shows conditions (??  satisfy. this form] Gaussian Processes fixed-point iteration employed search  part shows parameterization obtained maxima Lagrangian rar minima saddle-points. final part considers case concave shows global maximum obtained maximizing lagrangian. note concavity lower bound required part parts true irrespective concavity. detailed proof orem supplementary material. note conditions. ) restrict values?  take. values valid range gradients this unlike formulation] constrain variables, similar method]. algorithm makes problem infeasible values range. ranges variables vary depending likelihood however, show. ) strictly positive log-concave likelihoods. equality obtained. ), equality simply change variables equality obtained. ]. final inequality obtained convex logconcave likelihoods (5xx) denotes Hessian)).  ? ? ? 52hn? ) Optimization Algorithms Decoupled Variational Gaussian Inference orem suggests optimal solution obtained maximizing) Lagrangian maximization diﬃcult reasons. first, constraints c2n) non-linear function) concave. note easy apply augmented Lagrangian method first-order methods (see Chapter]) application require storage instead, method based linearization constraints avoid explicit computation storage first, show) concave, maximize minimizing sequence convex problems. solve convex problem dual-variational method].  Linearly Constrained Lagrangian (lcl) Method derive algorithm based linearly constrained Lagrangian (lcl) method]. LCL approach involves linearization non-linear constraints effective method large-scale optimization. packages MINOS]. variants method globally \\x0cconvergent robust], variant Chapter]. final algorithm: See Algorithm start  iteration minimize dual:  fnk?  min log diag(? — ?,?  ?. functions fnk? obtained follows: here, fnk?  max    obtained previous iteration. ) Algorithm Linearly constrained Lagrangian (lcl) method approximation Initialize           repeat For compute predictive variances? linear regression. )) for parallel, compute? maximizes. ). find (?,  gradients?   ?   ?  convergence end constraint set box constraints global minimum. ) exists. show examples section. eﬃcient gradient computation: advantage approach gradient iteration computed eﬃciently, large gradient computation decoupled terms. term computed computing fnk? parallel, term involves prediction linear model. gradients respect (derived supplementary material?   ?  ?  ? ? maximizers. ?  computed follows,: diag(?  ,:  ? wtn wtn diag(?  ,:          ) quantities? computed parallel sometimes, closed form shown section), compute numerically optimizing two-dimensional functions. since problems two-dimensional, Newton method easily implemented obtain fast convergence.  interpreted predictive means variances pseudo terms? linear model. compare. .  rasmussen book]. hence gradient computation expressed Bayesian prediction linear model existing implementation. for example, binary multi-class classification, reuse eﬃcient implementation regression. general, Bayesian inference conjuate model compute gradient non-conjugate model. this method avoids forming work linear projections eﬃciently computed vector-matrix-vector products. ?decoupling? nature algorithm clear. non-linear \\x0cputations depending data, parallel compute?  completely decoupled linear computations  this summarized Algorithm). derivation: derive algorithm, linearize constraints. given multiplier point iteration, linearize constraints c2n?2nk) c2n 5c2n      wtn Vwn (wtn Vwn wtn) Since linearized constraint?2nk) close original constraint c2n), penalize difference two. c2n) ?2nk wtn Vwn   wtn Vwn ) key point term independent allowing obtain closed-form solution  this crucial extension non-concave case section.  subproblem defined linearized constraints penalization term: max)   wtn.     wtn Vwn This concave problem linear constraints optimized dual variational inference]. detailed derivation supplementary material. convergence: When LCL algorithm converges, quadratic convergence rates]. however, converge. globally convergent methods exist. ]) explore paper. below, present simple approach improves convergence log-concave likelihoods. augmented Lagrangian Methods log-concave likelihoods: When likelihood log-concave, lower bound local minimum, making optimization diﬃcult function). scenarios, algorithm converge starting values. convergence approach improved cases. simply add augmented Lagrangian term c2nk linearly constrained Lagrangian defined. ), shown]. here iteration subproblem: gaug)    subject constraints. ). sequence eir set constant increased slowly ensure convergence local maximum. more details setting sequence affect convergence found Chapter]. fact algorithm converge. this set examining primal function function respect deviations constraints. turns set larger largest eigenvalues Hessian primal function good discussion found Chapter]. fact linearized constraint?2nk) depend addition term affects computation fnk?  modify algorithm simply changing computation optimization function: max      clear augmented Lagrangian term ?convexify? non-convex function leading improved convergence. computation fnk? (?, functions obtained solving optimization problem shown. ). cases, compute functions closed form. for example, shown supplementary material, compute   closed form Poisson likelihood shown below. show range fnk? finite. ?    log expression Laplace likelihood derived supplementary material. when closed-form expression fnk?  Newton method optimization. facilitate convergence, warm-start optimization. when concave, converges iterations, parallelize significant speed obtained. significant engineering effort required parallelization experiments paper. issue remains open evaluation range fnk? finite. for now, simply set range gradients function shown. (also paragraph section). clear wher assure convergence optimization. prediction: Given  compute predictions equations similar regression. see details rasmussen book]. Results demonstrate advantages approach binary classification problem. model binary data bernoulli-logit likelihoods. function computed reasonable accuracy piecewise bound] pieces. apply model subproblem USPS digit data]. here, task classify. total 1540 data examples feature dimensionality 256. since compare convergence, show results data sizes subsampling randomly examples. set squared-exponential kernel entry defined exp[?   feature. show results log(?) log) corresponds diﬃcult case approximations converge slowly (due ill-conditioning kernel]. our conclusions hold parameter settings well. compare algorithm approach Opper Archambeau] Challis Barber]. refer ?opper? ?cholesky?, respectively. call approach ?decoupled?. for \\x0call methods-bfgs method optimization (implemented minfunc Mark schmidt), Newton method expensive large all algorithms stopped subsequent lower bound.  all methods randomly initialized. our results sensitive initialization. compare convergence terms lower bound. prediction errors show similar trend, refore present results summarized Figure each plot shows negative lower bound time seconds increasing data sizes 200, 500, 1000 1500. for Opper cholesky, show markers iteration. for decoupled, show markers completion subproblem. result subproblem here, visible marker obtained subproblem onwards. data size increases, Decoupled converges faster methods, showing clear advantage methods large dimensionality. Discussion Future Work paper, proposed decoupled inference method approximate Bayesian inference. obtain eﬃcient reparameterization Lagrangian lower bound. showed parameterization unique, log-concave likelihood functions, maximum lower bound obtained maximizing lagrangian. for concave likelihood function, method recovers global maximum. proposed linearly constrained Lagrangian method maximize lagrangian. algorithm desired property reduces gradient computation linear model computation, parallelizing non-linear computations data examples. our proposed algorithm capable attaining convergence rates similar convex methods. unlike methods mean-field approximation, method preserves posterior correlations generalizing stochastic variational inference (svi) methods] nonconjugate models. existing SVI methods rely mean-field approximations widely applied conjugate models. under method, stochastically include constraints maximize lagrangian. this amounts low-rank approximation covariance matrix construct unbiased estimate gradient. focused latent Gaussian models simplicity. easy extend approach non-gaussian latent models. sparse Bayesian linear model] Bayesian nonnegative matrix factorization]. similar decoupling method applied general latent variable models. note choice proper posterior distribution required eﬃcient parameterization posterior. sparse posterior covariance approximation decoupled formulation. one idea Hinge type loss approximate likelihood terms. using dualization similar shown give sparse posterior covariance. 200 500 Cholesky Opper Decoupled 545 \\x0cnegative Lower Bound Negative Lower Bound 1500 535 525 Cholesky Opper Decoupled 1480 1460 1440 1420 1400 1380 1360 Time seconds Time seconds 1000 1500 Cholesky Opper Decoupled 2840 Negative Lower Bound Negative Lower Bound 4280 2820 2800 2780 Cholesky Opper Decoupled 4270 4260 4250 4240 2760 100 150 4230 200 Time seconds 100 150 200 300 400 500 Time seconds Figure Convergence results classification usps-3vs5 data set. each plot shows negative lower bound time seconds data sizes 200, 500, 1000 1500. for Opper cholesky, show \\x0cmarkers iteration. for decoupled, show markers completion subproblem. result subproblem here, visible marker obtained subproblem. data size increases, Decoupled converges faster, showing clear advantage methods large dimensionality. weakness paper lack strong experiments showing decoupled method converge fast rate. implementation decoupled method requires good engineering effort scale big data. future, plan eﬃcient implementation method demonstrate enables variational inference scale large data. acknowledgments This work supported School Computer Science Communication epfl. specifically Matthias grossglauser, Rudiger urbanke, Jame Larus providing support funding work. personally Volkan cevher, Quoc trandinh, Matthias Seeger EPFL early discussions work Marc Desgroseilliers EPFL checkin proofs. reviewers valuable feedback. experiments paper extensive promised due time space constraints, add more experiments arxiv version paper.',\n",
       " 'PP5652': 'primate human retina, roughly distinct classes retinal ganglion cells (rgcs) send distinct visual information diverse targets brain]. complementary methods identification RGC types pursued extensively. anatomical studies relied indicators dendritic field size shape, stratification patterns synaptic connections] distinguish cell classes. functional studies leveraged differences responses stimulation variety visual stimuli, purpose. successful, methods diﬃcult, timeconsuming require significant expertise. thus, suitable large-scale, automated analysis existing large-scale physiological recording data. furrmore, clinical settings, inapplicable. specific scientific engineering goals demand development efficient methods cell type identification: discovery cell types.  morphologically distinct RGC types exist, characterized functionally. automated means detecting unknown cell types electrophysiological recordings make process massive amounts existing large-scale physiological data long analyze manually, order search poorly understood RGC types.  developing brain-machine interfaces future. blind patients suffering retinal degeneration, RGCs longer respond light. advanced retinal prosses previously demonstrated-vivo aim electrically restoring correct neural code RGC type diseased retina], requires cell type identification information light response properties rgcs. present paper, introduce eﬃcient computational methods cell type identification neural circuit, spatiotemporal voltage signals produced spiking cells recorded high-density, large-scale electrode array]. describe data study Section show raw descriptors classifiers extracted voltage recordings primate retina. introduce classifier leverages handspecified randomprojection based features electrical signatures unique rgcs, large unlabeled data sets, identify cell types (section). evaluate performance distinguishing midget, parasol small bistratified cells manually annotated datasets. section show matrix completion techniques identify populations unique cell types, assess accuracy algorithm predicting polarity off) RGCs datasets ground truth available. section devoted numerical experiments designed test modeling choices. finally, discuss future work Section extracting descriptors electrical recordings section, define electrical signatures cell classification, algorithms perform statistical inference cell type subsequent sections. exploit electrical signatures recorded neurons measured large-scale, highdensity recordings. first, electrical image) cell, average spatiotemporal pattern voltage measured entire electrode array spiking cell. measure information geometric electrical conduction properties cell itself. second, inter-spike interval distribution (isi), summarizes temporal separation spikes emitted cell. measure reﬂects specific ion channels cell distribution cell. third, cross-correlation function (ccf) firing cells. measure captures degree polarity interactions cells generation spike.  Electrophysiological image calculation, alignment filtering raw data numerical experiments consist extracellular voltage recordings electrical activity retinas male female \\x0cmacaque monkeys, sampled digitized khz channel 512 channels laid hexagonal lattice (see Appendix 100 sample movie electrical recording). emission action potential spiking neuron transient voltage ﬂuctuations anatomical features (soma, dendritic tree, axon). bringing extracellular matrix electrodes contact neural tissue, capture projection voltage plane recording electrodes (see Figure). dense multielectrode arrays, voltage activity single cell picked multiple electrodes. literature refers footprint electrophysiological electrical image) cell], inherently spatiotemporal characteristic neuron, due transient nature action potentials. essence, short movie ) average electrical activity array emission action potential spiking neuron, include properties cells firing correlated neuron. calculated electrical images identified RGC recording literature].  minute recording, typically detected,000?100,000 action potentials rgc. cell, averaged voltages recorded entire array window starting peak negative voltage sample action potential. cropped electrode array subset electrodes falls 125 radius RGC soma (see Figure order represent matrix (time points number electrodes 125 radius), equivalently 570 dimensional vector. augment training data exploiting symmetries (approximately) hexagonal grid electrode array. form training data EIs original eis, rotating,   reﬂection spatial symmetries total). characteristic radius (125 here) select central portion hyper-parameter method controls signal noise ratio input data (see Section Figure middle panel). appendix paper describe families (subdivided sub-families) filters manually built capture anatomical features cell. particular, included filters action potential propagation velocities level axon hard-coded parameter captures soma size. quantities believed indicative cell type.  Time Distance soma, 600 120 300 Time spike) 600 Distance soma Time spike 120 Time Figure EIs cell morphology. (top row) Multielectrode arrays record projection spatio-temporal action potentials, schematically illustrated midget (left) parasol (right) rgc. midget cells asymmetric dendritic field, parasol cells isotropic. (bottom row) Temporal evolution voltage recorded electrodes located 125 ?mradius electrode largest action potential detected, cell type classification. amplitude circles materialize signal amplitude. red circles positive voltages, blue circles negative voltages. filtered spatiotemporally aligned RGC electrical images hand defined filters create feature set. separate experiments filtered aligned EIs iid Gaussian random filters features) fashion], Table compare performances.  Interspike Intervals statistics timing action potential trains anor source information functional RGC types. interspike intervals (isis) estimate probability emission consecutive action potentials time difference spiking neuron. build histograms times elapsed consecutive action potentials cell form isi. estimate interspike intervals 100, time granularity, resulting 200 dimensional ISI vectors. isis begin refractory period. duration action potentials occur, action potential). period lasts. ISIs increase \\x0cdecaying back rates representative functional cell type (see Figure left hand side). describe ISI values time differences smood ISI reaches, 100% maximum slopes linear interpolations consecutive pair points.  cross-correlation functions electrical coupling cells retina high probability joint emission action potentials neighboring ganglion cells type, RGCs antagonistic polarities OFF cells) tend exhibit strongly negatively correlated firing patterns]. words, emission action potential pathway leads reduced probability observing action potential OFF pathway time. cross-correlation function RGCs characterizes probability joint emission action potentials pair cells latency, holds information functional coupling cells. cross-correlations functional RGC types studied extensively literature previously]. construction CCFs steps ISI computation: obtain CCF pairs cells building histograms time differences consecutive firing times. large CCF origin indicative positive functional coupling negative coupling corresponds negative CCF origin (see Figure panels right). .005 Parasols 100) 150 200 off?off Parasols?off Parasols correlation Frequency.015 correlation \\x0coff Parasol Parasol Midget Off Midget SBC correlation Interspike Intervals.025) Figure (left panel) Interspike Intervals major RGC types primate retina. (right panels) cross-correlation functions parasol cells. purple traces: single pairwise ccf. red line: population average. green arrow: strength correlation.  Learning electrical signatures retinal ganglion cells Learning dictionaries slices unlabeled data Learning descriptors unlabeled data, dictionary learning], successfully classification tasks high-dimensional data images, speech texts]. methodology learning discriminative features large amount unlabeled data closely steps]. extracting independent slices data. step approach consists extracting independent possible) slices data points. slice subset descriptors (nearly) independent subsets. image processing analogue object named patch. small sub-image. case, data slices. isi descriptors form slice, ors extracted eis. reasonable assume ISI features descriptors independent quantities. aligning EIs filtering collection filter banks (see Appendix description biologically motivated filters), group set filtered eis. group filters reacts specific patterns eis: rotational motion driven dendrites, radial propagation electrical signal axon direction propagation constitute behaviors captured distinct filter banks. reby, treat response data unique data slice. slice whitened], finally perform sparse-means slice separately, denotes integer parameter algorithm. , letting  denote slice data: number data points dimensionality slice denote set cluster assignment matrices  ] kui,? }, optimization problem min uvt k2f ?kvk1.    ) \\x0cwarm-starting-means warm started nmf. order solve optimization problem), propose coarse-fine strategy consists relaxing constraint  steps. initially relax constraint  completely set  , problem) substitute larger set run alternate minimization steps. replace clustering constraint nonnegativity constraint  retaining  steps nonnegative alternate minimization activate constraint  finally raise warm-start strategy systematically resulted lower values objective compared random-means] initializations.  Building feature vectors labeled data order extract feature vectors labeled data extract slice data point: extract ISI features hand filter data point filter families. slice separately whitened compared cluster centers slice. this, matrices) cluster centroids computed slices    letting(?, denote soft thresholding ) slice, operator, (sign max  compute soft-thresholded products slice data point) ) slices cluster centroids slice concatenate  resulting encoded point predict cell types) step performed eir toger label logistic regression feeding concatenated vectors classifier handles multiple classes one-versus-all fashion, random forest classifier. predicting cell polarities completing RGC coupling matrix additionally exploit pairwise spike train cross-correlations infer RGC polarities off) estimate polarity vector measure pairwise functional coupling strength cells. rationale approach neighboring cells polarity tend exhibit positive correlations action potential spike trains, positive functional coupling. cells antagonistic polarities, functional coupling strength negative. coupling neighboring cells refore modeled} } denote cell polarities. cells excite inhibit, avoid incorporating noise model, choose include estimates functional coupling strengths neighboring cells. neighborhood size hyperparameter approach study Section denotes graph neighboring cells recording, cross-correlations spike trains cells connected edge Since estimate position RGC lattice], refore form graph-dimensional regular geometric graph. number edges denote linear map returning values) cells located critical distance.  denote adjoint (transpose) operator. complete \\x0cmatrix pairwise couplings written observation noise yyt  vector cell polarities OFF cells). refore, observation modeled(yyt  observation noise. ) recovery yyt formulated standard matrix completion problem.  Minimizing nonconvex loss warm-started Newton steps section, show estimate observation(yyt minimizing non-convex loss(zzt )?ck22 minimizing degree polynomial loss function-hard general, propose Newton method warm-started spectral heuristic approaching solution (see Algorithm). similar contexts, sampling entries uniform, type spectral initialization alternate minimization proven converge global minimum least-squared loss, analogous]. sampling graph erdos-renyi graph, empirically observed regular structure enables reliable initial spectral guess falls basin attraction global minimum subsequent Newton scheme, iterate shifted Hessian matrix) (zzt  ensures positive definiteness) computing  expensive due potentially large number cells replacing diagonal scalar approximation ?/kzk22 reduces iteration cost resulting slower convergence. refer method first-order method minimizing nonconvex objective, ISTA] order method applied convex relaxation problem presented Appendix (see Figure middle panel). convex relaxation prove Appendix proposed estimator classification accuracy ?  . algorithm Polarity matrix completion require: observed couplings, projection operator Let leading eigenpair ) initialize  —#revealed entries—       Hessian approximation end Input Task ISI filters) ISI rand. filters) ISI rand. filters) filters) ISI CCF) ) table Comparing performance input data sources filters. cell type identification. polarity identification. : cell type polarity identification. eis cropped 125 central electrode. numerical experiments section, benchmark performance cell type classifiers introduced previously datasets ground truth available. RGCs datasets, experts manually hand-labeled light response properties cells manner previously literature]. unlabeled data contained,457 (spatial symmetries) data points. labeled data consists 436 OFF midget, 652 OFF parasol, 964 midget, 607 parasol 169 small bistratified cells assembled distinct recordings. rgc classification electrical features. numerical experiment consists hiding labeled recordings, learning cell classifiers ors testing classifier hidden recording. chose test performance classifier individual recordings reasons. firstly, wanted compare polarity prediction accuracy electrical features prediction made matrix completion (see Section matrix completion algorithm takes input pairwise data obtained single recording only. secondly, experimental parameters inﬂuence EIs ISIs recording temperature vary recording recording, remain consistent recording. reported scores reﬂect expected performance recordings, including points test distribution realistic proxy true test error. table report classification accuracies classification tasks: cell type identification): midget. parasol. small bistratified cells; polarity identification): versus OFF cells; cell type polarity-midget-parasol. off-midget. off-parasol. small bistratified. row table data input. column represents results method dictionary learning step performed, EIs recorded radius 125 central electrode electrodes array). compare method identical method replaced hand-specified filters random Gaussian filters] (second column). performance random filters opens perspectives learning deeper predictors random filters layer. impact filters Figure left-hand panel: larger bring furr information polarity prediction cell type classification, leads optimal choice-class problem. 4th 5th columns, part features sets disposal, EIs ISIs respectively. results confirm joint EIs ISIs cell classification beneficial. globally, cell type identification turns easier task polarity prediction cell descriptors. figure middle panel illustrates impact diameter classification accuracy. larger recording radius lets make signal, amount noise incorporated increases number electrodes \\x0caccount observe trade-off terms signal noise ratio tasks. interesting observation jump accuracy cell-type prediction diameter 325, point attain peak performance% . jump takes place axonal signals start incorporated, signals strong indicator cell type Accuracy (%) 100 100 100 100 150 200 250 300 350 Dictionary size) Electrical image radius) 100 150 200 250 300 Maximum cell distance) Figure (left panel) Effect dictionary size (middle panel) EIs radius cell classification. (right panel) Effect neighborhood size polarity prediction matrix completion. Cell index Cell index 100 102 100 \\x0c100 loss) OPT Coupling strength.)  loss) opt): observed couplings 100 110 100 Newton First order Convex (ista) PCA 101 Iteration) 102 100-mnf NMF RND 101 102 103 Iteration) Figure (left panel) Observed coupling matrix. (middle panel) Convergence matrix completion algorithms. (right panel-means initialization-nmf) versus choices. differences axonal conduction velocities]. prediction variance low cell-type prediction compared polarity prediction, predicting polarity turns significantly easier datasets ors. average, logistic regression classifier performed slightly %) random forests tasks data sets disposal. matrix completion based polarity prediction. matrix completion resulted% accuracy datasets average% accuracy datasets. report average performance Table inferior simpler classification approach reasons) idea matrix completion task) high potential, demonstrated Figure hand panel. datasets, matrix completion 100%accuracy. however, datasets, eir issues fragile spike-sorting, noise, approach well. figure (right hand side) examine effect neighborhood size prediction accuracy. colors correspond datasets. sake readability, show results datasets: best, worse intermediary. sensitivity maximum cell distance clear plot. bold curves correspond prediction resulting 100 steps Newton algorithm. dashed curves correspond predictions order (nonconvex) method stopped 100 steps, dots prediction accuracies leading singular vector. spectral initialization algorithm. overall, Newton algorithm perform rivals, appears optimal radius choose dataset corresponds characteristic distance pairs cells (here parasols). parameter varies dataset dataset requires parameter tuning extracting CCF data order performance algorithm. warm-start strategy dictionary learning. refer Figure hand panel illustration warm-start strategy minimizing) Section. , compare dense -means initialized double-warm start steps unconstrained alternate minimization steps nonnegative alternate minimization, referred-nmf), single spectral warm start steps unconstrained alternate minimization initialization) steps nonnegative alternate minimization (nmf) standard baselines random initialization-means++ initializations]. postpone oretical study initialization choice future work. note step alternate minimization involves matrix-matrix products element-wise operations matrices. NVIDIA Tesla K40 GPU drastically accelerated steps, allowing scale experiments. discussion developed accurate cell-type classifiers unique collection labeled unlabeled electrical recordings employing recent advances areas machine learning. results show strong empirical success methodology, highly scalable adapted major applications discussed below. matrix completion binary classification novel, heuristics minimizing non-convex objectives show convincing superiority existing baselines. future work dedicated studying properties algorithms. recording methods. major aspects electrical recordings critical successful cell type identification electrical signatures. first, high spatial resolution required detect fine features eis; widely spaced electrode arrays cortex perform well. second, high temporal resolution required measure ISI accurately; suggests optical measurements++ sensors electrical measurements. third, large-scale recordings required detect pairs cells estimate functional interactions; electrode arrays fewer channels suffice. thus, large-scale, high-density electrophysiological recordings uniquely suited task identifying cell types. future directions. probable source variability cell type classification differences retinal preparations, including eccentricity retina, inter-animal variability, experimental variables temperature signal-noise recording. present data, features defined assembled dozen recordings. motivates transfer learning account variability, exploiting fact features change preparations (target domains), underlying cell types fundamental differences electrical signatures expected remain. expect future work result models enjoy higher complexity training larger datasets, achieving invariance ambient conditions (eccentricity \\x0cand temperature) automatically. model interpreted single-layer neural network. straightforward development increase number layers. relative success random filters layer sign hope furr automated improvement building richer representations data minimum incorporation prior knowledge. application. major applications envisioned. first, extensive set large-scale, highdensity recordings primate retina mined information infrequently-recorded cell types. manual identification cell types light response properties extremely labor-intensive, however, present approach promises facilitate automated mining. second, identification cell types light responses fundamental development highresolution retinal prosses future]. devices, identify electrodes capable stimulating cells, drive spiking RGCs type order deliver meaningful visual signal brain. futuristic brain-machine interface application, results solve fundamental problem. finally, hoped applications retina relevant brain areas, identification neural cell types customized electrical stimulation high-resolution neural implants equally important future. acknowledgement grateful Montanari Palanker inspiring discussions valuable comments, Rhoades labeling data. acknowledges support grants afosr/darpa fa9550-10411 fa9550-0036. stanford Data Science Initiative financial support NVIDIA Corporation donation Tesla K40 GPU used. data collection supported National Eye Institute grants EY017992 EY018003 (ejc). contact EJC@stanford.edu) access data. primate human retina, roughly distinct classes retinal ganglion cells (rgcs) send distinct visual information diverse targets brain]. two complementary methods identification RGC types pursued extensively. anatomical studies relied indicators dendritic field size shape, stratification patterns synaptic connections] distinguish cell classes. functional studies leveraged differences responses stimulation variety visual stimuli, purpose. although successful, methods diﬃcult, timeconsuming require significant expertise. thus, suitable large-scale, automated analysis existing large-scale physiological recording data. furrmore, clinical settings, inapplicable. least specific scientific engineering goals demand development efficient methods cell type identification: discovery cell types. while morphologically distinct RGC types exist, characterized functionally. automated means detecting unknown cell types electrophysiological recordings make process massive amounts existing large-scale physiological data long analyze manually, order search poorly understood RGC types.  developing brain-machine interfaces future. blind patients suffering retinal degeneration, RGCs longer respond light. advanced retinal prosses previously demonstrated-vivo aim electrically restoring correct neural code RGC type diseased retina], requires cell type identification information light response properties rgcs. present paper, introduce eﬃcient computational methods cell type identification neural circuit, spatiotemporal voltage signals produced spiking cells recorded high-density, large-scale electrode array]. describe data study Section show raw descriptors classifiers extracted voltage recordings primate retina. introduce classifier leverages handspecified randomprojection based features electrical signatures unique rgcs, large unlabeled data sets, identify cell types (section). evaluate performance distinguishing midget, parasol small bistratified cells manually annotated datasets. Section show matrix completion techniques identify populations unique cell types, assess accuracy algorithm predicting polarity off) RGCs datasets ground truth available. section devoted numerical experiments designed test modeling choices. finally, discuss future work Section Extracting descriptors electrical recordings section, define electrical signatures cell classification, algorithms perform statistical inference cell type subsequent sections. exploit electrical signatures recorded neurons measured large-scale, highdensity recordings. first, electrical image) cell, average spatiotemporal pattern voltage measured entire electrode array spiking cell. this measure information geometric electrical conduction properties cell itself. second, inter-spike interval distribution (isi), summarizes temporal separation spikes emitted cell. this measure reﬂects specific ion channels cell distribution cell. third, cross-correlation function (ccf) firing cells. this measure captures degree polarity interactions cells generation spike.  Electrophysiological image calculation, alignment filtering raw data numerical experiments consist extracellular voltage recordings electrical activity retinas male female \\x0cmacaque monkeys, sampled digitized khz channel 512 channels laid hexagonal lattice (see Appendix 100 sample movie electrical recording). emission action potential spiking neuron transient voltage ﬂuctuations anatomical features (soma, dendritic tree, axon). bringing extracellular matrix electrodes contact neural tissue, capture projection voltage plane recording electrodes (see Figure). with dense multielectrode arrays, voltage activity single cell picked multiple electrodes. while literature refers footprint electrophysiological electrical image) cell], inherently spatiotemporal characteristic neuron, due transient nature action potentials. essence, short movie ) average electrical activity array emission action potential spiking neuron, include properties cells firing correlated neuron. calculated electrical images identified RGC recording literature].  minute recording, typically detected,000?100,000 action potentials rgc. for cell, averaged voltages recorded entire array window starting peak negative voltage sample action potential. cropped electrode array subset electrodes falls 125 radius RGC soma (see Figure order represent matrix (time points number electrodes 125 radius), equivalently 570 dimensional vector. augment training data exploiting symmetries (approximately) hexagonal grid electrode array. form training data EIs original eis, rotating,   reﬂection spatial symmetries total). characteristic radius (125 here) select central portion hyper-parameter method controls signal noise ratio input data (see Section Figure middle panel). Appendix paper describe families (subdivided sub-families) filters manually built capture anatomical features cell. particular, included filters action potential propagation velocities level axon hard-coded parameter captures soma size. quantities believed indicative cell type.  Time Distance soma, 600 120 300 Time spike) 600 Distance soma Time spike 120 Time Figure EIs cell morphology. (top row) Multielectrode arrays record projection spatio-temporal action potentials, schematically illustrated midget (left) parasol (right) rgc. midget cells asymmetric dendritic field, parasol cells isotropic. (bottom row) Temporal evolution voltage recorded electrodes located 125 ?mradius electrode largest action potential detected, cell type classification. amplitude circles materialize signal amplitude. red circles positive voltages, blue circles negative voltages. filtered spatiotemporally aligned RGC electrical images hand defined filters create feature set. separate experiments filtered aligned EIs iid Gaussian random filters features) fashion], Table compare performances.  Interspike Intervals statistics timing action potential trains anor source information functional RGC types. interspike intervals (isis) estimate probability emission consecutive action potentials time difference spiking neuron. build histograms times elapsed consecutive action potentials cell form isi. estimate interspike intervals 100, time granularity, resulting 200 dimensional ISI vectors. isis begin refractory period. duration action potentials occur, action potential). this period lasts. ISIs increase \\x0cdecaying back rates representative functional cell type (see Figure left hand side). describe ISI values time differences smood ISI reaches, 100% maximum slopes linear interpolations consecutive pair points.  cross-correlation functions electrical coupling cells retina high probability joint emission action potentials neighboring ganglion cells type, RGCs antagonistic polarities OFF cells) tend exhibit strongly negatively correlated firing patterns]. words, emission action potential pathway leads reduced probability observing action potential OFF pathway time. cross-correlation function RGCs characterizes probability joint emission action potentials pair cells latency, holds information functional coupling cells. cross-correlations functional RGC types studied extensively literature previously]. construction CCFs steps ISI computation: obtain CCF pairs cells building histograms time differences consecutive firing times. large CCF origin indicative positive functional coupling negative coupling corresponds negative CCF origin (see Figure panels right). .005 Parasols 100) 150 200 off?off Parasols?off Parasols correlation Frequency.015 correlation \\x0coff Parasol Parasol Midget Off Midget SBC correlation Interspike Intervals.025) Figure (left panel) Interspike Intervals major RGC types primate retina. (right panels) cross-correlation functions parasol cells. purple traces: single pairwise ccf. red line: population average. green arrow: strength correlation.  Learning electrical signatures retinal ganglion cells Learning dictionaries slices unlabeled data Learning descriptors unlabeled data, dictionary learning], successfully classification tasks high-dimensional data images, speech texts]. methodology learning discriminative features large amount unlabeled data closely steps]. extracting independent slices data. step approach consists extracting independent possible) slices data points. one slice subset descriptors (nearly) independent subsets. image processing analogue object named patch. small sub-image. case, data slices. ISI descriptors form slice, ors extracted eis. reasonable assume ISI features descriptors independent quantities. after aligning EIs filtering collection filter banks (see Appendix description biologically motivated filters), group set filtered eis. each group filters reacts specific patterns eis: rotational motion driven dendrites, radial propagation electrical signal axon direction propagation constitute behaviors captured distinct filter banks. reby, treat response data unique data slice. each slice whitened], finally perform sparse-means slice separately, denotes integer parameter algorithm. that, letting  denote slice data: number data points dimensionality slice denote set cluster assignment matrices  ] kui,? }, optimization problem min uvt k2f ?kvk1.    ) \\x0cwarm-starting-means warm started nmf. order solve optimization problem), propose coarse-fine strategy consists relaxing constraint  steps. initially relax constraint  completely set  that, problem) substitute larger set run alternate minimization steps. replace clustering constraint nonnegativity constraint  retaining  after steps nonnegative alternate minimization activate constraint  finally raise this warm-start strategy systematically resulted lower values objective compared random-means] initializations.  Building feature vectors labeled data order extract feature vectors labeled data extract slice data point: extract ISI features hand filter data point filter families. each slice separately whitened compared cluster centers slice. for this, matrices) cluster centroids computed slices    letting(?, denote soft thresholding ) slice, operator, (sign max  compute soft-thresholded products slice data point) ) slices cluster centroids slice concatenate  resulting encoded point predict cell types) step performed eir toger label logistic regression feeding concatenated vectors classifier handles multiple classes one-versus-all fashion, random forest classifier. Predicting cell polarities completing RGC coupling matrix additionally exploit pairwise spike train cross-correlations infer RGC polarities off) estimate polarity vector measure pairwise functional coupling strength cells. rationale approach neighboring cells polarity tend exhibit positive correlations action potential spike trains, positive functional coupling. cells antagonistic polarities, functional coupling strength negative. coupling neighboring cells refore modeled} } denote cell polarities. because cells excite inhibit, avoid incorporating noise model, choose include estimates functional coupling strengths neighboring cells. neighborhood size hyperparameter approach study Section denotes graph neighboring cells recording, cross-correlations spike trains cells connected edge Since estimate position RGC lattice], refore form graph-dimensional regular geometric graph. number edges denote linear map returning values) cells located critical distance.  denote adjoint (transpose) operator. complete \\x0cmatrix pairwise couplings written observation noise yyt  vector cell polarities OFF cells). refore, observation modeled(yyt  observation noise. ) recovery yyt formulated standard matrix completion problem.  Minimizing nonconvex loss warm-started Newton steps section, show estimate observation(yyt minimizing non-convex loss(zzt )?ck22 even minimizing degree polynomial loss function-hard general, propose Newton method warm-started spectral heuristic approaching solution (see Algorithm). similar contexts, sampling entries uniform, type spectral initialization alternate minimization proven converge global minimum least-squared loss, analogous]. while sampling graph erdos-renyi graph, empirically observed regular structure enables reliable initial spectral guess falls basin attraction global minimum subsequent Newton scheme, iterate shifted Hessian matrix) (zzt  ensures positive definiteness) whenever computing  expensive due potentially large number cells replacing diagonal scalar approximation ?/kzk22 reduces iteration cost resulting slower convergence. refer method first-order method minimizing nonconvex objective, ISTA] order method applied convex relaxation problem presented Appendix (see Figure middle panel). using convex relaxation prove Appendix proposed estimator classification accuracy ?  . algorithm Polarity matrix completion require: observed couplings, projection operator Let leading eigenpair ) initialize  —#revealed entries—       Hessian approximation end Input Task ISI filters) ISI rand. filters) ISI rand. filters) filters) ISI CCF) ) table Comparing performance input data sources filters. cell type identification. polarity identification. : cell type polarity identification. eis cropped 125 central electrode. Numerical experiments section, benchmark performance cell type classifiers introduced previously datasets ground truth available. for RGCs datasets, experts manually hand-labeled light response properties cells manner previously literature]. our unlabeled data contained,457 (spatial symmetries) data points. labeled data consists 436 OFF midget, 652 OFF parasol, 964 midget, 607 parasol 169 small bistratified cells assembled distinct recordings. rgc classification electrical features. our numerical experiment consists hiding labeled recordings, learning cell classifiers ors testing classifier hidden recording. chose test performance classifier individual recordings reasons. firstly, wanted compare polarity prediction accuracy electrical features prediction made matrix completion (see Section matrix completion algorithm takes input pairwise data obtained single recording only. secondly, experimental parameters inﬂuence EIs ISIs recording temperature vary recording recording, remain consistent recording. since reported scores reﬂect expected performance recordings, including points test distribution realistic proxy true test error. Table report classification accuracies classification tasks: cell type identification): midget. parasol. small bistratified cells; polarity identification): versus OFF cells; cell type polarity-midget-parasol. off-midget. off-parasol. small bistratified. each row table data input. column represents results method dictionary learning step performed, EIs recorded radius 125 central electrode electrodes array). compare method identical method replaced hand-specified filters random Gaussian filters] (second column). performance random filters opens perspectives learning deeper predictors random filters layer. impact filters Figure left-hand panel: larger bring furr information polarity prediction cell type classification, leads optimal choice-class problem. 4th 5th columns, part features sets disposal, EIs ISIs respectively. results confirm joint EIs ISIs cell classification beneficial. globally, cell type identification turns easier task polarity prediction cell descriptors. figure middle panel illustrates impact diameter classification accuracy. while larger recording radius lets make signal, amount noise incorporated increases number electrodes \\x0caccount observe trade-off terms signal noise ratio tasks. interesting observation jump accuracy cell-type prediction diameter 325, point attain peak performance% . jump takes place axonal signals start incorporated, signals strong indicator cell type Accuracy (%) 100 100 100 100 150 200 250 300 350 Dictionary size) Electrical image radius) 100 150 200 250 300 Maximum cell distance) Figure (left panel) Effect dictionary size (middle panel) EIs radius cell classification. (right panel) Effect neighborhood size polarity prediction matrix completion. Cell index Cell index 100 102 100 \\x0c100 loss) OPT Coupling strength.)  loss) opt): observed couplings 100 110 100 Newton First order Convex (ista) PCA 101 Iteration) 102 100-mnf NMF RND 101 102 103 Iteration) Figure (left panel) Observed coupling matrix. (middle panel) Convergence matrix completion algorithms. (right panel-means initialization-nmf) versus choices. differences axonal conduction velocities]. prediction variance low cell-type prediction compared polarity prediction, predicting polarity turns significantly easier datasets ors. average, logistic regression classifier performed slightly %) random forests tasks data sets disposal. matrix completion based polarity prediction. matrix completion resulted% accuracy datasets average% accuracy datasets. report average performance Table inferior simpler classification approach reasons) idea matrix completion task) high potential, demonstrated Figure hand panel. datasets, matrix completion 100%accuracy. however, datasets, eir issues fragile spike-sorting, noise, approach well. Figure (right hand side) examine effect neighborhood size prediction accuracy. colors correspond datasets. for sake readability, show results datasets: best, worse intermediary. sensitivity maximum cell distance clear plot. bold curves correspond prediction resulting 100 steps Newton algorithm. dashed curves correspond predictions order (nonconvex) method stopped 100 steps, dots prediction accuracies leading singular vector. spectral initialization algorithm. overall, Newton algorithm perform rivals, appears optimal radius choose dataset corresponds characteristic distance pairs cells (here parasols). this parameter varies dataset dataset requires parameter tuning extracting CCF data order performance algorithm. warm-start strategy dictionary learning. refer Figure hand panel illustration warm-start strategy minimizing) Section. , compare dense -means initialized double-warm start steps unconstrained alternate minimization steps nonnegative alternate minimization, referred-nmf), single spectral warm start steps unconstrained alternate minimization initialization) steps nonnegative alternate minimization (nmf) standard baselines random initialization-means++ initializations]. postpone oretical study initialization choice future work. note step alternate minimization involves matrix-matrix products element-wise operations matrices. using NVIDIA Tesla K40 GPU drastically accelerated steps, allowing scale experiments. Discussion developed accurate cell-type classifiers unique collection labeled unlabeled electrical recordings employing recent advances areas machine learning. results show strong empirical success methodology, highly scalable adapted major applications discussed below. matrix completion binary classification novel, heuristics minimizing non-convex objectives show convincing superiority existing baselines. future work dedicated studying properties algorithms. recording methods. three major aspects electrical recordings critical successful cell type identification electrical signatures. first, high spatial resolution required detect fine features eis; widely spaced electrode arrays cortex perform well. second, high temporal resolution required measure ISI accurately; suggests optical measurements++ sensors electrical measurements. third, large-scale recordings required detect pairs cells estimate functional interactions; electrode arrays fewer channels suffice. thus, large-scale, high-density electrophysiological recordings uniquely suited task identifying cell types. future directions. probable source variability cell type classification differences retinal preparations, including eccentricity retina, inter-animal variability, experimental variables temperature signal-noise recording. present data, features defined assembled dozen recordings. this motivates transfer learning account variability, exploiting fact features change preparations (target domains), underlying cell types fundamental differences electrical signatures expected remain. expect future work result models enjoy higher complexity training larger datasets, achieving invariance ambient conditions (eccentricity \\x0cand temperature) automatically. model interpreted single-layer neural network. straightforward development increase number layers. relative success random filters layer sign hope furr automated improvement building richer representations data minimum incorporation prior knowledge. application. two major applications envisioned. first, extensive set large-scale, highdensity recordings primate retina mined information infrequently-recorded cell types. manual identification cell types light response properties extremely labor-intensive, however, present approach promises facilitate automated mining. second, identification cell types light responses fundamental development highresolution retinal prosses future]. devices, identify electrodes capable stimulating cells, drive spiking RGCs type order deliver meaningful visual signal brain. for futuristic brain-machine interface application, results solve fundamental problem. finally, hoped applications retina relevant brain areas, identification neural cell types customized electrical stimulation high-resolution neural implants equally important future. acknowledgement grateful Montanari Palanker inspiring discussions valuable comments, Rhoades labeling data. acknowledges support grants afosr/darpa fa9550-10411 fa9550-0036. Stanford Data Science Initiative financial support NVIDIA Corporation donation Tesla K40 GPU used. data collection supported National Eye Institute grants EY017992 EY018003 (ejc). please contact EJC@stanford.edu) access data.',\n",
       " 'PP5669': 'variational inference computationally eﬃcient approach approximating posterior distributions. idea tractable family distributions latent variables minimize kullback-leibler divergence posterior. combined stochastic optimization, variational inference scale complex statistical models massive data sets]. computational complexity accuracy variational inference controlled factorization variational family. optimization tractable, algorithms fullyfactorized family, mean-field family, latent variable assumed independent. common, structured mean-field methods slightly relax assumption preserving original structure latent variables]. factorized distributions enable eﬃcient variational inference sacrifice accuracy. exact posterior, latent variables dependent mean-field methods, construction, fail capture dependency. end, develop copula variational inference (copula). copula augments traditional variational distribution copula, ﬂexible construction learning depen \\x0cdencies factorized distributions]. strategy advantages traditional: reduces bias; sensitive local optima; sensitive hyperparameters; helps characterize interpret dependency latent variables. variational inference previously restricted eir generic inference simple models?where dependency make significant difference writing model-specific variational updates. copula widens applicability, providing generic inference finds meaningful dependencies latent variables. detail, contributions following. generalization original procedure variational inference. copula generalizes variational inference mean-field structured factorizations: traditional corresponds running step method. coordinate descent, monotonically decreases divergence posterior alternating fitting mean-field parameters copula parameters. figure illustrates copula toy fitting bivariate gaussian. improving generic inference. copula applied inference procedure mean-field structured approach. furr, require specific knowledge Figure Approximations elliptical gaussian. mean-field (red) restricted fitting independent one-dimensional gaussians, step algorithm. step (blue) fits copula models dependency. iterations alternate: refits meanfield (green) fourth refits copula (cyan), demonstrating convergence true posterior. model, falls framework black box variational inference]. investigator write function evaluate model log-likelihood. rest algorithm calculations, sampling evaluating gradients, library. richer variational approximations. experiments, demonstrate copula standard Gaussian mixture models. found consistently estimates parameters, reduces sensitivity local optima, reduces sensitivity hyperparameters. examine copula captures dependencies latent space model]. copula outperforms competing methods significantly improves mean-field approximation.  Background Variational inference Let set observations, latent variables, free parameters variational distribution; ?). aim find approximation posterior variational distribution; quality approximation measured divergence. equivalent maximizing quantity (?) ;?) [log)] ;?) [log; ?)]. (?) evidence lower bound (elbo), variational free energy]. simpler computation, standard choice variational family mean-field approximation;  \\x0cwhere    note strong independence assumption. sophisticated approaches, structured variational inference], attempt restore dependencies latent variables. work, restore dependencies copulas. structured typically tailored individual models diﬃcult work mamatically. copulas learn general posterior dependencies inference, require investigator structure advance. furr, copulas augment structured factorization order introduce dependencies considered before; generalizes procedure. review copulas.  Copulas augment mean-field distribution copula. variational family    )).  Figure Example vine factorizes copula density random variables product pair copulas. edges tree nodes lower level tree edge determines bivariate copula conditioned random variables connected nodes share.  marginal cumulative distribution function (cdf) random variable joint distribution, random variables distribution called copula joint uniform marginal distrimultivariate density butions]. distribution, factorization product marginal densities copula exists integrates]. intuitively, copula captures information multivariate random variable eliminating marginal information., applying probability integral transform variable. copula captures dependencies. recall that, random variables uniform distributed. marginals copula give information. example, bivariate Gaussian copula defined     )). independent uniform distributed, inverse CDF standard normal transforms independent normals. cdf bivariate Gaussian distribution, Pearson correlation squashes transformed values back unit square. Gaussian  copula directly correlates Pearson correlation parameter  Vine copulas diﬃcult copula. find family distributions easy compute express broad range dependencies. work focuses two-dimensional copulas, student, clayton, gumbel, frank, Joe copulas]. however, multivariate extensions ﬂexibly model dependencies higher dimensions]. rar, successful approach recent literature combining sets conditional bivariate copulas; resulting joint called vine]. vine factorizes copula density   product conditional bivariate copulas, called pair copulas. makes easy high-dimensional copula. express dependence pair random variables conditioned subset ors. figure vine factorizes-dimensional copula product pair copulas. tree nodes representing random variables respectively. edge corresponds pair copula., symbolizes edges collapse nodes tree edges correspond conditional bivariate copulas symbolizes proceeds nested tree symbolizes overload notation marginal CDF depend names argument, occasionally clarity needed. analogous standard convention overloading probability density function(?).  vine structure specifies complete factorization multivariate copula, pair copula family set parameters formally, vine nested set trees  properties: tree nodes edges.  edges tree nodes tree  nodes tree joined edge edges tree share node. edge nested set trees  specifies pair copula, product edges comprise factorization copula density. total  edges, factorizes   product  pair copulas. edge, conditioning set), set variable indices   define cik) bivariate copula density conditioning set:    cik   ) ) Both copula cdf arguments conditional). vine specifies factorization copula, product edges levels     \\x0ccik)  highlight depends set parameters pair copulas. vine construction ﬂexibility model dependencies high dimensions decomposition pair copulas easier estimate. see, construction leads eﬃcient stochastic gradients taking individual (and easy) gradients pair copula. copula variational inference introduce copula variational inference (copula), method performing accurate scalable variational inference. simplicity, mean-field factorization augmented copula extend structured factorizations). copula-augmented variational family;         copula mean-field denotes mean-field parameters copula parameters. family, maximize augmented elbo, (?, ;?,?) [log)] ;?,?) [log; ?)]. copula alternates steps: fix copula parameters solve mean-field parameters fix mean-field parameters solve copula parameters generalizes mean-field approximation, special case initializing copula uniform stopping step. apply stochastic approximations] step gradients derived section.  set learning rate satisfy robbins-monro schedule summary outlined Algorithm alternating set optimizations falls class minorize-maximization methods, includes procedures algorithm], alternating squares algorithm, iterative procedure generalized method moments. step copula monotonically increases objective function refore approximates posterior distribution. algorithm Copula variational inference (copula) input: Data Model), Variational family Initialize randomly, uniform. change elbo threshold Fix maximize set iteration counter converged Draw sample unif update   ) Increment end Fix maximize set iteration counter converged Draw sample unif update   ) Increment end end output: Variational parameters (?, ?). copula generic input requirements black-box variational inference]? user joint model, order perform inference. furr, copula variational inference easily extends case original variational family structured factorization. vine construction, simply fixes pair copulas pre-existent dependence factorization independence copula. enables copula model dependence exist. optimization, assume tree structure copula families fixed. note, however, learned. study, learn tree structure sequential tree selection] learn families, choice bivariate families, Bayesian model selection] (see supplement). preliminary studies found-selection tree structure copula families significantly change future iterations.  Stochastic gradients elbo perform stochastic optimization, require stochastic gradients elbo respect mean-field copula parameters. copula objective leads eﬃcient stochastic gradients low variance. derive gradient respect mean-field parameters. general, apply score function estimator], leads gradient ;?,?) [?? log;  (log, log; ?))]. ) follow noisy unbiased estimates gradient sampling(?) evaluating expression. apply gradient discrete latent variables. latent variables differentiable, reparameterization trick] advantage first-order information model log). specifically, rewrite expectation terms random variable distribution) depend variational parameters latent variables deterministic function mean-field parameters; ?). reparameterization, gradients propagate inside expectation,  log, log; ?))?? ; ?)]. ) This estimator reduces variance stochastic gradients]. furrmore, copula variational family, type reparameterization uniform random variable deterministic function; possible. (see supplement.) reparameterized gradient) requires calculation terms log;  ;  tractable derived supplement; decomposes log;  log log     log log)  ,‘} summation pair copulas argument. words, gradient latent variable evaluated marginal pair copulas model correlation latent variable similar derivation holds calculating terms score function estimator. turn gradient respect copula parameters. copulas differentiable respect parameters. enables eﬃcient reparameterized gradient  log, log; ?))?? ; ?)]. requirements mean-field parameters. ) finally, note requirement model gradient log). calculated automatic differentiation tools]. Copula implemented library applied requiring manual derivations user.  Computational complexity vine factorization copula  pair copulas, number latent variables. stochastic gradients meanfield parameters copula parameters require complexity. generally, apply low rank approximation copula truncating number levels vine (see Figure). reduces number pair copulas leads computational complexity). sequential tree selection learning vine structure], correlated variables highest level vines. truncated low rank copula forgets weakest correlations. generalizes low rank Gaussian approximations) complexity]: special case mean-field distribution product independent gaussians, pair copula Gaussian copula.  Related work Preserving structure variational inference studied Saul Jordan] case probabilistic neural networks. revisited recently case conditionally conjugate exponential familes]. work differs line learn dependency structure inference, require explicit knowledge model. furr, augmentation strategy works broadly posterior distribution factorized variational family, generalizes approaches. similar augmentation strategy higher-order mean-field methods, Taylor series correction based difference posterior mean-field approximation]. recently, Giordano. ] covariance correction mean-field estimates. methods assume mean-field approximation reliable Taylor series expansion make sense, true general robust black box framework. approach alternates estimation mean-field copula, find empirically leads robust estimates estimating simultaneously, sensitive quality mean-field approximation. lambda method CVI LRVB All off?diagonal covariances method CVI LRVB Estimated \\x0cestimated Gibbs standard deviation Gibbs standard deviation Figure Covariance estimates copula variational inference (copula), mean-field), linear response variational Bayes (lrvb) ground truth (gibbs samples). copula lrvb effectively capture dependence underestimates variance forgets covariances. experiments study copula models: Gaussian mixtures latent space model]. gaussian mixture classical model diﬃcult capture posterior dependencies. latent space model modern Bayesian model mean-field approximation poor estimates posterior, modeling posterior dependencies crucial uncovering patterns data. implementation details copula. iteration, form stochastic gradient generating samples variational distribution taking average gradient. set 1024 follow asynchronous updates]. set step-size ADAM].  Mixture Gaussians follow goal Giordano. ], estimate posterior covariance Gaussian mixture. hidden variables-vector mixture proportions set -dimensional multivariate normals unknown -vector) precision matrix mixture gaussians, joint probability,  (?)     Dirichlet prior(?) normal-wishart prior  apply mean-field approximation), assigns independent factors perform copula copula-augmented mean-field distribution., includes pair copulas latent \\x0cvariables. compare results linear response variational Bayes (lrvb], posthoc correction technique covariance estimation variational inference. higher-order mean-field methods demonstrate similar behavior lrvb. comparisons structured approximations omitted require explicit factorizations black box. standard black box variational inference] corresponds approximation. simulate, 000 samples components dimensional gaussians. figure displays estimates standard deviations 100 simulations, plots ground truth 500 effective Gibb samples. plot displays off-diagonal covariance estimates. estimates  pattern supplement. initializing true mean-field parameters, copula lrvb achieve consistent estimates posterior variance. underestimates variance, well-known limitation]. note estimates initialized truth, copula converges true posterior step fitting copula. require alternating steps. variational inference methods Predictive Likelihood Runtime mean-field lrvb copula steps) copula steps) copula (converged) -383 -330 -303 min. min. min. . min. . table Predictive likelihood latent space model. copula step eir refits meanfield copula. copula converges roughly steps significantly outperforms mean-field lrvb fitting copula steps). copula robust lrvb. toy demonstration, analyze MNIST data set handwritten digits,665 training examples,115 test examples. perform ”unsupervised” classification., classify training labels: apply mixture Gaussians cluster, classify digit based membership assignment. copula reports test set error rate, lrvb ranges depending mean-field estimates. lrvb similar higher order mean-field methods correct existing solution sensitive local optima general quality solution. hand, copula-adjusts copula parameters fits, making robust initialization.  Latent space model study inference latent space model], Bernoulli latent factor model network analysis. node -node network -dimensional latent variable (?,  edges pairs nodes observed high probability nodes close latent space. formally, edge pair, observed probability logit)    model parameter. generate 100, 000 node network latent node attributes dimensional gaussian. learn posterior latent attributes \\x0corder predict likelihood held-out edges. applies independent factors lrvb applies correction, copula fully dependent variational distribution. table displays likelihood held-out edges runtime. attempted Hamiltonian Monte Carlo converge hours. copula dominates methods accuracy convergence, copula estimation refitting steps) dominates lrvb runtime accuracy. note lrvb requires invert  matrix. scale method achieve faster estimates copula applied stochastic approximations inversion. however, copula outperforms lrvb fast 100,000 node network. conclusion developed copula variational inference (copula). copula variational inference algorithm augments mean-field variational distribution copula; captures posterior dependencies latent variables. derived scalable generic algorithm performing inference expressive variational distribution. found copula significantly reduces bias mean-field approximation, estimates posterior variance, accurate forms capturing posterior dependency variational approximations. acknowledgments Luke bornn, Robin gong, Alp Kucukelbir insightful comments. work supported NSF iis-0745520, iis-1247664, iis-1009542, ONR n00014-10651, DARPA fa8750-0009, n66001-4032, facebook, adobe, amazon, John Templeton foundation. Variational inference computationally eﬃcient approach approximating posterior distributions. idea tractable family distributions latent variables minimize kullback-leibler divergence posterior. combined stochastic optimization, variational inference scale complex statistical models massive data sets]. both computational complexity accuracy variational inference controlled factorization variational family. optimization tractable, algorithms fullyfactorized family, mean-field family, latent variable assumed independent. less common, structured mean-field methods slightly relax assumption preserving original structure latent variables]. factorized distributions enable eﬃcient variational inference sacrifice accuracy. exact posterior, latent variables dependent mean-field methods, construction, fail capture dependency. end, develop copula variational inference (copula). copula augments traditional variational distribution copula, ﬂexible construction learning depen \\x0cdencies factorized distributions]. this strategy advantages traditional: reduces bias; sensitive local optima; sensitive hyperparameters; helps characterize interpret dependency latent variables. variational inference previously restricted eir generic inference simple models?where dependency make significant difference writing model-specific variational updates. copula widens applicability, providing generic inference finds meaningful dependencies latent variables. detail, contributions following. generalization original procedure variational inference. copula generalizes variational inference mean-field structured factorizations: traditional corresponds running step method. coordinate descent, monotonically decreases divergence posterior alternating fitting mean-field parameters copula parameters. figure illustrates copula toy fitting bivariate gaussian. improving generic inference. copula applied inference procedure mean-field structured approach. furr, require specific knowledge Figure Approximations elliptical gaussian. mean-field (red) restricted fitting independent one-dimensional gaussians, step algorithm. step (blue) fits copula models dependency. more iterations alternate: refits meanfield (green) fourth refits copula (cyan), demonstrating convergence true posterior. model, falls framework black box variational inference]. investigator write function evaluate model log-likelihood. rest algorithm calculations, sampling evaluating gradients, library. richer variational approximations. experiments, demonstrate copula standard Gaussian mixture models. found consistently estimates parameters, reduces sensitivity local optima, reduces sensitivity hyperparameters. examine copula captures dependencies latent space model]. copula outperforms competing methods significantly improves mean-field approximation.  Background Variational inference Let set observations, latent variables, free parameters variational distribution; ?). aim find approximation posterior variational distribution; quality approximation measured divergence. this equivalent maximizing quantity (?) ;?) [log)] ;?) [log; ?)]. (?) evidence lower bound (elbo), variational free energy]. for simpler computation, standard choice variational family mean-field approximation;  \\x0cwhere    note strong independence assumption. more sophisticated approaches, structured variational inference], attempt restore dependencies latent variables. work, restore dependencies copulas. structured typically tailored individual models diﬃcult work mamatically. copulas learn general posterior dependencies inference, require investigator structure advance. furr, copulas augment structured factorization order introduce dependencies considered before; generalizes procedure. review copulas.  Copulas augment mean-field distribution copula. variational family    )).  Figure Example vine factorizes copula density random variables product pair copulas. edges tree nodes lower level tree edge determines bivariate copula conditioned random variables connected nodes share. here marginal cumulative distribution function (cdf) random variable joint distribution, random variables distribution called copula joint uniform marginal distrimultivariate density butions]. for distribution, factorization product marginal densities copula exists integrates]. intuitively, copula captures information multivariate random variable eliminating marginal information., applying probability integral transform variable. copula captures dependencies. recall that, random variables uniform distributed. thus marginals copula give information. for example, bivariate Gaussian copula defined     )). independent uniform distributed, inverse CDF standard normal transforms independent normals. CDF bivariate Gaussian distribution, Pearson correlation squashes transformed values back unit square. thus Gaussian  copula directly correlates Pearson correlation parameter  Vine copulas diﬃcult copula. find family distributions easy compute express broad range dependencies. much work focuses two-dimensional copulas, student, clayton, gumbel, frank, Joe copulas]. however, multivariate extensions ﬂexibly model dependencies higher dimensions]. rar, successful approach recent literature combining sets conditional bivariate copulas; resulting joint called vine]. vine factorizes copula density   product conditional bivariate copulas, called pair copulas. this makes easy high-dimensional copula. one express dependence pair random variables conditioned subset ors. figure vine factorizes-dimensional copula product pair copulas. tree nodes representing random variables respectively. edge corresponds pair copula., symbolizes edges collapse nodes tree edges correspond conditional bivariate copulas symbolizes this proceeds nested tree symbolizes overload notation marginal CDF depend names argument, occasionally clarity needed. this analogous standard convention overloading probability density function(?).  vine structure specifies complete factorization multivariate copula, pair copula family set parameters formally, vine nested set trees  properties: tree nodes edges.  edges tree nodes tree  two nodes tree joined edge edges tree share node. each edge nested set trees  specifies pair copula, product edges comprise factorization copula density. since total  edges, factorizes   product  pair copulas. each edge, conditioning set), set variable indices   define cik) bivariate copula density conditioning set:    cik   ) ) Both copula cdf arguments conditional). vine specifies factorization copula, product edges levels     \\x0ccik)  highlight depends set parameters pair copulas. vine construction ﬂexibility model dependencies high dimensions decomposition pair copulas easier estimate. see, construction leads eﬃcient stochastic gradients taking individual (and easy) gradients pair copula. Copula variational inference introduce copula variational inference (copula), method performing accurate scalable variational inference. for simplicity, mean-field factorization augmented copula extend structured factorizations). copula-augmented variational family;         copula mean-field denotes mean-field parameters copula parameters. with family, maximize augmented elbo, (?, ;?,?) [log)] ;?,?) [log; ?)]. copula alternates steps: fix copula parameters solve mean-field parameters fix mean-field parameters solve copula parameters this generalizes mean-field approximation, special case initializing copula uniform stopping step. apply stochastic approximations] step gradients derived section.  set learning rate satisfy robbins-monro schedule summary outlined Algorithm this alternating set optimizations falls class minorize-maximization methods, includes procedures algorithm], alternating squares algorithm, iterative procedure generalized method moments. each step copula monotonically increases objective function refore approximates posterior distribution. Algorithm Copula variational inference (copula) input: Data Model), Variational family Initialize randomly, uniform. change elbo threshold Fix maximize set iteration counter converged Draw sample unif update   ) Increment end Fix maximize set iteration counter converged Draw sample unif update   ) Increment end end output: Variational parameters (?, ?). copula generic input requirements black-box variational inference]? user joint model, order perform inference. furr, copula variational inference easily extends case original variational family structured factorization. vine construction, simply fixes pair copulas pre-existent dependence factorization independence copula. this enables copula model dependence exist. throughout optimization, assume tree structure copula families fixed. note, however, learned. study, learn tree structure sequential tree selection] learn families, choice bivariate families, Bayesian model selection] (see supplement). preliminary studies found-selection tree structure copula families significantly change future iterations.  Stochastic gradients elbo perform stochastic optimization, require stochastic gradients elbo respect mean-field copula parameters. copula objective leads eﬃcient stochastic gradients low variance. derive gradient respect mean-field parameters. general, apply score function estimator], leads gradient ;?,?) [?? log;  (log, log; ?))]. ) follow noisy unbiased estimates gradient sampling(?) evaluating expression. apply gradient discrete latent variables. when latent variables differentiable, reparameterization trick] advantage first-order information model log). specifically, rewrite expectation terms random variable distribution) depend variational parameters latent variables deterministic function mean-field parameters; ?). following reparameterization, gradients propagate inside expectation,  log, log; ?))?? ; ?)]. ) This estimator reduces variance stochastic gradients]. furrmore, copula variational family, type reparameterization uniform random variable deterministic function; possible. (see supplement.) reparameterized gradient) requires calculation terms log;  ;  tractable derived supplement; decomposes log;  log log     log log)  ,‘} summation pair copulas argument. words, gradient latent variable evaluated marginal pair copulas model correlation latent variable similar derivation holds calculating terms score function estimator. turn gradient respect copula parameters. copulas differentiable respect parameters. this enables eﬃcient reparameterized gradient  log, log; ?))?? ; ?)]. requirements mean-field parameters. ) finally, note requirement model gradient log). this calculated automatic differentiation tools]. thus Copula implemented library applied requiring manual derivations user.  Computational complexity vine factorization copula  pair copulas, number latent variables. thus stochastic gradients meanfield parameters copula parameters require complexity. more generally, apply low rank approximation copula truncating number levels vine (see Figure). this reduces number pair copulas leads computational complexity). using sequential tree selection learning vine structure], correlated variables highest level vines. thus truncated low rank copula forgets weakest correlations. this generalizes low rank Gaussian approximations) complexity]: special case mean-field distribution product independent gaussians, pair copula Gaussian copula.  Related work Preserving structure variational inference studied Saul Jordan] case probabilistic neural networks. revisited recently case conditionally conjugate exponential familes]. our work differs line learn dependency structure inference, require explicit knowledge model. furr, augmentation strategy works broadly posterior distribution factorized variational family, generalizes approaches. similar augmentation strategy higher-order mean-field methods, Taylor series correction based difference posterior mean-field approximation]. recently, Giordano. ] covariance correction mean-field estimates. all methods assume mean-field approximation reliable Taylor series expansion make sense, true general robust black box framework. our approach alternates estimation mean-field copula, find empirically leads robust estimates estimating simultaneously, sensitive quality mean-field approximation. Lambda method CVI LRVB All off?diagonal covariances method CVI LRVB Estimated \\x0cestimated Gibbs standard deviation Gibbs standard deviation Figure Covariance estimates copula variational inference (copula), mean-field), linear response variational Bayes (lrvb) ground truth (gibbs samples). copula lrvb effectively capture dependence underestimates variance forgets covariances. Experiments study copula models: Gaussian mixtures latent space model]. Gaussian mixture classical model diﬃcult capture posterior dependencies. latent space model modern Bayesian model mean-field approximation poor estimates posterior, modeling posterior dependencies crucial uncovering patterns data. implementation details copula. iteration, form stochastic gradient generating samples variational distribution taking average gradient. set 1024 follow asynchronous updates]. set step-size ADAM].  Mixture Gaussians follow goal Giordano. ], estimate posterior covariance Gaussian mixture. hidden variables-vector mixture proportions set -dimensional multivariate normals unknown -vector) precision matrix mixture gaussians, joint probability,  (?)     Dirichlet prior(?) normal-wishart prior  apply mean-field approximation), assigns independent factors perform copula copula-augmented mean-field distribution., includes pair copulas latent \\x0cvariables. compare results linear response variational Bayes (lrvb], posthoc correction technique covariance estimation variational inference. higher-order mean-field methods demonstrate similar behavior lrvb. comparisons structured approximations omitted require explicit factorizations black box. standard black box variational inference] corresponds approximation. simulate, 000 samples components dimensional gaussians. figure displays estimates standard deviations 100 simulations, plots ground truth 500 effective Gibb samples. plot displays off-diagonal covariance estimates. estimates  pattern supplement. when initializing true mean-field parameters, copula lrvb achieve consistent estimates posterior variance. underestimates variance, well-known limitation]. note estimates initialized truth, copula converges true posterior step fitting copula. require alternating steps. Variational inference methods Predictive Likelihood Runtime mean-field lrvb copula steps) copula steps) copula (converged) -383 -330 -303 min. min. min. . min. . table Predictive likelihood latent space model. each copula step eir refits meanfield copula. copula converges roughly steps significantly outperforms mean-field lrvb fitting copula steps). copula robust lrvb. toy demonstration, analyze MNIST data set handwritten digits,665 training examples,115 test examples. perform ”unsupervised” classification., classify training labels: apply mixture Gaussians cluster, classify digit based membership assignment. copula reports test set error rate, lrvb ranges depending mean-field estimates. lrvb similar higher order mean-field methods correct existing solution sensitive local optima general quality solution. hand, copula-adjusts copula parameters fits, making robust initialization.  Latent space model study inference latent space model], Bernoulli latent factor model network analysis. each node -node network -dimensional latent variable (?,  edges pairs nodes observed high probability nodes close latent space. formally, edge pair, observed probability logit)    model parameter. generate 100, 000 node network latent node attributes dimensional gaussian. learn posterior latent attributes \\x0corder predict likelihood held-out edges. applies independent factors lrvb applies correction, copula fully dependent variational distribution. table displays likelihood held-out edges runtime. attempted Hamiltonian Monte Carlo converge hours. copula dominates methods accuracy convergence, copula estimation refitting steps) dominates lrvb runtime accuracy. note lrvb requires invert  matrix. scale method achieve faster estimates copula applied stochastic approximations inversion. however, copula outperforms lrvb fast 100,000 node network. Conclusion developed copula variational inference (copula). copula variational inference algorithm augments mean-field variational distribution copula; captures posterior dependencies latent variables. derived scalable generic algorithm performing inference expressive variational distribution. found copula significantly reduces bias mean-field approximation, estimates posterior variance, accurate forms capturing posterior dependency variational approximations. acknowledgments Luke bornn, Robin gong, Alp Kucukelbir insightful comments. this work supported NSF iis-0745520, iis-1247664, iis-1009542, ONR n00014-10651, DARPA fa8750-0009, n66001-4032, facebook, adobe, amazon, John Templeton foundation.',\n",
       " 'PP5684': 'detecting emergence abrupt change-points classic problem statistics machine learning. sequence samples,   domain interested detecting change-point  samples .   -called background distribution, change-point, samples .  , post-change distribution. time horizon eir fixed number (called oﬄine fixed-sample problem), fixed samples (called sequential online problem). goal detect existence change-point oﬄine setting, detect emergence change-point \\x0coccurs online setting. restrict attention detecting change-point, arises monitoring problems. seismic event detection], detect onset event precisely retrospect understand earthquakes quickly streaming data. ideally, detection algorithm robust distributional assumptions detect kinds seismic events background. typically large amount background data (since seismic events rare), algorithm exploit data computationally eﬃcient. classical approaches change-point detection parametric, meaning rely strong assumptions distribution. nonparametric kernel approaches distribution free robust provide consistent results larger classes data distributions possibly powerful settings clear distributional assumption made). particular, kernel based statistics proposed machine learning literature, typically work real data assumptions. however, existing kernel statistics provided computationally eﬃcient characterize tail probability extremal statistics. characterization tail probability crucial setting correct detection thresholds oﬄine online cases. furrmore, eﬃciency important consideration typically amount background data large. case, freedom restructure sample background data statistical design gain computational eﬃciency. hand, change-point detection problems related statistical two-sample test problems; however, diﬃcult change-point detection, search unknown change-point location  instance, oﬄine case, corresponds taking maximum series statistics putative change-point location similar idea] oﬄine case), online case, characterize average run length test statistic hitting threshold, necessarily results taking maximum statistics time. moreover, statistics maxed highly correlated. hence, analyzing tail probabilities test statistic change-point detection typically requires sophisticated probabilistic tools. paper, design related -statistics change-point detection based kernel maximum discrepancy (mmd) two-sample test]. MMD nice unbiased minimum variance -statistic estimator (mmdu directly applied MMDu costs compute based sample data points. change-point detection case, translates complexity quadratically grows number background observations detection time horizon refore, adopt strategy inspired recently developed-test statistic] design) statistic changepoint detection. high level, methods sample blocks background data size compute quadratic-time MMDu reference block post-change block, average results. however, \\x0csimple two-sample test case, order provide accurate change-point detection threshold, background block designed structured oﬄine setting updated recursively online setting. presenting -statistics, contributions include) deriving accurate approximations significance level oﬄine case, average run length online case, -statistics, enable determine thresholds eﬃciently recurring onerous simulations. repeated bootstrapping) obtaining closed-form variance estimator form -statistic easily) developing structured ways design background blocks oﬄine setting rules update online setting, leads desired correlation structures statistics enable accurate approximations tail probability. approximate asymptotic tail probabilities, adopt highly sophisticated technique based change-measure, recently developed series paper Yakir Siegmund. ]. numerical accuracy approximations validated numerical examples. demonstrate good performance method real speech human activity data. find that, twosample testing scenario, beneficial increase block size distribution statistic null alternative separated; however, longer case online change-point detection, larger block size inevitably larger detection delay. finally, point future directions relax Gaussian approximation correct skewness kernel-based statistics. background Related Work brieﬂy review kernel-based methods maximum discrepancy. reproducing kernel Hilbert space (rkhs) kernel, Hilbert space functions (?)  product element, satisfies reproducing property: , ), consequently,  , meaning view evaluation function point product. assume sets observations domain distribution distribution maximum discrepancy (mmd) defined] MMD0, supf)]} unbiased estimate MMD20 obtained -statistic MMD2u, drawn. drawn.     (?) kernel -statistic defined intuitively, empirical test statistic MMD2u expected small (close zero) large apart. complexity evaluating \\x0chave form-called Gram matrix data. ), -statistic degenerate distributed infinite sum Chisquare variables. improve computational eﬃciency obtain easy-tocompute threshold hyposis testing, recently] proposed alternative statistic MMD20 called-test. key idea approach partition samples non-overlapping blocks,      constant size MMD2u, computed pair blocks averaged blocks result MMD2B MMD2u, constant, ), computational complexity MMD2B), significant reduction compared MMD2u, furrmore, averaging MMD2u, independent blocks-statistic asymptotically normal leveraging central limit orem. property simple threshold derived two-sample test rar resorting expensive bootstrapping approach. statistics inspired bstatistic. however, change-point detection setting requires significant derivations obtain test threshold cares maximum MMD2B, computed point time. moreover, changepoint detection case consists sum highly correlated MMD statistics, MMD2B formed common test block data. inevitable change-point detection problems test data reference data. hence, central limit orem (even martingale version), adopt aforementioned change-ofmeasure approach. related work. nonparametric change-point detection approach proposed literature. oﬄine setting] designs kernel-based test statistic, based-called running maximum partition strategy test presence change-point] studies related problem anomalous sequences sequences detected construct test statistic mmd. online setting] presents meta-algorithm compares data ?reference window? data current window, empirical distance measures (not kernelbased] detects abrupt comparing sets descriptors extracted online signal time instant: past set future set; based soft margin single-class support vector machine (svm), build dissimilarity measure (which asymptotically equivalent Fisher ratio Gaussian case) feature space sets estimating densities intermediate step] densityratio estimation detect change-point, models density-ratio non-parametric Gaussian kernel model, parameters updated online stochastic gradient decent. work lack oretical analysis extremal behavior statistics average run length. -statistic oﬄine online change-point detection Give sequence observations         denoting sequence background reference) data. assume large amount reference data available. goal detect existence change-point change-point, samples. distribution change-point, samples. distribution location change-point occurs unknown. formulate problem hyposis test, null hyposis states change-point, alternative hyposis exists change-point time  construct kernelbased -statistic maximum discrepancy (mmd) measure difference distributions reference test data. denote block data potentially change-point (also referred post-change block test block). oﬄine setting, assume size Bmax search location change-point observations distribution. inspired idea-test], sample reference blocks size Bmax independently reference pool, index XiBmax  search location  bmax change-point, construct sub-block taking contiguous data points, denote form statistic, correspondingly construct sub-blocks reference block taking contiguous data points) block, index sub-blocks (illustrated fig. )). compute  block potential change point Pool reference data MMD2u max (bmax max Bmax Bmax Bmax Bmax Bmax Bmax Bmax MMD2u  Bmax Bmax Block potential change point ,?  Pool reference data sample time  Pool reference data sample time ): oﬄine): sequential Figure Illustration) oﬄine case: data split blocks size Bmax indexed backwards time blocks size   bmax) online case. assuming large amount reference background data null distribution. ) MMD2u) average blocks) MMD2u denotes jth sample denotes sample due property MMD2u null hyposis var denote variance null. expression) section. variance depends block size number blocks increases var decreases (also illustrated Figure appendix). this, standardize statistic, maximize values define oﬄine -statistic, detect change-point -statistic exceeds threshold max var {oﬄine change-point detection,...,bmax varying block-size Bmax corresponds searching online setting, suppose post unknown change-point location. change block size construct sliding window. case, potential change-point declared end block form statistic, samples replacement (since assume reference data.with distribution reference pool form reference blocks, compute quadratic MMD2u statistics reference block post-change block, average sample (time moves), append sample reference block, remove oldest sample post-change block, move reference pool. reference blocks updated accordingly: end point reference block moved reference pool, point sampled appended front reference block, shown fig. ). sliding window scheme above, similarly, define online -statistic forming standardized average MMD2u post-change block sliding window reference block: ZB0) MMD2u) fixed block-size, ith reference block size time) post-change block size time online case, characterize average run length test statistic hitting threshold, necessarily results taking maximum statistics time. online change-point detection procedure stopping time, detect change-point normalized ZB0 exceeds pre-determined threshold inf ZB0 var[zb0}. {online change-point detection) Note online case, maximum standardized statistics time. recursive calculate online -statistic eﬃciently, explained Section appendix. stopping time claim exists change-point. tradeoff choosing block size online setting: small block size incur smaller computational cost, important online case, enables smaller detection delay strong change4 point magnitude; however, disadvantage small lower power, \\x0cwhich corresponds longer detection delay change-point magnitude weak (for example, amplitude shift small). examples oﬄine online -statistics demonstrated fig. based syntic data segment real seismic signal. proposed oﬄine -statistic powerfully detects existence change-point accurately pinpoints change occurs; online -statistic quickly hits threshold change happens. 100 Normal) Laplace) Seismic Signal Normal) Signal Laplace 100 200 300 Time 400 500 100 200 300 Time 400 500 300 200 100): oﬄine, null 500 100 200 300 Time 400 500 Peak 400 \\x0cstatistic Statistic 500 400 300 200 100): oﬄine, 250 ?100 200 400 600 Time 100 Statistic Statistic Normal) Signal Signal 800 1000 bandw=med bandw=100med bandw.1med 100 200 300 Time 400 500): online, 250 200 400 600 Time 800 1000) seismic signal \\x0cfigure Examples oﬄine online -statistic), oﬄine case change-point (bmax 500 maximum obtained 263) online case change-point 250, stopping-time 268 (detection delay) real seismic signal -statistic kernel bandwidth. thresholds oretical values marked red. oretical Performance Analysis obtain analytical expression variance var), leveraging correspondence MMD2u statistics -statistic] (since form -statistic), exploiting properties -statistic. derive covariance structure online oﬄine standardized statistics, crucial proving orems lemma (variance null.) fixed block size number blocks null hyposis,   var, Cov(x00 x000) x00 x000. null distribution lemma suggests easy estimate variance var reference data. estimate), estimate, )], time drawing samples replacement reference data, evaluate sampled function value, form Monte Carlo average. similarly, estimate Cov(x00 x000 )]. lemma (covariance structure standardized statistics.) null hyposis, Bmax oﬄine case      Cov max}, online case cov oﬄine setting, choice threshold involves tradeoff standard performance metrics) significant level), probability -statistic exceeds threshold null hyposis., change-point) power, probability statistic exceeds threshold alternative hyposis. online setting, analogous performance metrics commonly analyzing change-point detection procedures) expected stopping time change, average run length (arl) expected detection delay (edd), defined expected stopping time extreme case change occurs immediately  focus analyzing ARL methods, play key roles setting thresholds. derive accurate approximations quantities functions threshold prescribed arl, solve analytically. denote, respectively, probability measure expectation null. orem oﬄine case.)  bmax constant significant level oﬄine -statistic defined) max max  ,...,bmax var) special function ) ) probability density function) cumulative distribution function standard normal distribution, respectively. proof orem change-measure argument, based likelihood ratio identity (see]). likelihood ratio identity relates computing tail probability null computing sum expectations alternative distribution indexed parameter value. illustrate, assume probability density function (pdf) null). function ), index set ?,, introduce family alternative distributions pdf ! ) (?) ), (?) log!  log moment generating function, parameter assign arbitrary value. easily verified ) pdf. family alternative, calculate probability event original distribution calculating sum expectations: ?  ‘! ? ? }], indicator function} event true orwise, expectation pdf ), log! ! )  log-likelyhood ratio, freedom choose   basic idea change-measure setting treat /var random field indexed characterize, study tail probability maximum random field. relate setting above, corresponds ), corresponds corresponds threshold crossing event. compute expectations alternative measures, steps. first, choose parameter pdf parameter  equivalent setting alternative probability threshold local central limit orem alternative measure boundary cross larger probability. second, express random quantities involved expectations, functions-called local field terms  = show asymptotically independent grows order furr simplifies calculation. step analyze covariance structure random field (lemma following), approximate Gaussian random field. note terms Zu0 Zv0 non-negligible correlation due construction: share post-change block) apply localization orem (orem]) obtain final result. orem (arl online case.)   constant average run length (arl) stopping time defined) (2b0(2b0  ).  -centered log-likelihood ratios: proof orem similar orem due fact \\x0cfor  max ) hence,pwe study tail probability maximum random field ZB0 ZB0 fixed block size similar changeof-measure approach used, covariance structure online case slightly oﬄine case. tail probability turns form ). similar argu6 ments], asymptotically exponentially distributed. hence, exp)]   leads). orem shows ARL  and, hence, log arl). hand, EDD typically order wald identity] (although careful analysis carried future work), kullback-leibler) divergence null alternative distributions order constant). hence, desired ARL (typically order 5000 10000), error made estimated threshold translated linearly edd. blessing means typically accurate performance loss edd. similarly, orem shows  similar argument made oﬄine case. numerical examples test performance -statistic simulation real world data. highlight main results. details found Appendix examples, Gaussian kernel, exp kernel bandwidth ?median trick? , bandwidth estimated background data. accuracy Lemma estimating var fig. appendix shows empirical distributions 200, cases, generate 10000 random instances, computed data), r20 represent null distribution. moreover, plot Gaussian pdf sample sample variance, matches empirical distribution. note approximation works block size decreases. skewness statistic corrected; discussions Section). accuracy oretical results estimating threshold. oﬄine case, compare thresholds obtained numerical simulations, bootstrapping, approximation orem values choose maximum block size Bmax. appendix, fig. ) demonstrates threshold obtained simulation, , threshold corresponds% quantile empirical distribution oﬄine statistic. range values, fig. ) compares empirical simulation predicted orem shows ory accurate small desirable care small obtain thresholds. table shows approximation works determine thresholds : thresholds obtained ory matches obtained Monte Carlo simulation null distribution), r20 bootstrapping real data scenario. here, ?bootstrap? thresholds speech signal censrec dataset. case, \\x0cnull distribution unknown, 3000 samples speech signals. generate bootstrap samples estimate threshold, shown fig. appendix.  obtained oretical approximations performance degradation, discuss improve Section table Comparison thresholds oﬄine case, determined simulation, bootstrapping ory respectively,   (sim Bmax (boot (sim Bmax (boot (sim Bmax (boot For online case, compare thresholds obtained simulation (using 5000 instances) ARL orem respectively. predicated ory, threshold consistently accurate null distributions (shown fig. ). note fig. precision improves increases. null distributions include), exponential distribution erdos-renyi random graph nodes probability forming random edges, Laplace distribution. expected detection delays (edd). online setting, compare EDD (with assumption detecting change-point signal dimensional transition gaussian) exp) Random Graph (node) laplace) ory ory gaussian) exp) Random Graph (node) laplace arl(104 arl(104 gaussian) exp) Random Graph (node) laplace) ory arl(104): 200 Figure online case, range ARL values, comparison obtained simulation orem null distributions. zero-mean Gaussian, I20 non-zero Gaussian (?, I20 postchange vector element-wise equal constant shift. setting, fig. ) demonstrates tradeoff choosing block size: block size small statistical power -statistic weak EDD large; hand, block size large, statistical power good, EDD large update test block. refore, optimal block size case. fig. ) shows optimal block size decreases shift increases, expected. real-data test performance -statistics real data. datasets include) censrec1: real-world speech dataset Speech Resource Consortium (src) corpora provided National Institute Informatics (nii) Human Activity Sensing Consortium (hasc) challenge 2011 data2 compare -statistic state--art algorithm, relative densityratio (rdr) estimate] (one limitation RDR algorithm, however, suitable high-dimensional data estimating density ratio high-dimensional setting illposed). achieve reasonable performance RDR algorithm, adjust bandwidth regularization parameter time step and, hence, RDR algorithm computationally expensive -statistics method. area Under Curve (auc] larger better) performance metric. -statistics competitive performance compared baseline RDR algorithm real data testing. report main results details found Appendix For speech data, goal detect onset speech signal emergent background noise background noises real acoustic signals, background noise highway, airport subway stations). auc statistic .8014 baseline algorithm .7578. human activity detection data, aim detection onset transitioning activity anor. data consists human activity information collected portable three-axis accelerometers. auc -statistic .8871 baseline algorithm .7161. discussions improve precision tail probability approximation orems account skewness change-measurement argument, choose parameter values  currently, Gaussian assumption , and, hence, (?)  , improve precision estimate skewness  particular, include skewness log moment generating function approximation (?)  +?  estimate change-measurement parameter: setting derivative solving quadratic equation    change leading exponent term) similar improvement ARL approximation orem acknowledgments This research supported part cmmi-1538746 ccf-1442635. Detecting emergence abrupt change-points classic problem statistics machine learning. given sequence samples,   domain interested detecting change-point  samples .   -called background distribution, change-point, samples .  , post-change distribution. here time horizon eir fixed number (called oﬄine fixed-sample problem), fixed samples (called sequential online problem). our goal detect existence change-point oﬄine setting, detect emergence change-point \\x0coccurs online setting. restrict attention detecting change-point, arises monitoring problems. one seismic event detection], detect onset event precisely retrospect understand earthquakes quickly streaming data. ideally, detection algorithm robust distributional assumptions detect kinds seismic events background. typically large amount background data (since seismic events rare), algorithm exploit data computationally eﬃcient. classical approaches change-point detection parametric, meaning rely strong assumptions distribution. nonparametric kernel approaches distribution free robust provide consistent results larger classes data distributions possibly powerful settings clear distributional assumption made). particular, kernel based statistics proposed machine learning literature, typically work real data assumptions. however, existing kernel statistics provided computationally eﬃcient characterize tail probability extremal statistics. characterization tail probability crucial setting correct detection thresholds oﬄine online cases. furrmore, eﬃciency important consideration typically amount background data large. case, freedom restructure sample background data statistical design gain computational eﬃciency. hand, change-point detection problems related statistical two-sample test problems; however, diﬃcult change-point detection, search unknown change-point location  for instance, oﬄine case, corresponds taking maximum series statistics putative change-point location similar idea] oﬄine case), online case, characterize average run length test statistic hitting threshold, necessarily results taking maximum statistics time. moreover, statistics maxed highly correlated. hence, analyzing tail probabilities test statistic change-point detection typically requires sophisticated probabilistic tools. paper, design related -statistics change-point detection based kernel maximum discrepancy (mmd) two-sample test]. although MMD nice unbiased minimum variance -statistic estimator (mmdu directly applied MMDu costs compute based sample data points. change-point detection case, translates complexity quadratically grows number background observations detection time horizon refore, adopt strategy inspired recently developed-test statistic] design) statistic changepoint detection. high level, methods sample blocks background data size compute quadratic-time MMDu reference block post-change block, average results. however, \\x0csimple two-sample test case, order provide accurate change-point detection threshold, background block designed structured oﬄine setting updated recursively online setting. besides presenting -statistics, contributions include) deriving accurate approximations significance level oﬄine case, average run length online case, -statistics, enable determine thresholds eﬃciently recurring onerous simulations. repeated bootstrapping) obtaining closed-form variance estimator form -statistic easily) developing structured ways design background blocks oﬄine setting rules update online setting, leads desired correlation structures statistics enable accurate approximations tail probability. approximate asymptotic tail probabilities, adopt highly sophisticated technique based change-measure, recently developed series paper Yakir Siegmund. ]. numerical accuracy approximations validated numerical examples. demonstrate good performance method real speech human activity data. find that, twosample testing scenario, beneficial increase block size distribution statistic null alternative separated; however, longer case online change-point detection, larger block size inevitably larger detection delay. finally, point future directions relax Gaussian approximation correct skewness kernel-based statistics. Background Related Work brieﬂy review kernel-based methods maximum discrepancy. reproducing kernel Hilbert space (rkhs) kernel, Hilbert space functions (?)  product its element, satisfies reproducing property: , ), consequently,  , meaning view evaluation function point product. assume sets observations domain distribution distribution maximum discrepancy (mmd) defined] MMD0, supf)]} unbiased estimate MMD20 obtained -statistic MMD2u, drawn. drawn.     (?) kernel -statistic defined intuitively, empirical test statistic MMD2u expected small (close zero) large apart. complexity evaluating \\x0chave form-called Gram matrix data. under), -statistic degenerate distributed infinite sum Chisquare variables. improve computational eﬃciency obtain easy-tocompute threshold hyposis testing, recently] proposed alternative statistic MMD20 called-test. key idea approach partition samples non-overlapping blocks,      constant size MMD2u, computed pair blocks averaged blocks result MMD2B MMD2u, since constant, ), computational complexity MMD2B), significant reduction compared MMD2u, furrmore, averaging MMD2u, independent blocks-statistic asymptotically normal leveraging central limit orem. this property simple threshold derived two-sample test rar resorting expensive bootstrapping approach. our statistics inspired bstatistic. however, change-point detection setting requires significant derivations obtain test threshold cares maximum MMD2B, computed point time. moreover, changepoint detection case consists sum highly correlated MMD statistics, MMD2B formed common test block data. this inevitable change-point detection problems test data reference data. hence, central limit orem (even martingale version), adopt aforementioned change-ofmeasure approach. related work. nonparametric change-point detection approach proposed literature. oﬄine setting] designs kernel-based test statistic, based-called running maximum partition strategy test presence change-point] studies related problem anomalous sequences sequences detected construct test statistic mmd. online setting] presents meta-algorithm compares data ?reference window? data current window, empirical distance measures (not kernelbased] detects abrupt comparing sets descriptors extracted online signal time instant: past set future set; based soft margin single-class support vector machine (svm), build dissimilarity measure (which asymptotically equivalent Fisher ratio Gaussian case) feature space sets estimating densities intermediate step] densityratio estimation detect change-point, models density-ratio non-parametric Gaussian kernel model, parameters updated online stochastic gradient decent. work lack oretical analysis extremal behavior statistics average run length. -statistic oﬄine online change-point detection Give sequence observations         denoting sequence background reference) data. assume large amount reference data available. our goal detect existence change-point change-point, samples. distribution change-point, samples. distribution location change-point occurs unknown. formulate problem hyposis test, null hyposis states change-point, alternative hyposis exists change-point time  construct kernelbased -statistic maximum discrepancy (mmd) measure difference distributions reference test data. denote block data potentially change-point (also referred post-change block test block). oﬄine setting, assume size Bmax search location change-point observations distribution. inspired idea-test], sample reference blocks size Bmax independently reference pool, index XiBmax  since search location  bmax change-point, construct sub-block taking contiguous data points, denote form statistic, correspondingly construct sub-blocks reference block taking contiguous data points) block, index sub-blocks (illustrated fig. )). compute  block potential change point Pool reference data MMD2u max (bmax max Bmax Bmax Bmax Bmax Bmax Bmax Bmax MMD2u  Bmax Bmax Block potential change point ,?  Pool reference data sample time  Pool reference data sample time ): oﬄine): sequential Figure Illustration) oﬄine case: data split blocks size Bmax indexed backwards time blocks size   Bmax) online case. assuming large amount reference background data null distribution. ) MMD2u) average blocks) MMD2u denotes jth sample denotes sample due property MMD2u null hyposis let var denote variance null. expression) section. variance depends block size number blocks increases var decreases (also illustrated Figure appendix). considering this, standardize statistic, maximize values define oﬄine -statistic, detect change-point -statistic exceeds threshold max var {oﬄine change-point detection,...,bmax varying block-size Bmax corresponds searching online setting, suppose post unknown change-point location. change block size construct sliding window. case, potential change-point declared end block form statistic, samples replacement (since assume reference data.with distribution reference pool form reference blocks, compute quadratic MMD2u statistics reference block post-change block, average when sample (time moves), append sample reference block, remove oldest sample post-change block, move reference pool. reference blocks updated accordingly: end point reference block moved reference pool, point sampled appended front reference block, shown fig. ). using sliding window scheme above, similarly, define online -statistic forming standardized average MMD2u post-change block sliding window reference block: ZB0) MMD2u) fixed block-size, ith reference block size time) post-change block size time online case, characterize average run length test statistic hitting threshold, necessarily results taking maximum statistics time. online change-point detection procedure stopping time, detect change-point normalized ZB0 exceeds pre-determined threshold inf ZB0 var[zb0}. {online change-point detection) Note online case, maximum standardized statistics time. recursive calculate online -statistic eﬃciently, explained Section appendix. stopping time claim exists change-point. tradeoff choosing block size online setting: small block size incur smaller computational cost, important online case, enables smaller detection delay strong change4 point magnitude; however, disadvantage small lower power, \\x0cwhich corresponds longer detection delay change-point magnitude weak (for example, amplitude shift small). examples oﬄine online -statistics demonstrated fig. based syntic data segment real seismic signal. proposed oﬄine -statistic powerfully detects existence change-point accurately pinpoints change occurs; online -statistic quickly hits threshold change happens. 100 Normal) Laplace) Seismic Signal Normal) Signal Laplace 100 200 300 Time 400 500 100 200 300 Time 400 500 300 200 100): oﬄine, null 500 100 200 300 Time 400 500 Peak 400 \\x0cstatistic Statistic 500 400 300 200 100): oﬄine, 250 ?100 200 400 600 Time 100 Statistic Statistic Normal) Signal Signal 800 1000 bandw=med bandw=100med bandw.1med 100 200 300 Time 400 500): online, 250 200 400 600 Time 800 1000) seismic signal \\x0cfigure Examples oﬄine online -statistic), oﬄine case change-point (bmax 500 maximum obtained 263) online case change-point 250, stopping-time 268 (detection delay) real seismic signal -statistic kernel bandwidth. all thresholds oretical values marked red. oretical Performance Analysis obtain analytical expression variance var), leveraging correspondence MMD2u statistics -statistic] (since form -statistic), exploiting properties -statistic. derive covariance structure online oﬄine standardized statistics, crucial proving orems lemma (variance null.) given fixed block size number blocks null hyposis,   var, Cov(x00 x000) x00 x000. null distribution lemma suggests easy estimate variance var reference data. estimate), estimate, )], time drawing samples replacement reference data, evaluate sampled function value, form Monte Carlo average. similarly, estimate Cov(x00 x000 )]. lemma (covariance structure standardized statistics.) under null hyposis, Bmax oﬄine case      Cov max}, online case cov oﬄine setting, choice threshold involves tradeoff standard performance metrics) significant level), probability -statistic exceeds threshold null hyposis., change-point) power, probability statistic exceeds threshold alternative hyposis. online setting, analogous performance metrics commonly analyzing change-point detection procedures) expected stopping time change, average run length (arl) expected detection delay (edd), defined expected stopping time extreme case change occurs immediately  focus analyzing ARL methods, play key roles setting thresholds. derive accurate approximations quantities functions threshold prescribed arl, solve analytically. let denote, respectively, probability measure expectation null. orem oﬄine case.) when Bmax constant significant level oﬄine -statistic defined) max max  ,...,bmax var) special function ) ) probability density function) cumulative distribution function standard normal distribution, respectively. proof orem change-measure argument, based likelihood ratio identity (see]). likelihood ratio identity relates computing tail probability null computing sum expectations alternative distribution indexed parameter value. illustrate, assume probability density function (pdf) null). given function ), index set ?,, introduce family alternative distributions pdf ! ) (?) ), (?) log!  log moment generating function, parameter assign arbitrary value. easily verified ) pdf. using family alternative, calculate probability event original distribution calculating sum expectations: ?  ‘! ? ? }], indicator function} event true orwise, expectation pdf ), log! ! )  log-likelyhood ratio, freedom choose   basic idea change-measure setting treat /var random field indexed characterize, study tail probability maximum random field. relate setting above, corresponds ), corresponds corresponds threshold crossing event. compute expectations alternative measures, steps. first, choose parameter pdf parameter  this equivalent setting alternative probability threshold local central limit orem alternative measure boundary cross larger probability. second, express random quantities involved expectations, functions-called local field terms  = show asymptotically independent grows order furr simplifies calculation. step analyze covariance structure random field (lemma following), approximate Gaussian random field. note terms Zu0 Zv0 non-negligible correlation due construction: share post-change block) apply localization orem (orem]) obtain final result. orem (arl online case.) when  constant average run length (arl) stopping time defined) (2b0(2b0  ).  -centered log-likelihood ratios: proof orem similar orem due fact \\x0cfor  max ) hence,pwe study tail probability maximum random field ZB0 ZB0 fixed block size similar changeof-measure approach used, covariance structure online case slightly oﬄine case. this tail probability turns form ). using similar argu6 ments], asymptotically exponentially distributed. hence, exp)]  consequently leads). orem shows ARL  and, hence, log arl). hand, EDD typically order wald identity] (although careful analysis carried future work), kullback-leibler) divergence null alternative distributions order constant). hence, desired ARL (typically order 5000 10000), error made estimated threshold translated linearly edd. this blessing means typically accurate performance loss edd. similarly, orem shows  similar argument made oﬄine case. Numerical examples test performance -statistic simulation real world data. here highlight main results. more details found Appendix examples, Gaussian kernel, exp kernel bandwidth ?median trick? , bandwidth estimated background data. accuracy Lemma estimating var fig. appendix shows empirical distributions 200, cases, generate 10000 random instances, computed data), r20 represent null distribution. moreover, plot Gaussian pdf sample sample variance, matches empirical distribution. note approximation works block size decreases. skewness statistic corrected; discussions Section). accuracy oretical results estimating threshold. for oﬄine case, compare thresholds obtained numerical simulations, bootstrapping, approximation orem values choose maximum block size Bmax. appendix, fig. ) demonstrates threshold obtained simulation, , threshold corresponds% quantile empirical distribution oﬄine statistic. for range values, fig. ) compares empirical simulation predicted orem shows ory accurate small desirable care small obtain thresholds. table shows approximation works determine thresholds : thresholds obtained ory matches obtained Monte Carlo simulation null distribution), r20 bootstrapping real data scenario. here, ?bootstrap? thresholds speech signal censrec dataset. case, \\x0cnull distribution unknown, 3000 samples speech signals. thus generate bootstrap samples estimate threshold, shown fig. appendix.  obtained oretical approximations performance degradation, discuss improve Section table Comparison thresholds oﬄine case, determined simulation, bootstrapping ory respectively,   (sim Bmax (boot (sim Bmax (boot (sim Bmax (boot For online case, compare thresholds obtained simulation (using 5000 instances) ARL orem respectively. predicated ory, threshold consistently accurate null distributions (shown fig. ). also note fig. precision improves increases. null distributions include), exponential distribution erdos-renyi random graph nodes probability forming random edges, Laplace distribution. expected detection delays (edd). online setting, compare EDD (with assumption detecting change-point signal dimensional transition gaussian) exp) Random Graph (node) laplace) ory ory gaussian) exp) Random Graph (node) laplace arl(104 arl(104 gaussian) exp) Random Graph (node) laplace) ory arl(104): 200 Figure online case, range ARL values, comparison obtained simulation orem null distributions. zero-mean Gaussian, I20 non-zero Gaussian (?, I20 postchange vector element-wise equal constant shift. setting, fig. ) demonstrates tradeoff choosing block size: block size small statistical power -statistic weak EDD large; hand, block size large, statistical power good, EDD large update test block. refore, optimal block size case. fig. ) shows optimal block size decreases shift increases, expected. real-data test performance -statistics real data. our datasets include) censrec1: real-world speech dataset Speech Resource Consortium (src) corpora provided National Institute Informatics (nii) Human Activity Sensing Consortium (hasc) challenge 2011 data2 compare -statistic state--art algorithm, relative densityratio (rdr) estimate] (one limitation RDR algorithm, however, suitable high-dimensional data estimating density ratio high-dimensional setting illposed). achieve reasonable performance RDR algorithm, adjust bandwidth regularization parameter time step and, hence, RDR algorithm computationally expensive -statistics method. Area Under Curve (auc] larger better) performance metric. our -statistics competitive performance compared baseline RDR algorithm real data testing. here report main results details found Appendix For speech data, goal detect onset speech signal emergent background noise background noises real acoustic signals, background noise highway, airport subway stations). AUC statistic .8014 baseline algorithm .7578. for human activity detection data, aim detection onset transitioning activity anor. each data consists human activity information collected portable three-axis accelerometers. AUC -statistic .8871 baseline algorithm .7161. Discussions improve precision tail probability approximation orems account skewness change-measurement argument, choose parameter values  currently, Gaussian assumption , and, hence, (?)  , improve precision estimate skewness  particular, include skewness log moment generating function approximation (?)  +?  estimate change-measurement parameter: setting derivative solving quadratic equation    this change leading exponent term) similar improvement ARL approximation orem acknowledgments This research supported part cmmi-1538746 ccf-1442635.',\n",
       " 'PP5704': 'completion low rank matrices entries task practical applications. aspects problem: detectability. ability estimate rank reliably fewest random entries, performance achieving small reconstruction error. propose spectral algorithm tasks called MaCBetH (for Matrix Completion hessian). rank estimated number negative eigenvalues Hessian matrix, eigenvectors initial condition minimization discrepancy estimated matrix revealed entries. analyze performance random matrix setting results statistical mechanics \\x0chopfield neural network, show that?macbeth eﬃciently detects rank large matrix entries) constant close evaluate root-mean-square error empirically show MaCBetH compares favorably existing approaches. matrix completion task inferring missing entries matrix subset entries. typically, matrix completed approximately) low rank This problem witnessed burst activity. ], motivated applications collaborative filtering], quantum tomography] physics, analysis covariance matrix]. commonly studied model matrix completion assumes matrix low rank, entries chosen uniformly random observed noise. widely considered question setting entries revealed matrix completed computationally eﬃcient]. present paper assumes model, main questions investigate different. question address detectability: random entries reveal order estimate rank reliably. motivated generic problem detecting structure case, low rank) hidden partially observed data. reasonable expect existence region exact completion hard impossible rank estimation tractable. question address minimum achievable root-mean-square error (rmse) estimating unknown elements matrix. practice, exact reconstruction possible, procedure small RMSE suﬃcient. paper propose algorithm called MaCBetH empirical performance tasks rank small. rank algorithm estimated number negative eigenvalues Hessian matrix], eigenvectors initial condition local optimization cost function commonly considered matrix completion (see. ]). particular, random matrix setting, show macbeth detects rank large matrix entries) small constant, fig. )   rmse evaluated empirically and, regime close, compares favorably existing approache OptSpace]. paper organized follows. define problem present generally approach context existing work sec.  sec. describe algorithm motivate construction spectral relaxation Hopfield model neural network. next, sec. show performance proposed spectral method analyzed using, parts, results spin glass ory phase transitions, rigorous results spectral density large random matrices. finally, sec. present numerical simulations demonstrate eﬃciency macbeth. implementations algorithms Julia Matlab programming languages SPHINX webpage http://www.lps.ens/krzakala/wasp.html. problem definition relation work Let Mtrue rank matrix Mtrue)   (unknown) tall matrices. observe small fraction elements Mtrue chosen uniformly random. call subset observed entries, (sparse) matrix supported nonzero elements revealed entries Mtrue aim reconstruct rank matrix Mtrue important parameter controls diﬃculty problem. case square matrix average number revealed entries line column. numerical examples oretical justifications generate low rank matrix Mtrue tall matrices iid Gaussian elements, call random matrix setting. macbeth algorithm, however, non-parametric prior knowledge analysis perform applies limit   ). matrix completion problem popularized] proposed nuclear norm minimization convex relaxation problem. algorithmic complexity semidefinite programming, however low complexity procedure solve problem proposed] based singular decomposition (svd). considerable step oretical understanding matrix completion entries made] proved trimming performance svd-based matrix completion improved RMSE proportional— achieved. algorithm] referred optspace, empirically achieves state--art RMSE regime revealed entries. optspace proceeds steps]. first, trims observed matrix setting rows (resp. columns) revealed entries average number revealed entries row (resp. column). second, singular decompositions performed matrix components kept. rank unknown estimated index ratio consecutive singular values minimum. third, local minimization discrepancy observed entries estimate performed. initial condition minimization left singular vectors step. work improve OptSpace replacing steps spectral procedure detects rank initial condition discrepancy minimization. method leverages recent progress made task detecting communities stochastic block model, spectral methods. community detection matrix completion, traditional spectral methods fail sparse regime due existence spurious large eigenvalues singular values) localized eigenvectors]. authors, showed non-backtracking matrix closely related Hessian basis spectral method community detection reliable rank estimation inference performance. present paper analogous improvement matrix completion problem. particular, analyze algorithm tools spin glass ory statistical mechanics, show exists phase transition phase detect rank, phase unable.  Algorithm motivation MaCBetH algorithm standard approach completion problem (see. ]) minimize cost function min [mij    function non-convex, global optimization hard. refore resorts local optimization technique careful choice initial conditions method, matrix weighted bipartite undirected graph adjacency matrix )? ) ) refer graph defined define Hessian matrix(?)  )? ) matrix elements hij (?) sinh ?aik sinh?aij)  parameter fix well-defined depending data, stands neighbors graph Expression) corresponds matrix introduced], applied case graphical model). macbeth algorithm main subject paper matrix assume centered: Algorithm (macbeth) numerically solve   (?)  tanh2 (?mij  build Hessian(? . ).  compute negative eigenvalues   ? eigenvectors   ?.  estimate rank Set (resp. lines (resp. lines) matrix   ?].  perform local optimization cost function) rank initial condition step  approximation optimal(?) maximum number negative eigenvalues (see section). approximation, chosen maximize number negative eigenvalues. observed numerically algorithm robust imprecision  step non-backtracking matrix weighted tanh ?mij shown] spectrum Hessian non-backtracking matrix closely related. section, motivate analyze algorithm setting Mtrue generated element-wise random show case MaCBetH infer rank fig. illustrates spectral properties Hessian justify algorithm: spectrum composed informative negative eigenvalues, separated bulk (which remains positive). particular, observed], avoids spurious eigenvalues localized eigenvectors make \\x0ctrimming case]. algorithm computationally eﬃcient based eigenvalue decomposition sparse, symmetric matrix.  Motivation Hopfield model motivate construction MaCBetH algorithm graphical model perspective spectral relaxation. observed matrix previous section, graphical model  }) exp mij  binary variables, parameter controlling strength interactions. model (generalized) Hebbian Hopfield model bipartite sparse graph, refore modes symmetries) correlated lines]. study, standard approximation widely believed exact problems large random graphs]. approximation means moments variable approximated parameters minimize-called free energy FBe reads FBe mij     ) ) degrees nodes graph Neural network models. ) extensively studied decades (see. ] references rein) phenomenology, review brieﬂy here, known. particular, small enough, global minimum free energy corresponds-called paramagnetic state tanh (?mij ) increase model enters retrieval phase, free energy local minima correlated factors local minima, called retrieval states ({bli {clj indexed   that, large limit,    bli clj  retrieval states refore convenient initial conditions local optimization. ), expect number correct rank. increasing critical system eventually enters spin glass phase, marked appearance spurious minima. tempting continue approach leading belief propagation, simpler spectral relaxation problem, strategy, graph clustering. first, fact paramagnetic state) stationary point free energy, ]. order detect retrieval states, study stability negative eigenvalues Hessian free energy evaluated paramagnetic state). point, elements Hessian involving derivative respect vanish, block involving derivatives diagonal positive definite matrix]. remaining part matrix called Hessian] (which considers graphical model)). eigenvectors negative eigenvalues expected give approximation retrieval states). picture exposed section summarized Figure motivates MaCBetH algorithm. note similar approach] detect retrieval states Hopfield model weighted non-backtracking matrix], linearizes belief propagation equations rar free energy, resulting larger, non-symmetric matrix. hessian, mamatically closely related, simpler handle practice. analysis performance detection show performance MaCBetH analyzed, spectral properties matrix characterized tools statistical mechanics rigorous arguments.   .12824   direct diag ?(?) ?(?)  Direct diag  Direct diag \\x0c?(?) ?(?)   Direct diag  Figure Spectral density Hessian values parameter red dots result direct diagonalisation Hessian rank 104 matrix, revealed entries row average. black curves solutions) computed belief propagation graph size 105 isolated smallest eigenvalues, represented small bars convenience, inset zoom smallest eigenvalues.  small (top plots), Hessian positive definite, signaling paramagnetic state) local minimum free energy.  increases, spectrum shifted negative region negative eigenvalues approximate .12824 compared.0832 case) evaluated algorithm (lower left plot). eigenvalues, retrieval states), positive eventually merge bulk furr increased (lower plot), bulk uninformative eigenvalues remains values positive region.  Analysis phase transition start investigating phase transition spectral method detect correct rank. (xlp (ypl random vectors empirical distribution lines respectively. statistical mechanics correspondence negative eigenvalues Hessian appearance phase transitions model), compute values instabilities towards, respectively, retrieval states spurious glassy states, arise. repeated computations] case model), cavity method]. refer reader interested technical details statistical mechanics approach neural networks]. standard computation locating phase transitions approximation (see. ]), stability paramagnetic state) phases monitored terms parameters) ?(?) lim tanh2 xlp ypl tanh2 xlp ypl??  ?(?) lim tanh ?—x1p yp1 xlp ypl tanh ?—x1p yp1 xlp ypl??  expectation distribution vectors parameter ?(?) controls sensitivity paramagnetic solution random noise, ?(?) measures sensitivity perturbation direction retrieval state.  defined implicitly  . perturbation diverges. existence retrieval phase equivalent condition exists range values retrieval states exist, spurious ones. condition met, setting  algorithm, ensure presence meaningful negative eigenvalues hessian. define critical general, closed-form formula critical value, defined implicitly terms functions  computed numerically population dynamics algorithm] results ) presented Figure remarkably, definition, critical depend ratio, rank  .812) limit large obtain simple closed-form formula. case observed entries matrix jointly Gaussian distributed, uncorrelated, refore independent. expression) simplifies ?(?) ?? tanh2   Note MaCBetH algorithm empirical estimator (?) ?(?) ) quantity compute approximation  purely revealed entries. large regime decay furr approximate Figure Location critical function rank MaCBetH estimate correct rank entries. population dynamics algorithm ?? ) population size compute funcp tions  ). dotted line fit ?? ) suggesting)  reach simple asymptotic expression, large limit, equivalently) interestingly, result obtained detectability threshold completion rank) matrices entries Bayes optimal setting].  notice, however, exact completion setting: detection exact completion phenomena. previous analysis extended random setting assumption, long empirical distribution entries defined, lines (resp. approximately orthogonal centered. condition related standard incoherence property].  Computation spectral density section, show spectral density Hessian computed analytically tree-like graphs generated picking uniformly random observed entries matrix furr motivates algorithm choice   independently section spectral density defined ?(?)  ?(?  ??  lim) eigenvalues hessian. cavity method, shown] spectral density potential delta peaks removed) ?(?)  (?) ??   lim) complex variables living vertices graph: sinh2?ail)  sinh2 ?aik   set neighbors  (linearly stable) solution belief propagation recursion  sinh2 ?aik sinh2?ail )   Rank Rank Mean inferred rank 500 2000 8000 16000 Transition  figure Mean inferred rank function sizes, averaged 100 samples matrices. entries drawn Gaussian distribution variance oretical transition computed population dynamics algorithm (see section). finite size effects considerable consistent asymptotic prediction. formula derived turning computation spectral density marginalization problem graphical model graph solving loopy belief propagation. remarkably, approach leads asymptotically exact (and rigorous]) description spectral density erd?nyi random graphs. solving equation) numerically obtain results shown fig. bulk spectrum, particular, positive. demonstrate  exists open set spectral density vanishes. justifies independently choice parameter proof] begins noticing cosh (?aij fixed point recursion)  fixed point real, spectral density small perturbation solution cosh (?aij cosh (?aij linearized version) writes  tanh2 (?ail linear operator defined weighted version \\x0cnon-backtracking matrix]. spectral radius ?(?  defined. particular,  straightforward application] implicit function orem show exists neighborhood  exists real, linearly stable fixed point), yielding spectral density equal    informative eigenvalues (those bulk), refore negative ones, motivates independently algorithm. numerical tests Figure illustrates ability Hessian infer rank critical limit large size (see section). figure demonstrate suitability eigenvectors Hessian starting point minimization cost function). compare final RMSE achieved reconstructed matrix initializations optimization, including largest singular vectors trimmed matrix]. macbeth systematically outperforms choices initial conditions, providing initial condition optimization). remarkably, performance achieved MaCBetH inferred rank essentially achieved oracle rank. contrast, estimating correct rank (trimmed) SVD challenging. note choice parameters consider, trimming negligible effect. lines, OptSpace] minimization procedure, tests difference performance due that. Alternating Least Squares] optimization method, obtained similar improvement reconstruction eigenvectors hessian, singular vectors initial condition. (rmse Rank Macbeth-svd Random Macbeth-svd(rmse Rank  figure RMSE function number revealed entries row comparison initializations optimization cost function). top row shows probability achieved RMSE smaller bottom row shows probability final RMSE smaller probabilities estimated frequency success 100 samples matrices size 10000 10000, \\x0centries drawn Gaussian distribution variance methods optimize cost function) low storage BFGS algorithm] part NLopt], starting initial conditions. maximum number iterations set 1000. initial conditions compared MaCBetH oracle rank (macbeth) inferred rank (macbeth), SVD observed matrix trimming, oracle rank-svd), inferred rank-svd, note equivalent OptSpace] regime), random initial conditions oracle rank (random). -svd method, inferred rank SVD index ratio consecutive eigenvalues minimized, suggested]. conclusion paper, presented macbeth, algorithm matrix completion eﬃcient distinct, complementary, tasks) ability estimate finite rank reliably fewer random entries existing approaches) lower root-mean-square reconstruction errors competitors. algorithm built Hessian matrix leverages recent progresses construction eﬃcient spectral methods clustering sparse networks], OptSpace approach] matrix completion. method presented offers number future directions, including replacing minimization cost function message-passing type algorithm, neural network models, oretical direction involving computation information oretically optimal transitions detectability. acknowledgment Our research received funding European search Council European union 7th Framework Programme/20072013/erc Grant Agreement 307087-sparcs). completion low rank matrices entries task practical applications. aspects problem: detectability. ability estimate rank reliably fewest random entries, performance achieving small reconstruction error. propose spectral algorithm tasks called MaCBetH (for Matrix Completion hessian). rank estimated number negative eigenvalues Hessian matrix, eigenvectors initial condition minimization discrepancy estimated matrix revealed entries. analyze performance random matrix setting results statistical mechanics \\x0chopfield neural network, show that?macbeth eﬃciently detects rank large matrix entries) constant close evaluate root-mean-square error empirically show MaCBetH compares favorably existing approaches. matrix completion task inferring missing entries matrix subset entries. typically, matrix completed approximately) low rank This problem witnessed burst activity. ], motivated applications collaborative filtering], quantum tomography] physics, analysis covariance matrix]. commonly studied model matrix completion assumes matrix low rank, entries chosen uniformly random observed noise. widely considered question setting entries revealed matrix completed computationally eﬃcient]. while present paper assumes model, main questions investigate different. question address detectability: random entries reveal order estimate rank reliably. this motivated generic problem detecting structure case, low rank) hidden partially observed data. reasonable expect existence region exact completion hard impossible rank estimation tractable. question address minimum achievable root-mean-square error (rmse) estimating unknown elements matrix. practice, exact reconstruction possible, procedure small RMSE suﬃcient. paper propose algorithm called MaCBetH empirical performance tasks rank small. rank algorithm estimated number negative eigenvalues Hessian matrix], eigenvectors initial condition local optimization cost function commonly considered matrix completion (see. ]). particular, random matrix setting, show MaCBetH detects rank large matrix entries) small constant, fig. )   RMSE evaluated empirically and, regime close, compares favorably existing approache OptSpace]. this paper organized follows. define problem present generally approach context existing work sec.  sec. describe algorithm motivate construction spectral relaxation Hopfield model neural network. next, sec. show performance proposed spectral method analyzed using, parts, results spin glass ory phase transitions, rigorous results spectral density large random matrices. finally, sec. present numerical simulations demonstrate eﬃciency macbeth. implementations algorithms Julia Matlab programming languages SPHINX webpage http://www.lps.ens/krzakala/wasp.html. problem definition relation work Let Mtrue rank matrix Mtrue)   (unknown) tall matrices. observe small fraction elements Mtrue chosen uniformly random. call subset observed entries, (sparse) matrix supported nonzero elements revealed entries Mtrue aim reconstruct rank matrix Mtrue important parameter controls diﬃculty problem. case square matrix average number revealed entries line column. numerical examples oretical justifications generate low rank matrix Mtrue tall matrices iid Gaussian elements, call random matrix setting. MaCBetH algorithm, however, non-parametric prior knowledge analysis perform applies limit   ). matrix completion problem popularized] proposed nuclear norm minimization convex relaxation problem. algorithmic complexity semidefinite programming, however low complexity procedure solve problem proposed] based singular decomposition (svd). considerable step oretical understanding matrix completion entries made] proved trimming performance svd-based matrix completion improved RMSE proportional— achieved. algorithm] referred optspace, empirically achieves state--art RMSE regime revealed entries. optspace proceeds steps]. first, trims observed matrix setting rows (resp. columns) revealed entries average number revealed entries row (resp. column). second, singular decompositions performed matrix components kept. when rank unknown estimated index ratio consecutive singular values minimum. third, local minimization discrepancy observed entries estimate performed. initial condition minimization left singular vectors step. work improve OptSpace replacing steps spectral procedure detects rank initial condition discrepancy minimization. our method leverages recent progress made task detecting communities stochastic block model, spectral methods. both community detection matrix completion, traditional spectral methods fail sparse regime due existence spurious large eigenvalues singular values) localized eigenvectors]. authors, showed non-backtracking matrix closely related Hessian basis spectral method community detection reliable rank estimation inference performance. present paper analogous improvement matrix completion problem.  particular, analyze algorithm tools spin glass ory statistical mechanics, show exists phase transition phase detect rank, phase unable.  Algorithm motivation MaCBetH algorithm standard approach completion problem (see. ]) minimize cost function min [mij    this function non-convex, global optimization hard. one refore resorts local optimization technique careful choice initial conditions method, matrix weighted bipartite undirected graph adjacency matrix )? ) ) refer graph defined define Hessian matrix(?)  )? ) matrix elements Hij (?) sinh ?aik sinh?aij)  parameter fix well-defined depending data, stands neighbors graph Expression) corresponds matrix introduced], applied case graphical model). MaCBetH algorithm main subject paper matrix assume centered: Algorithm (macbeth) numerically solve   (?)  tanh2 (?mij  build Hessian(? . ).  compute negative eigenvalues   ? eigenvectors   ?.  estimate rank Set (resp. lines (resp. lines) matrix   ?].  perform local optimization cost function) rank initial condition step  approximation optimal(?) maximum number negative eigenvalues (see section). instead approximation, chosen maximize number negative eigenvalues. observed numerically algorithm robust imprecision  step non-backtracking matrix weighted tanh ?mij shown] spectrum Hessian non-backtracking matrix closely related. section, motivate analyze algorithm setting Mtrue generated element-wise random show case MaCBetH infer rank fig. illustrates spectral properties Hessian justify algorithm: spectrum composed informative negative eigenvalues, separated bulk (which remains positive). particular, observed], avoids spurious eigenvalues localized eigenvectors make \\x0ctrimming case]. this algorithm computationally eﬃcient based eigenvalue decomposition sparse, symmetric matrix.  Motivation Hopfield model motivate construction MaCBetH algorithm graphical model perspective spectral relaxation. given observed matrix previous section, graphical model  }) exp mij  binary variables, parameter controlling strength interactions. this model (generalized) Hebbian Hopfield model bipartite sparse graph, refore modes symmetries) correlated lines]. study, standard approximation widely believed exact problems large random graphs]. approximation means moments variable approximated parameters minimize-called free energy FBe reads FBe mij     ) ) degrees nodes graph Neural network models. ) extensively studied decades (see. ] references rein) phenomenology, review brieﬂy here, known. particular, small enough, global minimum free energy corresponds-called paramagnetic state tanh (?mij ) increase model enters retrieval phase, free energy local minima correlated factors local minima, called retrieval states ({bli {clj indexed   that, large limit,    bli clj  retrieval states refore convenient initial conditions local optimization. ), expect number correct rank. increasing critical system eventually enters spin glass phase, marked appearance spurious minima. tempting continue approach leading belief propagation, simpler spectral relaxation problem, strategy, graph clustering. first, fact paramagnetic state) stationary point free energy, ]. order detect retrieval states, study stability negative eigenvalues Hessian free energy evaluated paramagnetic state). point, elements Hessian involving derivative respect vanish, block involving derivatives diagonal positive definite matrix]. remaining part matrix called Hessian] (which considers graphical model)). eigenvectors negative eigenvalues expected give approximation retrieval states). picture exposed section summarized Figure motivates MaCBetH algorithm. note similar approach] detect retrieval states Hopfield model weighted non-backtracking matrix], linearizes belief propagation equations rar free energy, resulting larger, non-symmetric matrix. hessian, mamatically closely related, simpler handle practice. Analysis performance detection show performance MaCBetH analyzed, spectral properties matrix characterized tools statistical mechanics rigorous arguments.   .12824   direct diag ?(?) ?(?)  Direct diag  Direct diag \\x0c?(?) ?(?)   Direct diag  Figure Spectral density Hessian values parameter red dots result direct diagonalisation Hessian rank 104 matrix, revealed entries row average. black curves solutions) computed belief propagation graph size 105 isolated smallest eigenvalues, represented small bars convenience, inset zoom smallest eigenvalues. for small (top plots), Hessian positive definite, signaling paramagnetic state) local minimum free energy.  increases, spectrum shifted negative region negative eigenvalues approximate .12824 compared.0832 case) evaluated algorithm (lower left plot). eigenvalues, retrieval states), positive eventually merge bulk furr increased (lower plot), bulk uninformative eigenvalues remains values positive region.  Analysis phase transition start investigating phase transition spectral method detect correct rank. let (xlp (ypl random vectors empirical distribution lines respectively. using statistical mechanics correspondence negative eigenvalues Hessian appearance phase transitions model), compute values instabilities towards, respectively, retrieval states spurious glassy states, arise. repeated computations] case model), cavity method]. refer reader interested technical details statistical mechanics approach neural networks]. following standard computation locating phase transitions approximation (see. ]), stability paramagnetic state) phases monitored terms parameters) ?(?) lim tanh2 xlp ypl tanh2 xlp ypl??  ?(?) lim tanh ?—x1p yp1 xlp ypl tanh ?—x1p yp1 xlp ypl??  expectation distribution vectors parameter ?(?) controls sensitivity paramagnetic solution random noise, ?(?) measures sensitivity perturbation direction retrieval state.  defined implicitly  . perturbation diverges. existence retrieval phase equivalent condition exists range values retrieval states exist, spurious ones. condition met, setting  algorithm, ensure presence meaningful negative eigenvalues hessian. define critical general, closed-form formula critical value, defined implicitly terms functions  computed numerically population dynamics algorithm] results ) presented Figure quite remarkably, definition, critical depend ratio, rank  .812) limit large obtain simple closed-form formula. case observed entries matrix jointly Gaussian distributed, uncorrelated, refore independent. expression) simplifies ?(?) ?? tanh2   Note MaCBetH algorithm empirical estimator (?) ?(?) ) quantity compute approximation  purely revealed entries. large regime decay furr approximate Figure Location critical function rank MaCBetH estimate correct rank entries.  population dynamics algorithm ?? ) population size compute funcp tions  ). dotted line fit ?? ) suggesting)  reach simple asymptotic expression, large limit, equivalently) interestingly, result obtained detectability threshold completion rank) matrices entries Bayes optimal setting].  notice, however, exact completion setting: detection exact completion phenomena. previous analysis extended random setting assumption, long empirical distribution entries defined, lines (resp. approximately orthogonal centered. this condition related standard incoherence property].  Computation spectral density section, show spectral density Hessian computed analytically tree-like graphs generated picking uniformly random observed entries matrix this furr motivates algorithm choice   independently section spectral density defined ?(?)  ?(?  ??  lim) eigenvalues hessian. using cavity method, shown] spectral density potential delta peaks removed) ?(?)  (?) ??   lim) complex variables living vertices graph: sinh2?ail)  sinh2 ?aik   set neighbors  (linearly stable) solution belief propagation recursion  sinh2 ?aik sinh2?ail )   Rank Rank Mean inferred rank 500 2000 8000 16000 Transition  figure Mean inferred rank function sizes, averaged 100 samples matrices. entries drawn Gaussian distribution variance oretical transition computed population dynamics algorithm (see section). finite size effects considerable consistent asymptotic prediction. this formula derived turning computation spectral density marginalization problem graphical model graph solving loopy belief propagation. quite remarkably, approach leads asymptotically exact (and rigorous]) description spectral density erd?nyi random graphs. solving equation) numerically obtain results shown fig. bulk spectrum, particular, positive. demonstrate  exists open set spectral density vanishes. this justifies independently choice parameter proof] begins noticing cosh (?aij fixed point recursion)  since fixed point real, spectral density now small perturbation solution cosh (?aij cosh (?aij linearized version) writes  tanh2 (?ail linear operator defined weighted version \\x0cnon-backtracking matrix]. its spectral radius ?(?  defined. particular,  straightforward application] implicit function orem show exists neighborhood  exists real, linearly stable fixed point), yielding spectral density equal    informative eigenvalues (those bulk), refore negative ones, motivates independently algorithm. Numerical tests Figure illustrates ability Hessian infer rank critical limit large size (see section). Figure demonstrate suitability eigenvectors Hessian starting point minimization cost function). compare final RMSE achieved reconstructed matrix initializations optimization, including largest singular vectors trimmed matrix]. macbeth systematically outperforms choices initial conditions, providing initial condition optimization). remarkably, performance achieved MaCBetH inferred rank essentially achieved oracle rank. contrast, estimating correct rank (trimmed) SVD challenging. note choice parameters consider, trimming negligible effect. along lines, OptSpace] minimization procedure, tests difference performance due that. when Alternating Least Squares] optimization method, obtained similar improvement reconstruction eigenvectors hessian, singular vectors initial condition. (rmse Rank Macbeth-svd Random Macbeth-svd(rmse Rank  figure RMSE function number revealed entries row comparison initializations optimization cost function). top row shows probability achieved RMSE smaller bottom row shows probability final RMSE smaller probabilities estimated frequency success 100 samples matrices size 10000 10000, \\x0centries drawn Gaussian distribution variance all methods optimize cost function) low storage BFGS algorithm] part NLopt], starting initial conditions. maximum number iterations set 1000. initial conditions compared MaCBetH oracle rank (macbeth) inferred rank (macbeth), SVD observed matrix trimming, oracle rank-svd), inferred rank-svd, note equivalent OptSpace] regime), random initial conditions oracle rank (random). for-svd method, inferred rank SVD index ratio consecutive eigenvalues minimized, suggested]. Conclusion paper, presented macbeth, algorithm matrix completion eﬃcient distinct, complementary, tasks) ability estimate finite rank reliably fewer random entries existing approaches) lower root-mean-square reconstruction errors competitors. algorithm built Hessian matrix leverages recent progresses construction eﬃcient spectral methods clustering sparse networks], OptSpace approach] matrix completion. method presented offers number future directions, including replacing minimization cost function message-passing type algorithm, neural network models, oretical direction involving computation information oretically optimal transitions detectability. acknowledgment Our research received funding European search Council European union 7th Framework Programme/20072013/erc Grant Agreement 307087-sparcs).',\n",
       " 'PP5862': 'neural networks today achieving state--art performance competitions range fields]. success raises hope begin move networks lab embedded systems tackle real world problems. necessitates shift darpa: Approved Public release, Distribution Unlimited thinking system design, neural network hardware substrate collectively meet performance, power, space, speed requirements. neuron-for-neuron basis, eﬃcient substrates neural network operation today dedicated neuromorphic designs]. achieve high eﬃciency, neuromorphic architectures spikes provide event based computation communication consumes energy necessary, low precision synapses colocate memory computation keeping data movement local allowing parallel distributed operation, constrained connectivity implement neuron fan-out eﬃciently dramatically reducing network traﬃc-chip. however, design choices introduce apparent incompatibility backpropagation algorithm] training today successful deep networks, continuousoutput neurons high-precision synapses, typically operates limits number inputs neuron. build systems advantage algorithmic insights deep learning, operational eﬃciency neuromorphic hardware? main contribution here, demonstrate learning rule network topology reconciles apparent incompatibility backpropagation neuromorphic hardware. essence learning rule train network oﬄine hardware supported connectivity, continuous valued input, neuron output, synaptic weights, values constrained range]. furr impose constrained values represent probabilities, eir spike occurring synapse. network trained backpropagation, direct representation spiking, low synaptic precision deployment system, reby bridging worlds. network topology progressive mixing approach, neuron access limited set inputs previous layer, sources chosen neurons successive layers access progressively network input. previous efforts shown success subsets elements bring toger here. backpropagation train networks spiking neurons high-precision weights], converse, networks trinary synapses continuous output neurons]. probabilistic backpropagation approaches demonstrated networks binary neurons binary trinary synapses full inter-layer connectivity]. work presented demonstrate time oﬄine training methodology backpropagation create network employs spiking neurons, synapses requiring bits precision trinary weights, constrained connectivity) achieve accuracy date MNIST%) compared networks spiking neurons, high precision synapses], networks binary synapses neurons], iii) demonstrate network running real-time TrueNorth chip], achieving published power eﬃciency digit recognition classification% accuracy running 1000 images second) compared low power approaches classification% accuracy running images second]. deployment Hardware TrueNorth neurosynaptic chip] deployment system, approach generalized neuromorphic hardware]. truenorth chip consists 4096 cores, core 256 axons (inputs), 256 256 synapse crossbar, 256 spiking neurons. information ﬂows spikes neuron axon cores, axon potentially neurons core, gated binary synapses crossbar. neurons considered variety dynamics], including below. axon assigned axon types, index lookup table-values, unique neuron, signed-bit integer synaptic strength synapse. approach requires bit synapse/off state additional bits synapse lookup table scheme. network Training approach, employ types multilayer networks. deployment network runs platform supporting spiking neurons, discrete synapses low precision, limited connectivity. training network learn binary synaptic connectivity states biases. network shares topology deployment network, represents input data, neuron outputs, synaptic connections continuous values constrained range overview provided Figure Table). values correspond probabilities spike occurring synapse?, providing means mapping training network deployment network, providing continuous differentiable space backpropagation. below, describe deployment network, training methodology, procedure mapping training network deployment network.  Deployment network Deployment Our deployment network feedforward methodology neurons sequentially updated layer. input network represented stochastically generated spikes, input unit probability. write spike state input unit continuous range, derived-scaling input data (pixels). scheme representation data binary spikes, preserving data precision expectation. training Input Input Neuron Connected synapses Synaptic connection probabilities Neuron Spikes Synapse strength Spike probabilities Figure Diagram showing input, synapses, output neuron deployment training network. simplicity, synapses depicted. summed neuron input computed cij sij) target neuron index, cij binary indicator variable representing wher synapse, sij synaptic strength, bias term. identical common practice neural networks, factored synaptic weight cij sij focus learning efforts reasons below. neuron activation function history-free thresholding equation orwise. dynamics implemented TrueNorth setting neuron leak equal learned bias term (dropping fractional portion), threshold membrane potential ﬂoor setting synapse parameters scheme below. represent class label multiple output neurons layer network, found improves prediction performance. network prediction class simply average output neurons assigned class. table Network components Network input Synaptic connection Synaptic strength Neuron output Deployment Network Variable Values, Correspondance      training Network Variable Values , ,  Training network Training backpropagation methodology iteratively running forward pass layer layer) comparing network output desired output loss function, iii) propagating loss backwards network determine loss gradient synapse bias term) gradient update network parameters. training network forward pass probabilistic representation deployment network forward pass. synaptic connections represented probabilities (cij  synaptic strength represented sij deployment network. assumed sij drawn limited set values additional constraint set ?blocks? multiple synapses share value TrueNorth eﬃciency. conceivable learn optimal values sij conditions, requires stepwise allowed values optimization local synapse. simpler approach here, learn biases synapse connection probabilities, intelligently fix synapse strengths approach Network Initialization section. input training network represented probability input spike occurring deployment network. neurons, note Equation summation weighted Bernoulli variables bias term. assume independence inputs suﬃcient numbers, approximate probability distribution summation Gaussian sij variance  )s2ij ) derive probability neuron firing complementary cumulative distribution function gaussian:        erf ) erf error function,    layers first, replaced input previous layer, represents probability neuron produces spike. variety loss functions suitable approach, found training converged fastest log loss=?  log log class binary class label class present orwise, probability average spike count class greater. conveniently, Gaussian approximation Equation this,  variance terms set averaging process. training network backward pass adaptation backpropagation neuron synapse equations above. gradient synapse, chain rule compute   cij  cij For bias, similar computation made replacing equation differentiate Equation produce sij   cij (??  s2ij  s2ij      (??   ) below, assume synapse strengths neuron balanced positive negative values neuron receives 256 inputs, expect close zero,  refore, term Equation denominator expected smaller left term \\x0ccontaining denominator conditions, computational eﬃciency approximate Equation dropping term factoring remainder    cij  cij    (??   sij  cij similar treatment show gradient respect bias term equals one. network updated loss gradient synapse bias term. iteration, synaptic connection probability cij  cij learning rate. synaptic connection probabilities fall range, result update rule ?snapped? nearest valid value. bias term handled similar fashion, values clipped fall range [?255, 255], largest values supported TrueNorth neuron parameters. training procedure amenable methods heuristics applied standard backpropagation. results shown below, mini batch size 100, momentum, dropout], learning rate decay fixed schedule training iterations starting multiplying 250 epochs, transformations training data iteration rotation? shift pixels rescale%.  Mapping training network deployment network Training performed oﬄine, resulting network mapped deployment network hardware operation. deployment, depending system requirements, utilize ensemble samplings training network increase output performance. unlike ensemble methods, train sample training network member. system output class determined averaging neurons member networks assigned class. synaptic connection states set (cij  independent random number draws synapse ensemble member. data converted spiking representation input independent random number draws input member ensemble.  Network initialization approach network initialization optimize eﬃcient neuromorphic hardware employs bits synapse. approach, synaptic connection probability initialized uniform random distribution range]. initialize synapse strength values, begin principle core maximize information transfer maximizing information neuron minimizing redundancy neurons. methods explored detail approaches infomax]. goals data dependent, pursue initialization time tuning space weights core, represented matrix synapse strength values, approach, minimize redundancy neurons \\x0ccore attempting induce product distribution outputs pair neurons. simplify problem, note summed weighted inputs pair neurons wellapproximated-variate Gaussian distribution. thus, forcing covariance summed weighted inputs guarantees inputs independent. furrmore, functions pairwise independent random variables remain pair-wise independent, neuron outputs guaranteed independent. Axons Type Axons-128 Type Axons 129-192 Type Axons 193-256 Type Synapse strength Neurons 246-256 Neurons Neurons summed weighted input neuron ... Equation desirable purposes maintaining balance neuron dynamics configure weights mix positive negative values sum zero. Figure Synapse strength values dex sij) picted axons (rows) neurons (columns) array. learning procedure fixes values network initialized implies assuming inputs learns probability synapse synaptic connection states decorrelated transmitting state. blocky bias term simplifies covariance ance strength matrix result inputs neurons core shared synaptic strength approach  truenorth reduce memory footprint.  cij sij cqr sqr   Rearranging terms cij sij cqr sqr[x2i cij sij cqr sqr  next, note equation covariance  assumption inputs equal variance[x2i   constant. furr assuming covariance inputs   constant. equation), Equation hcj cij sij (?cir sir hcj  hcj   hcj minimizing absolute product columns forces maximally uncorrelated constraints. inspired observation, apriori., knowledge input data) choose strength \\x0cvalues absolute product columns effective weight matrix minimized, sum effective weights neuron zero. practically, achieved assigning half neuron svalues half balancing permutations assignments occur equally neurons core, evenly distributing axon types axons core. resulting matrix synaptic strength values Figure configuration optimal weight subspace, constraints, backpropagation operate datadriven fashion find desirable synaptic/off states. core (256 neurons) input window stride 256 neurons layer input input Core Network Core Network Figure Two network configurations results here, core network designed minimize core count core network designed maximize accuracy. board socketed TrueNorth chip run deployment networks. chip cm2 runs real time neuron updates), consumes running benchmark network million neuron]. measured accuracy measured energy network configurations running chip. ensemble size shown data point. network topology network topology designed support neurons responses local, regional global features respecting ?core-core? connectivity TrueNorth architecture neurons core share access set inputs, number inputs limited. network multilayer feedforward scheme, layer consists input elements rows columns channels array, image, remaining layers consist TrueNorth cores. connections layers made sliding window approach. input core layer drawn  input window (figure), represents row column dimensions, represents feature dimension. input layer, rows columns units input elements features input channels, input remaining layers rows columns units cores features neurons. core target layer \\x0clocates input window upper left corner source layer, core target layer shifts input window stride Successive cores slide window edge source layer reached, window returned left, shifted process repeated. features sub-selected randomly, constraint neuron selected target core. input elements selected multiple times. scheme similar respects convolution network, employ independent synapses location. specific networks employed here, parameters, shown Figure. results applied training method MNIST dataset], examining accuracy. energy tradeoffs networks running TrueNorth chip (figure). network smallest multilayer TrueNorth network number pixels present dataset, consisting cores distributed layers, 512 neurons. network built primary goal maximizing accuracy, composed cores distributed layers (figure), 3840 neurons. networks configured layer networks, core network core network, subsequent layers networks, parameters result ?pyramid? shape, cores layer final layer draw input source cores neurons sources. core employs neurons core targets, maximum 256 neurons. tested network ensemble, members running TrueNorth chip real-time. image encoded single time step), spike sampling input line targeted pixel. instrumentation measures active power network operation leakage power entire chip, consists 4096 cores. report energy numbers active power fraction leakage power cores use. highest performance observed% achieved core trained network member ensemble, total 1920 cores, measured 108 classification. lowest energy achieved core network operating ensemble measured.268 classification achieving% accuracy. results plotted showing accuracy. energy Figure. networks classified 1000 images second. discussion Our results show backpropagation operating probabilistic domain train networks naturally map neuromorphic hardware spiking neurons extremely lowprecision synapses. approach succinctly summarized constrain-train, constrain network provide direct representation deployment system train \\x0cwithin constraints. contrasted train-constrain approach, network agnostic final deployment system trained, training constrained normalization discretization methods provide spiking representation low precision weights. requiring customized training rule, constrain-train approach offers advantage decrease training error direct correspondence decrease error deployment network. conversely, train-constrain approach shelf training methods, unconstrained training guaranteed produce reduction error hardware constraints applied. forward, avenues expanding approach complex datasets. first, deep convolution networks] great deal success backpropagation learn weights convolutional filters. learning method introduced independent specific network structure sparsity constraint, adapted convolution networks. second, biology number examples, retina cochlea, mapping high-precision sensory data binary spiking representation. drawing inspiration approaches improve performance linear mapping scheme work. third, approach adaptable gradient based learning methods, methods existing probabilistic components contrastive divergence]. furr, describe approach TrueNorth provide concrete case, reason training approach spiking neuromorphic hardware]. work timely, recent years backpropagation achieved high level performance number tasks reﬂecting real world tasks, including object detection complex scenes], pedestrian detection], speech recognition]. wide range sensors found mobile devices ranging phones automobiles, platforms TrueNorth provide low power substrate processing sensory data. bridging backpropagation energy eﬃcient neuromorphic computing, hope work important step building low-power, scalable brain-inspired systems real world applicability. acknowledgments This research sponsored Defense Advanced Research Projects Agency contracts. hr001109-0002. fa9453-0055. views, opinions, and findings contained paper authors interpreted representing oﬃcial views policies Department Defense. government. Neural networks today achieving state--art performance competitions range fields]. such success raises hope begin move networks lab embedded systems tackle real world problems. this necessitates shift darpa: Approved Public release, Distribution Unlimited thinking system design, neural network hardware substrate collectively meet performance, power, space, speed requirements. neuron-for-neuron basis, eﬃcient substrates neural network operation today dedicated neuromorphic designs]. achieve high eﬃciency, neuromorphic architectures spikes provide event based computation communication consumes energy necessary, low precision synapses colocate memory computation keeping data movement local allowing parallel distributed operation, constrained connectivity implement neuron fan-out eﬃciently dramatically reducing network traﬃc-chip. however, design choices introduce apparent incompatibility backpropagation algorithm] training today successful deep networks, continuousoutput neurons high-precision synapses, typically operates limits number inputs neuron. how build systems advantage algorithmic insights deep learning, operational eﬃciency neuromorphic hardware? main contribution here, demonstrate learning rule network topology reconciles apparent incompatibility backpropagation neuromorphic hardware. essence learning rule train network oﬄine hardware supported connectivity, continuous valued input, neuron output, synaptic weights, values constrained range]. furr impose constrained values represent probabilities, eir spike occurring synapse. such network trained backpropagation, direct representation spiking, low synaptic precision deployment system, reby bridging worlds. network topology progressive mixing approach, neuron access limited set inputs previous layer, sources chosen neurons successive layers access progressively network input. previous efforts shown success subsets elements bring toger here. backpropagation train networks spiking neurons high-precision weights], converse, networks trinary synapses continuous output neurons]. probabilistic backpropagation approaches demonstrated networks binary neurons binary trinary synapses full inter-layer connectivity]. work presented demonstrate time oﬄine training methodology backpropagation create network employs spiking neurons, synapses requiring bits precision trinary weights, constrained connectivity) achieve accuracy date MNIST%) compared networks spiking neurons, high precision synapses], networks binary synapses neurons], iii) demonstrate network running real-time TrueNorth chip], achieving published power eﬃciency digit recognition classification% accuracy running 1000 images second) compared low power approaches classification% accuracy running images second]. deployment Hardware TrueNorth neurosynaptic chip] deployment system, approach generalized neuromorphic hardware]. TrueNorth chip consists 4096 cores, core 256 axons (inputs), 256 256 synapse crossbar, 256 spiking neurons. information ﬂows spikes neuron axon cores, axon potentially neurons core, gated binary synapses crossbar. neurons considered variety dynamics], including below. each axon assigned axon types, index lookup table-values, unique neuron, signed-bit integer synaptic strength synapse. this approach requires bit synapse/off state additional bits synapse lookup table scheme. Network Training approach, employ types multilayer networks. deployment network runs platform supporting spiking neurons, discrete synapses low precision, limited connectivity. training network learn binary synaptic connectivity states biases. this network shares topology deployment network, represents input data, neuron outputs, synaptic connections continuous values constrained range overview provided Figure Table). values correspond probabilities spike occurring synapse?, providing means mapping training network deployment network, providing continuous differentiable space backpropagation. below, describe deployment network, training methodology, procedure mapping training network deployment network.  Deployment network Deployment Our deployment network feedforward methodology neurons sequentially updated layer. input network represented stochastically generated spikes, input unit probability. write spike state input unit continuous range, derived-scaling input data (pixels). this scheme representation data binary spikes, preserving data precision expectation. training Input Input Neuron Connected synapses Synaptic connection probabilities Neuron Spikes Synapse strength Spike probabilities Figure Diagram showing input, synapses, output neuron deployment training network. for simplicity, synapses depicted. summed neuron input computed cij sij) target neuron index, cij binary indicator variable representing wher synapse, sij synaptic strength, bias term. this identical common practice neural networks, factored synaptic weight cij sij focus learning efforts reasons below. neuron activation function history-free thresholding equation orwise. dynamics implemented TrueNorth setting neuron leak equal learned bias term (dropping fractional portion), threshold membrane potential ﬂoor setting synapse parameters scheme below. represent class label multiple output neurons layer network, found improves prediction performance. network prediction class simply average output neurons assigned class. table Network components Network input Synaptic connection Synaptic strength Neuron output Deployment Network Variable Values, Correspondance      training Network Variable Values , ,  Training network Training backpropagation methodology iteratively running forward pass layer layer) comparing network output desired output loss function, iii) propagating loss backwards network determine loss gradient synapse bias term) gradient update network parameters. training network forward pass probabilistic representation deployment network forward pass. synaptic connections represented probabilities (cij  synaptic strength represented sij deployment network. assumed sij drawn limited set values additional constraint set ?blocks? multiple synapses share value TrueNorth eﬃciency. while conceivable learn optimal values sij conditions, requires stepwise allowed values optimization local synapse. simpler approach here, learn biases synapse connection probabilities, intelligently fix synapse strengths approach Network Initialization section. input training network represented probability input spike occurring deployment network. for neurons, note Equation summation weighted Bernoulli variables bias term. assume independence inputs suﬃcient numbers, approximate probability distribution summation Gaussian sij variance  )s2ij ) derive probability neuron firing complementary cumulative distribution function gaussian:        erf ) erf error function,    for layers first, replaced input previous layer, represents probability neuron produces spike. variety loss functions suitable approach, found training converged fastest log loss=?  log log class binary class label class present orwise, probability average spike count class greater. conveniently, Gaussian approximation Equation this,  variance terms set averaging process. training network backward pass adaptation backpropagation neuron synapse equations above. gradient synapse, chain rule compute   cij  cij For bias, similar computation made replacing equation differentiate Equation produce sij   cij (??  s2ij  s2ij      (??   ) below, assume synapse strengths neuron balanced positive negative values neuron receives 256 inputs, expect close zero,  refore, term Equation denominator expected smaller left term \\x0ccontaining denominator under conditions, computational eﬃciency approximate Equation dropping term factoring remainder    cij  cij    (??   sij  cij similar treatment show gradient respect bias term equals one. network updated loss gradient synapse bias term. for iteration, synaptic connection probability cij  cij learning rate. any synaptic connection probabilities fall range, result update rule ?snapped? nearest valid value. changes bias term handled similar fashion, values clipped fall range [?255, 255], largest values supported TrueNorth neuron parameters. training procedure amenable methods heuristics applied standard backpropagation. for results shown below, mini batch size 100, momentum, dropout], learning rate decay fixed schedule training iterations starting multiplying 250 epochs, transformations training data iteration rotation? shift pixels rescale%.  Mapping training network deployment network Training performed oﬄine, resulting network mapped deployment network hardware operation. for deployment, depending system requirements, utilize ensemble samplings training network increase output performance. unlike ensemble methods, train sample training network member. system output class determined averaging neurons member networks assigned class. synaptic connection states set (cij  independent random number draws synapse ensemble member. data converted spiking representation input independent random number draws input member ensemble.  Network initialization approach network initialization optimize eﬃcient neuromorphic hardware employs bits synapse. approach, synaptic connection probability initialized uniform random distribution range]. initialize synapse strength values, begin principle core maximize information transfer maximizing information neuron minimizing redundancy neurons. such methods explored detail approaches infomax]. while goals data dependent, pursue initialization time tuning space weights core, represented matrix synapse strength values, approach, minimize redundancy neurons \\x0ccore attempting induce product distribution outputs pair neurons. simplify problem, note summed weighted inputs pair neurons wellapproximated-variate Gaussian distribution. thus, forcing covariance summed weighted inputs guarantees inputs independent. furrmore, functions pairwise independent random variables remain pair-wise independent, neuron outputs guaranteed independent. Axons Type Axons-128 Type Axons 129-192 Type Axons 193-256 Type Synapse strength Neurons 246-256 Neurons Neurons summed weighted input neuron ... Equation desirable purposes maintaining balance neuron dynamics configure weights mix positive negative values sum zero. thus Figure Synapse strength values dex sij) picted axons (rows) neurons (columns) array. learning procedure fixes values network initialized implies assuming inputs learns probability synapse synaptic connection states decorrelated transmitting state. blocky bias term this simplifies covariance ance strength matrix result inputs neurons core shared synaptic strength approach  TrueNorth reduce memory footprint.  cij sij cqr sqr   Rearranging terms cij sij cqr sqr[x2i cij sij cqr sqr  next, note equation covariance  under assumption inputs equal variance[x2i   constant. furr assuming covariance inputs   constant. using equation), Equation hcj cij sij (?cir sir hcj  hcj   hcj minimizing absolute product columns forces maximally uncorrelated constraints. inspired observation, apriori., knowledge input data) choose strength \\x0cvalues absolute product columns effective weight matrix minimized, sum effective weights neuron zero. practically, achieved assigning half neuron svalues half balancing permutations assignments occur equally neurons core, evenly distributing axon types axons core. resulting matrix synaptic strength values Figure this configuration optimal weight subspace, constraints, backpropagation operate datadriven fashion find desirable synaptic/off states. core (256 neurons) input window stride 256 neurons layer input input Core Network Core Network Figure Two network configurations results here, core network designed minimize core count core network designed maximize accuracy. Board socketed TrueNorth chip run deployment networks. chip cm2 runs real time neuron updates), consumes running benchmark network million neuron]. Measured accuracy measured energy network configurations running chip. ensemble size shown data point. Network topology network topology designed support neurons responses local, regional global features respecting ?core-core? connectivity TrueNorth architecture neurons core share access set inputs, number inputs limited. network multilayer feedforward scheme, layer consists input elements rows columns channels array, image, remaining layers consist TrueNorth cores. connections layers made sliding window approach. input core layer drawn  input window (figure), represents row column dimensions, represents feature dimension. for input layer, rows columns units input elements features input channels, input remaining layers rows columns units cores features neurons. core target layer \\x0clocates input window upper left corner source layer, core target layer shifts input window stride Successive cores slide window edge source layer reached, window returned left, shifted process repeated. features sub-selected randomly, constraint neuron selected target core. input elements selected multiple times. this scheme similar respects convolution network, employ independent synapses location. specific networks employed here, parameters, shown Figure. Results applied training method MNIST dataset], examining accuracy. energy tradeoffs networks running TrueNorth chip (figure). network smallest multilayer TrueNorth network number pixels present dataset, consisting cores distributed layers, 512 neurons. network built primary goal maximizing accuracy, composed cores distributed layers (figure), 3840 neurons. networks configured layer networks, core network core network, subsequent layers networks, parameters result ?pyramid? shape, cores layer final layer draw input source cores neurons sources. each core employs neurons core targets, maximum 256 neurons. tested network ensemble, members running TrueNorth chip real-time. each image encoded single time step), spike sampling input line targeted pixel. instrumentation measures active power network operation leakage power entire chip, consists 4096 cores. report energy numbers active power fraction leakage power cores use. highest performance observed% achieved core trained network member ensemble, total 1920 cores, measured 108 classification. lowest energy achieved core network operating ensemble measured.268 classification achieving% accuracy. results plotted showing accuracy. energy Figure. both networks classified 1000 images second. Discussion Our results show backpropagation operating probabilistic domain train networks naturally map neuromorphic hardware spiking neurons extremely lowprecision synapses. our approach succinctly summarized constrain-train, constrain network provide direct representation deployment system train \\x0cwithin constraints. this contrasted train-constrain approach, network agnostic final deployment system trained, training constrained normalization discretization methods provide spiking representation low precision weights. while requiring customized training rule, constrain-train approach offers advantage decrease training error direct correspondence decrease error deployment network. conversely, train-constrain approach shelf training methods, unconstrained training guaranteed produce reduction error hardware constraints applied. looking forward, avenues expanding approach complex datasets. first, deep convolution networks] great deal success backpropagation learn weights convolutional filters. learning method introduced independent specific network structure sparsity constraint, adapted convolution networks. second, biology number examples, retina cochlea, mapping high-precision sensory data binary spiking representation. drawing inspiration approaches improve performance linear mapping scheme work. third, approach adaptable gradient based learning methods, methods existing probabilistic components contrastive divergence]. furr, describe approach TrueNorth provide concrete case, reason training approach spiking neuromorphic hardware]. work timely, recent years backpropagation achieved high level performance number tasks reﬂecting real world tasks, including object detection complex scenes], pedestrian detection], speech recognition]. wide range sensors found mobile devices ranging phones automobiles, platforms TrueNorth provide low power substrate processing sensory data. bridging backpropagation energy eﬃcient neuromorphic computing, hope work important step building low-power, scalable brain-inspired systems real world applicability. acknowledgments This research sponsored Defense Advanced Research Projects Agency contracts. hr001109-0002. fa9453-0055. views, opinions, and findings contained paper authors interpreted representing oﬃcial views policies Department Defense. government.',\n",
       " 'PP5918': 'problem minimizing sum functions projected iterations convex parameter set  regime, algorithms utilize sub-sampling techniques effective. paper, sub-sampling techniques toger low-rank approximation design randomized batch algorithm possesses comparable convergence rate newton method, smaller per-iteration cost. proposed algorithm robust terms starting point step size, \\x0cenjoys composite convergence rate, namely, quadratic convergence start linear convergence iterate close minimizer. develop oretical analysis select near-optimal algorithm parameters. oretical results obtain convergence rates previously proposed sub-sampling based algorithms well. demonstrate results apply well-known machine learning problems. lastly, evaluate performance algorithm datasets scenarios. introduction focus minimization problem, minimize (?)  ) most machine learning models expressed above, function corresponds observation. examples include logistic regression, support vector machines, neural networks graphical models. optimization algorithms developed solve minimization problem [bis95, bv04, nes04]. convex set denote Euclidean projection set updates form      ) step size suitable scaling matrix curvature information. updates form. ) extensively studied optimization literature (for simplicity, assume introduction). case equal identity matrix corresponds Gradient Descent) which, smoothness assumptions, achieves linear convergence rate) per-iteration cost. precisely, ideal step size yields where, limt     largest   ,  eigenvalue Hessian (?) minimizer  order methods newton Method) Natural Gradient Descent (ngd) [ama98] recovered taking inverse Hessian Fisher information evaluated current iterate, respectively. methods achieve quadratic convergence rates(np2 per-iteration cost [bis95, nes04]. particular, large enough, newton method yields     k22 insensitive condition number hessian. however, number samples grows large, computing extremely expensive. popular line research construct matrix update computationally feasible, suﬃcient order information. attempts resulted quasi-newton methods, gradients iterates utilized, resulting eﬃcient update celebrated QuasiNewton method broyden-fletcher-goldfarb-shanno (bfgs) algorithm requires per-iteration cost [bis95, nes04]. alternative approach sub-sampling techniques, scaling matrix based randomly selected set data points [mar10, bcnn11, vp12, erd15]. subsampling widely order methods, studied \\x0capproximating scaling matrix. particular, oretical guarantees missing. key challenge sub-sampled Hessian close actual Hessian directions large eigenvalues (large curvature directions )), poor approximation directions small eigenvalues (ﬂatter directions (?)). order overcome problem, low-rank approximation. precisely, treat eigenvalues equal. yields desired stability respect sub-sample: call algorithm newsamp. paper, establish following: newsamp composite convergence rate: quadratic start linear minimizer, illustrated Figure formally, prove bound form       k22 coeﬃcient explicitly (and computable data).  asymptiotic behavior linear convergence coeﬃcient limt small. condition number controls convergence, replaced milder datasets strong spectral features, large improvement, shown Figure  results achived tuning step-size, particular, setting  complexity iteration NewSamp— sample size.  oretical results obtain convergence rates previously proposed subsampling algorithms. rest paper organized follows: Section surveys related work. section describe proposed algorithm provide intuition. next, present oretical results Section., convergence rates sub-sampling schemes, discussion choose algorithm parameters. applications algorithm discussed Section compare algorithm existing methods datasets Section finally, Section conclude discussion.  Related Work Even syntic review optimization algorithms large-scale machine learning page limits paper. here, emphasize method choice depends crucially amount data used, dimensionality., respectively, parameters). paper, focus regime large large make gradient computations order) matrix manipulations order prohibitive. online algorithms option choice large computation update independent case Stochastic Gradient Descent (sgd), descent direction formed randomly selected gradient. improvements SGD developed incorporating previous gradient directions current update equation [srb13, bot10, dhs11]. batch algorithms, hand, achieve faster convergence exploit order information. competitive intermediate Several methods category aim quadratic, super-linear convergence rates. particular, quasi-newton methods proven effective [bis95, nes04]. anor approach goal utilize sub-sampling form approximate Hessian [mar10, bcnn11, vp12, \\x0cerd15]. sub-sampled Hessian close true hessian, methods approach terms convergence rate, neverless, enjoy Algorithm NewSamp input:   define: (?) argmin euclidean projection TruncatedSVDk) rank truncated SVD     sub-sample set indices ]. let HSt —s1t i2st?   truncatedsvdr (hst        end output:  smaller complexity update. convergence rate analysis methods: analysis main contribution paper. knowledge, result direction proven [bcnn11] estabilishes asymptotic convergence quantitative bounds (exploiting general ory [gns09]). furr improvements subsampling algorithms, common approach Conjugate Gradient) methods and Krylov sub-spaces [mar10, bcnn11, vp12]. lastly, hybrid algorithms combine techniques increase performance. examples include, sub-sampling quasi-newton [bhns14], SGD [fs12], NGD [lrf10], NGD low-rank approximation [lrmb08]. newsamp newton-sampling method rank thresholding regime consider, main drawbacks classical order methods newton method. dominant issue computation Hessian matrix, requires(np2 operations, issue inverting hessian, requires computation. sub-sampling effective eﬃcient tackling issue. recent empirical studies show sub-sampling Hessian significant improvement terms computational cost, preserves fast convergence rate order methods [mar10, vp12]. uniform subsample used, sub-sampled Hessian random matrix expected true hessian, considered sample estimator mean. recent advances statistics shown performance estimators significantly improved simple procedures shrinkage and thresholding [ccs10, dgj13]. extent, low-rank approximation important order information generally contained largest eigenvalues/vectors hessian. newsamp presented Algorithm iteration step sub-sampled set indices, size sub-sampled Hessian denoted HSt respectively. assuming functions convex, eigenvalues symmetric matrix HSt non-negative. refore, SVD eigenvalue decomposition coincide. operation TruncatedSVDk (hst rank approximation., takes HSt input returns largest eigenvalues eigenvectors procedure requires(kp2 computation [hmt11]. operator projects current iterate feasible set Euclidean projection. assume \\x0cthis projection eﬃciently. construct curvature matrix basic rank approximation, fill eigenvalues eigenvalue sub-sampled Hessian largest eigenvalue threshold. compute truncated SVD operation results  simply sum scaled identity matrix rank matrix. note low-rank approximation suggested improve curvature estimation furr utilized reduce cost computing inverse matrix. final per-iteration cost NewSamp  newsamp takes parameters inputs. discuss Section, choose optimally, based ory Section Convergence Rate Convergence Coeﬃcients Value log(error sub?sample size NewSamp 100 NewSamp 200 NewSamp 500 200 Iterations Coeﬃcient linear quadratic 400 600 Rank Figure Left plot demonstrates convergence rate NewSamp starts quadratic rate transitions linear convergence true minimizer. plot shows effect eigenvalue thresholding convergence coeﬃcients scaling constant. -axis shows number eigenvalues. plots obtained Covertype dataset. construction NewSamp descent algorithm. enjoys quadratic convergence rate start transitions linear rate neighborhood minimizer. behavior observed Figure left plot Figure shows convergence behavior NewSamp sub-sample sizes. observe large sub-samples result convergence rates expected. sub-sample size increases, slope linear phase decreases, closer quadratic phase. explain phenomenon Section orems. plot Figure demonstrates coeﬃcients phases depend thresholded rank. coeﬃcient quadratic phase increases rank threshold, linear phase, relation reversed. oretical results section, provide convergence analysis NewSamp based subsampling schemes: Independent sub-sampling: iteration uniformly sampled}, independently sets?  replacement. : Sequentially dependent sub-sampling: iteration sampled], based distribution depend previous sets?  randomness data. sub-sampling scheme simple commonly optimization. drawback sub-sampled set current iteration independent previous sub-samples, samples previously form approximate curvature information. order prevent cycles obtain performance optimum, increase sample size iteration advances [mar10], including previously unused samples. process results sequence dependent sub-samples falls subsampling scheme. oretical analysis, make assumptions: Assumption (lipschitz continuity). subset — depending size?, khs (?)  —  assumption (bounded hessian). ? (?) upper bounded constant., max? (?)   Independent sub-sampling section, assume ] fact, stochastic sampled sub-sampling scheme. algorithms assume uniform subset], case sub-sampled Hessian unbiased estimator full hessian. , [hst (?)] ] expectation randomness show scaling matrix formed sub-samples iterations form. ) composite convergence rate., combination linear quadratic phases. lemma. assume parameter set convex ] based sub-sampling scheme suﬃciently large. furr, Assumptions hold absolute constant probability, updates form. ) satisfy         coeﬃcients defined HSt log remark initial point close algorithm start quadratic rate convergence transform linear rate close neighborhood optimum. lemma holds matrix particular, choose HSt1 obtain bound simple sub-sampled Hessian method. case, coeﬃcients depend kqt smallest eigenvalue subsampled hessian. note arbitrarily small blow coeﬃcients. following, NewSamp remedies issue. orem. assumptions Lemma hold. denote eigenvalue HSt  newsamp iteration step step size satisfies  have, probability,        k22 absolute constant coeﬃcients defined log  NewSamp composite convergence rate coeﬃcients linear quadratic terms, (see plot Figure). observe sub-sampling size significant effect linear term, quadratic term governed Lipschitz constant. emphasize case feasible conditions orem.  Sequentially dependent sub-sampling here, assume subsampling scheme generate?   distribution sub-sampled sets depend, randomness dataset. examples include fixed sub-samples sub-samples increasing size, sequentially covering unused data. addition Assumptions, assume following. assumption. observations). . observations distribution For fixed ], assume functions satisfy (?)  function  most statistical learning algorithms formulated above., classification problems, access. samples denote class label covariate, measures classification error (see Section examples). sub-sampling scheme, analogue Lemma stated Appendix Lemma, leads result. orem. assume parameter set convex ] based sub-sampling scheme. furr, Assumptions hold, surely. conditioned event {?? }, step size satisfies. , newsamp iteration probability),         coeﬃcients defined diam log absolute constants  denotes eigenvalue HSt   Compared orem, observe coeﬃcient quadratic term change. due Assumption however, bound linear term worse, uniform bound convex parameter set Dependence coeﬃcients convergence guarantees coeﬃcients depend iteration step undesirable aspect results. however, constants approximated analogues? ? evaluated optimum defined simply replacing definition eigenvalue full-hessian  sake simplicity, case functions  (?) quadratic. orem. assume functions (?) quadratic, based scheme full Hessian lower bounded suﬃciently large absolute constants probability log   log orem implies that, sub-sampling size suﬃciently large concentrate?  generalizing orem nonquadratic functions straightforward, case, additional terms involving difference   case scheme, fixed sub-samples, coeﬃcient depend corollary suﬃcient condition convergence. detailed discussion number iterations convergence furr local convergence properties found [erd15, em15]. corollary. assume well-approximated? ? error bound ? orem. initial point suﬃcient condition convergence?    ?  Choosing algorithm parameters Step size: Let(log —). suggest step size NewSamp iteration  Note) upper bound orems minimizes component terms linearly depend compensate that, shrink) contrary algorithms, optimal step size NewSamp larger rigorous derivation.  found [em15]. sample size: orem, sub-sample size/ log)) suﬃcient obtain small coeﬃcient linear phase. note sub-sample size scales quadratically condition number. rank threshold: For full-hessian effective rank (trace divided largest eigenvalue), suﬃces log)) samples [ver10]. effective rank upper bounded dimension hence, log) samples approximate full-hessian choose rank threshold retains important curvature information.  Examples Generalized Linear Models (glm) Maximum likelihood estimation GLM setting equivalent minimizing negative loglikelihood ‘(? minimize (?) (hxi) hxi cumulant generating function, denote rows design matrix coeﬃcient vector. here, denotes product vectors function defines type glm) ordinary squares (ols) log logistic regression). results Section perform convergence analysis algorithm GLM problem. corollary.  ] uniform sub-sample, parameter set. assume derivative cumulant generating function) bounded Lipschitz continuous Lipschitz constant.pfurr, assume covariates contained ball radius. maxi2] kxi   newsamp constant step size iteration probability,  constants defined    absolute constant crx  log k22 LRx ith eigenvalue HSt  support Vector Machines (svm) linear SVM separating hyperplane maximizes margin., distance hyperplane support vectors. vast majority literature focuses dual problem [ss02], SVMs trained primal well. dual problem scale number data points (some approaches complexity) primal better-suited optimization linear SVMs [cha07]. primal problem linear SVM written minimize (?) ?k22 denote data samples, defines separating hyperplane, loss function. commonly loss functions include hinge loss, Huber loss smood versions [cha07]. smoothing approximating losses stable functions crucial optimization. case NewSamp requires loss function differentiable (almost everywhere), suggest eir smood Huber loss, hinge loss [cha07]. case hinge loss) max} combining offset normal vector hyperplane single parameter vector denoting SVt set indices support vectors iteration write hessian? (?)  xti SVt}. —svt i2svt When —svt large, problem falls setup solved eﬃciently newsamp. note unlike GLM setting, Lipschitz condition orems apply here. however, empirically demonstrate NewSamp works assumptions. experiments section, validate performance NewSamp numerical studies. experimented optimization problems, namely, Logistic Regression) svm. minimizes.  logistic function, SVM minimizes.  hinge loss. following, brieﬂy describe algorithms experiments: gradient Descent), iteration, takes step proportional negative full gradient evaluated current iterate. regularity conditions, exhibits linear convergence rate.  accelerated Gradient Descent (agd) proposed Nesterov [nes83], improves gradient descent momentum term.  newton Method) achieves quadratic convergence rate utilizing inverse Hessian evaluated current iterate.  broyden-fletcher-goldfarb-shanno (bfgs) popular stable quasi-newton method. formed accumulating information iterates gradients.  limited Memory BFGS-bfgs) variant bfgs, recent iterates gradients construct providing improvement terms memory usage.  stochastic Gradient Descent (sgd) simplified version where, iteration, randomly selected gradient used. follow guidelines [bot10] step size. dataset:) syn) Logistic regression, rank msd) Logistic regression, rank log(error) Method NewSamp BFGS LBFGS Newton AGD SGD AdaGrad time(sec) log(error) log(error)slices) Logistic regression, rank Method NewSamp BFGS LBFGS Newton AGD SGD AdaGrad svm, rank time(sec) Method NewSamp BFGS LBFGS Newton AGD SGD AdaGrad svm, rank time(sec) svm, rank Method NewSamp BFGS LBFGS Newton AGD SGD AdaGrad time(sec 100 log(error) log(error) log(error) Method NewSamp BFGS LBFGS Newton AGD SGD AdaGrad time(sec) Method NewSamp BFGS LBFGS Newton AGD SGD AdaGrad time(sec) 120 Figure Performance algorithms datasets. newsamp represented red color  adaptive Gradient Scaling (adagrad) adaptive learning rate based previous gradients. adagrad significantly improves performance stability SGD [dhs11]. batch algorithms, constant step size algorithms, step size fastest convergence chosen. stochastic algorithms, optimized parameters define step size. parameters NewSamp selected guidelines Section. experimented datasets Table dataset consists design matrix observations (classes) syntic data generated multivariate Gaussian distribution. methodological choice, selected moderate values newton method implemented, neverless demonstrate improvement. larger values comparison favorable approach. effects sub-sampling size rank threshold demonstrated Figure comparison aforementioned optimization techniques presented Figure case, observe stochastic methods enjoy fast convergence start, slows epochs. algorithm close NewSamp terms performance bfgs. case svm, closest algorithm NewSamp note global convergence BFGS [nes04]. condition super-linear rate which, initial point close optimum required [dm77]. condition rarely satisfied practice, affects performance order methods. newsamp, rank thresholding level robustness, found initial point important factor. details Figure additional experiments found Appendix Dataset slices Covertype MSD Syntic 53500 581012 515345 500000 386 300 Reference [gks, lic13] [bd99, lic13] [mewl, lic13] table Datasets experiments. conclusion paper, proposed sub-sampling based order method utilizing low-rank Hessian estimation. proposed method target regime complexity per-iteration. showed convergence rate NewSamp composite widely sub-sampling schemes., starts quadratic convergence transforms linear convergence optimum. convergence behavior sub-sampling schemes interesting line research. numerical experiments demonstrate performance proposed algorithm compared classical optimization methods. problem minimizing sum functions projected iterations convex parameter set  regime, algorithms utilize sub-sampling techniques effective. paper, sub-sampling techniques toger low-rank approximation design randomized batch algorithm possesses comparable convergence rate newton method, smaller per-iteration cost. proposed algorithm robust terms starting point step size, \\x0cenjoys composite convergence rate, namely, quadratic convergence start linear convergence iterate close minimizer. develop oretical analysis select near-optimal algorithm parameters. our oretical results obtain convergence rates previously proposed sub-sampling based algorithms well. demonstrate results apply well-known machine learning problems. lastly, evaluate performance algorithm datasets scenarios. Introduction focus minimization problem, minimize (?)  ) Most machine learning models expressed above, function corresponds observation. examples include logistic regression, support vector machines, neural networks graphical models. many optimization algorithms developed solve minimization problem [bis95, bv04, nes04]. for convex set denote Euclidean projection set updates form      ) step size suitable scaling matrix curvature information. updates form. ) extensively studied optimization literature (for simplicity, assume introduction). case equal identity matrix corresponds Gradient Descent) which, smoothness assumptions, achieves linear convergence rate) per-iteration cost. more precisely, ideal step size yields where, limt     largest   ,  eigenvalue Hessian (?) minimizer  second order methods newton Method) Natural Gradient Descent (ngd) [ama98] recovered taking inverse Hessian Fisher information evaluated current iterate, respectively. such methods achieve quadratic convergence rates(np2 per-iteration cost [bis95, nes04]. particular, large enough, newton method yields     k22 insensitive condition number hessian. however, number samples grows large, computing extremely expensive. popular line research construct matrix update computationally feasible, suﬃcient order information. such attempts resulted quasi-newton methods, gradients iterates utilized, resulting eﬃcient update celebrated QuasiNewton method broyden-fletcher-goldfarb-shanno (bfgs) algorithm requires per-iteration cost [bis95, nes04]. alternative approach sub-sampling techniques, scaling matrix based randomly selected set data points [mar10, bcnn11, vp12, erd15]. subsampling widely order methods, studied \\x0capproximating scaling matrix. particular, oretical guarantees missing. key challenge sub-sampled Hessian close actual Hessian directions large eigenvalues (large curvature directions )), poor approximation directions small eigenvalues (ﬂatter directions (?)). order overcome problem, low-rank approximation. more precisely, treat eigenvalues equal. this yields desired stability respect sub-sample: call algorithm newsamp. paper, establish following: newsamp composite convergence rate: quadratic start linear minimizer, illustrated Figure formally, prove bound form       k22 coeﬃcient explicitly (and computable data).  asymptiotic behavior linear convergence coeﬃcient limt small. condition number controls convergence, replaced milder for datasets strong spectral features, large improvement, shown Figure  results achived tuning step-size, particular, setting  complexity iteration NewSamp— sample size.  our oretical results obtain convergence rates previously proposed subsampling algorithms. rest paper organized follows: Section surveys related work. Section describe proposed algorithm provide intuition. next, present oretical results Section., convergence rates sub-sampling schemes, discussion choose algorithm parameters. two applications algorithm discussed Section compare algorithm existing methods datasets Section finally, Section conclude discussion.  Related Work Even syntic review optimization algorithms large-scale machine learning page limits paper. here, emphasize method choice depends crucially amount data used, dimensionality., respectively, parameters). paper, focus regime large large make gradient computations order) matrix manipulations order prohibitive. online algorithms option choice large computation update independent case Stochastic Gradient Descent (sgd), descent direction formed randomly selected gradient. improvements SGD developed incorporating previous gradient directions current update equation [srb13, bot10, dhs11]. batch algorithms, hand, achieve faster convergence exploit order information. competitive intermediate Several methods category aim quadratic, super-linear convergence rates. particular, quasi-newton methods proven effective [bis95, nes04]. anor approach goal utilize sub-sampling form approximate Hessian [mar10, bcnn11, vp12, \\x0cerd15]. sub-sampled Hessian close true hessian, methods approach terms convergence rate, neverless, enjoy Algorithm NewSamp input:   define: (?) argmin Euclidean projection TruncatedSVDk) rank truncated SVD     sub-sample set indices ]. Let HSt —s1t i2st?   truncatedsvdr (hst        end output:  smaller complexity update. convergence rate analysis methods: analysis main contribution paper. knowledge, result direction proven [bcnn11] estabilishes asymptotic convergence quantitative bounds (exploiting general ory [gns09]). furr improvements subsampling algorithms, common approach Conjugate Gradient) methods and Krylov sub-spaces [mar10, bcnn11, vp12]. lastly, hybrid algorithms combine techniques increase performance. examples include, sub-sampling quasi-newton [bhns14], SGD [fs12], NGD [lrf10], NGD low-rank approximation [lrmb08]. NewSamp newton-sampling method rank thresholding regime consider, main drawbacks classical order methods newton method. dominant issue computation Hessian matrix, requires(np2 operations, issue inverting hessian, requires computation. sub-sampling effective eﬃcient tackling issue. recent empirical studies show sub-sampling Hessian significant improvement terms computational cost, preserves fast convergence rate order methods [mar10, vp12]. uniform subsample used, sub-sampled Hessian random matrix expected true hessian, considered sample estimator mean. recent advances statistics shown performance estimators significantly improved simple procedures shrinkage and thresholding [ccs10, dgj13]. extent, low-rank approximation important order information generally contained largest eigenvalues/vectors hessian. newsamp presented Algorithm iteration step sub-sampled set indices, size sub-sampled Hessian denoted HSt respectively. assuming functions convex, eigenvalues symmetric matrix HSt non-negative. refore, SVD eigenvalue decomposition coincide. operation TruncatedSVDk (hst rank approximation., takes HSt input returns largest eigenvalues eigenvectors this procedure requires(kp2 computation [hmt11]. operator projects current iterate feasible set Euclidean projection. assume \\x0cthis projection eﬃciently. construct curvature matrix basic rank approximation, fill eigenvalues eigenvalue sub-sampled Hessian largest eigenvalue threshold. compute truncated SVD operation results  simply sum scaled identity matrix rank matrix. note low-rank approximation suggested improve curvature estimation furr utilized reduce cost computing inverse matrix. final per-iteration cost NewSamp  newsamp takes parameters inputs. discuss Section, choose optimally, based ory Section Convergence Rate Convergence Coeﬃcients Value log(error sub?sample size NewSamp 100 NewSamp 200 NewSamp 500 200 Iterations Coeﬃcient linear quadratic 400 600 Rank Figure Left plot demonstrates convergence rate NewSamp starts quadratic rate transitions linear convergence true minimizer. plot shows effect eigenvalue thresholding convergence coeﬃcients scaling constant. -axis shows number eigenvalues. plots obtained Covertype dataset. construction NewSamp descent algorithm. enjoys quadratic convergence rate start transitions linear rate neighborhood minimizer. this behavior observed Figure left plot Figure shows convergence behavior NewSamp sub-sample sizes. observe large sub-samples result convergence rates expected. sub-sample size increases, slope linear phase decreases, closer quadratic phase. explain phenomenon Section orems. plot Figure demonstrates coeﬃcients phases depend thresholded rank. coeﬃcient quadratic phase increases rank threshold, linear phase, relation reversed. oretical results section, provide convergence analysis NewSamp based subsampling schemes: Independent sub-sampling: iteration uniformly sampled}, independently sets?  replacement. : Sequentially dependent sub-sampling: iteration sampled], based distribution depend previous sets?  randomness data. sub-sampling scheme simple commonly optimization. one drawback sub-sampled set current iteration independent previous sub-samples, samples previously form approximate curvature information. order prevent cycles obtain performance optimum, increase sample size iteration advances [mar10], including previously unused samples. this process results sequence dependent sub-samples falls subsampling scheme. oretical analysis, make assumptions: Assumption (lipschitz continuity). for subset — depending size?, khs (?)  —  assumption (bounded hessian). ? (?) upper bounded constant., max? (?)   Independent sub-sampling section, assume ] fact, stochastic sampled sub-sampling scheme. algorithms assume uniform subset], case sub-sampled Hessian unbiased estimator full hessian. that, [hst (?)] ] expectation randomness show scaling matrix formed sub-samples iterations form. ) composite convergence rate., combination linear quadratic phases. Lemma. assume parameter set convex ] based sub-sampling scheme suﬃciently large. furr, Assumptions hold absolute constant probability, updates form. ) satisfy         coeﬃcients defined HSt log remark initial point close algorithm start quadratic rate convergence transform linear rate close neighborhood optimum. lemma holds matrix particular, choose HSt1 obtain bound simple sub-sampled Hessian method. case, coeﬃcients depend kqt smallest eigenvalue subsampled hessian. note arbitrarily small blow coeﬃcients. following, NewSamp remedies issue. orem. let assumptions Lemma hold. denote eigenvalue HSt  NewSamp iteration step step size satisfies  have, probability,        k22 absolute constant coeﬃcients defined log  NewSamp composite convergence rate coeﬃcients linear quadratic terms, (see plot Figure). observe sub-sampling size significant effect linear term, quadratic term governed Lipschitz constant. emphasize case feasible conditions orem.  Sequentially dependent sub-sampling here, assume subsampling scheme generate?   distribution sub-sampled sets depend, randomness dataset. examples include fixed sub-samples sub-samples increasing size, sequentially covering unused data. addition Assumptions, assume following. assumption. observations). let. observations distribution For fixed ], assume functions satisfy (?)  function  Most statistical learning algorithms formulated above., classification problems, access. samples denote class label covariate, measures classification error (see Section examples). for sub-sampling scheme, analogue Lemma stated Appendix Lemma, leads result. orem. assume parameter set convex ] based sub-sampling scheme. furr, Assumptions hold, surely. conditioned event {?? }, step size satisfies. , NewSamp iteration probability),         coeﬃcients defined diam log absolute constants  denotes eigenvalue HSt   Compared orem, observe coeﬃcient quadratic term change. this due Assumption however, bound linear term worse, uniform bound convex parameter set Dependence coeﬃcients convergence guarantees coeﬃcients depend iteration step undesirable aspect results. however, constants approximated analogues? ? evaluated optimum defined simply replacing definition eigenvalue full-hessian  for sake simplicity, case functions  (?) quadratic. orem. assume functions (?) quadratic, based scheme let full Hessian lower bounded suﬃciently large absolute constants probability log   log orem implies that, sub-sampling size suﬃciently large concentrate?  generalizing orem nonquadratic functions straightforward, case, additional terms involving difference   case scheme, fixed sub-samples, coeﬃcient depend corollary suﬃcient condition convergence. detailed discussion number iterations convergence furr local convergence properties found [erd15, em15]. corollary. assume well-approximated? ? error bound ? orem. for initial point suﬃcient condition convergence?    ?  Choosing algorithm parameters Step size: Let(log —). suggest step size NewSamp iteration  Note) upper bound orems minimizes component terms linearly depend compensate that, shrink) contrary algorithms, optimal step size NewSamp larger rigorous derivation.  found [em15]. sample size: orem, sub-sample size/ log)) suﬃcient obtain small coeﬃcient linear phase. also note sub-sample size scales quadratically condition number. rank threshold: For full-hessian effective rank (trace divided largest eigenvalue), suﬃces log)) samples [ver10]. effective rank upper bounded dimension hence, log) samples approximate full-hessian choose rank threshold retains important curvature information.  Examples Generalized Linear Models (glm) Maximum likelihood estimation GLM setting equivalent minimizing negative loglikelihood ‘(? minimize (?) (hxi) hxi cumulant generating function, denote rows design matrix coeﬃcient vector. here, denotes product vectors function defines type glm) ordinary squares (ols) log logistic regression). using results Section perform convergence analysis algorithm GLM problem. Corollary. let ] uniform sub-sample, parameter set. assume derivative cumulant generating function) bounded Lipschitz continuous Lipschitz constant.pfurr, assume covariates contained ball radius. maxi2] kxi   NewSamp constant step size iteration probability,  constants defined    absolute constant crx  log k22 LRx ith eigenvalue HSt  support Vector Machines (svm) linear SVM separating hyperplane maximizes margin., distance hyperplane support vectors. although vast majority literature focuses dual problem [ss02], SVMs trained primal well. since dual problem scale number data points (some approaches complexity) primal better-suited optimization linear SVMs [cha07]. primal problem linear SVM written minimize (?) ?k22 denote data samples, defines separating hyperplane, loss function. commonly loss functions include hinge loss, Huber loss smood versions [cha07]. smoothing approximating losses stable functions crucial optimization. case NewSamp requires loss function differentiable (almost everywhere), suggest eir smood Huber loss, hinge loss [cha07]. case hinge loss) max} combining offset normal vector hyperplane single parameter vector denoting SVt set indices support vectors iteration write hessian? (?)  xti SVt}. —svt i2svt When —svt large, problem falls setup solved eﬃciently newsamp. note unlike GLM setting, Lipschitz condition orems apply here. however, empirically demonstrate NewSamp works assumptions. Experiments section, validate performance NewSamp numerical studies. experimented optimization problems, namely, Logistic Regression) svm. minimizes.  logistic function, SVM minimizes.  hinge loss. following, brieﬂy describe algorithms experiments: gradient Descent), iteration, takes step proportional negative full gradient evaluated current iterate. under regularity conditions, exhibits linear convergence rate.  accelerated Gradient Descent (agd) proposed Nesterov [nes83], improves gradient descent momentum term.  newton Method) achieves quadratic convergence rate utilizing inverse Hessian evaluated current iterate.  broyden-fletcher-goldfarb-shanno (bfgs) popular stable quasi-newton method. formed accumulating information iterates gradients.  limited Memory BFGS-bfgs) variant bfgs, recent iterates gradients construct providing improvement terms memory usage.  stochastic Gradient Descent (sgd) simplified version where, iteration, randomly selected gradient used. follow guidelines [bot10] step size. dataset:) syn) Logistic regression, rank msd) Logistic regression, rank log(error) Method NewSamp BFGS LBFGS Newton AGD SGD AdaGrad time(sec) log(error) log(error)slices) Logistic regression, rank Method NewSamp BFGS LBFGS Newton AGD SGD AdaGrad svm, rank time(sec) Method NewSamp BFGS LBFGS Newton AGD SGD AdaGrad svm, rank time(sec) svm, rank Method NewSamp BFGS LBFGS Newton AGD SGD AdaGrad time(sec 100 log(error) log(error) log(error) Method NewSamp BFGS LBFGS Newton AGD SGD AdaGrad time(sec) Method NewSamp BFGS LBFGS Newton AGD SGD AdaGrad time(sec) 120 Figure Performance algorithms datasets. newsamp represented red color  adaptive Gradient Scaling (adagrad) adaptive learning rate based previous gradients. adagrad significantly improves performance stability SGD [dhs11]. for batch algorithms, constant step size algorithms, step size fastest convergence chosen. for stochastic algorithms, optimized parameters define step size. parameters NewSamp selected guidelines Section. experimented datasets Table each dataset consists design matrix observations (classes) syntic data generated multivariate Gaussian distribution. methodological choice, selected moderate values newton method implemented, neverless demonstrate improvement. for larger values comparison favorable approach. effects sub-sampling size rank threshold demonstrated Figure comparison aforementioned optimization techniques presented Figure case, observe stochastic methods enjoy fast convergence start, slows epochs. algorithm close NewSamp terms performance bfgs. case svm, closest algorithm NewSamp note global convergence BFGS [nes04]. condition super-linear rate which, initial point close optimum required [dm77]. this condition rarely satisfied practice, affects performance order methods. for newsamp, rank thresholding level robustness, found initial point important factor. details Figure additional experiments found Appendix Dataset slices Covertype MSD Syntic 53500 581012 515345 500000 386 300 Reference [gks, lic13] [bd99, lic13] [mewl, lic13] table Datasets experiments. Conclusion paper, proposed sub-sampling based order method utilizing low-rank Hessian estimation. proposed method target regime complexity per-iteration. showed convergence rate NewSamp composite widely sub-sampling schemes., starts quadratic convergence transforms linear convergence optimum. convergence behavior sub-sampling schemes interesting line research. numerical experiments demonstrate performance proposed algorithm compared classical optimization methods.',\n",
       " 'PP5985': 'matrix factorization) techniques emerged powerful tool perform collaborative filtering large datasets]. algorithms decompose partially-observed matrix product smaller matrices,    variety-based methods proposed literature successfully applied domains. promise, challenges faced methods recommending user/item arrives system, problem coldstart. anor challenge recommending items online setting quickly adapting user feedback required real world applications including online advertising, serving person paper, alized content, link prediction product recommendations. address challenges problem online low-rank matrix completion combining matrix completion bandit algorithms. setting introduced previous work] work satisfactory solution problem. bandit setting, model problem repeated game environment chooses row learning agent chooses column rij revealed goal learning agent) minimize cumulative regret respect optimal solution, highest entry row key design principle bandit setting balance exploration exploitation solves problem cold start naturally. example, online advertising, exploration implies presenting ads, observing subsequent feedback, exploitation entails serving ads attract high click rate. while solutions proposed bandit problems, years, renewed interest Thompson sampling) originally proposed 1933]. addition competitive empirical performance, attractive due conceptual simplicity. agent choose action (column) set actions maximize reward certainty action optimal. , agent select probability action.  denotes unknown parameter governing reward structure history observations agent. agent chooses probability, max ? implemented simply sampling posterior arg maxa0 ?]. realistic scenarios (including matrix completion), sampling computationally eﬃcient recourse approximate methods required make practical. propose computationally-eﬃcient algorithm solving problem, call Particle Thompson sampling matrix factorization (pts). pts combination particle filtering online Bayesian parameter estimation non-conjugate case posterior closed form. particle filtering set weighted samples (particles) estimate posterior density. order overcome problem huge parameter space, utilize rao-blackwellization design suitable Monte Carlo kernel computationally statistically eﬃcient update set particles data arrives online fashion. unlike prior work] approximates posterior latent item features single point estimate, approach maintain approximation posterior latent features diverse set particles. results real datasets show substantial improvement cumulative regret vis-vis online methods. probabilistic Matrix Factorization review probabilistic matrix factorization approach low \\x0crank matrix completion problem. matrix completion, portion matrix (rij observed, goal infer unobserved entries probabilistic matrix factorization (pmf], assumed noisy perturbation termed rank matrix user item latent features typically small). full generative model PMF.  .   Rij¿ ) Figure Graphical model probabilistic matrix factoriza2 variances  parameters model. tion model full Bayesian treatment variances drawn inverse Gamma prior (while held fixed.,  (?,   (?, (this special case Bayesian PMF] isotropic gaussians generative model, observed ratings estimate parameters ?complete? matrix PMF MAP point-estimate finds maximize (stochastic) gradient ascend (alternate square]). bayesian PMF] attempts approximate full posterior ?). joint posterior intractable; however, structure graphical model (fig. exploited derive eﬃcient Gibbs sampler. rij.  provide expressions conditional probabilities interest. supposed known. vectors independent user rts—rij set items rated user observe ratings {rij rts)} generated. ] considers full covariance structure, noted isotropic Gaussians effective enough. simple conditional linear Gaussian model. thus, posterior closed form  —vrts,rts) rij   ?rts?rts) conditional posterior, similarly factorized precision similarly defined. posterior precision (and simiarly obtained conjugacy Gamma prior isotropic Gaussian,   ?). ) Although required Bayesian pmf, give likelihood expression(rij,  ) advantage Bayesian approach uncertainty estimate crucial exploration bandit setting. however, bandit setting requires maitaining online estimates posterior ratings arrive time makes rar awkward mcmc. paper, employ sequential monte-carlo (smc) method online Bayesian inference]. similar Gibbs sampler], exploit closed form updates design eﬃcient rao-blackwellized particle filter] maintaining posterior time. matrix-factorization Recommendation Bandit typical deployed recommendation system, users observed ratings (also called rewards) arrive time, task system recommend item user maximize accumulated expected rewards. bandit setting arises fact system learn time items ratings (for user) recommend, time suﬃciently explore items. formulate matrix factorization bandit follows. assume ratings generated. ) fixed unknown latent features   time environment chooses user system (learning agent) recommend item user rates recommended item rating rit  agent receives rating reward. abbreviate rto rit system recommends item policy takes account history observed ratings prior time  highest expected reward system earn time maxj achieved optimal item ) arg maxj? ? recommended.   unknown, optimal item ) priori. quality recommendation system measured expected cumulative regret:  rit  max Uit expectation respect choice user time randomness choice recommended items algorithm.  Particle Thompson Sampling Matrix Factorization Bandit While diﬃcult optimize cumulative regret directly, shown work practice contextual linear bandit]. matrix factorization bandit, main diﬃculty incrementally update posterior latent features, control reward structure. subsection, describe eﬃcient rao-blackwellized particle filter (rbpf) designed exploit specific structure probabilistic matrix factorization model.  (?, control parameters posterior time ?). standard Algorithm Particle Thompson Sampling Matrix Factorization (pts) Global control params: Bayesian version (pts):  initializeparticles()     current user Sample    pts]     sample   sample due rao-blackwellization arg maxj Recommend user observe rating rto  updateposterior(?  end procedure PDATE osterior(? )  structure, particles) particles) , ) )  . ) reweighting. , (rij. ,  .particles] ?.particles  resampling move) ) ) rvj) .     pts] Update norm ) ) )     )  pts   : end: return: end procedure . ) particle filter sample parameters, unfortunately, experiments, degeneracy highly problematic vanilla particle filter) assumed (see fig. )). RBPF algorithm maintains posterior distribution follows. particle conceptually represents point-mass integrated analytically possible thus, approximated  number particles. crucially, particle filter estimate set non-time-vayring parameters, effective eﬃcient mcmc-kernel move stationary. essential. design move kernel based observations. first, make auxiliary variables, effectively sampling, , , however, move highly ineﬃcient due number variables sampled update. observation key eﬃcient implementation. note latent features users current user independent current observed rating rto, refore time resample Uit resample furrmore, suﬃces resample latent feature current item \\x0cvjt leads eﬃcient implementation RBPF particle fact stores3, auxiliary variables, kernel move sample Uit, Vj0t,   ) PTS algorithm algo.  time complexity  maximum number users rated item maximum When fewer users items, similar strategy derived integrate instead. inconsistent previous statement conceptually particle represents pointmass distribution number items rated user, respectively. dependency arises invert precision matrix, concern rank typically small. line replaced incremental update caching: line, incrementally update item previously rated current user reduces complexity ), potentially significant improvement real recommendation systems user rate small number items. analysis regret PTS bounded. however, existing work bandits provide suﬃcient tools proper analysis algorithm. particular, existing techniques provide(log gap-independent) regret bounds problem, bounds typically linear number entries observation matrix linear number users), typically large, compared thus, ideal regret bound setting sublinear dependency dependency all) number users. key obstacle achieving that, conditional posteriors gaussians, neir marginal joint posteriors belong behaved classes., conjugate posteriors, closed forms). thus, tools, handle generic posteriors, needed eﬃcient analysis. moreover, general setting, correlation latent features non-linear (see] details). existing techniques typically designed eﬃciently learning linear regressions, suitable problem. neverless, show bound regret specific case rank matrices, leave generalization results future work. particular, analyze regret PTS setting Gopalan. ]. model problem follows. parameter space ,   ,   discretizations parameter spaces rank factors integer. sake oretical analysis, assume PTS sample full posterior. assume ?       note setting, highest-rated item expectation users. denote item arg max? assume uniquely optimal =  leverage properties analysis. random variable time pair random rating matrix random row   action time column   observation rit bound regret PTS follows. orem   , ), exists pts recom1+ mends items   steps log times probability constant independent proof. orem Gopalan. ], number recommendations bounded(log constant independent bound(log counting number times PTS selects models distinguished?  observing optimal action  let,   ??  maxk6 set models action optimal. suppose algorithm chooses model,  divergence distributions ratings models?  bounded ?      inequality fact ? ?? ?  uniquely optimal?    ?  granularity discretization Let row indices. divergence distributions ratings positions),    models?   DKL (uit?    orem Gopalan. ], models, chosen PTS steps DKL (uit?  log DKL?      log selections,  apply argument total, sum regrets.  remarks: Note orem implies log regret bound holds high probability. here, plays role gap smallest difference expected ratings item row sense, result log similar magnitude results Gopalan. ]. restrict    proof, affect algorithm. fact, proof focuses high probability events samples posterior concentrated true parameters, thus well. extending proof general setting trivial. particular, moving discretized parameters continuous space introduces abovementioned ill behaved posteriors. increasing violate fact item users, allowed eliminate regret bound. experiments Results goal experimental evaluation twofold) evaluate PTS algorithm making online recommendations respect baseline algorithms real-world datasets) understand qualitative performance intuition pts.  Dataset description syntic dataset real world datasets evaluate approach. syntic dataset generated generate user item latent features rank drawing Gaussian distribution respectively. true rating matrix  generate observed rating matrix adding Gaussian noise, true ratings. real world datasets follows: Movielens 100k, Movielens, Yahoo Music4 Book crossing5 EachMovie shown Table users items ratings Movielens 100k Movielens Yahoo Music Book crossing 943 6040 15400 6841 1682 3900 1000 5644 100k 311,704 90k Table Characteristics datasets study EachMovie 36656 1621.58m Baseline measures current approaches simultaneously learn user item factors sampling posterior bandit setting. algorithms, choose kinds baseline methods sequentially updates posterior user features fixing item features point estimate (icf) anor updates MAP estimates user item features stochastic gradient descent (sgd-eps). key challenge online algorithms unbiased oﬄine evaluation. problem oﬄine setting partial information user feedback., information items user rated. experiment, restrict recommendation space algorithms recommend items user rated entire dataset makes empirically measure regret interaction. baseline measures follows: Random iteration, recommend random movie user. most Popular iteration, recommend popular movie restricted movies rated user dataset. note unrealistically optimistic baseline online algorithm global popularity items beforehand. icf: ICF algorithm] proceeds estimating user item latent factors initial training period interaction reafter updates user features assuming item features fixed. run scenarios ICF algorithm% (icf% (icf) data training period respectively. period training, randomly recommend movie user compute regret. pmf implementation] estimating sgdeps: learn latent factors online variant PMF algorithm]. stochastic gradient descent update latent factors mini-batch size. order make recommendation, -greedy \\x0cstrategy recommend highest probability make random recommendations orwise. set experiments.) http://webscope.sandbox.yahoo.com/ http://www.bookcrossing.com Results Syntic Dataset Iterations 450 180 400 160 350 140 120 100 300 250 200 150 100 100 200 100 200 300 Iterations 400 500 200 400 600 Iterations 800 120 140 100 120 100 Cummulative Regret Cummulative Regret Cummulative Regret Cummulative Regret \\x0ccummulative Regret generated syntic dataset mentioned earlier run PTS algorithm 100 particles recommendations. simulate setting mentioned Section assume time random user arrives system recommends item user rates recommended item rit evaluate performance model computing expected cumulative regret defined). fig. shows cumulative regret algorithm syntic data averaged 100 runs size matrix latent features cumulative regret increases sub-linearly number interactions confidence approach works syntic dataset.  1000 Iterations 100 100 Iterations Figure Cumulative regret sizes syntic data averaged 100 runs. results Real Datasets PTS random popular icf icf sgd?eps pts Cummulative Regret Cummulative Regret PTS random popular icf icf sgd?eps pts PTS random popular icf icf sgd?eps pts Cummulative Regret Iterations Iterations) Movielens 100k PTS random popular icf sgd?eps pts Iterations) Yahoo Music Cummulative Regret Cummulative Regret) Movielens PTS random popular icf icf sgd?eps pts Iterations) Book Crossing Iterations) EachMovie Figure Comparison baseline methods datasets. next, evaluate algorithms real datasets compare baseline algorithms. subtract ratings data centre zero. simulate extreme cold-start scenario start empty set user rating. iterate datasets assume random user arrived time system recommends item constrained items rated user dataset. algorithms particles approach. PTS set  pts (bayesian version, algo. details), set  initial shape parameters Gamma distribution  . icf icf, set  fig. shows cumulative regret algorithms datasets6 approach performs significantly compared baseline algorithms diverse set datasets. pts parameter tuning performs slightly PTS achieves regret. important note PTS pts performs comparable ?most popular? baseline knowing global popularity advance. note ICF sensitive length initial training period; clear set apriori. icf fails run Bookcrossing dataset% data sparse PMF implementation. test error pmf icf pts pts?100 MSE MSE ?200 200 400 600 Iterations 1000) Movielens 800 1000 Iterations) particle filter 100) Movie feature vector Figure shows MSE movielens dataset, red line MSE PMF algorithm shows performance RBPF (blue line) compared vanilla (red line) syntic dataset shows movie feature vectors movie 384 ratings, red dot feature vector icf algorithm (using ratings). pts feature vector% data (green dots) pts-100 100% (blue dots). evaluate performance model oﬄine setting follows: divide datasets training test set iterate training data triplets pretending movie recommended approach update latent factors rbpf.  average prediction particles time compute recovered matrix step compute squared error (mse) test dataset iteration. unlike batch method PMF takes multiple passes data, method designed bounded update complexity iteration. ran algorithm% data training rest testing computed MSE averaging results runs. fig. ) shows average MSE movielens dataset. MSE.7925) comparable PMF MSE.7718) shown red line. demonstrates RBPF performing matrix factorization. addition, fig. ) shows syntic dataset, vanilla suffers degeneration high variance. understand intuition fixing latent item features ICF work, perform experiment follows: run ICF algorithm movielens 100k dataset% data training. point ICF algorithm fixes item features updates user features next, run algorithm obtain latent features. examined features selected movie particles time intervals ICF algorithm fixes% anor end shown fig. ). shows movie features evolved location fixing early good idea. related Work Probabilistic matrix completion bandit setting setting introduced previous work Zhao. ]. icf algorithm] approximates posterior latent item features single point estimate. bandit algorithms recommendations proposed. valko. ] proposed bandit algorithm content-based recommendations. approach, features items extracted similarity graph items, advance. preferences user features learned independently regressing ratings items features. key difference approach learn features items. words, learn user item factors] learn kocak. ] combine spectral bandit algorithm. gentile. ] propose bandit algorithm recommendations clusters users online fashion based similarity preferences. preferences learned regressing ratings items features. features items input learning algorithm learn maillard. ] study bandit problem arms partitioned unknown clusters unlike work general. conclusion proposed eﬃcient method carrying matrix factorization bandit setting. key novelty approach \\x0cbined rao-blackwellized particle filtering Thompson sampling (pts) matrix factorization recommendation. simultaneously update posterior probability online manner minimizing cumulative regret. state art, till now, eir point estimates point estimate factor., update posterior probability pts results substantially performance wide variety real world data sets. Matrix factorization) techniques emerged powerful tool perform collaborative filtering large datasets]. algorithms decompose partially-observed matrix product smaller matrices,    variety-based methods proposed literature successfully applied domains. despite promise, challenges faced methods recommending user/item arrives system, problem coldstart. anor challenge recommending items online setting quickly adapting user feedback required real world applications including online advertising, serving person paper, alized content, link prediction product recommendations. address challenges problem online low-rank matrix completion combining matrix completion bandit algorithms. this setting introduced previous work] work satisfactory solution problem. bandit setting, model problem repeated game environment chooses row learning agent chooses column Rij revealed goal learning agent) minimize cumulative regret respect optimal solution, highest entry row key design principle bandit setting balance exploration exploitation solves problem cold start naturally. for example, online advertising, exploration implies presenting ads, observing subsequent feedback, exploitation entails serving ads attract high click rate. While solutions proposed bandit problems, years, renewed interest Thompson sampling) originally proposed 1933]. addition competitive empirical performance, attractive due conceptual simplicity. agent choose action (column) set actions maximize reward certainty action optimal. following, agent select probability action. let denotes unknown parameter governing reward structure history observations agent. agent chooses probability, max ? implemented simply sampling posterior arg maxa0 ?]. however realistic scenarios (including matrix completion), sampling computationally eﬃcient recourse approximate methods required make practical. propose computationally-eﬃcient algorithm solving problem, call Particle Thompson sampling matrix factorization (pts). pts combination particle filtering online Bayesian parameter estimation non-conjugate case posterior closed form. particle filtering set weighted samples (particles) estimate posterior density. order overcome problem huge parameter space, utilize rao-blackwellization design suitable Monte Carlo kernel computationally statistically eﬃcient update set particles data arrives online fashion. unlike prior work] approximates posterior latent item features single point estimate, approach maintain approximation posterior latent features diverse set particles. our results real datasets show substantial improvement cumulative regret vis-vis online methods. Probabilistic Matrix Factorization review probabilistic matrix factorization approach low \\x0crank matrix completion problem. matrix completion, portion matrix (rij observed, goal infer unobserved entries probabilistic matrix factorization (pmf], assumed noisy perturbation termed rank matrix user item latent features typically small). full generative model PMF.  .   Rij¿ ) Figure Graphical model probabilistic matrix factoriza2 variances  parameters model. tion model full Bayesian treatment variances drawn inverse Gamma prior (while held fixed.,  (?,   (?, (this special case Bayesian PMF] isotropic gaussians given generative model, observed ratings estimate parameters ?complete? matrix PMF MAP point-estimate finds maximize (stochastic) gradient ascend (alternate square]). bayesian PMF] attempts approximate full posterior ?). joint posterior intractable; however, structure graphical model (fig. exploited derive eﬃcient Gibbs sampler. rij.  provide expressions conditional probabilities interest. supposed known. vectors independent user let rts—rij set items rated user observe ratings {rij rts)} generated. ] considers full covariance structure, noted isotropic Gaussians effective enough. simple conditional linear Gaussian model. thus, posterior closed form  —vrts,rts) rij   ?rts?rts) conditional posterior, similarly factorized precision similarly defined. posterior precision (and simiarly obtained conjugacy Gamma prior isotropic Gaussian,   ?). ) Although required Bayesian pmf, give likelihood expression(rij,  ) advantage Bayesian approach uncertainty estimate crucial exploration bandit setting. however, bandit setting requires maitaining online estimates posterior ratings arrive time makes rar awkward mcmc. paper, employ sequential monte-carlo (smc) method online Bayesian inference]. similar Gibbs sampler], exploit closed form updates design eﬃcient rao-blackwellized particle filter] maintaining posterior time. matrix-factorization Recommendation Bandit typical deployed recommendation system, users observed ratings (also called rewards) arrive time, task system recommend item user maximize accumulated expected rewards. bandit setting arises fact system learn time items ratings (for user) recommend, time suﬃciently explore items. formulate matrix factorization bandit follows. assume ratings generated. ) fixed unknown latent features   time environment chooses user system (learning agent) recommend item user rates recommended item rating rit  agent receives rating reward. abbreviate rto rit system recommends item policy takes account history observed ratings prior time  highest expected reward system earn time maxj achieved optimal item ) arg maxj? ? recommended. since  unknown, optimal item ) priori. quality recommendation system measured expected cumulative regret:  rit  max Uit expectation respect choice user time randomness choice recommended items algorithm.  Particle Thompson Sampling Matrix Factorization Bandit While diﬃcult optimize cumulative regret directly, shown work practice contextual linear bandit]. matrix factorization bandit, main diﬃculty incrementally update posterior latent features, control reward structure. subsection, describe eﬃcient rao-blackwellized particle filter (rbpf) designed exploit specific structure probabilistic matrix factorization model. let (?, control parameters posterior time ?). standard Algorithm Particle Thompson Sampling Matrix Factorization (pts) Global control params: Bayesian version (pts):  initializeparticles()     current user Sample    pts]     sample   sample due rao-blackwellization arg maxj Recommend user observe rating rto  updateposterior(?  end procedure PDATE osterior(? )  structure, particles) particles) , ) )  . ) reweighting. , (rij. ,  .particles] ?.particles  resampling move) ) ) rvj) .     pts] Update norm ) ) )     )  pts   : end: return: end procedure . ) particle filter sample parameters, unfortunately, experiments, degeneracy highly problematic vanilla particle filter) assumed (see fig. )). our RBPF algorithm maintains posterior distribution follows. each particle conceptually represents point-mass integrated analytically possible thus, approximated  number particles. crucially, particle filter estimate set non-time-vayring parameters, effective eﬃcient mcmc-kernel move stationary. essential. our design move kernel based observations. first, make auxiliary variables, effectively sampling, , , however, move highly ineﬃcient due number variables sampled update. our observation key eﬃcient implementation. note latent features users current user independent current observed rating rto, refore time resample Uit resample furrmore, suﬃces resample latent feature current item \\x0cvjt this leads eﬃcient implementation RBPF particle fact stores3, auxiliary variables, kernel move sample Uit, Vj0t,   ) PTS algorithm algo.  time complexity  maximum number users rated item maximum When fewer users items, similar strategy derived integrate instead. this inconsistent previous statement conceptually particle represents pointmass distribution number items rated user, respectively. dependency arises invert precision matrix, concern rank typically small. line replaced incremental update caching: line, incrementally update item previously rated current user this reduces complexity ), potentially significant improvement real recommendation systems user rate small number items. Analysis regret PTS bounded. however, existing work bandits provide suﬃcient tools proper analysis algorithm. particular, existing techniques provide(log gap-independent) regret bounds problem, bounds typically linear number entries observation matrix linear number users), typically large, compared thus, ideal regret bound setting sublinear dependency dependency all) number users. key obstacle achieving that, conditional posteriors gaussians, neir marginal joint posteriors belong behaved classes., conjugate posteriors, closed forms). thus, tools, handle generic posteriors, needed eﬃcient analysis. moreover, general setting, correlation latent features non-linear (see] details). existing techniques typically designed eﬃciently learning linear regressions, suitable problem. neverless, show bound regret specific case rank matrices, leave generalization results future work. particular, analyze regret PTS setting Gopalan. ]. model problem follows. parameter space ,   ,   discretizations parameter spaces rank factors integer. for sake oretical analysis, assume PTS sample full posterior. assume ?       note setting, highest-rated item expectation users. denote item arg max? assume uniquely optimal =  leverage properties analysis. random variable time pair random rating matrix random row   action time column   observation rit bound regret PTS follows. orem for  , ), exists PTS recom1+ mends items   steps log times probability constant independent proof. orem Gopalan. ], number recommendations bounded(log constant independent now bound(log counting number times PTS selects models distinguished?  observing optimal action  let,   ??  maxk6 set models action optimal. suppose algorithm chooses model,  divergence distributions ratings models?  bounded ?      inequality fact ? ?? ?  uniquely optimal?    ?  granularity discretization Let row indices. divergence distributions ratings positions),    models?   DKL (uit?    orem Gopalan. ], models, chosen PTS steps DKL (uit?  log this DKL?      log selections,  now apply argument total, sum regrets.  remarks: Note orem implies log regret bound holds high probability. here, plays role gap smallest difference expected ratings item row sense, result log similar magnitude results Gopalan. ]. while restrict    proof, affect algorithm. fact, proof focuses high probability events samples posterior concentrated true parameters, thus well. extending proof general setting trivial. particular, moving discretized parameters continuous space introduces abovementioned ill behaved posteriors. while increasing violate fact item users, allowed eliminate regret bound. Experiments Results goal experimental evaluation twofold) evaluate PTS algorithm making online recommendations respect baseline algorithms real-world datasets) understand qualitative performance intuition pts.  Dataset description syntic dataset real world datasets evaluate approach. syntic dataset generated generate user item latent features rank drawing Gaussian distribution respectively. true rating matrix  generate observed rating matrix adding Gaussian noise, true ratings. real world datasets follows: Movielens 100k, Movielens, Yahoo Music4 Book crossing5 EachMovie shown Table users items ratings Movielens 100k Movielens Yahoo Music Book crossing 943 6040 15400 6841 1682 3900 1000 5644 100k 311,704 90k Table Characteristics datasets study EachMovie 36656 1621.58m Baseline measures current approaches simultaneously learn user item factors sampling posterior bandit setting. from algorithms, choose kinds baseline methods sequentially updates posterior user features fixing item features point estimate (icf) anor updates MAP estimates user item features stochastic gradient descent (sgd-eps). key challenge online algorithms unbiased oﬄine evaluation. one problem oﬄine setting partial information user feedback., information items user rated. experiment, restrict recommendation space algorithms recommend items user rated entire dataset makes empirically measure regret interaction. baseline measures follows: Random iteration, recommend random movie user. Most Popular iteration, recommend popular movie restricted movies rated user dataset. note unrealistically optimistic baseline online algorithm global popularity items beforehand. icf: ICF algorithm] proceeds estimating user item latent factors initial training period interaction reafter updates user features assuming item features fixed. run scenarios ICF algorithm% (icf% (icf) data training period respectively. during period training, randomly recommend movie user compute regret. PMF implementation] estimating sgdeps: learn latent factors online variant PMF algorithm]. stochastic gradient descent update latent factors mini-batch size. order make recommendation, -greedy \\x0cstrategy recommend highest probability make random recommendations orwise. set experiments.) http://webscope.sandbox.yahoo.com/ http://www.bookcrossing.com Results Syntic Dataset Iterations 450 180 400 160 350 140 120 100 300 250 200 150 100 100 200 100 200 300 Iterations 400 500 200 400 600 Iterations 800 120 140 100 120 100 Cummulative Regret Cummulative Regret Cummulative Regret Cummulative Regret \\x0ccummulative Regret generated syntic dataset mentioned earlier run PTS algorithm 100 particles recommendations. simulate setting mentioned Section assume time random user arrives system recommends item user rates recommended item rit evaluate performance model computing expected cumulative regret defined). fig. shows cumulative regret algorithm syntic data averaged 100 runs size matrix latent features cumulative regret increases sub-linearly number interactions confidence approach works syntic dataset.  1000 Iterations 100 100 Iterations Figure Cumulative regret sizes syntic data averaged 100 runs. results Real Datasets PTS random popular icf icf sgd?eps pts Cummulative Regret Cummulative Regret PTS random popular icf icf sgd?eps pts PTS random popular icf icf sgd?eps pts Cummulative Regret Iterations Iterations) Movielens 100k PTS random popular icf sgd?eps pts Iterations) Yahoo Music Cummulative Regret Cummulative Regret) Movielens PTS random popular icf icf sgd?eps pts Iterations) Book Crossing Iterations) EachMovie Figure Comparison baseline methods datasets. next, evaluate algorithms real datasets compare baseline algorithms. subtract ratings data centre zero. simulate extreme cold-start scenario start empty set user rating. iterate datasets assume random user arrived time system recommends item constrained items rated user dataset. algorithms particles approach. for PTS set  for pts (bayesian version, algo. details), set  initial shape parameters Gamma distribution  . for icf icf, set  fig. shows cumulative regret algorithms datasets6 our approach performs significantly compared baseline algorithms diverse set datasets. pts parameter tuning performs slightly PTS achieves regret. important note PTS pts performs comparable ?most popular? baseline knowing global popularity advance. note ICF sensitive length initial training period; clear set apriori. icf fails run Bookcrossing dataset% data sparse PMF implementation. test error pmf icf pts pts?100 MSE MSE ?200 200 400 600 Iterations 1000) Movielens 800 1000 Iterations) particle filter 100) Movie feature vector Figure shows MSE movielens dataset, red line MSE PMF algorithm shows performance RBPF (blue line) compared vanilla (red line) syntic dataset shows movie feature vectors movie 384 ratings, red dot feature vector icf algorithm (using ratings). pts feature vector% data (green dots) pts-100 100% (blue dots). evaluate performance model oﬄine setting follows: divide datasets training test set iterate training data triplets pretending movie recommended approach update latent factors rbpf.  average prediction particles time compute recovered matrix step compute squared error (mse) test dataset iteration. unlike batch method PMF takes multiple passes data, method designed bounded update complexity iteration. ran algorithm% data training rest testing computed MSE averaging results runs. fig. ) shows average MSE movielens dataset. our MSE.7925) comparable PMF MSE.7718) shown red line. this demonstrates RBPF performing matrix factorization. addition, fig. ) shows syntic dataset, vanilla suffers degeneration high variance. understand intuition fixing latent item features ICF work, perform experiment follows: run ICF algorithm movielens 100k dataset% data training. point ICF algorithm fixes item features updates user features next, run algorithm obtain latent features. examined features selected movie particles time intervals ICF algorithm fixes% anor end shown fig. ). shows movie features evolved location fixing early good idea. Related Work Probabilistic matrix completion bandit setting setting introduced previous work Zhao. ]. ICF algorithm] approximates posterior latent item features single point estimate. several bandit algorithms recommendations proposed. valko. ] proposed bandit algorithm content-based recommendations. approach, features items extracted similarity graph items, advance. preferences user features learned independently regressing ratings items features. key difference approach learn features items. words, learn user item factors] learn kocak. ] combine spectral bandit algorithm. gentile. ] propose bandit algorithm recommendations clusters users online fashion based similarity preferences. preferences learned regressing ratings items features. features items input learning algorithm learn maillard. ] study bandit problem arms partitioned unknown clusters unlike work general. Conclusion proposed eﬃcient method carrying matrix factorization bandit setting. key novelty approach \\x0cbined rao-blackwellized particle filtering Thompson sampling (pts) matrix factorization recommendation. this simultaneously update posterior probability online manner minimizing cumulative regret. state art, till now, eir point estimates point estimate factor., update posterior probability pts results substantially performance wide variety real world data sets.',\n",
       " 'PP5995': 'firm relies ability make diﬃcult predictions gain lot large collection data. goal estimate values observations class hyposes describing relationship (for example, linear regression). classic statistical learning ory, goal formalized attempting approximately solve min loss loss(?) inutility function, drawn unknown distribution. present paper concerned case data drawn held central authority inherently distributed. data (disjointly) partitioned set agents, agent privately possessing portion dataset agents obvious incentive reveal data firm seeking. vast swaths data personal email accounts provide massive benefits range companies, example, users typically loa provide account credentials, asked politely. concerned design financial mechanisms provide community agents, holding private set data, incentive contribute solution large learning prediction task. term ?mechanism? algorithmic interface receive answer queries, engage monetary exchange (deposits payouts). aim design mechanism satisfies properties: mechanism eﬃcient approaches solution) amount data participation grows spending constant, fixed total budget.  mechanism incentive-compatible sense agents rewarded contributions provide marginal terms improved hyposes, rewarded bad misleading information.  mechanism reasonable privacy guarantees, agent observer) manipulate mechanism order infer contributions agent ultimately mechanism approach performance learning algorithm direct access data, spending constant budget acquire data improve predictions protecting participants? privacy. construction relies recent surge literature prediction markets], popular time field economics recently studied great detail computer science]. prediction market mechanism designed purpose information aggregation, underlying future event members population private information. instance, elicit predictions team win upcoming sporting event, candidate win election. predictions eventually scored actual outcome event. applying prediction market techniques participants essentially ?trade market? based data. (this approach similar prior work crowdsourcing contests].) members population private information, prediction markets case, data points beliefs goal incentivize reveal aggregate information final hyposis prediction. final profits tied outcome test set data, participant paid accordance information improved performance test set. techniques depart framework] significant aspects) focus problem data aggregation, results advantage kernel methods) mechanisms combine differential privacy guarantees data aggregation prediction-market framework. framework provide eﬃciency truthfulness. show achieve privacy scenarios. give mechanisms prices predictions published satisfy )-differential privacy] respect participant data. mechanism output give reasonable predictions observer infer participant input data. mechanisms Eliciting Aggregating Data give broad description mechanism study. brief, imagine central authority mechanism, market) maintaining \\x0cposis representing current aggregation contributions made far.  returning) participant query cost, evaluating quality predictions privately-held dataset, propose update possibly requires investment ?bet?). bets evaluated close market true data sample generated (analogous test set), payouts distributed quality updates. describing initial framework Mechanism based loosely setting], turn attention special case hyposes lie Reproducing Kernel Hilbert Space (rkhs] kernel(?, ?). kernel-based ?nonparametric mechanism? well-suited problem data aggregation, betting space participants consists essentially updates form data object offered participant ?magnitude? bet. drawback Mechanism lack privacy guarantees betting protocol: utilizing one data make bets investments mechanism lead loss privacy owner data. participant submits bet form sensitive personal information, anor participant infer querying mechanism. primary contributions present work, detailed Section technique productive participation mechanism maintaining guarantee privacy data submitted.  General Template space examples, features labels. mechanism designer chooses function space consisting  assumed Hilbert space structure; view eir hyposis class loss class, measures loss/performance hyposis observation label case refer hyposis, eliding distinction pricing scheme mechanism relies convex cost function (?)  parameterized elements domain set hyposes cost function publicly determined advance. interaction mechanism sequential process querying betting. round mechanism publishes hyposis ?state? market, participants query. participant arrives sequentially, round participant place ?bet?  called ?trade? ?update?, modifying hyposis  finally participation ends mechanism samples reveals) test example1, underlying distribution pays charges) participant relative performance marginal contributions. precisely, total reward participant bet, minus cost  mechanism Market Template ARKET announces  PARTICIPANT query functions, examples, PARTICIPANT submit bet ARKET ARKET    updates state ARKET observes true sample, participant receives payment design cost-function prediction markets area active research past years, starting] furr refinements generalizations]. general idea mechanism eﬃciently provide price quotes function(?) acts potential space outstandings shares] review. present work added additional twist function (?) additional parameterization observation dive deeply oretical aspects generalization, straightforward extension existing ory. key special case: exponential family mechanism. familiar statistics machine learning, natural canonical family problems cast general framework Mechanism call exponential family prediction mechanism]. assume parameterized?   suﬃcient statistics summary function   rrd function evaluation ?, . log exp? log exp?, )idy. words, chosen mechanism encode exponential family model, (?) chosen conditional log partition function distribution market settled function interpret aggregate market belief distribution  , exp?,  (?)) (?) log exp?, . view ?market aggregate? belief? notice trader observes market state bet form   eventual profit,  ?   log)  ., profit precisely conditional log likelihood ratio update   example: Logistic regression. }, set functions , (?¿   construction, log(exp)) exp))) log(exp(?¿ exp(? )),  payoff participant placing bet moves market state outcome:  ? ?¿ log) log(exp(?¿ exp(? )) log) log exp)) This easily extended test set taking average performance test set. simply negative logistic loss parameter choice?. participant wishing maximize profit belief distribution, refore choose logistic regression, arg min log exp)) )  Properties Market describe nice properties Mechanism incentive-compatibility bounded budget. recall that, exponential family markets discussed above, trader moving market hyposis compensated conditional log-likelihood ratio test data point. implication traders incentivized minimize divergence market estimate distribution true underlying distribution. refer property incentive-compatibility traders? interests aligned mechanism designer. property holds generally Mechanism divergence replaced general Bregman divergence Fenchel conjugate proposition appendix details. mechanism make sequence (possibly negative) payments traders, natural question wher potential large downside mechanism terms total payment (budget). context exponential family mechanism, question easy answer: sequence bets moving market state parameter      ?final total loss mechanism corresponds total payouts made traders, ,   log final, worst-case loss worst-case conditional log-likelihood ratio. context logistic regression quantity guaranteed log long initial parameter set  Mechanism generally, tight bounds worst-case loss results prediction markets], give detailed statement Proposition appendix. price sensitivity parameter choosing cost function family important consideration ?scale? quickly market hyposis translate ?instantaneous prices?  (which give marginal cost infinitesimal bet formally, captured price sensitivity defined upper bound operator norm (with respect norm) Hessian cost function (over). choice small translates small worst-case budget required mechanism. however, means market prices sensitive update prices quickly. protecting privacy trader updates Section privacy imposes restrictions price sensitivity.  Nonparametric Mechanism Kernel Methods framework discussed involved general function space ?state? mechanism, contributions participants form modifications functions. downsides generic template participants reason information optimal privately-held dataset   specific class functions parameterized actual data. brings well-studied type non-parametric hyposis class, reproducing kernel Hilbert space (rkhs). design market based rkhs, refer kernel market, brings toger number ideas including recent work] kernel exponential families]. positive semidefinite kernel  reproducing kernel Hilbert space basis (?) ,  }. reproducing property , ). hyposis expressed (?)  collection points )}. kernel approach nice properties. natural extension exponential family mechanism RKHS building block class exponential family distributions]. key assumption exponential family mechanism evaluating viewed product feature space; precisely kernel framework. specifically, assume PSD kernel  }. define classification kernel     rto, conditions], log exp, distribution form, rkhs exp)). again, participant updating market rewarded conditional log-likelihood ratio test data. nice property mirrors standard kernel learning methods, conditions search subset RKHS spanned basis  }, set data; direct result Representer orem]. context kernel market, suggests participants interact mechanism pushing updates lie span data. words, updates form), ?). naturally suggests idea directly purchasing data points traders. buying Data points. far, supposed participant trade prefers make. simply data point, drawn underlying distribution? give trader ?simple? trading interface sell data mechanism reason correct data point. proposal mimic behavior natural learning algorithms, stochastic gradient descent, presented). market offer trader purchase bundle update learning algorithm data point. principle, approach online learning algorithm. particular, stochastic gradient descent clean update rule, describe. expected profit (which negative expected loss) trade  )] draw), loss function gradient step   , gradient  (where indicator data point). suggests market offer participant trade  chosen arbitrarily ?learning rate?. interpreted buying unit shares participant data point), ?hedging? selling small amount shares proportion current prices (recall current prices kernel setting, choice stochastic gradient descent problematic, result non-sparse share purchases. desirable algorithms guarantee sparse updates modern discussion approaches found]. framework, participants access private set samples true underlying distribution simply opt ?standard bundle? data point, precisely stochastic gradient descent update. small learning rate, assuming data point independent current hyposis. , previously incorporated), trade guaranteed make positive profit expectation. sophisticated alternative strategies course, proposed simple bet type earning potential. protecting participants? privacy extend mechanism protect privacy participants: adversary observing hyposes prices mechanism, controlling trades participants, infer trader update relevant participants sell data mechanism data sensitive. medical data. here, privacy formalized )-differential privacy, defined shortly. intuitive characterization that, prior distribution adversary trader data, adversary posterior belief observing mechanism approximately trader participate all. idea that, rar posting exact prices trades made market, publish noisy versions, random noise giving guarantee. naive approach add independent noise participant trade. however, require prohibitively-large amount noise; final market hyposis determined random noise data trades. central challenge add carefully correlated noise large hide effects participant data point, large prices (equivalently, hyposis) meaningless. show adjusting ?price sensitivity? mechanism, measure fast prices change response trades defined. turn suﬃce set price sensitivity/polylog participants. roughly interpreted participant move market price noticeably privacy protected(polylog traders toger move prices completely. formally define differential privacy discuss tools disposal.  Differential Privacy Tools Differential privacy context defined follows. randomized function operating inputs form   outputs form )-differentially private, coordinate vector, distinct df1t df2t (measurable) set outputs df1t )]  df2t  notation means vector tth entry removed. intuitively, private modifying tth entry vector entry \\x0cnot change distribution outputs much. case, data protected trade participant space outputs entire sequence prices/predictions published mechanism. preserve privacy, trade bounded size. consist data point). enforce this, define parameter chosen mechanism designer: max hdf) allowed maximum trades allowed mechanism. , scalar capturing maximum allowed size trade. instance, trades restricted form,  max ). describe tools require. tool Private functions Gaussian processes. current market state   lies rkhs, construct ?private? version queries ?accurate?  close outputs private respect fact, convenient privately output partial sums trades, output Pt2 private approximates ft1 accomplished construction due]. orem], Corollary). sample path Gaussian process covariance kernel function /?)  ft1  )-differentially private respect     general infinite-dimensional object impossible finitely represent. case, orem implies releasing results number queries) differentially private.  course, queries released, larger chance high error query.) computationally feasible sample) simply sample Gaussian covariance previous samples drawn. unfortunately, suﬃcient independently release time amount noise required prohibitive. leads tool. formally) random variable and, finite subset variables distributed multivariate normal covariance                 10df 11df 12df 13df 14df 15df Figure Picturing continual observation technique preserving pri vacy. trade. data point sold market). goal release, time step noisy version , start follow arrow back). partial sum) add random noise. trace arrow)) anor partial sum add noise sum well. repeat reached, add toger noisy partial sums output time equal noise. key point-use noisy partial sums time steps. instance, noisy partial sum-used releasing    meanwhile, participates noisy partial sums number arrows passing). tool Continual observation technique. idea technique, pioneered], construct adding toger noisy partial sums form constructed Equation idea choosing partial sums pictured Figure For function) returns integer smaller)    specifically) determined writing binary, ﬂipping rightmost ?one? bit zero. pictured Figure intuition technique helps twofold. first, total noise sum noises partial sums, turns dlog terms. second, total noise add protect privacy governed partial sums participates, turns number dlog This privacy accuracy guarantees naively treating step independently.  Mechanism Results Combining market template Mechanism privacy tools, obtain Mechanism key differences. first, bound total number queries. (each query returns instantaneous prices market.) query reveals information participants, intuitively, allowing queries sacrifice eir privacy accuracy. fortunately, bound arbitrarily large polynomial number traders affecting quality results. second, pac-style guarantees accuracy: probability price queries return values true prices. third, longer straightforward compute represent market prices finite. leave general analysis Mechanism future work. eir approximately, Mechanism inherits desirable properties Mechanism bounded budget incentive-compatitibility (that, participants incentivized minimize risk market hyposis). addition, show preserves privacy maintaining accuracy, choice price sensitivity orem Mechanism maximimum trade size (equation—. mechanism differentially private and, traders price queries, accuracy guarantee: probability query returned prices satisfy ?   setting  log log takes exp [?polylog, )], superpolynomially low failure probability, Mechanism answers queries \\x0cwithin accuracy setting price sensitivity (?/polylog, )). note, however, weaker guarantee desired differential privacy literature, ideally exponentially small.   mechanism Privacy Protected Market parameters: (privacy), (accuracy), (kernel), (trade size), (#queries), (#traders) ARKET announces sets sets (orem participant proposes bet ARKET updates true position ARKET instantiates defined Equation BSERVER wishes make query BSERVER submits pricing query ARKET returns prices)    ARKET sets arket observes true sample,   participant receives payment, ,  Computing discussed limiting finite— order eﬃciently compute marginal prices however, immediately clear compute prices, implement Mechanism here, show problem solved exponential family, log exp. case, marginal prices gradient nice exponential-weights form) price shares, ptx) evaluating prices evaluating,  note worst-case bound greatly improved taking account structure kernel. ?smooth? cases Gaussian kernel, querying point close requires additional randomness builds additional error. gave worst-case bound holds kernels. adding transaction fee. appendix, discuss potential transaction fees. adding small ?(?) fee suﬃces deter arbitrage opportunities introduced noisy pricing. discussion main contribution work bring toger tools construct mechanism incentivized data aggregation ?contest-like? incentive properties, privacy guarantees, limited downside mechanism. proposed mechanisms extensions prediction market literature. building work Abernethy. ] introduce innovations: conditional markets. framework Mechanism interpreted prediction market conditional predictions) rar classic market elicit joint distribution), marginals. (this similar decision markets], incentive problems.) naturally couple conditional predictions restricted hyposis spaces, allowing capture., linear relationship nonparametric securities. extend nonparametric hyposis spaces kernels, kernel-based scoring rules].  privacy guarantees. provide private prediction market knowledge), showing information individual \\x0ctrades revealed. approach preserving privacy holds classic prediction market setting similar privacy accuracy guarantees. directions remain future work. mechanisms made practical privacy guarantees derived, nonparametric settings. explore connections similar settings, agents costs acquiring data. acknoledgements Abernethy acknowledges generous support National Science Foundation CAREER Grant iis-1453304 Grant iis-1421391. firm relies ability make diﬃcult predictions gain lot large collection data. goal estimate values observations class hyposes describing relationship (for example, linear regression). classic statistical learning ory, goal formalized attempting approximately solve min loss loss(?) inutility function, drawn unknown distribution. present paper concerned case data drawn held central authority inherently distributed. data (disjointly) partitioned set agents, agent privately possessing portion dataset agents obvious incentive reveal data firm seeking. vast swaths data personal email accounts provide massive benefits range companies, example, users typically loa provide account credentials, asked politely. concerned design financial mechanisms provide community agents, holding private set data, incentive contribute solution large learning prediction task. here term ?mechanism? algorithmic interface receive answer queries, engage monetary exchange (deposits payouts). our aim design mechanism satisfies properties: mechanism eﬃcient approaches solution) amount data participation grows spending constant, fixed total budget.  mechanism incentive-compatible sense agents rewarded contributions provide marginal terms improved hyposes, rewarded bad misleading information.  mechanism reasonable privacy guarantees, agent observer) manipulate mechanism order infer contributions agent Ultimately mechanism approach performance learning algorithm direct access data, spending constant budget acquire data improve predictions protecting participants? privacy. our construction relies recent surge literature prediction markets], popular time field economics recently studied great detail computer science]. prediction market mechanism designed purpose information aggregation, underlying future event members population private information. for instance, elicit predictions team win upcoming sporting event, candidate win election. predictions eventually scored actual outcome event. applying prediction market techniques participants essentially ?trade market? based data. (this approach similar prior work crowdsourcing contests].) members population private information, prediction markets case, data points beliefs goal incentivize reveal aggregate information final hyposis prediction. final profits tied outcome test set data, participant paid accordance information improved performance test set. our techniques depart framework] significant aspects) focus problem data aggregation, results advantage kernel methods) mechanisms combine differential privacy guarantees data aggregation prediction-market framework. this framework provide eﬃciency truthfulness. show achieve privacy scenarios. give mechanisms prices predictions published satisfy )-differential privacy] respect participant data. mechanism output give reasonable predictions observer infer participant input data. Mechanisms Eliciting Aggregating Data give broad description mechanism study. brief, imagine central authority mechanism, market) maintaining \\x0cposis representing current aggregation contributions made far.  returning) participant query cost, evaluating quality predictions privately-held dataset, propose update possibly requires investment ?bet?). bets evaluated close market true data sample generated (analogous test set), payouts distributed quality updates. after describing initial framework Mechanism based loosely setting], turn attention special case hyposes lie Reproducing Kernel Hilbert Space (rkhs] kernel(?, ?). this kernel-based ?nonparametric mechanism? well-suited problem data aggregation, betting space participants consists essentially updates form data object offered participant ?magnitude? bet. drawback Mechanism lack privacy guarantees betting protocol: utilizing one data make bets investments mechanism lead loss privacy owner data. when participant submits bet form sensitive personal information, anor participant infer querying mechanism. one primary contributions present work, detailed Section technique productive participation mechanism maintaining guarantee privacy data submitted.  General Template space examples, features labels. mechanism designer chooses function space consisting  assumed Hilbert space structure; view eir hyposis class loss class, measures loss/performance hyposis observation label case refer hyposis, eliding distinction pricing scheme mechanism relies convex cost function (?)  parameterized elements domain set hyposes cost function publicly determined advance. interaction mechanism sequential process querying betting. round mechanism publishes hyposis ?state? market, participants query. each participant arrives sequentially, round participant place ?bet?  called ?trade? ?update?, modifying hyposis  finally participation ends mechanism samples reveals) test example1, underlying distribution pays charges) participant relative performance marginal contributions. precisely, total reward participant bet, minus cost  mechanism Market Template ARKET announces  PARTICIPANT query functions, examples, PARTICIPANT submit bet ARKET ARKET    updates state ARKET observes true sample, PARTICIPANT receives payment design cost-function prediction markets area active research past years, starting] furr refinements generalizations]. general idea mechanism eﬃciently provide price quotes function(?) acts potential space outstandings shares] review. present work added additional twist function (?) additional parameterization observation dive deeply oretical aspects generalization, straightforward extension existing ory. key special case: exponential family mechanism. for familiar statistics machine learning, natural canonical family problems cast general framework Mechanism call exponential family prediction mechanism]. assume parameterized?   suﬃcient statistics summary function   rrd function evaluation ?, . log exp? log exp?, )idy. words, chosen mechanism encode exponential family model, (?) chosen conditional log partition function distribution market settled function interpret aggregate market belief distribution  , exp?,  (?)) (?) log exp?, . how view ?market aggregate? belief? notice trader observes market state bet form   eventual profit,  ?   log)  ., profit precisely conditional log likelihood ratio update   example: Logistic regression. let}, set functions , (?¿   construction, log(exp)) exp))) log(exp(?¿ exp(? )),  payoff participant placing bet moves market state outcome:  ? ?¿ log) log(exp(?¿ exp(? )) log) log exp)) This easily extended test set taking average performance test set. simply negative logistic loss parameter choice?. participant wishing maximize profit belief distribution, refore choose logistic regression, arg min log exp)) )  Properties Market describe nice properties Mechanism incentive-compatibility bounded budget. recall that, exponential family markets discussed above, trader moving market hyposis compensated conditional log-likelihood ratio test data point. implication traders incentivized minimize divergence market estimate distribution true underlying distribution. refer property incentive-compatibility traders? interests aligned mechanism designer. this property holds generally Mechanism divergence replaced general Bregman divergence Fenchel conjugate Proposition appendix details. given mechanism make sequence (possibly negative) payments traders, natural question wher potential large downside mechanism terms total payment (budget). context exponential family mechanism, question easy answer: sequence bets moving market state parameter      ?final total loss mechanism corresponds total payouts made traders, ,   log final, worst-case loss worst-case conditional log-likelihood ratio. context logistic regression quantity guaranteed log long initial parameter set  for Mechanism generally, tight bounds worst-case loss results prediction markets], give detailed statement Proposition appendix. price sensitivity parameter choosing cost function family important consideration ?scale? quickly market hyposis translate ?instantaneous prices?  (which give marginal cost infinitesimal bet formally, captured price sensitivity defined upper bound operator norm (with respect norm) Hessian cost function (over). choice small translates small worst-case budget required mechanism. however, means market prices sensitive update prices quickly. when protecting privacy trader updates Section privacy imposes restrictions price sensitivity.  Nonparametric Mechanism Kernel Methods framework discussed involved general function space ?state? mechanism, contributions participants form modifications functions. one downsides generic template participants reason information optimal privately-held dataset   specific class functions parameterized actual data. this brings well-studied type non-parametric hyposis class, reproducing kernel Hilbert space (rkhs). design market based rkhs, refer kernel market, brings toger number ideas including recent work] kernel exponential families]. positive semidefinite kernel  reproducing kernel Hilbert space basis (?) ,  }. reproducing property , ). now hyposis expressed (?)  collection points )}. kernel approach nice properties. one natural extension exponential family mechanism RKHS building block class exponential family distributions]. key assumption exponential family mechanism evaluating viewed product feature space; precisely kernel framework. specifically, assume PSD kernel  }. define classification kernel     Rto, under conditions], log exp, distribution form, RKHS exp)). and again, participant updating market rewarded conditional log-likelihood ratio test data. nice property mirrors standard kernel learning methods, conditions search subset RKHS spanned basis  }, set data; direct result Representer orem]. context kernel market, suggests participants interact mechanism pushing updates lie span data. words, updates form), ?). this naturally suggests idea directly purchasing data points traders. buying Data points. far, supposed participant trade prefers make. but simply data point, drawn underlying distribution? give trader ?simple? trading interface sell data mechanism reason correct data point. our proposal mimic behavior natural learning algorithms, stochastic gradient descent, presented). market offer trader purchase bundle update learning algorithm data point. principle, approach online learning algorithm. particular, stochastic gradient descent clean update rule, describe. expected profit (which negative expected loss) trade  )] given draw), loss function gradient step   , gradient  (where indicator data point). this suggests market offer participant trade  chosen arbitrarily ?learning rate?. this interpreted buying unit shares participant data point), ?hedging? selling small amount shares proportion current prices (recall current prices kernel setting, choice stochastic gradient descent problematic, result non-sparse share purchases. desirable algorithms guarantee sparse updates modern discussion approaches found]. given framework, participants access private set samples true underlying distribution simply opt ?standard bundle? data point, precisely stochastic gradient descent update. with small learning rate, assuming data point independent current hyposis. , previously incorporated), trade guaranteed make positive profit expectation. more sophisticated alternative strategies course, proposed simple bet type earning potential. Protecting participants? privacy extend mechanism protect privacy participants: adversary observing hyposes prices mechanism, controlling trades participants, infer trader update this relevant participants sell data mechanism data sensitive. medical data. here, privacy formalized )-differential privacy, defined shortly. one intuitive characterization that, prior distribution adversary trader data, adversary posterior belief observing mechanism approximately trader participate all. idea that, rar posting exact prices trades made market, publish noisy versions, random noise giving guarantee. naive approach add independent noise participant trade. however, require prohibitively-large amount noise; final market hyposis determined random noise data trades. central challenge add carefully correlated noise large hide effects participant data point, large prices (equivalently, hyposis) meaningless. show adjusting ?price sensitivity? mechanism, measure fast prices change response trades defined. turn suﬃce set price sensitivity/polylog participants. this roughly interpreted participant move market price noticeably privacy protected(polylog traders toger move prices completely. formally define differential privacy discuss tools disposal.  Differential Privacy Tools Differential privacy context defined follows. consider randomized function operating inputs form   outputs form )-differentially private, coordinate vector, distinct df1t df2t (measurable) set outputs df1t )]  df2t  notation means vector tth entry removed. intuitively, private modifying tth entry vector entry \\x0cnot change distribution outputs much. case, data protected trade participant space outputs entire sequence prices/predictions published mechanism. preserve privacy, trade bounded size. consist data point). enforce this, define parameter chosen mechanism designer: max hdf) allowed maximum trades allowed mechanism. that, scalar capturing maximum allowed size trade. for instance, trades restricted form,  max ). describe tools require. tool Private functions Gaussian processes. given current market state   lies rkhs, construct ?private? version queries ?accurate?  close outputs private respect fact, convenient privately output partial sums trades, output Pt2 private approximates ft1 this accomplished construction due]. orem], Corollary). let sample path Gaussian process covariance kernel function /?)  ft1  )-differentially private respect     general infinite-dimensional object impossible finitely represent. case, orem implies releasing results number queries) differentially private.  course, queries released, larger chance high error query.) this computationally feasible sample) simply sample Gaussian covariance previous samples drawn. unfortunately, suﬃcient independently release time amount noise required prohibitive. this leads tool. formally) random variable and, finite subset variables distributed multivariate normal covariance                 10df 11df 12df 13df 14df 15df Figure Picturing continual observation technique preserving pri vacy. each trade. data point sold market). goal release, time step noisy version , start follow arrow back). take partial sum) add random noise. trace arrow)) anor partial sum add noise sum well. repeat reached, add toger noisy partial sums output time equal noise. key point-use noisy partial sums time steps. for instance, noisy partial sum-used releasing    meanwhile, participates noisy partial sums number arrows passing). tool Continual observation technique. idea technique, pioneered], construct adding toger noisy partial sums form constructed Equation idea choosing partial sums pictured Figure For function) returns integer smaller)    specifically) determined writing binary, ﬂipping rightmost ?one? bit zero. this pictured Figure intuition technique helps twofold. first, total noise sum noises partial sums, turns dlog terms. second, total noise add protect privacy governed partial sums participates, turns number dlog This privacy accuracy guarantees naively treating step independently.  Mechanism Results Combining market template Mechanism privacy tools, obtain Mechanism key differences. first, bound total number queries. (each query returns instantaneous prices market.) this query reveals information participants, intuitively, allowing queries sacrifice eir privacy accuracy. fortunately, bound arbitrarily large polynomial number traders affecting quality results. second, pac-style guarantees accuracy: probability price queries return values true prices. third, longer straightforward compute represent market prices finite. leave general analysis Mechanism future work. eir approximately, Mechanism inherits desirable properties Mechanism bounded budget incentive-compatitibility (that, participants incentivized minimize risk market hyposis). addition, show preserves privacy maintaining accuracy, choice price sensitivity orem consider Mechanism maximimum trade size (equation—. Mechanism differentially private and, traders price queries, accuracy guarantee: probability query returned prices satisfy ?   setting  log log takes exp [?polylog, )], superpolynomially low failure probability, Mechanism answers queries \\x0cwithin accuracy setting price sensitivity (?/polylog, )). note, however, weaker guarantee desired differential privacy literature, ideally exponentially small.   mechanism Privacy Protected Market parameters: (privacy), (accuracy), (kernel), (trade size), (#queries), (#traders) ARKET announces sets sets (orem PARTICIPANT proposes bet ARKET updates true position ARKET instantiates defined Equation BSERVER wishes make query BSERVER submits pricing query ARKET returns prices)    ARKET sets ARKET observes true sample,   PARTICIPANT receives payment, ,  Computing discussed limiting finite— order eﬃciently compute marginal prices however, immediately clear compute prices, implement Mechanism here, show problem solved exponential family, log exp. case, marginal prices gradient nice exponential-weights form) price shares, ptx) thus evaluating prices evaluating,  note worst-case bound greatly improved taking account structure kernel. for ?smooth? cases Gaussian kernel, querying point close requires additional randomness builds additional error. gave worst-case bound holds kernels. adding transaction fee. appendix, discuss potential transaction fees. adding small ?(?) fee suﬃces deter arbitrage opportunities introduced noisy pricing. discussion main contribution work bring toger tools construct mechanism incentivized data aggregation ?contest-like? incentive properties, privacy guarantees, limited downside mechanism. our proposed mechanisms extensions prediction market literature. building work Abernethy. ] introduce innovations: conditional markets. our framework Mechanism interpreted prediction market conditional predictions) rar classic market elicit joint distribution), marginals. (this similar decision markets], incentive problems.) naturally couple conditional predictions restricted hyposis spaces, allowing capture., linear relationship nonparametric securities. extend nonparametric hyposis spaces kernels, kernel-based scoring rules].  privacy guarantees. provide private prediction market knowledge), showing information individual \\x0ctrades revealed. our approach preserving privacy holds classic prediction market setting similar privacy accuracy guarantees. many directions remain future work. mechanisms made practical privacy guarantees derived, nonparametric settings. one explore connections similar settings, agents costs acquiring data. acknoledgements Abernethy acknowledges generous support National Science Foundation CAREER Grant iis-1453304 Grant iis-1421391.',\n",
       " 'PP6068': 'deep learning methods storm areas computer vision, natural language processing speech recognition. key strengths ability leverage large quantities labelled data extract meaningful powerful representations. however, capability significant limitations large datasets train deep neural network option, necessity. known, fact, models prone overfitting. thus, deep networks goal learn concept, single shot learning. problems tackled generative models, discriminative setting-hoc solutions exemplar support vector machines (svms]. common discriminative approach one-shot learning learn off-line deep embedding \\x0cfunction define-line simple classification rules nearest neighbors embedding space]. however, computing embedding cry learning model object. paper, approach wher induce, single supervised example, full, deep discriminative model recognize instances object class. furrmore, solution require lengthy optimization process, computable, eﬃciently. formulate problem learning deep neural network, called learnet, that, single exemplar object class, predicts parameters network recognize objects type.  authors contributed equally, listed alphabetical order. 30th Conference Neural Information Processing Systems (nips 2016), barcelona, spain. model elements interest. firstly, learning process maps set images parameters model, ?learning learn? approach. clearly, learning single exemplar suﬃcient prior knowledge learning domain. prior knowledge incorporated learnet off-line phase solving millions small one-shot learning tasks back-propagating errors end-end. secondly, learnet feed-forward learning algorithm extracts exemplar final model parameters. iterative approaches exemplar SVMs complex inference processes generative modeling. demonstrates deep neural networks learn ?meta-level? predicting filter parameters network, interesting result right. thirdly, method competitive, eﬃcient, practical performing one-shot learning discriminative methods.  Related work Our work related ors literature. however, methods learn parameters complex discriminative models shot. one-shot learning widely studied context generative modeling, unlike work focused solving discriminative tasks. recent Rezende. ], recurrent spatial attention model generate images, learns optimizing measure reconstruction error variational inference]. demonstrate results sampling images classes generative model, solving discriminative tasks. anor notable work Lake. ], probabilistic program generative model. model constructs written characters compositions pen strokes, general programs envisioned, demonstrate Optical Character Recognition (ocr) applications. approach one-shot learning learn embedding space, typically siamese network]. exemplar category, classification performed embedding space simple rule nearest-neighbor. training performed classifying pairs distance], enforcing distance ranking triplet loss]. work departs paradigms generative modeling similarity learning, predicting parameters neural network single exemplar image. network effectively ?learns learn?, generalizing tasks defined exemplars. idea parameter prediction was, knowledge, explored Schmidhuber] recurrent architecture network modifies weights anor. parameter prediction zero-shot learning opposed one-shot learning), related problem learning object class single image, based solely description binary attributes text. framed modality transfer problem solved transfer learning], Noh. ] recently employed parameter prediction induce weights image classifier text problem visual question answering. denil. ] investigated redundancy neural network parameters, showing linearly predict% parameters layer remaining%. vastly proposition ours, predict parameters layer external exemplar image, non-linearly. one-shot learning dynamic parameter prediction Since one-shot learning discriminative task, starting point standard discriminative learning. generally consists finding parameters minimize average loss predictor function ; computed dataset samples labels(?  min) Unless model space small, generalization requires constraining choice model, regularization. however, extreme case goal learn single exemplar class interest, called one-shot learning, regularization insuﬃcient additional prior information injected learning process. main challenge siamese siamese learnet learnet Figure Our proposed architectures predict parameters network single example, replacing static convolutions (green) dynamic convolutions (red). siamese learnet predicts parameters embedding function applied inputs, single-stream learnet predicts parameters function applied input. linear layers denoted nonlinear layers dashed connections represent parameter sharing. discriminative one-shot learning find mechanism incorporate domain-specific information learner. learning learn. anor challenge, practical importance applications one-shot learning, avoid lengthy optimization process. ). propose address challenges learning parameters predictor single exemplar meta-prediction process. non-iterative feedforward function maps; practice function implemented deep neural network, call learnet. learnet depends exemplar single representative class interest, parameters own. learning learn posed problem optimizing learnet meta-parameters objective function defined below. furrmore, feed-forward learnet evaluation faster solving optimization problem). order train learnet, require produce good predictors exemplar empirically evaluated average training samples min(?   )), ) expression, performance predictor extracted learnet exemplar assessed single ?validation? pair comprising anor exemplar label hence, training data consists triplets notice meaning label subtly. ) class interest depending exemplar positive belong class negative orwise. triplets sampled uniformly respect cases. importantly, parameters original predictor . ) change dynamically exemplar note training data reminiscent siamese networks], learn labeled sample pairs. however, siamese networks apply model ; shared weights compute inner-product produce similarity score?  , min) key differences model. first, treat asymmetrically, results objective function. second, importantly, output ; parametrize linear layers determine intermediate representations network significantly computing single product layer. )). . ) specifies optimization objective one-shot learning dynamic parameter prediction. application chain rule, backpropagating derivatives computational blocks ; ; diﬃcult standard deep network. neverless, dive concrete implementations models face peculiar challenge, discussed next.  challenge naive parameter prediction order analyse practical diﬃculties implementing learnet, begin one-shot prediction fully-connected layer, simpler analyse. ) \\x0c?(?)     figure Factorized convolutional layer. )). channels input projected factorized space convolution), resulting channels convolved independently filter prediction), finally projected back input output weights  biases  replace weights biases functional counterparts), representing outputs learnet ; exemplar input avoid clutter, omit implicit dependence). ) While. ) drop replacement linear layers, careful analysis reveals scales extremely poorly. main unusually large output space learnet  comparable number input output units linear layer), output space learnet grows quadratically number units. concern large networks, extremely diﬃcult networks units. simple linear learnet) small fullyconnected layer 100 units 100), exemplar 100 features 100), learnet parameters learned. overfitting space time costs make learning regressor infeasible. furrmore, reducing number features exemplar achieve small constant-size reduction total number parameters. bottleneck quadratic size output space, size input space Factorized linear layers simple reduce size output space factorized set weights, replacing. ) with: diag). ) product diag)) factorized representation weights, analogous Singular Value decomposition. matrix  projects space elements) represent disentangled factors variation. projection  maps result back space. additional parameters learned, modest size compared case discussed sect. . importantly, one-shot branch) predict set diagonal elements (see. )), output space grows linearly number units layer. ):   Factorized convolutional layers factorization. ) generalized convolutional layers follows. input tensor  weights  (where filter support size), biases output convolutional layer ) denotes convolution, biases applied channels. projections analogous. ) incorporated filter bank ways obvious pick. view disentangle feature channels. dimension predicted filters) operate channel independently. such, factorization: ) ) ??? ??? ??? ??? ??? predicted filters) ??? activations Figure predicted filters output dynamic convolutional layer single-stream learnet trained OCR task. exemplars define filters). applying filters exemplar input yields responses. viewed colour. ??? ??? ??? ??? ??? ??? predicted filters) Activations Figure predicted filters output dynamic convolutional layer siamese learnet trained object tracking task. viewed colour.   )  convolution subscript denotes independent filtering channels. channel simply convolution channel practice, achieved filter tensors diagonal fourth dimensions, filter groups], group single filter. illustration fig.  predicted filters) interpreted filter basis, supplementary material (sec. ). notice that, factorization, number elements predicted one-shot branch) filter size typically small. ]). factorization, number elements. )). similarly case fully-connected layers (sect. ), number predicted elements growing quadratically number channels, allowing grow linearly. examples filters predicted learnets shown figs.  resulting activations confirm networks induced exemplars possess internal representations input. experiments evaluate learnets baseline one-shot architectures (sect. ) one-shot learning problems Optical Character Recognition (ocr; sect. ) visual object tracking (sect. ). experiments performed MatConvNet].  \\x0carchitectures noted sect. closest competitors method discriminative one-shot learning embedding learning siamese architectures. refore, structure experiments compare baseline. particular, choose implement learnets similar network topologies fairer comparison. baseline siamese architecture comprises parallel streams ; ; composed number layers, convolution, max-pooling, relu, sharing parameters (fig. ). outputs streams compared layer ?(? ; ; computing measure similarity dissimilarity. particular: dot product, vectors Euclidean distance , weighted -norm bk1 vector learnable weights Hadamard product). modification siamese baseline learnet predict intermediate shared stream parameters (fig. ). case ; siamese architecture writes ?(? ; ; )), ; ; ))). note siamese parameters Table Error rate character recognition foreign alphabets (chance%). inner-product (%) Euclidean dist. (%) Weighted dist. (%) Siamese (shared Siamese (unshared Siamese (unshared, fact.)   Siamese learnet (shared Learnet Modified Hausdorff distance streams, learnet subnetwork purpose map exemplar image shared weights. call model siamese learnet. modification single-stream learnet configuration, stream siamese architecture predicting parameter learnet case, comparison block reinterpreted layer stream (fig. ). note that: single predicted stream learnet asymmetric parameters) learnet predicts final comparison layer parameters intermediate filter parameters. single-stream learnet architecture understood predict discriminant function example, siamese learnet architecture predict embedding function comparison images. variants demonstrate versatility dynamic convolutional layer. ). finally, order ensure difference performance simply due asymmetry learnet architecture induced filter factorizations (sect.  sect. ), compare unshared siamese nets, distinct parameters stream, factorized siamese nets, convolutions replaced factorized convolutions learnet.  Character recognition foreign alphabets This section describes experiments one-shot learning ocr. this, Omniglot dataset], images handwritten characters alphabets. alphabets divided background evaluation alphabets. one-shot learning problem develop method determining wher, single exemplar character evaluation alphabet, image alphabet  represents character not. importantly, methods trained background alphabets tested evaluation alphabets. dataset evaluation protocol. character images resized pixels order explore eﬃciently variants proposed architectures. sample images character, average characters alphabet. dataset total,280 images background alphabets,180 evaluation alphabets. algorithms evaluated series recognition problems. recognition problem involves identifying image set shows character exemplar image match). characters single problem belong alphabet. test time, collection characters function evaluated pair, candidate highest score declared match. case learnet architectures, interpreted obtaining parameters ; evaluating static network  architecture. baseline stream siamese, siamese learnet, single-stream learnet architecture consists convolutional layers, max-pooling layers stride filter sizes   ,      512. siamese learnet single-stream learnet, consists layers number outputs 1600 element predicted filters size ). experiments simple, predict parameters convolutional layer. conducted cross-validation choose predicted layer found convolutional layer yields results proposed variants. siamese nets previously applied problem Koch. ] deeper networks applied images size 105 105. however, restricted investigation shallow networks enable exploration parameter space. powerful algorithm one-shot learning, Hierarchical Bayesian Program Learning], achieve human-level performance. however, approach involves computationally expensive inference test time, leverages extra information training time describes strokes drawn human author. learning. learning involves minimizing objective function specific method. . ) learnet. ) siamese architectures) stochastic gradient descent (sgd) cases. noted sect. objective obtained sampling triplets exemplars congruous) incongruous% probability. 100,000 random pairs training epoch, train epochs. conducted random search find hyper-parameters algorithm (initial learning rate geometric decay, standard deviation Gaussian parameter initialization, weight decay). results discussion. tab. shows classification error obtained variants architecture. dash failure converge large range hyper-parameters. learnet architectures combined weighted distance achieve significantly results methods. architecture reduced error% siamese network shared parameters% single-stream learnet. Euclidean distance gave results siamese networks shared parameters, results achieved learnets (and siamese networks unshared parameters) weighted distance. fact, alternative architectures achieve lower error Euclidean distance shared siamese net. dot product was, general, effective metrics. introduction factorization convolutional layer expected improve quality estimated model reducing number parameters, worsen diminishing capacity hyposis space. simple task character recognition, factorization large effect.   object tracking task single-target object tracking requires locate object interest sequence video frames. video frame collection   image windows; one-shot setting, exemplar object frame goal identify window frames  datasets. method trained ImageNet Large Scale Visual Recognition Challenge 2015,862 videos totalling million annotated frames. instances objects thirty classes (mostly vehicles animals) annotated video bounding boxes. tracking, instance labels retained object class labels ignored. % videos training% held-out monitor validation error network training. testing VOT 2015 benchmark]. architecture. experiment siamese siamese learnet architectures (fig. learnet predicts parameters (dynamic) convolutional layer siamese streams. siamese stream convolutional layers test variants those: variant) configuration AlexNet] stride layer, variants) reduce% number filters convolutional layers and, respectively% number filters layer. training. order train architecture eﬃciently windows, data prepared follows. object bounding box sampled random, crop double size extracted frame, padding average image color needed. border included order incorporate visual context exemplar object. next, } sampled random% probability positive. , image extracted choosing random frame object. orwise, frame object temporal samples selected random. that, patch centered object times bigger extracted. way, subwindows match images resized 127 127 255 255 pixels, respectively, triplet, formed. 127 127 subwindows considered match central . table Tracking accuracy number tracking failures VOT 2015 benchmark, reported toolkit]. architectures grouped size main network (see text). group, entry column bold. report scores recent trackers. Method Accuracy Failures Siamese.465 105 Siamese; unshared.447 131 Siamese; factorized.444 138 Siamese learnet.500 Siamese learnet.497 DAT.442 113-dlt.540 108 Method Accuracy Failures Siamese.466 120 Siamese; factorized.435 132 Siamese learnet.483 105 Siamese learnet.491 106 DSST.483 163 MEEM.458 107 MUSTer.471 132 All networks trained scratch SGD epoch,000 sample triplets multiple windows contained compared eﬃciently making comparison layer convolutional (fig. ), accumulating logistic loss spatial locations. hyperparameters (learning rate geometrically decaying weight decay.005, small mini-batches size experiments, found work baseline proposed architectures. weights initialized improved Xavier] method, batch normalization] linear layers. testing. adopting initial crop exemplar, object sought frame radius previous position, proceeding sequentially. evaluating pupil net convolutionally, searching scales order track object scale space. approach detail Bertinetto. ]. results discussion. tab. compares methods terms oﬃcial metrics (accuracy number failures) VOT 2015 benchmark]. ranking plot produced VOT toolkit provided supplementary material (fig. ). tab. observed factorizing filters siamese architecture significantly diminishes performance, learnet predict filters factorization recovers gap fact achieves performance original siamese net. performance learnet architectures adversely affected slimmer prediction networks (with channels). elementary tracker based learnet compares favourably recent tracking systems, make features online model update strategies: DAT], DSST], MEEM], MUSTer-dlt]. -dlt good direct adaptation standard batch deep learning methodology online learning, SGD tracking fine-tune ensemble deep convolutional networks. however, online adaptation model big computational cost affects speed method, runs frames-persecond (fps) gpu. due feed-forward nature one-shot learnets, track objects real-time framerates excess fps, achieving tracking failures. consider, however, implementation serves proof-concept, tracking interesting demonstration one-shot-learning, orthogonal technical improvements found tracking literature]. conclusions work, shown obtain parameters deep neural network single, feed-forward prediction network. approach desirable iterative methods slow, large sets annotated training samples available. demonstrated feasibility feed-forward parameter prediction demanding one-shot learning tasks OCR visual tracking. results hint promising avenue research ?learning learn? solving millions small discriminative problems oﬄine phase. extensions include domain adaptation sharing single learnet pupil networks. acknowledgements This research supported Apical ltd. ERC grants erc-2012-adg 321162-helios, helios-dfr00200 ?integrated Detailed Image understanding? /l024683). Deep learning methods storm areas computer vision, natural language processing speech recognition. one key strengths ability leverage large quantities labelled data extract meaningful powerful representations. however, capability significant limitations large datasets train deep neural network option, necessity. known, fact, models prone overfitting. thus, deep networks goal learn concept, single shot learning. problems tackled generative models, discriminative setting-hoc solutions exemplar support vector machines (svms]. perhaps common discriminative approach one-shot learning learn off-line deep embedding \\x0cfunction define-line simple classification rules nearest neighbors embedding space]. however, computing embedding cry learning model object. paper, approach wher induce, single supervised example, full, deep discriminative model recognize instances object class. furrmore, solution require lengthy optimization process, computable, eﬃciently. formulate problem learning deep neural network, called learnet, that, single exemplar object class, predicts parameters network recognize objects type.  authors contributed equally, listed alphabetical order. 30th Conference Neural Information Processing Systems (nips 2016), barcelona, spain. our model elements interest. firstly, learning process maps set images parameters model, ?learning learn? approach. clearly, learning single exemplar suﬃcient prior knowledge learning domain. this prior knowledge incorporated learnet off-line phase solving millions small one-shot learning tasks back-propagating errors end-end. secondly, learnet feed-forward learning algorithm extracts exemplar final model parameters. this iterative approaches exemplar SVMs complex inference processes generative modeling. demonstrates deep neural networks learn ?meta-level? predicting filter parameters network, interesting result right. thirdly, method competitive, eﬃcient, practical performing one-shot learning discriminative methods.  Related work Our work related ors literature. however, methods learn parameters complex discriminative models shot. one-shot learning widely studied context generative modeling, unlike work focused solving discriminative tasks. one recent Rezende. ], recurrent spatial attention model generate images, learns optimizing measure reconstruction error variational inference]. demonstrate results sampling images classes generative model, solving discriminative tasks. anor notable work Lake. ], probabilistic program generative model. this model constructs written characters compositions pen strokes, general programs envisioned, demonstrate Optical Character Recognition (ocr) applications. approach one-shot learning learn embedding space, typically siamese network]. given exemplar category, classification performed embedding space simple rule nearest-neighbor. training performed classifying pairs distance], enforcing distance ranking triplet loss]. our work departs paradigms generative modeling similarity learning, predicting parameters neural network single exemplar image. network effectively ?learns learn?, generalizing tasks defined exemplars. idea parameter prediction was, knowledge, explored Schmidhuber] recurrent architecture network modifies weights anor. parameter prediction zero-shot learning opposed one-shot learning), related problem learning object class single image, based solely description binary attributes text. whereas framed modality transfer problem solved transfer learning], Noh. ] recently employed parameter prediction induce weights image classifier text problem visual question answering. denil. ] investigated redundancy neural network parameters, showing linearly predict% parameters layer remaining%. this vastly proposition ours, predict parameters layer external exemplar image, non-linearly. one-shot learning dynamic parameter prediction Since one-shot learning discriminative task, starting point standard discriminative learning. generally consists finding parameters minimize average loss predictor function ; computed dataset samples labels(?  min) Unless model space small, generalization requires constraining choice model, regularization. however, extreme case goal learn single exemplar class interest, called one-shot learning, regularization insuﬃcient additional prior information injected learning process. main challenge siamese siamese learnet learnet Figure Our proposed architectures predict parameters network single example, replacing static convolutions (green) dynamic convolutions (red). siamese learnet predicts parameters embedding function applied inputs, single-stream learnet predicts parameters function applied input. linear layers denoted nonlinear layers dashed connections represent parameter sharing. discriminative one-shot learning find mechanism incorporate domain-specific information learner. learning learn. anor challenge, practical importance applications one-shot learning, avoid lengthy optimization process. ). propose address challenges learning parameters predictor single exemplar meta-prediction process. non-iterative feedforward function maps; since practice function implemented deep neural network, call learnet. learnet depends exemplar single representative class interest, parameters own. learning learn posed problem optimizing learnet meta-parameters objective function defined below. furrmore, feed-forward learnet evaluation faster solving optimization problem). order train learnet, require produce good predictors exemplar empirically evaluated average training samples min(?   )), ) expression, performance predictor extracted learnet exemplar assessed single ?validation? pair comprising anor exemplar label hence, training data consists triplets notice meaning label subtly. ) class interest depending exemplar positive belong class negative orwise. triplets sampled uniformly respect cases. importantly, parameters original predictor . ) change dynamically exemplar note training data reminiscent siamese networks], learn labeled sample pairs. however, siamese networks apply model ; shared weights compute inner-product produce similarity score?  , min) key differences model. first, treat asymmetrically, results objective function. second, importantly, output ; parametrize linear layers determine intermediate representations network this significantly computing single product layer. )). . ) specifies optimization objective one-shot learning dynamic parameter prediction. application chain rule, backpropagating derivatives computational blocks ; ; diﬃcult standard deep network. neverless, dive concrete implementations models face peculiar challenge, discussed next.  challenge naive parameter prediction order analyse practical diﬃculties implementing learnet, begin one-shot prediction fully-connected layer, simpler analyse. this) \\x0c?(?)     figure Factorized convolutional layer. )). channels input projected factorized space convolution), resulting channels convolved independently filter prediction), finally projected back input output weights  biases  replace weights biases functional counterparts), representing outputs learnet ; exemplar input avoid clutter, omit implicit dependence). ) While. ) drop replacement linear layers, careful analysis reveals scales extremely poorly. main unusually large output space learnet  for comparable number input output units linear layer), output space learnet grows quadratically number units. while concern large networks, extremely diﬃcult networks units. consider simple linear learnet) even small fullyconnected layer 100 units 100), exemplar 100 features 100), learnet parameters learned. overfitting space time costs make learning regressor infeasible. furrmore, reducing number features exemplar achieve small constant-size reduction total number parameters. bottleneck quadratic size output space, size input space Factorized linear layers simple reduce size output space factorized set weights, replacing. ) with: diag). ) product diag)) factorized representation weights, analogous Singular Value decomposition. matrix  projects space elements) represent disentangled factors variation. projection  maps result back space. both additional parameters learned, modest size compared case discussed sect. . importantly, one-shot branch) predict set diagonal elements (see. )), output space grows linearly number units layer. ):   Factorized convolutional layers factorization. ) generalized convolutional layers follows. given input tensor  weights  (where filter support size), biases output convolutional layer ) denotes convolution, biases applied channels. projections analogous. ) incorporated filter bank ways obvious pick. here view disentangle feature channels. dimension predicted filters) operate channel independently. such, factorization: ) ) ??? ??? ??? ??? ??? Predicted filters) ??? activations Figure predicted filters output dynamic convolutional layer single-stream learnet trained OCR task. different exemplars define filters). applying filters exemplar input yields responses. best viewed colour. ??? ??? ??? ??? ??? ??? Predicted filters) Activations Figure predicted filters output dynamic convolutional layer siamese learnet trained object tracking task. best viewed colour.   )  convolution subscript denotes independent filtering channels. channel simply convolution channel practice, achieved filter tensors diagonal fourth dimensions, filter groups], group single filter. illustration fig.  predicted filters) interpreted filter basis, supplementary material (sec. ). notice that, factorization, number elements predicted one-shot branch) filter size typically small. ]). without factorization, number elements. )). similarly case fully-connected layers (sect. ), number predicted elements growing quadratically number channels, allowing grow linearly. examples filters predicted learnets shown figs.  resulting activations confirm networks induced exemplars possess internal representations input. Experiments evaluate learnets baseline one-shot architectures (sect. ) one-shot learning problems Optical Character Recognition (ocr; sect. ) visual object tracking (sect. ). all experiments performed MatConvNet].  \\x0carchitectures noted sect. closest competitors method discriminative one-shot learning embedding learning siamese architectures. refore, structure experiments compare baseline. particular, choose implement learnets similar network topologies fairer comparison. baseline siamese architecture comprises parallel streams ; ; composed number layers, convolution, max-pooling, relu, sharing parameters (fig. ). outputs streams compared layer ?(? ; ; computing measure similarity dissimilarity. particular: dot product, vectors Euclidean distance , weighted -norm bk1 vector learnable weights Hadamard product). modification siamese baseline learnet predict intermediate shared stream parameters (fig. ). case ; siamese architecture writes ?(? ; ; )), ; ; ))). note siamese parameters Table Error rate character recognition foreign alphabets (chance%). inner-product (%) Euclidean dist. (%) Weighted dist. (%) Siamese (shared Siamese (unshared Siamese (unshared, fact.)   Siamese learnet (shared Learnet Modified Hausdorff distance streams, learnet subnetwork purpose map exemplar image shared weights. call model siamese learnet. modification single-stream learnet configuration, stream siamese architecture predicting parameter learnet case, comparison block reinterpreted layer stream (fig. ). note that: single predicted stream learnet asymmetric parameters) learnet predicts final comparison layer parameters intermediate filter parameters. single-stream learnet architecture understood predict discriminant function example, siamese learnet architecture predict embedding function comparison images. variants demonstrate versatility dynamic convolutional layer. ). finally, order ensure difference performance simply due asymmetry learnet architecture induced filter factorizations (sect.  sect. ), compare unshared siamese nets, distinct parameters stream, factorized siamese nets, convolutions replaced factorized convolutions learnet.  Character recognition foreign alphabets This section describes experiments one-shot learning ocr. for this, Omniglot dataset], images handwritten characters alphabets. alphabets divided background evaluation alphabets. one-shot learning problem develop method determining wher, single exemplar character evaluation alphabet, image alphabet  represents character not. importantly, methods trained background alphabets tested evaluation alphabets. dataset evaluation protocol. character images resized pixels order explore eﬃciently variants proposed architectures. sample images character, average characters alphabet. dataset total,280 images background alphabets,180 evaluation alphabets. algorithms evaluated series recognition problems. each recognition problem involves identifying image set shows character exemplar image match). all characters single problem belong alphabet. test time, collection characters function evaluated pair, candidate highest score declared match. case learnet architectures, interpreted obtaining parameters ; evaluating static network  architecture. baseline stream siamese, siamese learnet, single-stream learnet architecture consists convolutional layers, max-pooling layers stride filter sizes   ,      512. for siamese learnet single-stream learnet, consists layers number outputs 1600 element predicted filters size ). experiments simple, predict parameters convolutional layer. conducted cross-validation choose predicted layer found convolutional layer yields results proposed variants. siamese nets previously applied problem Koch. ] deeper networks applied images size 105 105. however, restricted investigation shallow networks enable exploration parameter space. powerful algorithm one-shot learning, Hierarchical Bayesian Program Learning], achieve human-level performance. however, approach involves computationally expensive inference test time, leverages extra information training time describes strokes drawn human author. learning. learning involves minimizing objective function specific method. . ) learnet. ) siamese architectures) stochastic gradient descent (sgd) cases. noted sect. objective obtained sampling triplets exemplars congruous) incongruous% probability. 100,000 random pairs training epoch, train epochs. conducted random search find hyper-parameters algorithm (initial learning rate geometric decay, standard deviation Gaussian parameter initialization, weight decay). results discussion. tab. shows classification error obtained variants architecture. dash failure converge large range hyper-parameters. learnet architectures combined weighted distance achieve significantly results methods. architecture reduced error% siamese network shared parameters% single-stream learnet. while Euclidean distance gave results siamese networks shared parameters, results achieved learnets (and siamese networks unshared parameters) weighted distance. fact, alternative architectures achieve lower error Euclidean distance shared siamese net. dot product was, general, effective metrics. introduction factorization convolutional layer expected improve quality estimated model reducing number parameters, worsen diminishing capacity hyposis space. for simple task character recognition, factorization large effect.   object tracking task single-target object tracking requires locate object interest sequence video frames. video frame collection   image windows; one-shot setting, exemplar object frame goal identify window frames  datasets. method trained ImageNet Large Scale Visual Recognition Challenge 2015,862 videos totalling million annotated frames. instances objects thirty classes (mostly vehicles animals) annotated video bounding boxes. for tracking, instance labels retained object class labels ignored. % videos training% held-out monitor validation error network training. testing VOT 2015 benchmark]. architecture. experiment siamese siamese learnet architectures (fig. learnet predicts parameters (dynamic) convolutional layer siamese streams. each siamese stream convolutional layers test variants those: variant) configuration AlexNet] stride layer, variants) reduce% number filters convolutional layers and, respectively% number filters layer. training. order train architecture eﬃciently windows, data prepared follows. given object bounding box sampled random, crop double size extracted frame, padding average image color needed. border included order incorporate visual context exemplar object. next, } sampled random% probability positive. , image extracted choosing random frame object. orwise, frame object temporal samples selected random. from that, patch centered object times bigger extracted. way, subwindows match images resized 127 127 255 255 pixels, respectively, triplet, formed. all 127 127 subwindows considered match central . table Tracking accuracy number tracking failures VOT 2015 benchmark, reported toolkit]. architectures grouped size main network (see text). for group, entry column bold. report scores recent trackers. Method Accuracy Failures Siamese.465 105 Siamese; unshared.447 131 Siamese; factorized.444 138 Siamese learnet.500 Siamese learnet.497 DAT.442 113-dlt.540 108 Method Accuracy Failures Siamese.466 120 Siamese; factorized.435 132 Siamese learnet.483 105 Siamese learnet.491 106 DSST.483 163 MEEM.458 107 MUSTer.471 132 All networks trained scratch SGD epoch,000 sample triplets multiple windows contained compared eﬃciently making comparison layer convolutional (fig. ), accumulating logistic loss spatial locations. hyperparameters (learning rate geometrically decaying weight decay.005, small mini-batches size experiments, found work baseline proposed architectures. weights initialized improved Xavier] method, batch normalization] linear layers. testing. adopting initial crop exemplar, object sought frame radius previous position, proceeding sequentially. this evaluating pupil net convolutionally, searching scales order track object scale space. approach detail Bertinetto. ]. results discussion. tab. compares methods terms oﬃcial metrics (accuracy number failures) VOT 2015 benchmark]. ranking plot produced VOT toolkit provided supplementary material (fig. ). from tab. observed factorizing filters siamese architecture significantly diminishes performance, learnet predict filters factorization recovers gap fact achieves performance original siamese net. performance learnet architectures adversely affected slimmer prediction networks (with channels). elementary tracker based learnet compares favourably recent tracking systems, make features online model update strategies: DAT], DSST], MEEM], MUSTer-dlt]. -dlt good direct adaptation standard batch deep learning methodology online learning, SGD tracking fine-tune ensemble deep convolutional networks. however, online adaptation model big computational cost affects speed method, runs frames-persecond (fps) gpu. due feed-forward nature one-shot learnets, track objects real-time framerates excess fps, achieving tracking failures. consider, however, implementation serves proof-concept, tracking interesting demonstration one-shot-learning, orthogonal technical improvements found tracking literature]. conclusions work, shown obtain parameters deep neural network single, feed-forward prediction network. this approach desirable iterative methods slow, large sets annotated training samples available. demonstrated feasibility feed-forward parameter prediction demanding one-shot learning tasks OCR visual tracking. our results hint promising avenue research ?learning learn? solving millions small discriminative problems oﬄine phase. possible extensions include domain adaptation sharing single learnet pupil networks. acknowledgements This research supported Apical ltd. ERC grants erc-2012-adg 321162-helios, helios-dfr00200 ?integrated Detailed Image understanding? /l024683).',\n",
       " 'PP6171': 'primary objective linear regression determine relationships multiple variables affect outcome. standard medical diagnosis, data gared patient information susceptibility illnesses. major drawback process work collect data, requires running numerous tests person, discomforting. cases impose limitations amount data example. medical diagnosis, patient undergo small subset tests. formal setting capturing regression learning limits number attribute observations Limited Attribute Observation (lao) setting, introduced ben-david Dichterman]. example, regression problem, learner access distribution data, fits (generalized) linear model loss function., approximately solves optimization problem min¿ LAO setting, learner complete access examples reader attributes patient. rar, learner observe fixed number attributes, denoted standard regression problem solved arbitrary precision. main question address: compute arbitrarily accurate solution number observations example, strictly formally, compute vector) min? ?  eﬃcient algorithms regression squared loss shown previous work], sample complexity bounds tightened]. however, similar results 30th Conference Neural Information Processing Systems (nips 2016), barcelona, spain. common loss functions. absolute loss shown relaxing hard limit attributes]. paper show, time, fact problem solved general. main result shows regression absolute loss function, information-oretic lower bound error attainable algorithm. , -optimal solution determined, irrespective number examples learner sees. formally, constant probability, algorithm returning vector satisfy) min?   furr show ultimate achievable precision parameter bounded polynomial dimension.,  additionally, basic setting Ridge regression (with squared loss), give tight lower bound LAO setting. cesa-bianchi. ] provided eﬃcient algorithm setting sample complexity” error. hazan Koren] improved result gave tight sample complexity” achieve error. cases, however, algorithms work complete picture show attributes fact obtain arbitrarily low error. , attribute example, information-oretic limit accuracy attainable regression algorithm. remark similar impossibility result proven cesa-bianchi. ] related setting learning noisy examples. classification similarly cast LAO setting. classification hinge loss, soft-margin svm, give related lower bound, showing impossible achieve arbitrarily low error number observed attributes bounded  however, unlike lower bound regression, lower bound prove classification scales exponentially dimension. Hazan. ] showed classification missing data, work includes low rank assumptions contradiction lower bounds presented here. similar LAO setting, setting learning missing data] presents learner examples attributes randomly \\x0cobserved. missing data setting diﬃcult LAO setting, lower bounds extend case well. complement lower bounds general purpose algorithm regression classification missing data that, suﬃcient number samples, achieve error). result leaves small polynomial gap compared information-oretic lower bound prove. setup Statement Results general framework linear regression involves set instances, form, attribute vector target value. typical statistical learning framework, pair drawn joint distribution learner objective determine linear predictor predicting quality prediction measured loss function two commonly loss functions regression squared loss¿ absolute loss—. examples drawn arbitrary distribution expected loss  ¿ learner goal determine regressor minimizes expected loss). avoid overfitting, regularization term typically added, constant factor equivalent min. kwk w2r regularization parameter standard norm, common variants regression Ridge regression squared loss) Lasso regression squared loss). framework classification identical linear regression. main distinction meaning acts label example. loss function learning classifier, paper interested hinge loss max, }. goal learner, however, remains same: namely, determine classifier) minimized. paper, denote minimizer).  Main Results step, Lasso Ridge regressions, show observe attributes learn regressor arbitrary precision. formally orem orem squared loss. exists distribution  ?  regression algorithm observe attribute training training set output regressor ?  )] Corollary squared loss. exists distribution  ?  regression algorithm observe attribute training training set output regressor ?  )] lower bounds tight?recall attributes, possi \\x0cble learn regressor arbitrary precision]. also, notice order quantification orems: turns exists distribution hard algorithms (rar hard distribution algorithm). regression absolute loss, setting learner limited fewer attributes training sample. orem shows case learner hope learn ”-optimal regressor orem  (mod), absolute loss. exists distribution  ?  regression algorithm observe attributes training training set )] ?  output regressor Corollary absolute loss. exists distribution  ?  regression algorithm observe attributes training training set output regressor ?  )] complement findings regression analogous lower bound classification hinge loss., soft margin svm). orem  (mod), hinge loss. exists holds: exists distribution  ?  classification algorithm observe attributes training )] ?  training set output regressor Lower Bounds section discuss lower bounds regression missing attributes. warm, prove orem regression squared loss. proof simple, illustrates main ideas lower bounds. give proof orem regression absolute loss. proofs remaining bounds deferred supplementary material.  Lower bounds squared loss Proof orem prove orem deterministic learning algorithms, namely, algorithms external randomization., randomization random samples drawn data distribution itself). randomized algorithm thought distribution deterministic algorithms, independent data distribution. now, suppose )}, uniform distributions } }, respectively. main observation learner observe attribute distinguish distributions probability greater matter samples given. marginal distributions individual attributes same. thus prove orem show sets ”-optimal solutions distributions disjoint. indeed, suppose learning algorithm emits ) vector?  (where expectation random samples ? algorithm). markov inequality, holds) probability. hence, output algorithm distinguish distributions probability, contradicting indistinguishability property. set characterize sets ”-optimal solutions  x2x) x2x note set ”-optimal regressors— ”}, set 1k2 ”}. S20¿ ”}.  s20 suﬃcient show disjoint. since¿ meaning however, S20 meaning member argued earlier, suﬃces prove orem.   Lower bounds absolute loss proof orem main idea show design distributions indistinguishable learner observe attributes sample distribution., marginals choice attributes identical), respective sets ”-optimal regressors disjoint. however, contrast orem handling general switching absolute loss introduce additional complexities proof require techniques. start constructing distributions   , kxk1 (mod)}  , kxk1 (mod)}, uniform } }, respectively. construction, hard choice attributes, marginals attributes distributions identical: uniform distribution bits. thus, distributions indistinguishable learner observe attributes example. ¿  —. x2x1 x2x2 turns subgradients), denote) respectively, expressed precisely. fact, full subgradient set point domain functions made explicit. representations hand, show minimizers), respectively. figure Geometric intuition Lemmas lower bounding absolute function acts relaxation true expected loss (depicted cone). fact, subgradient sets prove stronger property expected losses akin ?directional strong convexity? property respective minimizers. geometric idea property shown Figure lower bounded absolute function. lemma     lemma      given Lemmas proof orem immediate. proof orem direct consequence Lemmas obtain sets        sets ”-optimal regressors), respectively. needed show separation ”-optimal sets showing separation manageable sets indeed, fix observe   hand, ,     exist sets disjoint. orem reasoning conclude proof orem  remains prove Lemmas proofs similar, prove Lemma defer proof Lemma supplementary material. proof Lemma write) Letting¿ x2x1 sign¿ x2x1   sign?¿ x2x1 sign?¿ x2x x2x1, sign?¿ x2x1, sign) x2x1, x2x1, sign?¿ \\x0cx2x1,   sign) number]. next, compute x2x1, x2x1            equality elementary identity prove Lemma supplementary material. now, kxk1 ,    matrix formed  express entire subgradient set explicitly      thus, choice result specific subgradient  choices: note Xr1 Xr2 equality, fixed coordinate notice number elements non-zero values coordinate equal number ways choose remaining non-zero coordinates coordinates. observe subgradients   xr1        note that, set subgradients convex set, taking convex combination minimizer). xr2 Given handle subgradient coeﬃcients polynomial set, show Observe that, fact(           d2e4          written convex combination Let ?   similarly             again, written convex combination vectors subgradient set, conclude  well. subgradient inequality that toger imply   required.      general Algorithm Limited Precision Although established limits attainable precision learning problems, possibility reaching limit. section provide general algorithm, learner observe attributes achieve expected loss). provide pseudo-code Algorithm similar AERR algorithm Hazan Koren]?which designed work squared loss?algorithm avoids necessity unbiased gradient estimator replacing original loss function slightly biased one. long loss function chosen \\x0ccarefully (and functions Lipschitz bounded), samples, algorithm return regressor limited precision. contrast AERR arbitrarily precise regressor achieved samples. formally, Algorithm prove (proof supplementary material). orem  -lipschitz function defined]. assume distribution kxk2 — probability  max},   output Algorithm run g2b ?      )] algorithm General algorithm regression/classification missing attributes input: Loss function training set],    output: Regressor Initialize kw1 arbitrarily Uniformly choose subset indices] replacement Set  regression case: Choose¿ Classification case: Choose update    max{kwt   : end : Return particular   )] learner observes optimum. attributes, expected loss)-away Conclusions Future Work limited attribute observation setting, shown informationoretic lower bounds variants regression, proving distributionindependent algorithm regression absolute loss attains error exist closing gap ridge regression suggested Hazan Koren]. shown proof technique applied regression absolute loss extended show similar bound classification hinge loss. addition, general purpose algorithm complements results providing means achieving error precision limit. interesting possibility future work bridge gap upper lower bounds precision limits, case exponential gap classification hinge loss. anor direction develop comprehensive understanding lower bounds terms general functions, classification logistic loss. primary objective linear regression determine relationships multiple variables affect outcome. standard medical diagnosis, data gared patient information susceptibility illnesses. major drawback process work collect data, requires running numerous tests person, discomforting. cases impose limitations amount data example. for medical diagnosis, patient undergo small subset tests. formal setting capturing regression learning limits number attribute observations Limited Attribute Observation (lao) setting, introduced ben-david Dichterman]. for example, regression problem, learner access distribution data, fits (generalized) linear model loss function., approximately solves optimization problem min¿  LAO setting, learner complete access examples reader attributes patient. rar, learner observe fixed number attributes, denoted standard regression problem solved arbitrary precision. main question address: compute arbitrarily accurate solution number observations example, strictly more formally, compute vector) min? ?  eﬃcient algorithms regression squared loss shown previous work], sample complexity bounds tightened]. however, similar results 30th Conference Neural Information Processing Systems (nips 2016), barcelona, spain. common loss functions. absolute loss shown relaxing hard limit attributes]. paper show, time, fact problem solved general. our main result shows regression absolute loss function, information-oretic lower bound error attainable algorithm. that, -optimal solution determined, irrespective number examples learner sees. formally, constant probability, algorithm returning vector satisfy) min?   furr show ultimate achievable precision parameter bounded polynomial dimension.,  additionally, basic setting Ridge regression (with squared loss), give tight lower bound LAO setting. cesa-bianchi. ] provided eﬃcient algorithm setting sample complexity” error. hazan Koren] improved result gave tight sample complexity” achieve error. cases, however, algorithms work complete picture show attributes fact obtain arbitrarily low error. that, attribute example, information-oretic limit accuracy attainable regression algorithm. remark similar impossibility result proven cesa-bianchi. ] related setting learning noisy examples. classification similarly cast LAO setting. for classification hinge loss, soft-margin svm, give related lower bound, showing impossible achieve arbitrarily low error number observed attributes bounded  however, unlike lower bound regression, lower bound prove classification scales exponentially dimension. although Hazan. ] showed classification missing data, work includes low rank assumptions contradiction lower bounds presented here. similar LAO setting, setting learning missing data] presents learner examples attributes randomly \\x0cobserved. since missing data setting diﬃcult LAO setting, lower bounds extend case well. complement lower bounds general purpose algorithm regression classification missing data that, suﬃcient number samples, achieve error). this result leaves small polynomial gap compared information-oretic lower bound prove. Setup Statement Results general framework linear regression involves set instances, form, attribute vector target value. under typical statistical learning framework, pair drawn joint distribution learner objective determine linear predictor predicting quality prediction measured loss function Two commonly loss functions regression squared loss¿ absolute loss—. since examples drawn arbitrary distribution expected loss  ¿ learner goal determine regressor minimizes expected loss). avoid overfitting, regularization term typically added, constant factor equivalent min. kwk w2r regularization parameter standard norm, two common variants regression Ridge regression squared loss) Lasso regression squared loss). framework classification identical linear regression. main distinction meaning acts label example. loss function learning classifier, paper interested hinge loss max, }. goal learner, however, remains same: namely, determine classifier) minimized. throughout paper, denote minimizer).  Main Results step, Lasso Ridge regressions, show observe attributes learn regressor arbitrary precision. this formally orem orem let squared loss. exists distribution  ?  regression algorithm observe attribute training training set output regressor ?  )] Corollary let squared loss. exists distribution  ?  regression algorithm observe attribute training training set output regressor ?  )] lower bounds tight?recall attributes, possi \\x0cble learn regressor arbitrary precision]. also, notice order quantification orems: turns exists distribution hard algorithms (rar hard distribution algorithm). for regression absolute loss, setting learner limited fewer attributes training sample. orem shows case learner hope learn ”-optimal regressor orem let (mod), absolute loss. exists distribution  ?  regression algorithm observe attributes training training set )] ?  output regressor Corollary let absolute loss. exists distribution  ?  regression algorithm observe attributes training training set output regressor ?  )] complement findings regression analogous lower bound classification hinge loss., soft margin svm). orem let (mod), hinge loss. exists holds: exists distribution  ?  classification algorithm observe attributes training )] ?  training set output regressor Lower Bounds section discuss lower bounds regression missing attributes. warm, prove orem regression squared loss. while proof simple, illustrates main ideas lower bounds. give proof orem regression absolute loss. proofs remaining bounds deferred supplementary material.  Lower bounds squared loss Proof orem prove orem deterministic learning algorithms, namely, algorithms external randomization., randomization random samples drawn data distribution itself). this randomized algorithm thought distribution deterministic algorithms, independent data distribution. now, suppose let)}, uniform distributions } }, respectively. main observation learner observe attribute distinguish distributions probability greater matter samples given. this marginal distributions individual attributes same. thus prove orem show sets ”-optimal solutions distributions disjoint. indeed, suppose learning algorithm emits ) vector?  (where expectation random samples ? algorithm). markov inequality, holds) probability. hence, output algorithm distinguish distributions probability, contradicting indistinguishability property. set characterize sets ”-optimal solutions for x2x) x2x Note set ”-optimal regressors— ”}, set 1k2 ”}. let S20¿ ”}.  s20 suﬃcient show disjoint. Since¿ meaning however, S20 meaning member argued earlier, suﬃces prove orem.   Lower bounds absolute loss proof orem main idea show design distributions indistinguishable learner observe attributes sample distribution., marginals choice attributes identical), respective sets ”-optimal regressors disjoint. however, contrast orem handling general switching absolute loss introduce additional complexities proof require techniques. start constructing distributions let  , kxk1 (mod)}  , kxk1 (mod)}, uniform } }, respectively. from construction, hard choice attributes, marginals attributes distributions identical: uniform distribution bits. thus, distributions indistinguishable learner observe attributes example. let¿  —. x2x1 x2x2 turns subgradients), denote) respectively, expressed precisely. fact, full subgradient set point domain functions made explicit. with representations hand, show minimizers), respectively. Figure Geometric intuition Lemmas lower bounding absolute function acts relaxation true expected loss (depicted cone). fact, subgradient sets prove stronger property expected losses akin ?directional strong convexity? property respective minimizers. geometric idea property shown Figure lower bounded absolute function. lemma let for   Lemma let for    Given Lemmas proof orem immediate. proof orem direct consequence Lemmas obtain sets        sets ”-optimal regressors), respectively. all needed show separation ”-optimal sets showing separation manageable sets indeed, fix observe   hand, and,     exist sets disjoint. orem reasoning conclude proof orem  remains prove Lemmas proofs similar, prove Lemma defer proof Lemma supplementary material. proof Lemma write) Letting¿ x2x1 sign¿ x2x1   sign?¿ x2x1 sign?¿ x2x x2x1, sign?¿ x2x1, sign) x2x1, x2x1, sign?¿ \\x0cx2x1,   sign) number]. next, compute x2x1, x2x1            equality elementary identity prove Lemma supplementary material. now, kxk1 ,    matrix formed  express entire subgradient set explicitly      thus, choice result specific subgradient consider choices: note Xr1 Xr2 equality, fixed coordinate notice number elements non-zero values coordinate equal number ways choose remaining non-zero coordinates coordinates. observe subgradients   Xr1        Note that, set subgradients convex set, taking convex combination minimizer). Xr2 Given handle subgradient coeﬃcients polynomial set, show Observe that, fact(           d2e4         since written convex combination Let ?   similarly             again, written convex combination vectors subgradient set, conclude  well. subgradient inequality that toger imply   required.      general Algorithm Limited Precision Although established limits attainable precision learning problems, possibility reaching limit. section provide general algorithm, learner observe attributes achieve expected loss). provide pseudo-code Algorithm although similar AERR algorithm Hazan Koren]?which designed work squared loss?algorithm avoids necessity unbiased gradient estimator replacing original loss function slightly biased one. long loss function chosen \\x0ccarefully (and functions Lipschitz bounded), samples, algorithm return regressor limited precision. this contrast AERR arbitrarily precise regressor achieved samples. formally, Algorithm prove (proof supplementary material). orem let -lipschitz function defined]. assume distribution kxk2 — probability let max},   output Algorithm run G2B ?      )] Algorithm General algorithm regression/classification missing attributes input: Loss function training set],    output: Regressor Initialize kw1 arbitrarily Uniformly choose subset indices] replacement Set  Regression case: Choose¿ Classification case: Choose Update    max{kwt   : end : Return particular   )] learner observes optimum. attributes, expected loss)-away Conclusions Future Work limited attribute observation setting, shown informationoretic lower bounds variants regression, proving distributionindependent algorithm regression absolute loss attains error exist closing gap ridge regression suggested Hazan Koren]. shown proof technique applied regression absolute loss extended show similar bound classification hinge loss. addition, general purpose algorithm complements results providing means achieving error precision limit. interesting possibility future work bridge gap upper lower bounds precision limits, case exponential gap classification hinge loss. anor direction develop comprehensive understanding lower bounds terms general functions, classification logistic loss.',\n",
       " 'PP6173': 'this work studies problem detecting community structure dynamic network framework evolving graphs]. model underlying graph evolves time, subject probabilistic process modifies vertices edges graph. algorithm learn place network probing graph limited rate. main question evolving graph model design strategies \\x0cfor probing graph, obtain information suﬃcient maintain solution competitive solution computed entire underlying graph known. motivation studying model inadequacy classical computational paradigm, assumes perfect knowledge input data algorithm terminates. evolving graph model captures evolving decentralized nature large-scale online social networks. important part model limited number probes made time step. assumption motivated limitations imposed social network platforms Twitter facebook, network constantly evolving access structure API implements rate-limited oracle. cases rate-limits exogenously imposed., network consideration web), 30th Conference Neural Information Processing Systems (nips 2016), barcelona, spain. resource constraints prohibit making probes time step (probing large graphs stored machines costly operation). evolving graph model considered PageRank computation] connectivity problems]. work address problem community detection evolving graph model. probabilistic model evolution community structure network based stochastic block model (sbm]. widely accepted model probabilistic networks study community-detection methods, generates graphs embodied community structure. basic form model, vertices graph partitioned disjoint communities probabilistic manner. nodes community linked probability nodes distinct communities linked probability connections mutually independent. make step study community detection evolving-graph model evolving stochastic block model, nodes change communities stochastic process.  Our Contributions Our step define meaningful model community detection evolving graphs. extending stochastic block model evolving setting. evolving stochastic block model generates-node graph, nodes partitioned communities. time step, nodes change communities random fashion. namely, probability node reassigned; moved ith community (which call cluster probability form probability distribution. reassigned, neighborhood node updated accordingly. performed, unaware yet, step, budget queries perform graph (later values obtain meaningful results small algorithm catch changes; large makes problem trivial unrealistic). query algorithm consists choosing single node. result query list neighbors chosen node \\x0cmoment query. goal design algorithm issue queries time step report partitioning    close real    diﬃculty evolving-graph model that, observe process infinite amount time, events negligible probability place. design algorithms provide guarantees time recover highly events place. present results high level. simplicity description, assume query model slightly different, algorithm discover entire contents cluster query study algorithms step pick cluster query independently random predefined distribution. natural idea pick cluster proportionally size (which essentially querying cluster node chosen uniformly random). however, show strategy query cluster proportionally square root size. strategies equivalent cluster probabilities uniform, case skewed distributions. example clusters, probabilities  strategy incorrectly classifies nodes step expectation), compared(log2 nodes misclassified strategy. furrmore, experimental analysis suggests strategy probing cluster frequency proportional square root size eﬃcient ory, good choice practical application well. improve result give algorithm mixture cluster node queries. considered  step reports clusterings) misclassified nodes expectation). query strategy error bound expressed analysis show assumption query model dropped cost increasing number queries perform time step constant factor. terms complex, show algorithm optimal, giving matching lower bound. finally, show deal case   case querying node partial information cluster: connected subset nodes case impose assumptions provide algorithm node discover entire contents cluster(log) node queries. algorithm extend previous results case (and suﬃciently), cost performing (log) queries step. evolving graph model requires algorithm issue low number queries, analysis shows (under reasonable assumptions small number queries suﬃcient maintain high-quality clustering. oretical results hold large refore, perform simulations, demonstrate final oretically optimal algorithm beat algorithms small values Related Work Clustering community-detection techniques studied hundreds researchers. social networks, detecting clustering structure basic primitive finding communities users, sets users sharing similar interests aﬃliations]. recommendation networks cluster discovery improve quality recommendation systems]. relevant applications clustering found image processing, bioinformatics, image analysis text classification. prior evolving model, number dynamic computation models studied, online computation input data revealed step step), dynamic algorithms data structures input data modified dynamically), streaming computation input data revealed step step algorithm space constrained). hartamann. ] presented survey results clustering dynamic networks previously mentioned models. however, aforementioned models capture relevant features dynamic evolution large-scale data sets: data evolves slow pace algorithm learn data probing specific portions graph cost. stochastic block model, sociologists], recently received growing attention computer science, machine learning, statistics]. oretical level, work studied range parameters, communities recovered generated graph, case, communities. anor line research focused studying dynamic versions stochastic block model]. yet, lack oretical work modeling analyzing stochastic block models, generally community detection evolving graph. paper makes step direction. model paper analyze evolving extension stochastic block model]. call model evolving stochastic block model. model graph nodes, assigned clusters, probability nodes edge depends clusters assigned. formally, probability distribution   ). loss generality, paper assume     also,  assume  constant  beginning, node independently picks clusters. probability node picks cluster denote clustering nodes Nodes pick cluster connected fixed probability (which depend), pairs nodes pick clusters connected probability qij (also possibly dependent). note qij qji edges independent. denote min1 max1  far, model similar classic stochastic block model. introduce main distinctive property, evolution dynamics. evo \\x0clution model: analysis, assume graph evolves discrete time steps indexed natural numbers. nodes change cluster random manner. time step, node reassigned probability (independently nodes). happens, deletes edges neighbors, selects cluster probability finally adds edges probability nodes cluster probability qij nodes cluster    denote Cit set nodes assigned cluster reassignments time step Note denote cluster itself, Cit denote contents. query model: assume algorithm gar information clusters issuing queries. single query algorithm chooses single node learns list current neighbors time step, graph probed reassignments made. study algorithms learn cluster structure graph. goal algorithm report approximate clustering graph end time step, close true clustering define distance clusterings (partitions        nodes min,   ??  minimum permutations , }, denotes symmetric called difference sets., A4B  distance, error algorithm returned clustering). finally, analysis assume apart, formally assume that: Assumption  ], parameters fix later, have)  log (iii log Let discuss assumptions. observe (iii). however, prefer make separate, rely (iii). assumption (iii) assure nodes cluster single edge anor node cluster. analysis, set large (yet, constant), assure time node (log edges nodes cluster, high probability. assumption algorithm that, node finds nodes cluster (correctly high probability3 issues(log) queries. algorithm), slightly stronger (iii implies nodes cluster neighbors common), guarantees average) neighbors node belong cluster discussion: assumed graph model simple?certainly complex claim accurately models real-world graphs. neverless, work attempt formally study clustering dynamic graphs simplifying assumptions obtain provable guarantees. basic model, analysis rar involved. dealing diﬃcult features advanced model overshadow main findings. number queries low(log), Assumption relaxed considerably, close. time, recovery clusters (nonevolving) stochastic block model studied stricter ranges parameters. however, algorithms settings inspect considerably nodes require cluster probabilities close uniform]. results apply case clusters nonuniform sizes require apart. note studying classic stochastic block model standard assumption assume work sake simplicity. note extend definition pairs clusterings numbers clusters adding empty clusters clustering smaller number clusters. define term high probability Section our model assumes expectation) node cluster time step. however, analysis extended case nodes change cluster step expectation) cost times queries. generalizing results paper general models challenging open problem. interesting directions are, example, graphs models overlapping communities analyzing general model moving nodes clusters. algorithms Main Results section outline main results. simplicity, omit technical details, probability. particular, event high probability, probability  constant section constant defined interested studying behavior algorithm arbitrary time step. start stating lemma showing obtain algorithm run indefinitely long, suﬃces designing algorithm queries step, initializes log steps works high probability steps. lemma assume exists algorithm clustering evolving graphs issues queries step time step log reports clustering expected error correctly high probability. exists algorithm clustering evolving graphs issues queries step time step log reports clustering expected error). prove lemma, show suﬃces run instance assumed algorithm steps. way, instance longer guaranteed work, finished initializing report clusterings.  Simulating Node Queries show reduce problem setting algorithm query entire contents cluster. steps. step, give algorithm detecting cluster node(log) node queries. algorithm maintains score node graph. initially, scores equal algorithm queries(log) \\x0cneighbors adds score neighbor neighbor Assumption prove step, high probability gap minimum score node inside cluster maximum score node. lemma suppose Assumption holds. exists algorithm that, node correctly identifies nodes cluster high probability. issues(log) queries. observe Lemma effectively reduces problem case single execution algorithm entire cluster node, single query node case step, give data structure maintains approximate clustering nodes detects number cluster toger (approximate) cluster probabilities. internally, algorithm Lemma lemma suppose Assumption holds. exists data structure time step ) answer queries: cluster number return node cit .  Cit contents cluster return  return sequence       . usually, constant made arbitrarily large, tuning constants Assumption data structure runs correctly steps high probability issues(log) queries step. furrmore makes query step. note data structure node queries access graph, imposes numbering clusters consistently. describe high-level idea. step data structure selects node uniformly random discovers entire cluster algorithm Lemma show implies time steps cluster queried high probability. main challenge lies refreshing knowledge clusters. data structure internally maintains clustering  however, queries cluster clear   correspond. deal show number cluster time steps low (again, high probability), single cluster     . data structure Lemma simulate queries cluster way. assume discover contents cluster first, data structure node cit . algorithm Lemma entire cluster node finally, data structure verify wher Cit case probability. moreover, data structure assume algorithms initially number nodes values data structure provide algorithms number clusters (approximate) probabilities.   clustering Algorithms Using results Section, assume algorithms       queries.  query clusters directly. give simple clustering algorithm. algorithm computes probability distribution clusters, function cluster probability distribution   cluster probability distribution part input data, approximate distribution  data structure Lemma?this increases error algorithm step algorithm picks cluster independently constant factor. random distribution  order express upper determine probability distribution bound error terms distribution find sequence   minimizes error. orem suppose Assumption holds. exists algorithm clustering evolving graphs issues(log)queries stepand time step ) reports clustering expected error furrmore issues) queries step. clusterings algorithm low error, give result. algorithm orem queries cluster finds correct cluster assignment nodes reassigned queried. nodes immediately assigned cluster. however, querying algorithm discovers nodes recently reassigned queried, now). improved algorithm maintains queue nodes step removes nodes queue locates order locate single node discover cluster) (using algorithm Lemma data structure Lemma find cluster number). that, assign cluster immediately. results bound error. orem assume      suppose Assumption holds. exists algorithm clustering evolving graphs issues(log) queries step time step log reports clustering expected error    Furrmore issues) queries step. note assumption     needed orem true. however, ordering minimizes bound orem statement. compare upper bounds orems uniform distributions,   , analyses give upper bound), means average constant number nodes cluster contribute error. distribution, log   error algorithm(log2) expected error. furrmore cases, difference bigger. namely, define distribution follows. / log ((log set    error algorithm). log Lower Bound finally, provide lower bound problem detecting clusters \\x0cevolving stochastic block model. particular, implies case algorithm orem optimal constant factor). orem algorithm issues query time step detecting clusters evolving stochastic block model runs log steps average expected error         note orem extended algorithms allowed queries losing multiplicative factor/?. proof based observation algorithm queried clusters long enough, unaware nodes reassigned particular, node moves time algorithm query clusters time small chances guessing cluster Some nontrivial analysis needed show suﬃciently large number nodes exist, choices algorithm. experiments section compare optimal algorithm benchmarks show experimentally effectiveness. precisely, compare strategies select node explore step algorithm: optimal algorithm orem strategy probes random node, strategy selects random cluster probes random node cluster. compare probing strategies construct syntic instance model follows. build graph 10000 nodes communities expected size 250. number communities expected size proportional distribution communities? size power-law distribution parameter }. generate random communities experiment.001. note experiments number communities depends parameters. simplicity remaining section denote number communities specific experiment instance. step experiment generate random graph parameters above. random evolution starts step single node cluster. 10k evolution steps, construct data structure Lemma exploring clusters single random node step. finally, run strategies 25k additional steps update clusterings exploring single node step retrieving cluster.  Figure Comparing performance algorithms graphs community distributions. point execution algorithm compute cluster node exploring neighbors. figure show experimental results values }. repeat \\x0cexperiments times show average standard deviation. interesting note optimal queue algorithm outperforms significantly strategies. interesting note quality clustering worsens time, fact steps data structure reliable. finally, notice decreases communities? size distribution skewed, performance algorithms worsens closer anor, suggested oretical analysis. acknowledgments Marek Adamczyk helping mamatical derivations. work partly supported FET project MULTIPLEX. 317532 Google Focused Award ?algorithms large-scale Data analysis.? This work studies problem detecting community structure dynamic network framework evolving graphs]. model underlying graph evolves time, subject probabilistic process modifies vertices edges graph. algorithm learn place network probing graph limited rate. main question evolving graph model design strategies \\x0cfor probing graph, obtain information suﬃcient maintain solution competitive solution computed entire underlying graph known. motivation studying model inadequacy classical computational paradigm, assumes perfect knowledge input data algorithm terminates. evolving graph model captures evolving decentralized nature large-scale online social networks. important part model limited number probes made time step. this assumption motivated limitations imposed social network platforms Twitter facebook, network constantly evolving access structure API implements rate-limited oracle. even cases rate-limits exogenously imposed., network consideration web), 30th Conference Neural Information Processing Systems (nips 2016), barcelona, spain. resource constraints prohibit making probes time step (probing large graphs stored machines costly operation). evolving graph model considered PageRank computation] connectivity problems]. this work address problem community detection evolving graph model. our probabilistic model evolution community structure network based stochastic block model (sbm]. widely accepted model probabilistic networks study community-detection methods, generates graphs embodied community structure. basic form model, vertices graph partitioned disjoint communities probabilistic manner. nodes community linked probability nodes distinct communities linked probability all connections mutually independent. make step study community detection evolving-graph model evolving stochastic block model, nodes change communities stochastic process.  Our Contributions Our step define meaningful model community detection evolving graphs. extending stochastic block model evolving setting. evolving stochastic block model generates-node graph, nodes partitioned communities. time step, nodes change communities random fashion. namely, probability node reassigned; moved ith community (which call cluster probability form probability distribution. after reassigned, neighborhood node updated accordingly. while performed, unaware yet, step, budget queries perform graph (later values obtain meaningful results small algorithm catch changes; large makes problem trivial unrealistic). query algorithm consists choosing single node. result query list neighbors chosen node \\x0cmoment query. our goal design algorithm issue queries time step report partitioning    close real    diﬃculty evolving-graph model that, observe process infinite amount time, events negligible probability place. thus design algorithms provide guarantees time recover highly events place. let present results high level. for simplicity description, assume query model slightly different, algorithm discover entire contents cluster query study algorithms step pick cluster query independently random predefined distribution. one natural idea pick cluster proportionally size (which essentially querying cluster node chosen uniformly random). however, show strategy query cluster proportionally square root size. while strategies equivalent cluster probabilities uniform, case skewed distributions. for example clusters, probabilities  strategy incorrectly classifies nodes step expectation), compared(log2 nodes misclassified strategy. furrmore, experimental analysis suggests strategy probing cluster frequency proportional square root size eﬃcient ory, good choice practical application well. improve result give algorithm mixture cluster node queries. considered  step reports clusterings) misclassified nodes expectation). although query strategy error bound expressed analysis show assumption query model dropped cost increasing number queries perform time step constant factor. terms complex, show algorithm optimal, giving matching lower bound. finally, show deal case   case querying node partial information cluster: connected subset nodes case impose assumptions provide algorithm node discover entire contents cluster(log) node queries. this algorithm extend previous results case (and suﬃciently), cost performing (log) queries step. even evolving graph model requires algorithm issue low number queries, analysis shows (under reasonable assumptions small number queries suﬃcient maintain high-quality clustering. our oretical results hold large refore, perform simulations, demonstrate final oretically optimal algorithm beat algorithms small values Related Work Clustering community-detection techniques studied hundreds researchers. social networks, detecting clustering structure basic primitive finding communities users, sets users sharing similar interests aﬃliations]. recommendation networks cluster discovery improve quality recommendation systems]. relevant applications clustering found image processing, bioinformatics, image analysis text classification. prior evolving model, number dynamic computation models studied, online computation input data revealed step step), dynamic algorithms data structures input data modified dynamically), streaming computation input data revealed step step algorithm space constrained). hartamann. ] presented survey results clustering dynamic networks previously mentioned models. however, aforementioned models capture relevant features dynamic evolution large-scale data sets: data evolves slow pace algorithm learn data probing specific portions graph cost. stochastic block model, sociologists], recently received growing attention computer science, machine learning, statistics]. oretical level, work studied range parameters, communities recovered generated graph, case, communities. anor line research focused studying dynamic versions stochastic block model]. yet, lack oretical work modeling analyzing stochastic block models, generally community detection evolving graph. this paper makes step direction. Model paper analyze evolving extension stochastic block model]. call model evolving stochastic block model. model graph nodes, assigned clusters, probability nodes edge depends clusters assigned. more formally, probability distribution   ). without loss generality, paper assume     also,  assume  constant  beginning, node independently picks clusters. probability node picks cluster denote clustering nodes Nodes pick cluster connected fixed probability (which depend), pairs nodes pick clusters connected probability qij (also possibly dependent). note qij qji edges independent. denote min1 max1  far, model similar classic stochastic block model. now introduce main distinctive property, evolution dynamics. evo \\x0clution model: analysis, assume graph evolves discrete time steps indexed natural numbers. nodes change cluster random manner. time step, node reassigned probability (independently nodes). when happens, deletes edges neighbors, selects cluster probability finally adds edges probability nodes cluster probability qij nodes cluster for   denote Cit set nodes assigned cluster reassignments time step Note denote cluster itself, Cit denote contents. query model: assume algorithm gar information clusters issuing queries. single query algorithm chooses single node learns list current neighbors time step, graph probed reassignments made. study algorithms learn cluster structure graph. goal algorithm report approximate clustering graph end time step, close true clustering define distance clusterings (partitions        nodes min,   ??  minimum permutations , }, denotes symmetric called difference sets., A4B  distance, error algorithm returned clustering). finally, analysis assume apart, formally assume that: Assumption for ], parameters fix later, have)  log (iii log Let discuss assumptions. observe (iii). however, prefer make separate, rely (iii). assumption (iii) assure nodes cluster single edge anor node cluster. analysis, set large (yet, constant), assure time node (log edges nodes cluster, high probability. Assumption algorithm that, node finds nodes cluster (correctly high probability3 issues(log) queries. our algorithm), slightly stronger (iii implies nodes cluster neighbors common), guarantees average) neighbors node belong cluster discussion: assumed graph model simple?certainly complex claim accurately models real-world graphs. neverless, work attempt formally study clustering dynamic graphs simplifying assumptions obtain provable guarantees. even basic model, analysis rar involved. dealing diﬃcult features advanced model overshadow main findings. number queries low(log), Assumption relaxed considerably, close. time, recovery clusters (nonevolving) stochastic block model studied stricter ranges parameters. however, algorithms settings inspect considerably nodes require cluster probabilities close uniform]. results apply case clusters nonuniform sizes require apart. note studying classic stochastic block model standard assumption assume work sake simplicity. Note extend definition pairs clusterings numbers clusters adding empty clusters clustering smaller number clusters. define term high probability Section Our model assumes expectation) node cluster time step. however, analysis extended case nodes change cluster step expectation) cost times queries. generalizing results paper general models challenging open problem. some interesting directions are, example, graphs models overlapping communities analyzing general model moving nodes clusters. Algorithms Main Results section outline main results. for simplicity, omit technical details, probability. particular, event high probability, probability  constant section constant defined interested studying behavior algorithm arbitrary time step. start stating lemma showing obtain algorithm run indefinitely long, suﬃces designing algorithm queries step, initializes log steps works high probability steps. lemma assume exists algorithm clustering evolving graphs issues queries step time step log reports clustering expected error correctly high probability. exists algorithm clustering evolving graphs issues queries step time step log reports clustering expected error). prove lemma, show suﬃces run instance assumed algorithm steps. way, instance longer guaranteed work, finished initializing report clusterings.  Simulating Node Queries show reduce problem setting algorithm query entire contents cluster. this steps. step, give algorithm detecting cluster node(log) node queries. this algorithm maintains score node graph. initially, scores equal algorithm queries(log) \\x0cneighbors adds score neighbor neighbor Assumption prove step, high probability gap minimum score node inside cluster maximum score node. lemma suppose Assumption holds. exists algorithm that, node correctly identifies nodes cluster high probability. issues(log) queries. observe Lemma effectively reduces problem case single execution algorithm entire cluster node, single query node case step, give data structure maintains approximate clustering nodes detects number cluster toger (approximate) cluster probabilities. internally, algorithm Lemma lemma suppose Assumption holds. exists data structure time step ) answer queries: given cluster number return node cit .  given Cit contents cluster return  return sequence       . usually, constant made arbitrarily large, tuning constants Assumption data structure runs correctly steps high probability issues(log) queries step. furrmore makes query step. note data structure node queries access graph, imposes numbering clusters consistently. let describe high-level idea. step data structure selects node uniformly random discovers entire cluster algorithm Lemma show implies time steps cluster queried high probability. main challenge lies refreshing knowledge clusters. data structure internally maintains clustering  however, queries cluster clear   correspond. deal show number cluster time steps low (again, high probability), single cluster     . data structure Lemma simulate queries cluster way. assume discover contents cluster first, data structure node cit . algorithm Lemma entire cluster node finally, data structure verify wher Cit this case probability. moreover, data structure assume algorithms initially number nodes values data structure provide algorithms number clusters (approximate) probabilities.   clustering Algorithms Using results Section, assume algorithms       queries.  query clusters directly. this give simple clustering algorithm. algorithm computes probability distribution clusters, function cluster probability distribution   although cluster probability distribution part input data, approximate distribution  data structure Lemma?this increases error algorithm step algorithm picks cluster independently constant factor. random distribution  order express upper determine probability distribution bound error terms distribution find sequence   minimizes error. orem suppose Assumption holds. exists algorithm clustering evolving graphs issues(log)queries stepand time step ) reports clustering expected error furrmore issues) queries step. clusterings algorithm low error, give result. whenever algorithm orem queries cluster finds correct cluster assignment nodes reassigned queried. nodes immediately assigned cluster. however, querying algorithm discovers nodes recently reassigned queried, now). our improved algorithm maintains queue nodes step removes nodes queue locates order locate single node discover cluster) (using algorithm Lemma data structure Lemma find cluster number). once that, assign cluster immediately. this results bound error. orem assume      suppose Assumption holds. exists algorithm clustering evolving graphs issues(log) queries step time step log reports clustering expected error    Furrmore issues) queries step. Note assumption     needed orem true. however, ordering minimizes bound orem statement. let compare upper bounds orems for uniform distributions,   , analyses give upper bound), means average constant number nodes cluster contribute error. now distribution, log   error algorithm(log2) expected error. furrmore cases, difference bigger. namely, define distribution follows. let/ log ((log set    error algorithm). log Lower Bound finally, provide lower bound problem detecting clusters \\x0cevolving stochastic block model. particular, implies case algorithm orem optimal constant factor). orem every algorithm issues query time step detecting clusters evolving stochastic block model runs log steps average expected error         note orem extended algorithms allowed queries losing multiplicative factor/?. proof based observation algorithm queried clusters long enough, unaware nodes reassigned particular, node moves time algorithm query clusters time small chances guessing cluster Some nontrivial analysis needed show suﬃciently large number nodes exist, choices algorithm. Experiments section compare optimal algorithm benchmarks show experimentally effectiveness. more precisely, compare strategies select node explore step algorithm: optimal algorithm orem strategy probes random node, strategy selects random cluster probes random node cluster. compare probing strategies construct syntic instance model follows. build graph 10000 nodes communities expected size 250. number communities expected size proportional distribution communities? size power-law distribution parameter }. generate random communities experiment.001. note experiments number communities depends parameters. for simplicity remaining section denote number communities specific experiment instance. step experiment generate random graph parameters above. random evolution starts step single node cluster. 10k evolution steps, construct data structure Lemma exploring clusters single random node step. finally, run strategies 25k additional steps update clusterings exploring single node step retrieving cluster.  Figure Comparing performance algorithms graphs community distributions. point execution algorithm compute cluster node exploring neighbors. Figure show experimental results values }. repeat \\x0cexperiments times show average standard deviation. interesting note optimal queue algorithm outperforms significantly strategies. interesting note quality clustering worsens time, fact steps data structure reliable. finally, notice decreases communities? size distribution skewed, performance algorithms worsens closer anor, suggested oretical analysis. acknowledgments Marek Adamczyk helping mamatical derivations. this work partly supported FET project MULTIPLEX. 317532 Google Focused Award ?algorithms large-scale Data analysis.?',\n",
       " 'PP6264': 'many machine learning successes supervised learning, typically involves employing annotators label large quantities data task. however, humans learn acting learning consequences, feedback from) actions. humans act dialogs., make speech utterances) feedback human responses, rich information. pronounced student/teacher scenario teacher positive feedback successful communication corrections unsuccessful]. however, general reply dialog partner, teacher not, informative training signal learning language subsequent conversations. paper explore wher train machine learning models learn dialogs. ultimate goal develop intelligent dialog agent learn conducting conversations. learn feedback supplied natural language. however, machine learning tasks natural language processing literature form: \\x0care eir hand labeled word level (part speech tagging, named entity recognition), segment (chunking) sentence level (question answering) labelers. subsequently, learning algorithms developed learn kind supervision. refore develop evaluation datasets dialog-based language learning setting, developing models algorithms learn regime. contribution present work thus: introduce set tasks model natural feedback teacher assess feasibility dialog-based language learning.  evaluate baseline models data, comparing standard supervised learning.  introduce forward prediction model, learner predict teacher replies actions, yielding promising results, reward signal all. 30th Conference Neural Information Processing Systems (nips 2016), barcelona, spain. related Work human language learning usefulness social interaction natural infant directed conversations emphasized. review paper], usefulness feedback learning grammar disputed]. support usefulness feedback found language learning] learning students]. machine learning, line research focused supervised learning dialogs neural models]. question answering eir database knowledge] short stories] considered simple case dialog easy evaluate. tasks typically feedback. work feedback dialog learning, notably collecting knowledge answer questions], natural language instruction learning symbolic rules] binary feedback (rewards) learning parsers]. anor setting feedback setting reinforcement learning. ] summary dialog. however, approaches reward feedback model rar exploiting dialog feedback. neverless, reinforcement learning ideas good effect tasks well, understanding text adventure games], machine translation summarization]. recently] proposed reward-based learning framework learning learn. finally, forward prediction models, make work, learning eye tracking], controlling robot arms] vehicles], action-conditional video prediction atari games]. aware dialog. dialog-based Supervision Tasks dialog-based supervision forms. aware unsolved problem type learning strategy work setting. section refore identify modes dialog-based supervision, build learning problem each. goal evaluate learners type supervision. begin selecting existing datasets) single supporting fact problem babi datasets] consists short stories simulated world questions; \\x0cand) MovieQA dataset] large-scale dataset 100k questions 75k entities) based questions answers open movie database (omdb). dataset ten modes dialog-based supervision. supervision modes summarized fig. snippet babi dataset example. setups movieqa, examples Fig describe supervision setups. imitating Expert Student Task dialogs place teacher expert student semantically coherent answers. hence, task learner imitate expert student, expert mselves. example, imagine real-world scenario child observes parents talking, learn taking part conversation. note main goal paper examine non-expert learn improve dialog skills conversing. rest tasks concentrate goal. task natural baseline rest tasks input dialogs questions. positive Negative Feedback Task learner answers question teacher replies eir positive negative feedback. experiments subsequent responses variants, that incorrect? ?yes, that right?. datasets build templates positive feedback templates negative feedback. ?sorry, that. ?wrong?, etc. separate notion positive negative (orwise signal words notion) assume additional external reward signal part text. shown fig. task (+) denotes positive reward external dialog. feedback provided anor medium, nod head teacher). provided positive response. note difference supervision compared Task answer positive supervision. here, answers learner correct positive supervision. problem learner unskilled: supply incorrect answers ever) receive positive responses. figure Sample dialogs differing supervision signals (tasks). case story simplicity (shown first). black text spoken teacher, red text denotes responses learner, blue text provided expert student (which learner imitate), (+) denotes positive reward external dialog. feedback provided anor medium, nod head teacher). story Context (same examples below) Mary hallway. john moved bathroom. mary travelled kitchen. task Imitating Expert Student Where mary? :kitchen Where john? :bathroom Task Positive Negative Feedback Where mary? :playground, that incorrect. john? :bathroom yes, that right! (+) Task Answers Supplied Teacher Where mary? :bedroom, answer kitchen. john? :bathroom correct! (+) Task Hints Supplied Teacher Where mary? :bathroom, downstairs. john? :kitchen, upstairs. task Supporting Facts Supplied Teacher Where mary? :kitchen \\x0cyes, that right! (+) Where john? :hallway, John moved bathroom. task Partial Feedback Where mary? :kitchen yes, that right! john? :bathroom yes, that correct! (+) Task Feedback Where mary? :kitchen yes, that right! john? :bathroom yes, that correct! task Imitation Feedback Mixture Where mary? :kitchen Where john? :bathroom that right! (+) Task Asking For Corrections Where mary? :kitchen yes, that right! (+) Where john? :hallway, that right. :can? bathroom. task: Asking For Supporting Facts Where mary? :kitchen yes, that right! (+) Where john? :hallway, that right. :can? relevant fact John moved bathroom. figure Samples MovieQA dataset]. experiments language learning setups Figure sec.  examples tasks questions black answers red, (+) receiving positive reward. task Positive Negative Feedback What movies open source? revolution that right! (+) What movies Darren McGavin star? carmen sorry, that. directed film White elephant? curtiz, incorrect. task Answers Supplied Teacher What films hawaii? first Dates correct! (+) Who acted Licence kill? billy Madison, answer Timothy dalton. genre Saratoga Trunk? drama yes! (+) Answers Supplied Teacher Task teacher positive negative feedback Task learner answer incorrect, teacher responds correction. ?where mary?? answered incorrect answer ?bedroom? teacher responds, answer kitchen? fig. task learner extra information, effectively supervision signal Task Task hints Supplied Teacher Task corrections provided teacher provide exact answer Task hint. setting meant mimic real life occurrence provided partial information wrong. datasets providing class correct answer. , downstairs? answer kitchen, director? question ?who directed monsters, inc.?? (using OMDB metadata). supervision signal Task supporting Facts Supplied Teacher Task anor providing partial supervision incorrect answer explored. here, teacher reason (explanation) answer wrong referring fact supports true answer incorrect answer contradict. , John moved bathroom? incorrect answer ?where john? fig. task related termed strong supervision] \\x0csupporting facts answers question answering tasks. partial Feedback Task considers case external rewards) time correct answers, setting orwise identical Task attempts mimic realistic situation learning closely supervised teacher rewarding answers right) dialogs supervision external rewards). task attempts assess impact partial supervision. feedback Task external rewards all, text, orwise identical Tasks task explores wher learn answer setting. find experiments answer surprisingly yes, conditions. imitation Feedback Mixture Task combines Tasks goal learner learn successfully forms supervision once. mimics child observing pairs experts talking (task talk (task). For Corrections Anor natural collecting supervision learner questions teacher wrong. task tests simple instances, ?can?? wrong obtains teacher correct answer. related supervision Task learner dialog. potentially harder model relevant information spread larger context. Supporting Facts finally, Task, direct form supervision learner receive hint rar correct answer, relevant fact John moved bathroom? ?can? fig. task. related supervision Task learner request help. experiments constructed ten supervision tasks datasets download http/babi. built way: task fixed policy1 performing actions (answering questions) questions correct probability ?acc. chance red text correct figs. ). compare learning algorithms task values ?acc). cases training, validation test set provided. babi dataset consists 1000, 100 1000 questions task, movieqa 96k, 10k 10k respectively. movieqa includes knowledge base) 85k facts omdb, memory network model employ inverted index retrieval based question form relevant memories set] details. note policies fixed experiments paper reinforcement learning setting. learning Models Our main goal explore training strategies execute dialog-based language learning. end evaluate strategies: imitation learning, reward-based imitation, forward prediction, combination reward-based imitation forward prediction. subsequently describe turn. test approaches model architec \\x0cture: end-end memory network (memn2n]. memory networks recently introduced model shown Since policy fixed depend model learnt, coming anor agent agent past) eir case imperfect expert. figure Architectures (reward-based) imitation forward prediction. answer (action taken) Supervision (direct reward-based) Output candidate( answers( Output Predict Response Answer read addressin read addressin Memory Module Memory vectors Input addressin Internal state Vector (initially: query) Memory vectors) Model (reward-based) imitation learning Controller module read addressin addressin Memory Module Controller module read read Input Internal state Vector (initially: query) Model forward prediction \\x0cwell number text understanding tasks, including question answering, dialog] language modeling]. particular, outperform LSTMs baselines babi datasets] employ dialog-based learning modifications sec.  natural baseline model order explore differing modes learning setup. review memory networks, detailing explicit choices architecture made, show modified applied setting dialog-based language learning. memory Networks high-level description memory network architecture fig. ). input utterance dialog, set memories (context encode short-term memory. recent previous utterances replies, long-term memories. facts answering questions. context inputs converted vectors embeddings stored memory. goal produce output processing input address read memory, possibly multiple times, order form coherent reply. figure memory read twice, termed multiple ?hops? attention. step, input embedded matrix size embedding dimension size vocabulary, giving, input bag-ofwords vector. memory embedded matrix, giving Aci output addressing reading memory hop: p1i p1i softmax   here, match input memories computed taking product softmax, yielding giving probability vector memories. goal select memories relevant utterance. relevant large values p1i output memory representation constructed weighted sum memories. weighted memory output added original input), form state controller, rotation matrix2 attention memory repeated addressing vector, yielding: p2i p2i softmax¿ controller state updated anor matrix learnt. two-hop model final output defined: softmax¿ Ay1   ayc) optionally, dictionaries inputs, memories puts shared. candidate answers experiments set actions occur training set babi tasks, MovieQA set words retrieved. basic architecture, detail training strategies employ tasks. imitation Learning This approach involves simply imitating speakers observed dialogs, essentially supervised learning objective3  setting existing dialog learning, question answer systems, employ learning. examples arrive, triples, (assumed) good response utterance context case, memory network model defined trained stochastic gradient descent minimizing standard cross-entropy loss label reward-based Imitation actions poor choices, repeat shouldn treat supervised objective. setting positive reward obtained immediately (some) correct actions, zero. simple strategy apply imitation learning rewarded actions. rest actions simply discarded training set. strategy derived naturally degenerate case obtains applying policy gradient] setting policy fixed (see end sec. ). complex settings. actions made lead long-term environment delayed rewards) applying reinforcement learning algorithms necessary. policy gradient train MemN2N applied model policy. forward Prediction alternative method training perform forward prediction: aim, utterance speaker answer speaker., learner), predict response answer speaker , general predict changed state world action case involves utterance learn data propose modification memory networks, shown fig. ): essentially chop final output original network fig. ) replace additional layers compute forward prediction. part network remains access input context before. computation before. point observe computation output original network, scoring candidate answers. ) similar addressing memory. key idea perform anor ?hop? attention candidate answers rar memories. crucially, incorporate information action (candidate) selected dialog. ). ?hop?, resulting state controller forward prediction. concretely, compute: p3i (ayi  ]), p3i softmax¿ Ayi)  -dimensional vector, learnt, represents output action selected. obtaining forward prediction computed: softmax¿   ?    , computes scores responses answer candidates. mechanism. ) model compare answers answer terms supervision critical. example question answering answer incorrect model assign high correct answer output small amount  conversely, large amount  correct. thus, informs model response teacher. training performed cross-entropy loss label similar before. event large number candidates subsample negatives, keeping set. set answers similarly sampled, making method highly scalable. imitation learning algorithms strictly supervised algorithms, depend agent actions. setting here, task imitate speakers dialog. table Test accuracy (%) Single Supporting Fact babi dataset supervision approachess (training 1000 examples each) policies ?acc task successfully passed % accuracy obtained (shown blue). supervision Type ?acc Imitating Expert Student Positive Negative Feedback Answers Supplied Teacher Hints Supplied Teacher Supporting Facts Supplied Teacher Partial Feedback Feedback Imitation Feedback Mixture Asking For Corrections Asking For Supporting Facts Number completed tasks %) MemN2N imitation learning 100 100 100 MemN2N reward-based imitation (rbi 100 100 100 100 MemN2N forward prediction 100 100 100 100 MemN2N RBI 100 100 100 100 100 100 100 major benefit architectural design forward prediction training forward prediction criterion, test time ?chop off? top model retrieve original memory network model fig. ). predict answers evaluate performance directly goal well. finally, importantly, answer response carries pertinent supervision information choosing settings sec. (and fig. ), backpropagated model. simply case imitation, reward-shaping] reward-based imitation learning strategies concentrate pairs. reward-based Imitation Forward Prediction reward-based imitation learning architecture fig. ), forward prediction architecture additional layers Fig), learn jointly strategies. simply shares weights networks, performs gradient steps criteria, type action. makes reward signal signal fails potential supervision feedback subsequent utterances above. effectively ignores dialogs carrying reward. forward prediction contrast makes dialog-based feedback train reward. hand rewards handicap. hence, mixture strategies potentially powerful combination. table Test accuracy (%) MovieQA dataset dataset supervision approaches. numbers bold winners task choice ?acc supervision Type ?acc Imitating Expert Student Positive Negative Feedback Answers Supplied Teacher Hints Supplied Teacher Supporting Facts Supplied Teacher Partial Feedback Feedback Imitation Feedback Mixture Asking For Corrections Asking For Supporting Facts Mean Accuracy MemN2N imitation learning MemN2N reward-based imitation (rbi MemN2N forward prediction MemN2N RBI Experiments conducted experiments datasets Section before, task fixed policy performing actions (answering questions) questions correct probability ?acc compare training strategies sec. task values ?acc hyperparameters methods optimized validation sets. summary results reported Table babi dataset Table movieqa. observed results: imitation learning, ignoring rewards, poor learning strategy imitating inaccurate answers. ?acc. imitating expert (task hard beat.  reward-based imitation (rbi) performs rewards available, Table degrades sparse. ?acc. forward prediction) robust stable performance levels ?acc predicts answers implicitly make rewards outperformed RBI tasks, notably Tasks (because supervised learning) Task (because advantage positive rewards).  makes dialog feedback Tasks RBI not. explains feedback (tasks) (task), RBI cannot.  supplying full answers (task hints (task hints yes answers extra information (task).  positive feedback missing (task RBI suffers Table feedback.  surprising results experiments performs overall, feedback, \\x0cwill attempt explain subsequently. evident Task feedback) RBI hope succeeding positive examples. hand learns adequately.  tasks harder question immediately feedback.  combining RBI ameliorates failings each, yielding results. interesting aspects results works rewards. task ?know? difference words ?yes? ??correct? . words ?wrong? ?incorrect?, tend predict actions lead response ?yes, that right?? natural coherence predicting true answers leads greater accuracy forward prediction. , predict ?right? ?wrong? response teacher don answer. experiments policies ?acc sample negative answers equally, make learning simpler. conducted experiment Task (positive negative feedback) babi dataset biased policy: ?acc policy predicts incorrectly probability choosing random guess before choosing fixed answer bathroom. case method obtains% accuracy showing method works regime, before. conclusion presented set evaluation datasets models dialog-based language learning. ultimate goal line research move learner capable talking humans, humans effectively teach dialog. dialog-based language learning approach small step goal. paper studies restricted types feedback, positive feedback corrections types. however, potentially reply dialog feedback, learning. studied forward prediction, approaches tried, work too. future work develop furr evaluation methodologies test models presented here, ones, work settings. complex settings actions made lead long-term environment delayed rewards. extending reinforcement learning setting, full language generation. finally, dialog-based feedback medium learn non-dialog based skills. natural language dialog completing visual physical tasks. acknowledgments Arthur szlam-lan boureau, marc?aurelio ranzato, Ronan collobert, Michael auli, David grangier, Alexander miller, Sumit chopra, Antoine Bordes Leon Bottou helpful discussions feedback, Facebook Research team general supporting work. Many machine learning successes supervised learning, typically involves employing annotators label large quantities data task. however, humans learn acting learning consequences, feedback from) actions. when humans act dialogs., make speech utterances) feedback human responses, rich information. this pronounced student/teacher scenario teacher positive feedback successful communication corrections unsuccessful]. however, general reply dialog partner, teacher not, informative training signal learning language subsequent conversations. paper explore wher train machine learning models learn dialogs. ultimate goal develop intelligent dialog agent learn conducting conversations. learn feedback supplied natural language. however, machine learning tasks natural language processing literature form: \\x0care eir hand labeled word level (part speech tagging, named entity recognition), segment (chunking) sentence level (question answering) labelers. subsequently, learning algorithms developed learn kind supervision. refore develop evaluation datasets dialog-based language learning setting, developing models algorithms learn regime. contribution present work thus: introduce set tasks model natural feedback teacher assess feasibility dialog-based language learning.  evaluate baseline models data, comparing standard supervised learning.  introduce forward prediction model, learner predict teacher replies actions, yielding promising results, reward signal all. 30th Conference Neural Information Processing Systems (nips 2016), barcelona, spain. Related Work human language learning usefulness social interaction natural infant directed conversations emphasized. review paper], usefulness feedback learning grammar disputed]. support usefulness feedback found language learning] learning students]. machine learning, line research focused supervised learning dialogs neural models]. question answering eir database knowledge] short stories] considered simple case dialog easy evaluate. those tasks typically feedback. work feedback dialog learning, notably collecting knowledge answer questions], natural language instruction learning symbolic rules] binary feedback (rewards) learning parsers]. anor setting feedback setting reinforcement learning. ] summary dialog. however, approaches reward feedback model rar exploiting dialog feedback. neverless, reinforcement learning ideas good effect tasks well, understanding text adventure games], machine translation summarization]. recently] proposed reward-based learning framework learning learn. finally, forward prediction models, make work, learning eye tracking], controlling robot arms] vehicles], action-conditional video prediction atari games]. aware dialog. dialog-based Supervision Tasks dialog-based supervision forms. aware unsolved problem type learning strategy work setting. section refore identify modes dialog-based supervision, build learning problem each. goal evaluate learners type supervision. begin selecting existing datasets) single supporting fact problem babi datasets] consists short stories simulated world questions; \\x0cand) MovieQA dataset] large-scale dataset 100k questions 75k entities) based questions answers open movie database (omdb). for dataset ten modes dialog-based supervision. supervision modes summarized fig. snippet babi dataset example. setups movieqa, examples Fig describe supervision setups. imitating Expert Student Task dialogs place teacher expert student semantically coherent answers. hence, task learner imitate expert student, expert mselves. for example, imagine real-world scenario child observes parents talking, learn taking part conversation. note main goal paper examine non-expert learn improve dialog skills conversing. rest tasks concentrate goal. this task natural baseline rest tasks input dialogs questions. positive Negative Feedback Task learner answers question teacher replies eir positive negative feedback. experiments subsequent responses variants, that incorrect? ?yes, that right?. datasets build templates positive feedback templates negative feedback. ?sorry, that. ?wrong?, etc. separate notion positive negative (orwise signal words notion) assume additional external reward signal part text. shown fig. Task (+) denotes positive reward external dialog. feedback provided anor medium, nod head teacher). this provided positive response. note difference supervision compared Task answer positive supervision. here, answers learner correct positive supervision. this problem learner unskilled: supply incorrect answers ever) receive positive responses. Figure Sample dialogs differing supervision signals (tasks). case story simplicity (shown first). black text spoken teacher, red text denotes responses learner, blue text provided expert student (which learner imitate), (+) denotes positive reward external dialog. feedback provided anor medium, nod head teacher). story Context (same examples below) Mary hallway. john moved bathroom. mary travelled kitchen. task Imitating Expert Student Where mary? :kitchen Where john? :bathroom Task Positive Negative Feedback Where mary? :playground, that incorrect. where john? :bathroom yes, that right! (+) Task Answers Supplied Teacher Where mary? :bedroom, answer kitchen. where john? :bathroom correct! (+) Task Hints Supplied Teacher Where mary? :bathroom, downstairs. where john? :kitchen, upstairs. task Supporting Facts Supplied Teacher Where mary? :kitchen \\x0cyes, that right! (+) Where john? :hallway, John moved bathroom. task Partial Feedback Where mary? :kitchen yes, that right! where john? :bathroom yes, that correct! (+) Task Feedback Where mary? :kitchen yes, that right! where john? :bathroom yes, that correct! task Imitation Feedback Mixture Where mary? :kitchen Where john? :bathroom that right! (+) Task Asking For Corrections Where mary? :kitchen yes, that right! (+) Where john? :hallway, that right. :can? bathroom. task: Asking For Supporting Facts Where mary? :kitchen yes, that right! (+) Where john? :hallway, that right. :can? relevant fact John moved bathroom. figure Samples MovieQA dataset]. experiments language learning setups Figure sec.  examples tasks questions black answers red, (+) receiving positive reward. task Positive Negative Feedback What movies open source? revolution that right! (+) What movies Darren McGavin star? carmen sorry, that. who directed film White elephant? Curtiz, incorrect. task Answers Supplied Teacher What films hawaii? First Dates correct! (+) Who acted Licence kill? billy Madison, answer Timothy dalton. what genre Saratoga Trunk? drama yes! (+) Answers Supplied Teacher Task teacher positive negative feedback Task learner answer incorrect, teacher responds correction. for ?where mary?? answered incorrect answer ?bedroom? teacher responds, answer kitchen? fig. Task learner extra information, effectively supervision signal Task Task hints Supplied Teacher Task corrections provided teacher provide exact answer Task hint. this setting meant mimic real life occurrence provided partial information wrong. datasets providing class correct answer. , downstairs? answer kitchen, director? question ?who directed monsters, inc.?? (using OMDB metadata). supervision signal Task supporting Facts Supplied Teacher Task anor providing partial supervision incorrect answer explored. here, teacher reason (explanation) answer wrong referring fact supports true answer incorrect answer contradict. for, John moved bathroom? incorrect answer ?where john? fig. Task this related termed strong supervision] \\x0csupporting facts answers question answering tasks. partial Feedback Task considers case external rewards) time correct answers, setting orwise identical Task this attempts mimic realistic situation learning closely supervised teacher rewarding answers right) dialogs supervision external rewards). task attempts assess impact partial supervision. Feedback Task external rewards all, text, orwise identical Tasks this task explores wher learn answer setting. find experiments answer surprisingly yes, conditions. imitation Feedback Mixture Task combines Tasks goal learner learn successfully forms supervision once. this mimics child observing pairs experts talking (task talk (task). asking For Corrections Anor natural collecting supervision learner questions teacher wrong. task tests simple instances, ?can?? wrong obtains teacher correct answer. this related supervision Task learner dialog. this potentially harder model relevant information spread larger context. asking Supporting Facts finally, Task, direct form supervision learner receive hint rar correct answer, relevant fact John moved bathroom? ?can? fig. Task. this related supervision Task learner request help. experiments constructed ten supervision tasks datasets download http/babi. built way: task fixed policy1 performing actions (answering questions) questions correct probability ?acc. chance red text correct figs. ). compare learning algorithms task values ?acc). cases training, validation test set provided. for babi dataset consists 1000, 100 1000 questions task, movieqa 96k, 10k 10k respectively. movieqa includes knowledge base) 85k facts omdb, memory network model employ inverted index retrieval based question form relevant memories set] details. note policies fixed experiments paper reinforcement learning setting. Learning Models Our main goal explore training strategies execute dialog-based language learning. end evaluate strategies: imitation learning, reward-based imitation, forward prediction, combination reward-based imitation forward prediction. subsequently describe turn. test approaches model architec \\x0cture: end-end memory network (memn2n]. memory networks recently introduced model shown Since policy fixed depend model learnt, coming anor agent agent past) eir case imperfect expert. Figure Architectures (reward-based) imitation forward prediction. answer (action taken) Supervision (direct reward-based) Output candidate( answers( Output Predict Response Answer read addressin read addressin Memory Module Memory vectors Input addressin Internal state Vector (initially: query) Memory vectors) Model (reward-based) imitation learning Controller module read addressin addressin Memory Module Controller module read read Input Internal state Vector (initially: query) Model forward prediction \\x0cwell number text understanding tasks, including question answering, dialog] language modeling]. particular, outperform LSTMs baselines babi datasets] employ dialog-based learning modifications sec.  natural baseline model order explore differing modes learning setup. review memory networks, detailing explicit choices architecture made, show modified applied setting dialog-based language learning. memory Networks high-level description memory network architecture fig. ). input utterance dialog, set memories (context encode short-term memory. recent previous utterances replies, long-term memories. facts answering questions. context inputs converted vectors embeddings stored memory. goal produce output processing input address read memory, possibly multiple times, order form coherent reply. figure memory read twice, termed multiple ?hops? attention. step, input embedded matrix size embedding dimension size vocabulary, giving, input bag-ofwords vector. each memory embedded matrix, giving Aci output addressing reading memory hop: p1i p1i softmax   here, match input memories computed taking product softmax, yielding giving probability vector memories. goal select memories relevant utterance. relevant large values p1i output memory representation constructed weighted sum memories. weighted memory output added original input), form state controller, rotation matrix2 attention memory repeated addressing vector, yielding: p2i p2i softmax¿ controller state updated anor matrix learnt. two-hop model final output defined: softmax¿ Ay1   AyC) optionally, dictionaries inputs, memories puts shared. candidate answers experiments set actions occur training set babi tasks, MovieQA set words retrieved. having basic architecture, detail training strategies employ tasks. imitation Learning This approach involves simply imitating speakers observed dialogs, essentially supervised learning objective3 this setting existing dialog learning, question answer systems, employ learning. examples arrive, triples, (assumed) good response utterance context case, memory network model defined trained stochastic gradient descent minimizing standard cross-entropy loss label reward-based Imitation actions poor choices, repeat shouldn treat supervised objective. setting positive reward obtained immediately (some) correct actions, zero. simple strategy apply imitation learning rewarded actions. rest actions simply discarded training set. this strategy derived naturally degenerate case obtains applying policy gradient] setting policy fixed (see end sec. ). complex settings. actions made lead long-term environment delayed rewards) applying reinforcement learning algorithms necessary. policy gradient train MemN2N applied model policy. forward Prediction alternative method training perform forward prediction: aim, utterance speaker answer speaker., learner), predict response answer speaker that, general predict changed state world action case involves utterance learn data propose modification memory networks, shown fig. ): essentially chop final output original network fig. ) replace additional layers compute forward prediction. part network remains access input context before. computation before. point observe computation output original network, scoring candidate answers. ) similar addressing memory. our key idea perform anor ?hop? attention candidate answers rar memories. crucially, incorporate information action (candidate) selected dialog. ). after ?hop?, resulting state controller forward prediction. concretely, compute: p3i (ayi  ]), p3i softmax¿ Ayi)  -dimensional vector, learnt, represents output action selected. after obtaining forward prediction computed: softmax¿   ?    that, computes scores responses answer candidates. mechanism. ) model compare answers answer terms supervision critical. for \\x0cexample question answering answer incorrect model assign high correct answer output small amount  conversely, large amount  correct. thus, informs model response teacher. training performed cross-entropy loss label similar before. event large number candidates subsample negatives, keeping set. set answers similarly sampled, making method highly scalable. Imitation learning algorithms strictly supervised algorithms, depend agent actions. that setting here, task imitate speakers dialog. Table Test accuracy (%) Single Supporting Fact babi dataset supervision approachess (training 1000 examples each) policies ?acc task successfully passed % accuracy obtained (shown blue). supervision Type ?acc Imitating Expert Student Positive Negative Feedback Answers Supplied Teacher Hints Supplied Teacher Supporting Facts Supplied Teacher Partial Feedback Feedback Imitation Feedback Mixture Asking For Corrections Asking For Supporting Facts Number completed tasks %) MemN2N imitation learning 100 100 100 MemN2N reward-based imitation (rbi 100 100 100 100 MemN2N forward prediction 100 100 100 100 MemN2N RBI 100 100 100 100 100 100 100 major benefit architectural design forward prediction training forward prediction criterion, test time ?chop off? top model retrieve original memory network model fig. ). one predict answers evaluate performance directly goal well. finally, importantly, answer response carries pertinent supervision information choosing settings sec. (and fig. ), backpropagated model. this simply case imitation, reward-shaping] reward-based imitation learning strategies concentrate pairs. reward-based Imitation Forward Prediction reward-based imitation learning architecture fig. ), forward prediction architecture additional layers Fig), learn jointly strategies. one simply shares weights networks, performs gradient steps criteria, type action. makes reward signal signal fails potential supervision feedback subsequent utterances above. effectively ignores dialogs carrying reward. forward prediction contrast makes dialog-based feedback train reward. hand rewards handicap. hence, mixture strategies potentially powerful combination. table Test accuracy (%) MovieQA dataset dataset supervision approaches. numbers bold winners task choice ?acc supervision Type ?acc Imitating Expert Student Positive Negative Feedback Answers Supplied Teacher Hints Supplied Teacher Supporting Facts Supplied Teacher Partial Feedback Feedback Imitation Feedback Mixture Asking For Corrections Asking For Supporting Facts Mean Accuracy MemN2N imitation learning MemN2N reward-based imitation (rbi MemN2N forward prediction MemN2N RBI Experiments conducted experiments datasets Section before, task fixed policy performing actions (answering questions) questions correct probability ?acc compare training strategies sec. task values ?acc hyperparameters methods optimized validation sets. summary results reported Table babi dataset Table movieqa. observed results: imitation learning, ignoring rewards, poor learning strategy imitating inaccurate answers. ?acc. for imitating expert (task hard beat.  reward-based imitation (rbi) performs rewards available, Table degrades sparse. ?acc. forward prediction) robust stable performance levels ?acc however predicts answers implicitly make rewards outperformed RBI tasks, notably Tasks (because supervised learning) Task (because advantage positive rewards).  makes dialog feedback Tasks RBI not. this explains feedback (tasks) (task), RBI cannot.  supplying full answers (task hints (task hints yes answers extra information (task).  when positive feedback missing (task RBI suffers Table feedback.  one surprising results experiments performs overall, feedback, \\x0cwill attempt explain subsequently. this evident Task feedback) RBI hope succeeding positive examples. hand learns adequately.  tasks harder question immediately feedback.  combining RBI ameliorates failings each, yielding results. one interesting aspects results works rewards. Task ?know? difference words ?yes? ??correct? . words ?wrong? ?incorrect?, tend predict actions lead response ?yes, that right?? this natural coherence predicting true answers leads greater accuracy forward prediction. that, predict ?right? ?wrong? response teacher don answer. experiments policies ?acc sample negative answers equally, make learning simpler. conducted experiment Task (positive negative feedback) babi dataset biased policy: ?acc policy predicts incorrectly probability choosing random guess before choosing fixed answer bathroom. case method obtains% accuracy showing method works regime, before. Conclusion presented set evaluation datasets models dialog-based language learning. ultimate goal line research move learner capable talking humans, humans effectively teach dialog. dialog-based language learning approach small step goal. this paper studies restricted types feedback, positive feedback corrections types. however, potentially reply dialog feedback, learning. studied forward prediction, approaches tried, work too. future work develop furr evaluation methodologies test models presented here, ones, work settings. complex settings actions made lead long-term environment delayed rewards. extending reinforcement learning setting, full language generation. finally, dialog-based feedback medium learn non-dialog based skills. natural language dialog completing visual physical tasks. acknowledgments Arthur szlam-lan boureau, marc?aurelio ranzato, Ronan collobert, Michael auli, David grangier, Alexander miller, Sumit chopra, Antoine Bordes Leon Bottou helpful discussions feedback, Facebook Research team general supporting work.',\n",
       " 'PP6272': 'collaborative preference completion task jointly learning bipartite dyadic) preferences set entities shared list items., user?item interactions recommender system]. commonly assumed entity?item preferences generated small number latent hidden factors, equivalently, underlying preference matrix assumed low rank. furr, observed aﬃnity scores explicit implicit feedback treated exact mildly perturbed) entries unobserved preference matrix, preference completion task \\x0cnaturally fits framework low rank matrix completion]. generally, low rank matrix completion involves predicting missing entries low rank matrix vanishing fraction entries observed noisy channel. low rank matrix completion estimators algorithms developed literature, strong oretical guarantees empirical performance]. recent research preference completion literature noted matrix completion estimator collaborative preference estimation misguided] observed entity?item aﬃnity scores implicit/explicit feedback potentially subject systematic monotonic transformations arising limitations feedback collection., quantization 30th Conference Neural Information Processing Systems (nips 2016), barcelona, spain. inherent biases. simple user biases linear transofmations handled low rank matrix framework, complex transformations quantization potentially increase rank observed preference score matrix significantly, adversely affecting recovery standard low rank matrix completion]. furr, common practice measuring preferences numerical scores, predictions deployed evaluated based item ranking. recommender systems, user recommendations presented ranked list items underlying scores. authors shown favorable empirical/oretical performance square error preference matrix translate performance performance measured ranking metrics]. thus, collaborative preference estimation posed collection coupled learning rank (letor) problems], seek jointly learn preference rankings set entities, exploiting low dimensional latent structure underlying preference values. paper considers preference completion general collaborative LETOR setting. importantly, observations assumed reliable indicators relative preference ranking, numerical scores deviant ground truth low rank preference matrix. refore, aim addressing preference completion generalizations: simple setting, entity, score vector representing observed aﬃnity interactions assumed generated arbitrary monotonic transformation entries ground truth preference matrix. make furr generative assumptions observed scores monotonicity respect underlying low rank preference matrix.  general setting, observed preferences entity represent specifications partial ranking form directed acyclic graph (dag) nodes represent subset items, edge represents strict ordering pair nodes. rankings encountered preference scores consolidated multiple sources feedback., comparative feedback (pairwise listwise) solicited independent subsets items. generalized setting handled standard matrix completion transforming DAG orderings score vector. work part motivated application neuroimaging meta-analysis outlined following. cognitive neuroscience aims quantify link brain function behavior. interaction measured humans Functional Magnetic Resonance Imaging (fmri) experiments measure brain activity response behavioral tasks. analysis, conclusions summarized neuroscience publications include table brain locations actively activated response experimental stimulus. results synsized meta-analysis techniques derive accurate predictions brain activity cognitive terms (also forward inference) prediction cognitive terms brain regions (also reverse inference). study, data neurosynth] public repository1 automatically scrapes information published associations brain regions terms cognitive neuroscience experiments. key contributions paper summarized below.  propose convex estimator low rank preference completion limited supervision, addressing) arbitrary monotonic transformations preference scores) partial rankings items (section). derive generalization error bounds surrogate ranking loss quantifies trade?off data?fit regularization (section).  propose eﬃcient algorithms estimate total partially ordered observations. case total orders, spite increased generality, computational complexity algorithm log factor standard convex algorithms matrix completion (section).  proposed algorithm evaluated application identifying associations brain?regions cognitive terms neurosynth dataset] (section). large scale meta-analysis synsizing information literature related tasks potential lead insights role brain regions cognition behavior.  Notation For matrix     singular values  nuclear norm operator norm kop Frobenius norm http://neurosynth.org,    vector set indexed denoted simply unambiguous.     denote subset indices matrix Rd1      denotes subset entries —?— column.     —?—},  (xis —?— linear subsampling operator?? —?— rd1 adjoint, ?? . conciseness, notation denote ). related Work Matrix completion: Low rank matrix completion extensive literature; examples include] ors. however, bulk works including context ranking/recommendation] Koyejo. plications focus) fitting observed numerical scores squared loss) evaluating results parameter/rating recovery metrics root squared error (rmse). shortcomings estimators results squared loss ranking applications studied recent research]. motivated collaborative ranking applications, growing interest addressing matrix completion explicit LETOR framework. weimer. ] propose estimators involve non?convex optimization problems algorithmic convergence generalization behavior understood. recent works provide parameter recovery guarantees pairwise/listwise ranking observations specific probabilistic distributional assumptions observed rankings]. comparison, estimators algorithms paper agnostic generative distribution, wider applicability. learning rank (letor): LETOR structured prediction task rank ordering relevance list items function pre?selected features]. currently, leading algorithms LETOR listwise methods approach paper), fully exploit ranking structure ordered observations, offer modeling ﬂexibility compared pointwise] pairwise methods]. recent listwise LETOR algorithm proposed idea monotone retargeting], elegantly addresses listwise learning rank (letor) task maintaining relative simplicity scalability pointwise estimation. furr extended incorporate margins margin equipped monotonic retargeting (memr) formulation] preclude trivial solutions arise scale invariance initial estimate Acharyya. ]. estimator proposed paper inspired idea revisited paper. collaborative preference completion, rar learning functional mapping features ranking, seek exploit low rank structure jointly modeling preferences collection entities access preference indicative features. single Index Models (sims) finally, literature monotonic single index models (sims) considers estimation unknown monotonic transformations]. however, algorithms SIMs designed solve harder problem estimating non?parametric monotonic transformation evaluated parameter recovery rar ranking performance. general, furr assumptions, sample complexity SIM estimators lends unsuitable high dimensional estimation. existing high dimensional estimators learning SIMs typically assume Lipschitz continuity monotonic transformation explicitly observed score values bounding Lipsciptz constant monotonic transformation]. comparison, proposed model completely agnostic numerical values preference scores. preference Completion Partial Rankings Let unobserved true preference scores entities items denoted rank min matrix  rd1 entity  observe partial total ordering preferences subset \\x0citems denoted   denotes number items relative preferences entity observed, denotes entity-item index set denotes index set collected entities.  denote sampling distribution observed preferences entity typically represented listwise preference score vector) rnj   (??  arbitrary unknown monotonic transformations, rd1 non?adversarial noise matrix sampled distribution preference completion task estimate unseen rankings column subset orderings?   arbitrary, exact values) inconsequential, observed preference order constraint set parameterized margin parameter follows: Definition (?margin Isotonic set) set vectors isotonic margin parameter? )   ],    addition score vectors, isotonic sets form? ) equivalently defined DAG], denotes partial ranking vertices, convention,   ? ),   note Definition ties broken random., yi1 yi2 ? ), xi1  xi2  ordering xi1 xi2 specified. ) denote smallest entry  distinguish special cases observation representing partial ranking]. ) Strict Total order)   ) ) Blockwise Total order) )     ) unique values. ) Arbitrary dag: Partial order induced DAG).  Monotone Retargeted Low Rank Estimator Consider scalable pointwise learning algorithm fits model exact preferences scores. generative model (besides monotonicity) assumed raw numerical scores observations, principle, scores) entity replaced retargeted rankingn preserving scores., vector) monotone Retargeting] exploits observation address combinatorial listwise ranking problem] maintaining relative simplicity scalability pointwise estimates (regression). key idea alternately fit pointwise algorithm current relevance scores, retarget scores searching space monotonic transformations scores. approach extends generalizes monotone retargeting preference prediction task. begin motivating algorithm noise free setting, clear  ) seek estimate candidate preference matrix intersection) data constraints observed preference rankings ) model constraints case low rankness induced constraining nuclear norm kxk?  robust estimation presence noise, extend noise free approach incorporating soft penalty constraint violations.  —?— slight abuse notation rnj denote vector entries —?—  incorporating soft penalties, monotone retargeted low rank estimator: Argmin min ?kxk?   )k22.  —?— parameter controls trade?off nuclear norm regularization data fit, set minimizers). note? ) convex,  scaling?   ? )} ? ). estimate computed eﬃcient convex optimization algorithms handle arbitrary monotonic transformation preference scores, providing higher ﬂexibility compared standard matrix completion. ) terms parameters, due geometry problem, turns jointly identifiable, discussed proposition. proposition optimization) jointly convex). furr, (?,  lead equivalent estimators, specifically(?,  (?  since, positive scaling preserves resultant preference order, Proposition loss generality, requires tuning remaining fixed. optimization Algorithm optimization problem) jointly convex). furr, proximal show operator non?differential component estimate ?kxk?  ) eﬃciently computable. motivates proximal gradient descent algorithm] jointly update). step size  resulting updates follows: update: Singular Value Thresholding proximal operator ? singular thresholding operator  singular decomposition   )  , soft thresholding operator  max }.  update: Parallel Projections For hard constraints proximal operator Euclidean projection constraints argminz?vk22.  )  updates decouple entity (column trivially parallelized. eﬃcient projections) discussed Section. ) Algorithm Proximal Gradient Descent) input paramter    until (stopping criterion?? )   ProjRnj ) ) Projection  begin definitions characterizing? ). definition (adjacent difference operator) adjacent difference operator denoted  defined   ]. definition (incidence matrix) For directed graph), incidence matrix — that: directed edge ith node node Projection? ) closely related isotonic regression problem finding univariate squares fit consistent order constraints (without margins). isotonic regression problem solved) complexity classical Pool Adjacent Violators (pav) algorithm: pav) argmin . zi0    discuss, simple adaptations isotonic regression projection -margin isotonic sets special cases interest summarized Table ) Strict Total order)   ) setting, constraint set characterized? ) }, vector ones. case projection? ) differs) requiring ?separation straight forward extension PAV algorithm] used. dsl vector dsl simple substitutions, ProjRn) pav dsl dsl   ) Blockwise Total order) )    ) This common setting supervision preference completion applications, listwise ranking preferences obtained ratings discrete quantized levels   prevalent. partitioned blocks   entries partition equal, blocks mselves strictly ordered.,  inf inf  let dbl dbl vector block indices, .., set valid permutations permute entries blocks ? )   )  dbl propose steps compute ProjRn) case: step  .  ],   sort(xpk) Step  ) dbl dbl correctness) summarized lemma. lemma Estimate) unique minimizer argminkz xk22.    )  dbl ) Arbitrary dag], arbitrary DAG (not necessarily connected) represent consistent order constraints vertices., partial rankings consolidated multiple listwise/pairwise scores. case, ?margin isotonic set? ) . definition). ddag ith entry ddag length longest directed chain connecting topological descendants node easily verified that, isotonic regression algorithm arbitrary DAGs applied ddag projection? ). general setting, isotonic regression algorithm exact solution requires(nm2 log computation], number edges While case), computation prohibitive, include case completeness. note case partial DAG ordering handled standard matrix completion setting consolidating partial ranks total order.  ) ProjRn  }   ) } ) Computation pav ? pav? ) dbl dbl isoreg ddag)+ddag log log Table Summary algorithms ProjRn)  Computational Complexity easily verified gradient? ) zk22?lipschitz continuous. thus, standard results convegence proximal gradient descent], Algorithm,converges error objective/) iterations. compared proximal algorithms standard matrix completion], additional complexity Algorithm arises update) simple substitution) standard matrix completion. total orders, update) highly eﬃcient asymptotically additional log —?— factor computational costs standard matrix completion. generalization Error Recall (noisy) partial rankings subset items user, obtained noise matrix unknown arbitrary transformations preserve ranking order column. estimator algorithms independent sampling distribution generating }). section quantify simple generalization error bounds). assumption (sampling? fixed assume sampling distribution. fixed constant pre?specified parameter denoting length single listwise observation.    — log) uniform )) ) randsample? ) (?? )). ) furr, define notation  column listwise scores) jointly define consistent partial ranking scores subsets monotonically transformed preference vector  consistent ordering represented DAG) partialorder}). note log samples ensures column included sampling high probability. definition (projection loss) Let], define partial ordering total order respectively. define convex surrogate loss partial rankings.  , minz? ) zk2 estimate). scaling orem (generalization bound) Let kxkf constants holds probability greater  observed rankings)  drawn— log— log1 log log/?  ))  )) ),? )   — orem quantifies test projection loss random length items) drawn random entity/user). bound trade?off observable training error complexity defined nuclear norm estimate. experiments evaluate model collaborative preference estimation tasks) standard user-item recommendataion task benchmarked dataset movielens) identifying associations brain?regions cognitive terms neurosynth dataset]. baselines: baseline models compared experiments: retargeted Matrix Completion (rmc): estimator proposed).  standard Matrix Completion (smc]: primarily compare estimator standard convex estimator matrix completion nuclear norm minimization.  collaborative Filtering Ranking cofi-rank]: This work addresses collaborative filtering task listwise ranking setting. SMC mrpc, hyperparameters tuned grid search logarithmic scale. due high computational cost tuning parameters cofirank, code default parameters provided authors. evaluation metrics: performance preference estimation tasks evaluated ranking metrics) Normalized Discounted Cummulative Gains (ndcg) precision) Spearmann rho) Kendall tau, metrics measure correlation complete ordering list, metrics primarily evaluate correctness ranking top list (see Liu. . ] furr details metrics). Movielens dataset (blockwise total order) Movielens movie recommendation website administered GroupLens research. competitive benchmarked movielens 100k dataset. ?fold train/test splits provided dataset test splits non-overlapping). discarded small number users ratings training data splits. resultant dataset consists 923 users 1682 items. ratings blockwise ordered taking values set,   }. testing, user, competing models return ranking test-items, performance averaged test-users. table presents results evaluation averaged train/test splits Movielens dataset, standard deviation. proposed retargeted matrix completion (rmc) significantly consistently outperforms SMC cofi-rank] ranking metrics. ndcg precision Spearman Rho Kendall Tau RMC.7984.0213.7546.0320.4137.0099.3383.0117) SMC.7863.0243.7429.0295.3722.0106.3031.0117) cofi-rank.7731.0213.7314.0293.3681.0082.2993.0110) Table Ranking performance recommendations Movielens 100k. table shows standard deviation fold train/test splits. reported metrics, higher values]. neurosynth Dataset (almost total order) neurosynth] publicly database consisting data automatically extracted large collection functional magnetic resonance imaging (fmri) publications,362 publications current version). publication database abstract text reported-dimensional peak activation coordinates study. text pre-processed remove common stop-words, text% frequency, leaving total 3169 terms. applied standard brain map activations, removing voxels grey matter. activations downsampled 2mm3 voxels 10mm3 voxels nilearn python package, resulting total 1231 dense voxels. aﬃnity measure 3169 terms 1231 consolidated voxels obtained multiplying term publication publication voxels matrices. resulting data dense high-rank preference matrix. tied preference values, setting fits case total ordered observations (case Section). data, reverse inference task ranking cognitive concepts (terms) brain region (voxel]. trainval-test% randomly sampled entries matrix test data anor% validation. created training datasets sample sizes subsampling remaining% data. random split replicated multiple times obtain bootstrapped datasplits (note unlike cross validation, test datasets overlapping entries). results fig. show proposed estimate) outperforms standard matrix completion terms popular ranking metrics. figure Ranking performance reverse inference Neurosynth data. -axis denotes fraction aﬃnity matrix entries observations training. plots show errorbars standard deviation bootstrapped train/test splits. reported ranking metrics, higher values better]. conclusion \\x0cour work addresses problem collaboratively ranking; task growing importance modern problems recommender systems, large scale metaanalysis, related areas. proposed convex estimator collaborative LETOR sparsely observed preferences, observations eir score vectors representing total order, generally directed acyclic graphs representing partial orders. remarkably, case complete order, complexity algorithm log factor state??art algorithms standard matrix completion. estimator empirically evaluated real data experiments. acknowledgments acknowledge funding NSF grants iis-1421729 SCH 1418511. Collaborative preference completion task jointly learning bipartite dyadic) preferences set entities shared list items., user?item interactions recommender system]. commonly assumed entity?item preferences generated small number latent hidden factors, equivalently, underlying preference matrix assumed low rank. furr, observed aﬃnity scores explicit implicit feedback treated exact mildly perturbed) entries unobserved preference matrix, preference completion task \\x0cnaturally fits framework low rank matrix completion]. more generally, low rank matrix completion involves predicting missing entries low rank matrix vanishing fraction entries observed noisy channel. several low rank matrix completion estimators algorithms developed literature, strong oretical guarantees empirical performance]. recent research preference completion literature noted matrix completion estimator collaborative preference estimation misguided] observed entity?item aﬃnity scores implicit/explicit feedback potentially subject systematic monotonic transformations arising limitations feedback collection., quantization 30th Conference Neural Information Processing Systems (nips 2016), barcelona, spain. inherent biases. while simple user biases linear transofmations handled low rank matrix framework, complex transformations quantization potentially increase rank observed preference score matrix significantly, adversely affecting recovery standard low rank matrix completion]. furr, common practice measuring preferences numerical scores, predictions deployed evaluated based item ranking. recommender systems, user recommendations presented ranked list items underlying scores. indeed authors shown favorable empirical/oretical performance square error preference matrix translate performance performance measured ranking metrics]. thus, collaborative preference estimation posed collection coupled learning rank (letor) problems], seek jointly learn preference rankings set entities, exploiting low dimensional latent structure underlying preference values. this paper considers preference completion general collaborative LETOR setting. importantly, observations assumed reliable indicators relative preference ranking, numerical scores deviant ground truth low rank preference matrix. refore, aim addressing preference completion generalizations: simple setting, entity, score vector representing observed aﬃnity interactions assumed generated arbitrary monotonic transformation entries ground truth preference matrix. make furr generative assumptions observed scores monotonicity respect underlying low rank preference matrix.  general setting, observed preferences entity represent specifications partial ranking form directed acyclic graph (dag) nodes represent subset items, edge represents strict ordering pair nodes. such rankings encountered preference scores consolidated multiple sources feedback., comparative feedback (pairwise listwise) solicited independent subsets items. this generalized setting handled standard matrix completion transforming DAG orderings score vector. our work part motivated application neuroimaging meta-analysis outlined following. cognitive neuroscience aims quantify link brain function behavior. this interaction measured humans Functional Magnetic Resonance Imaging (fmri) experiments measure brain activity response behavioral tasks. after analysis, conclusions summarized neuroscience publications include table brain locations actively activated response experimental stimulus. results synsized meta-analysis techniques derive accurate predictions brain activity cognitive terms (also forward inference) prediction cognitive terms brain regions (also reverse inference). for study, data neurosynth] public repository1 automatically scrapes information published associations brain regions terms cognitive neuroscience experiments. key contributions paper summarized below.  propose convex estimator low rank preference completion limited supervision, addressing) arbitrary monotonic transformations preference scores) partial rankings items (section). derive generalization error bounds surrogate ranking loss quantifies trade?off data?fit regularization (section).  propose eﬃcient algorithms estimate total partially ordered observations. case total orders, spite increased generality, computational complexity algorithm log factor standard convex algorithms matrix completion (section).  proposed algorithm evaluated application identifying associations brain?regions cognitive terms neurosynth dataset] (section). such large scale meta-analysis synsizing information literature related tasks potential lead insights role brain regions cognition behavior.  Notation For matrix     singular values  nuclear norm operator norm kop Frobenius norm let http://neurosynth.org,    vector set indexed denoted simply unambiguous. let    denote subset indices matrix Rd1 for     denotes subset entries —?— column. given    —?—},  (xis —?— linear subsampling operator?? —?— rd1 adjoint, ?? . for conciseness, notation denote ). Related Work Matrix completion: Low rank matrix completion extensive literature; examples include] ors. however, bulk works including context ranking/recommendation] Koyejo. plications focus) fitting observed numerical scores squared loss) evaluating results parameter/rating recovery metrics root squared error (rmse). shortcomings estimators results squared loss ranking applications studied recent research]. motivated collaborative ranking applications, growing interest addressing matrix completion explicit LETOR framework. weimer. ] propose estimators involve non?convex optimization problems algorithmic convergence generalization behavior understood. some recent works provide parameter recovery guarantees pairwise/listwise ranking observations specific probabilistic distributional assumptions observed rankings]. comparison, estimators algorithms paper agnostic generative distribution, wider applicability. learning rank (letor): LETOR structured prediction task rank ordering relevance list items function pre?selected features]. currently, leading algorithms LETOR listwise methods approach paper), fully exploit ranking structure ordered observations, offer modeling ﬂexibility compared pointwise] pairwise methods]. recent listwise LETOR algorithm proposed idea monotone retargeting], elegantly addresses listwise learning rank (letor) task maintaining relative simplicity scalability pointwise estimation. furr extended incorporate margins margin equipped monotonic retargeting (memr) formulation] preclude trivial solutions arise scale invariance initial estimate Acharyya. ]. estimator proposed paper inspired idea revisited paper. collaborative preference completion, rar learning functional mapping features ranking, seek exploit low rank structure jointly modeling preferences collection entities access preference indicative features. single Index Models (sims) finally, literature monotonic single index models (sims) considers estimation unknown monotonic transformations]. however, algorithms SIMs designed solve harder problem estimating non?parametric monotonic transformation evaluated parameter recovery rar ranking performance. general, furr assumptions, sample complexity SIM estimators lends unsuitable high dimensional estimation. existing high dimensional estimators learning SIMs typically assume Lipschitz continuity monotonic transformation explicitly observed score values bounding Lipsciptz constant monotonic transformation]. comparison, proposed model completely agnostic numerical values preference scores. Preference Completion Partial Rankings Let unobserved true preference scores entities items denoted rank min matrix  rd1 for entity  observe partial total ordering preferences subset \\x0citems denoted  let denotes number items relative preferences entity observed, denotes entity-item index set denotes index set collected entities. let denote sampling distribution observed preferences entity typically represented listwise preference score vector) rnj   (??  arbitrary unknown monotonic transformations, rd1 non?adversarial noise matrix sampled distribution preference completion task estimate unseen rankings column subset orderings?   arbitrary, exact values) inconsequential, observed preference order constraint set parameterized margin parameter follows: Definition (?margin Isotonic set) set vectors isotonic margin parameter? )   ],    addition score vectors, isotonic sets form? ) equivalently defined DAG], denotes partial ranking vertices, convention,   ? ),   note Definition ties broken random., yi1 yi2 ? ), xi1  xi2  ordering xi1 xi2 specified. let) denote smallest entry  distinguish special cases observation representing partial ranking]. ) Strict Total order)   ) ) Blockwise Total order) )     ) unique values. ) Arbitrary dag: Partial order induced DAG).  Monotone Retargeted Low Rank Estimator Consider scalable pointwise learning algorithm fits model exact preferences scores. since generative model (besides monotonicity) assumed raw numerical scores observations, principle, scores) entity replaced retargeted rankingn preserving scores., vector) monotone Retargeting] exploits observation address combinatorial listwise ranking problem] maintaining relative simplicity scalability pointwise estimates (regression). key idea alternately fit pointwise algorithm current relevance scores, retarget scores searching space monotonic transformations scores. our approach extends generalizes monotone retargeting preference prediction task. begin motivating algorithm noise free setting, clear  ) seek estimate candidate preference matrix intersection) data constraints observed preference rankings ) model constraints case low rankness induced constraining nuclear norm kxk?  for robust estimation presence noise, extend noise free approach incorporating soft penalty constraint violations. let —?— slight abuse notation rnj denote vector entries —?—  upon incorporating soft penalties, monotone retargeted low rank estimator: Argmin min ?kxk?   )k22.  —?— parameter controls trade?off nuclear norm regularization data fit, set minimizers). note? ) convex,  scaling?   ? )} ? ). estimate computed eﬃcient convex optimization algorithms handle arbitrary monotonic transformation preference scores, providing higher ﬂexibility compared standard matrix completion. although) terms parameters, due geometry problem, turns jointly identifiable, discussed proposition. proposition optimization) jointly convex). furr, (?,  lead equivalent estimators, specifically(?,  (?  since, positive scaling preserves resultant preference order, Proposition loss generality, requires tuning remaining fixed. Optimization Algorithm optimization problem) jointly convex). furr, proximal show operator non?differential component estimate ?kxk?  ) eﬃciently computable. this motivates proximal gradient descent algorithm] jointly update). for step size  resulting updates follows: update: Singular Value Thresholding proximal operator ? singular thresholding operator  for singular decomposition   )  , soft thresholding operator  max }.  update: Parallel Projections For hard constraints proximal operator Euclidean projection constraints argminz?vk22.  )  updates decouple entity (column trivially parallelized. Eﬃcient projections) discussed Section. ) Algorithm Proximal Gradient Descent) input paramter    Until (stopping criterion?? )   ProjRnj ) ) Projection  begin definitions characterizing? ). definition (adjacent difference operator) adjacent difference operator denoted  defined   ]. definition (incidence matrix) For directed graph), incidence matrix — that: directed edge ith node node Projection? ) closely related isotonic regression problem finding univariate squares fit consistent order constraints (without margins). this isotonic regression problem solved) complexity classical Pool Adjacent Violators (pav) algorithm: pav) argmin . zi0    discuss, simple adaptations isotonic regression projection -margin isotonic sets special cases interest summarized Table ) Strict Total order)   ) setting, constraint set characterized? ) }, vector ones. for case projection? ) differs) requiring ?separation straight forward extension PAV algorithm] used. let dsl vector dsl simple substitutions, ProjRn) pav dsl dsl   ) Blockwise Total order) )    ) This common setting supervision preference completion applications, listwise ranking preferences obtained ratings discrete quantized levels   prevalent. let partitioned blocks   entries partition equal, blocks mselves strictly ordered.,  inf inf  Let dbl dbl vector block indices, .., let set valid permutations permute entries blocks ? )   )  dbl propose steps compute ProjRn) case: step  .  ],   sort(xpk) Step  ) dbl dbl correctness) summarized lemma. lemma Estimate) unique minimizer argminkz xk22.    )  dbl ) Arbitrary dag], arbitrary DAG (not necessarily connected) represent consistent order constraints vertices., partial rankings consolidated multiple listwise/pairwise scores. case, ?margin isotonic set? ) . definition). consider ddag ith entry ddag length longest directed chain connecting topological descendants node easily verified that, isotonic regression algorithm arbitrary DAGs applied ddag projection? ). this general setting, isotonic regression algorithm exact solution requires(nm2 log computation], number edges While case), computation prohibitive, include case completeness. note case partial DAG ordering handled standard matrix completion setting consolidating partial ranks total order.  ) ProjRn  }   ) } ) Computation pav ? pav? ) dbl dbl isoreg ddag)+ddag log log Table Summary algorithms ProjRn)  Computational Complexity easily verified gradient? ) zk22?lipschitz continuous. thus, standard results convegence proximal gradient descent], Algorithm,converges error objective/) iterations. compared proximal algorithms standard matrix completion], additional complexity Algorithm arises update) simple substitution) standard matrix completion. for total orders, update) highly eﬃcient asymptotically additional log —?— factor computational costs standard matrix completion. Generalization Error Recall (noisy) partial rankings subset items user, obtained noise matrix unknown arbitrary transformations preserve ranking order column. estimator algorithms independent sampling distribution generating }). section quantify simple generalization error bounds). assumption (sampling? for fixed assume sampling distribution. let fixed constant pre?specified parameter denoting length single listwise observation. for   — log) uniform )) ) randsample? ) (?? )). ) furr, define notation  for column listwise scores) jointly define consistent partial ranking scores subsets monotonically transformed preference vector  this consistent ordering represented DAG) partialorder}). note log samples ensures column included sampling high probability. definition (projection loss) Let], define partial ordering total order respectively. define convex surrogate loss partial rankings.  , minz? ) zk2 estimate). with scaling orem (generalization bound) Let kxkf constants holds probability greater  observed rankings)  drawn— log— log1 log log/?  ))  )) ),? )   — orem quantifies test projection loss random length items) drawn random entity/user). bound trade?off observable training error complexity defined nuclear norm estimate. Experiments evaluate model collaborative preference estimation tasks) standard user-item recommendataion task benchmarked dataset movielens) identifying associations brain?regions cognitive terms neurosynth dataset]. baselines: baseline models compared experiments: retargeted Matrix Completion (rmc): estimator proposed).  standard Matrix Completion (smc]: primarily compare estimator standard convex estimator matrix completion nuclear norm minimization.  collaborative Filtering Ranking cofi-rank]: This work addresses collaborative filtering task listwise ranking setting. for SMC mrpc, hyperparameters tuned grid search logarithmic scale. due high computational cost tuning parameters cofirank, code default parameters provided authors. evaluation metrics: performance preference estimation tasks evaluated ranking metrics) Normalized Discounted Cummulative Gains (ndcg) precision) Spearmann rho) Kendall tau, metrics measure correlation complete ordering list, metrics primarily evaluate correctness ranking top list (see Liu. . ] furr details metrics). Movielens dataset (blockwise total order) Movielens movie recommendation website administered GroupLens research. competitive benchmarked movielens 100k dataset. ?fold train/test splits provided dataset test splits non-overlapping). discarded small number users ratings training data splits. resultant dataset consists 923 users 1682 items. ratings blockwise ordered taking values set,   }. during testing, user, competing models return ranking test-items, performance averaged test-users. table presents results evaluation averaged train/test splits Movielens dataset, standard deviation. proposed retargeted matrix completion (rmc) significantly consistently outperforms SMC cofi-rank] ranking metrics. ndcg precision Spearman Rho Kendall Tau RMC.7984.0213.7546.0320.4137.0099.3383.0117) SMC.7863.0243.7429.0295.3722.0106.3031.0117) cofi-rank.7731.0213.7314.0293.3681.0082.2993.0110) Table Ranking performance recommendations Movielens 100k. table shows standard deviation fold train/test splits. for reported metrics, higher values]. Neurosynth Dataset (almost total order) neurosynth] publicly database consisting data automatically extracted large collection functional magnetic resonance imaging (fmri) publications,362 publications current version). for publication database abstract text reported-dimensional peak activation coordinates study. text pre-processed remove common stop-words, text% frequency, leaving total 3169 terms. applied standard brain map activations, removing voxels grey matter. next activations downsampled 2mm3 voxels 10mm3 voxels nilearn python package, resulting total 1231 dense voxels. aﬃnity measure 3169 terms 1231 consolidated voxels obtained multiplying term publication publication voxels matrices. resulting data dense high-rank preference matrix. with tied preference values, setting fits case total ordered observations (case Section). using data, reverse inference task ranking cognitive concepts (terms) brain region (voxel]. trainval-test% randomly sampled entries matrix test data anor% validation. created training datasets sample sizes subsampling remaining% data. this random split replicated multiple times obtain bootstrapped datasplits (note unlike cross validation, test datasets overlapping entries). results fig. show proposed estimate) outperforms standard matrix completion terms popular ranking metrics. figure Ranking performance reverse inference Neurosynth data. -axis denotes fraction aﬃnity matrix entries observations training. plots show errorbars standard deviation bootstrapped train/test splits. for reported ranking metrics, higher values better]. Conclusion \\x0cour work addresses problem collaboratively ranking; task growing importance modern problems recommender systems, large scale metaanalysis, related areas. proposed convex estimator collaborative LETOR sparsely observed preferences, observations eir score vectors representing total order, generally directed acyclic graphs representing partial orders. remarkably, case complete order, complexity algorithm log factor state??art algorithms standard matrix completion. our estimator empirically evaluated real data experiments. acknowledgments acknowledge funding NSF grants iis-1421729 SCH 1418511.',\n",
       " 'PP6313': 'demand forecasting plays central role supply chain management, driving automated ordering-stock management, facilities planning. classical forecasting methods, exponential smoothing] ARIMA models], produce Gaussian predictive distributions. suﬃcient inventories thousand fast-selling items, Gaussian assumptions grossly violated extremely large catalogues maintained-commerce platforms. , demand highly intermittent bursty: long runs zeros, islands high counts. decision making requires quantiles predictive distributions], accuracy suffer erroneous assumptions. work, detail methodology intermittent demand forecasting operates industrial environment large-commerce platform. implemented Apache Spark], method process hundreds thousands items hundreds millions item-days. key requirements automated parameter learning expert interventions), scalability high degree operational robustness. system produces forecasts short (one weeks) longer lead times months), require feature maps depending holidays, sales days, promotions, price changes. previous work intermittent demand forecasting Statistics surveyed]: address longer lead times. modelling level, proposal related], novelties essential operating industrial scale target here. paper makes contributions: combination generalized linear models time series smoothing. enables medium longer term forecasts, temporal continuity reasonable distributions time. compared], provide empirical evidence usefulness combination.  algorithm maximum likelihood parameter learning state space models non-gaussian likelihood, approximate Bayesian inference. substantial related prior work, proposal stands robustness scalability. show approximate inference solved NewtonRaphson algorithm, fully reduced Kalman smoothing iteration. reduction scales linearly vanilla implementation 30th Conference Neural Information Processing Systems (nips 2016), barcelona, spain. scale cubically). previously Statistics, sect. ], reduction widely Machine learning. -bfgs proposed]), approximate inference fails real-world cases.  multi-stage likelihood, taylored intermittent bursty demand data (extension]), transfer function Poisson likelihood, robustifies Laplace approximation bursty data. demonstrate approach work novelties. structure paper follows. section introduce intermittent demand likelihood function generalized linear model baseline. latent state forecasting methodology detailed Section relate approach prior work Section section evaluate methods publicly data large dataset real-world demand context ecommerce, comparing state art intermittent forecasting methods. generalized Linear Models section, introduce likelihood function intermittent demand data, generalized linear model baseline. denote demand zit item, day). goal predict distributions zit aggregates future. fitting probabilistic model maximize likelihood training demand data, drawing sample paths fitted model, represent forecast distributions. sequel, fix item write zit model defined likelihood latent function poisson: Ppoi) ?? )  ) rate ) depends transfer function. demand data large inventories intermittent (many bursty (occasional large represented poisson. choice multistage likelihood, generalizing proposal]. likelihood decomposes stages, latent function) stage emit probability1 ) orwise, transfer stage emitted probability ) finally, stage draws poisson) rate ) latent function functions) linear, generalized linear model (glm]. features include kernels anchored holidays (christmas, halloween), seasonality indicators (dayofweek, monthofyear), promotion price change indicators. weights learned maximizing training data likelihood. multi-stage likelihood, amounts separate instances binary classification stages Poisson regression stage generalized linear forecasters work well, important drawbacks. lack temporal continuity: short term predictions, simple smoors outperform tuned glm. important, GLM predicts overly narrow forecast distributions, widths grow time, neglects temporal correlations. drawbacks alleviated Gaussian linear time series models, exponential smoothing]. major challenge combine technology general likelihood functions (poisson, multi-stage) enable intermittent demand forecasting. latent State Forecasting section, develop latent state forecasting intermittent demand, combining glms, general likelihoods, exponential smoothing time series models. begin single likelihood Poisson), multi-stage extension. latent process ). ) here, GLM deterministic linear function, latent state. innovation state space model (issm] defined prior  note ISSMs characterized single Gaussian innovation variable time step. experiments here, here,  logistic sigmoid. employ simple2 instance   meaning], latent state level component only. free parameters (weights),  collected vector  training. prediction. multiple Stages learn maximizing likelihood data compared GLM case, harder, latent (unobserved) variables   integrated out. likelihood gaussian, marginalization computed analytically Kalman smoothing]. non-gaussian likelihood, problem analytically intractable, amenable LaplaceRapproximation, sect. ]. exact log likelihood log—?) log—?) log) aﬃne mapping). proceed steps. first, find mode posterior: argmax log—? optimization problem. second, replace log—?) quadratic Taylor approximation; mode. criterion replace negative log likelihood ?(?)  log;? . precisely, denote log (?  posterior mode. log-concavity likelihood implies convex, ?00t quadratic Taylor approximation  log /?00t    now, laplace approximation log—?) written ?(?)  log )    ; ?). ) For log-concave3 optimization convex problem. newton-raphson algorithm solve. algorithm iterates fitting current criterion local order approximation minimizing quadratic surrogate. step, compute values forward pass), replace potentials  values determined order fit above  step amounts computing posterior (equal mode] resulting gaussian-linear model. inference problem solved Kalman smoothing computation fully reduced Kalman smoothing. finding mode effective optimization algorithm. this point crucial. find general, Newton step requires inversion Hessian matrix. reduce Kalman smoothing, robust algorithm scaling. shown Section newton-raphson reasonable time. essential here: commonly optimizers fail find Prediction samples obtained follows. denote observed demand unobserved demand prediction range  run newton-raphson time obtain Gaussian approximation posterior final state. sample path draw  ), compute forward pass,  drawing prediction samples expensive glm. finally, generalize latent state forecasting multi-stage likelihood. glm, learn parameters ) separately stage Stages binary classification, stage count regression. day active stage recall glms) simply drop non-active days. here, ISSMs full range ) non-active considered unobserved: likelihood potential Both Kalman smoothing mode finding (laplace approximation) adapted missing observations, presents diﬃculties (see Section). more advanced variants include damping, linear trend, seasonality fac      tors]. orwise said, likelihoods paper log-concave. numerically robust implementation Kalman smoothing, detailed, sect. ].  Some Details section, sketch technical details, contributions. demonstrated experiments, details essential approach work robustly intended scale diﬃcult real-world data. full details supplemental report. -bfgs outer optimization ?(? encoding constrained parameters:    log add quadratic regularizer   criterion, shared items. finally, recall multi-stage likelihood, day unobserved stage item, observed days stage, skip training fall back fixed parameters   high-dimensional Every single evaluation ?(?) requires finding posterior mode argmin;  log) optimizationp converge robustly iterations: log log). newton-raphson reduction linear-time Kalman smoothing noted above. algorithm extended line search procedure heuristic pick starting point (see supplemental report). compute gradient ?(? criterion). main diﬃculty  (?  (?).  computed indirect dependencies: (?, iterative algorithm, commonly automated differentiation tools sensibly apply here.  defined Maybe diﬃcult indirect term?  (??    first?  taking derivative. sides, obtain (??  ? ? ? ? ? ?). question compute invert?  ? ?  corresponds posterior ISSM Gaussian likelihood, depends?  means indirect gradient part costs run Kalman smoothing, independent number parameters note reasoning underlies reduction newton-raphson Kalman smoothing. final contribution essential making Laplace approximation work real-world bursty demand data. recall transfer function ) Poisson likelihood) highest stage shown Section exponential choice fails short term forecasts. glm, logistic transfer ) works well) log behaves grows linearly positive however, exhibits grave problems latent state forecasting. denote ) log) Poisson logistic transfer.    ? recall laplace approximation Section: ?(?) fit quadratic ?(?)  /? ),    ). large terms scale grow polynomially. real-world data, regularly exhibit sizable counts \\x0c(say, driving single point, huge values  arise, causing cancellation errors ?(? outer optimization terminates prematurely. root issues lies transfer function) large curvature behaves remedy propose logistic transfer function: )),   ) log) transfer function, ) behaves similar ) small negative crucially (?? )  large this means Laplace approximation terms/?). setting  resolves problems above. importantly, resulting Poisson likelihood log-concave   conjecture similar problems arise ?local? variational expectation propagation inference approximations well. logistic transfer function refore wider applicability. related Work Our work precursors Statistics Machine learning. maximum likelihood learning exponential smoothing models developed]. methods limited Gaussian likelihood, approximate Bayesian inference used. starting croston method, sect. ], sizable literature intermittent demand forecasting, reviewed]. best-performing method] negative binomial likelihood damped dynamic, parameters learned maximum likelihood. latent (random) state, neir non-gaussian inference Kalman smoothing required. combination glms. employ approximate Bayesian inference linear dynamical system, lot prior work Machine Learning]. laplace technique frequently deterministic approximation statistics, publications automated inference systems], techniques expectation propagation applicable models interest]. robustness predictable running time laplace approximation key application, inference driving parameter learning, running parallel hundreds thousands items. expectation propagation guaranteed converge, Markov chain Monte Carlo methods lack automated convergence tests. work closely related]. target intermittent demand forecasting, Laplace approximation maximum likelihood learning, combination glms, work transferring information items hierarchical prior distribution. work evaluated small datasets short term scenarios only. contrast, system runs robustly hundreds thousands items millions item-days, orders magnitude larger scale report. explore featurebased deterministic part, real-world data essential medium term forecasts. find number choices] limiting robustness scalability. first, choose likelihood log-concave reasons: negative binomial distribution poisson, zero-inﬂation multi-stage setup This \\x0cmeans optimization problem non-convex, jeopardizing robustness eﬃciency nested learning process. moreover, multi-stage setup, conditional probability versus represented exactly, zero-inﬂation caters time-independent probability only. next, exponential transfer function negative binomial rate, propose logistic function newton lbfgs (section). experiments exponen10 tial choice data resulted total failure, short term forecasts. huge curvature large results extremely large instable predictions holidays. fact, exponential function rapid growth predictions linear function ex10 tension, random process strongly damped. finally, standard-bfgs time] solver problem, evaluating criterion additional sparse matrix software. contrast, enable newton-raphson refigure Comparison NewtonRaphson. Lducing Kalman smoothing. figure BFGS optimization. sampled evaluate usefulness-bfgs mode evaluation ?(?). shown median (p10, p90) finding setup-bfgs fails. 1500 items. -bfgs fails converge attain decent accuracy reasonable amount decent accuracy. time, NewtonRaphson converges reliably. reliability key reaching goal fully automated learning industrial system. conclusion, lack public code] precludes direct comparison, approach, partly advanced, limited smaller problems, shorter forecast horizons, hard run industrial setting. gradient norm Experiments section, present experimental results, comparing variants approach related work.  Out Stock Treatment With large growing inventory, fraction items stock time, meaning order fulfillments delayed happen all. stock, item sold zero-inﬂation}  destroys log-concavity problem convex, criterion eﬃciently implemented dependence foreign code). situation] diﬃcult. ), elicit considerable customer demand. probabilistic nature latent state forecasting renders easy stock information. item stock day data explained away, \\x0ccorresponding likelihood term dropped. noted Section, presents diﬃculty framework. Sep 2015 Jun 2015 Mar 2015 Dec 2014 Sep 2014 Jun 2014 Mar 2014 Dec 2013 Sep 2015 Jun 2015 Mar 2015 Dec 2014 Sep 2014 Jun 2014 unobserveddays Mar 2014 Dec 2013 unobserveddays Figure Demand forecast item partially stock. panel: Training range left (green), prediction range (red), true targets black. color: median, P10 p90. bottom: Out stock % day) marked red. left: Out stock signal ignored. demand forecast drops zero, strong underbias prediction range. right: Out stock regions treated missing observations. demand uncertain stock region. underbias prediction range. figure show demand forecasts item stock periods training range. obvious ignoring stock signal leads systematic underbias (since interpreted demand?). underbias corrected treating stock regions unobserved targets. note item partially stock day, creating sales. cases, treat unobserved, lower-bounded sales, expectation maximization extension applied. however, situations comparatively rare data (compared full-day stock). rest section, latent state forecasting taking stock information account.  Comparative Study present experimental results obtained number datasets, intermittent counts time series. parts monthly demand spare parts automobile company, publicly available, previously]. furr results obtained internal daily-commerce sales data. eir case, subsampled sets stratified manner larger volume production setting. -sub medium size fast medium moving items. -all large dataset (more 500k items, 150m item-days), union-sub items slower moving. properties datasets Figure top left. demand highly intermittent bursty cases, witnessed large high proportion properties typical supply chain data. -all larger public demand forecasting dataset aware, internal datasets consists longer series?) bursty parts. methods compared. ets exponential smoothing Gaussian additive errors automatic model selection, frequently package]. negbin implementation negative binomial damped dynamic variant]. variants latent state forecaster-pure features-feats feature vector (basic seasonality, kernels holidays, price changes, stock). predictive distributions represented 100 samples prediction range (length parts, length 365 ors). employ quadratic regularization methods ETS (see Section). hyperparameters consist regularization constants centers (full details supplemental report). tune7 parameters random% data, evaluating test results remaining%. -pure-feats, sets tuned hyperparameters largest set-all: ECsub part, rest. metrics quantify forecast accuracy quantiles predictive distributions. defined terms spans, prediction range, lead times. general, ignore days items stock (see Figure top left-stock ratios). found careful hyperparameter tuning important obtaining good results, negbin. contrast, regularization mentioned] (our implementation NegBin includes quadratic regularization methods).  stock define zit   ), predicted ?-quantile ) denoted) predictions obtained sample paths summing span, estimating quantile sorting. ?-quantile loss8 defined ?)   }  ? } (?  100) risk metric, defined  )  left argument) computed test targets focus P50 risk ; absolute error) P90 risk -quantile relevant automated ordering). -sub-all 19874 month% 100% 656k 39700 day% 329 13m 534884 day% 293 157m) ETS NegBin-pure-feats true demand) ETS NegBin-pure-feats P90 risk ETS NegBin-pure-feats P50 risk) sum units Parts items Unit Median freq. -stock ratio avg. size series item-days  dataset. figure table: Dataset properties. var measures burstiness. ): Sum weekly P50 point (median) forecast one-year prediction range methods (lines) sum true demand): Weekly P50 risk; (shaded area), dataset-sub. )], ): Same) P90 risk. plot P50 P90 risk dataset-sub, sum P50 point (median) forecast true demand, panels Figure methods work week, considerable differences furr out. naturally, losses highest Christmas peak sales period. -feats strongly outperforms ors critical region (see Figure top right), means features (holidays, seasonality). gaussian predictive distributions ETS exhibit growing errors time. exception Christmas period, NegBin works rar P50 risk), uniformly outperformed-pure-feats particular. larger range results Table (parts-sub) Table-all), numbers relative negbin. note code ETS run large-all. parts, NegBin works best-pure close features dataset). -sub-feats outperforms ors scenarios. featureless NegBin-pure comparable dataset. largest set-all-feats generally outperforms ors, differences smaller. finally, report running times parameter learning (outer optimization-feats-sub. lbfgs run maxiters, gradtol experimental cluster consists 150 nodes, Intel Xeon-2670 CPUs cores) 30gb ram. profiling separately stage.180s.30s.15s.143s.11s.79s.138s.29s.25s). here, quote median (p50% percentiles, p95). largest time recorded. narrow spread numbers witnesses robustness predictability nested optimization process, crucial properties context production systems running parallel compute clusters. ? ?)] minimized ?-quantile. also?)  ?—. more precisely, filter )]:  }. , ETS-pure-feats NegBin \\x0cparts P90 risk P50 risk   -sub) P90 risk) P50 risk  Table Results dataset Parts (left-sub (right). metric values relative NegBin (each column). ): Average )],   ): Average ; )],   . -pure-feats NegBin) P90 risk) P50 risk Table Results dataset-all. metric values relative NegBin (each column). ets run scale. conclusions. future Work paper, developed framework maximum likelihood learning probabilistic latent state forecasting models, principled time series extensions generalized linear models. pay special attention intermittent bursty statistics demand, characteristic vast inventories maintained large retailers-commerce platforms. show approximate Bayesian inference techniques implemented robust highly scalable way, enable forecasting system runs safely hundred thousands items hundreds millions item-days. draw conclusions comparative study range real-world datasets. proposed method strongly outperforms competitors sales data \\x0cfrom fast medium moving items. good short term forecasts due temporal smoothness well-calibrated growth uncertainty, feature vector decisive medium term forecasts. slow moving items, simpler methods NegBin] competitive, lack signal models learned data. investigating directions future work. current system time-independent issms, [?] means amount innovation variance applied day. assumption violated data, lot variation weeks leading Christmas major holidays rest year. end, exploring learning parameters: high-variation periods, remaining days. plan augment state seasonality10 factors, sect. ] (both depend time). important future directions learn exploit dependencies demand time series items. fact, strategy learn forecast item independently suitable items short demand history, slow moving items. approach pursue couple latent processes shared (global) linear non-linear function. acknowledgements Maren Mahsereci determining running time figures, Wupper team hard work paper happened. currently, periodic seasonality dealt features Demand forecasting plays central role supply chain management, driving automated ordering-stock management, facilities planning. classical forecasting methods, exponential smoothing] ARIMA models], produce Gaussian predictive distributions. while suﬃcient inventories thousand fast-selling items, Gaussian assumptions grossly violated extremely large catalogues maintained-commerce platforms. , demand highly intermittent bursty: long runs zeros, islands high counts. decision making requires quantiles predictive distributions], accuracy suffer erroneous assumptions. work, detail methodology intermittent demand forecasting operates industrial environment large-commerce platform. implemented Apache Spark], method process hundreds thousands items hundreds millions item-days. key requirements automated parameter learning expert interventions), scalability high degree operational robustness. our system produces forecasts short (one weeks) longer lead times months), require feature maps depending holidays, sales days, promotions, price changes. previous work intermittent demand forecasting Statistics surveyed]: address longer lead times. modelling level, proposal related], novelties essential operating industrial scale target here. this paper makes contributions: combination generalized linear models time series smoothing. enables medium longer term forecasts, temporal continuity reasonable distributions time. compared], provide empirical evidence usefulness combination.  algorithm maximum likelihood parameter learning state space models non-gaussian likelihood, approximate Bayesian inference. while substantial related prior work, proposal stands robustness scalability. show approximate inference solved NewtonRaphson algorithm, fully reduced Kalman smoothing iteration. this reduction scales linearly vanilla implementation 30th Conference Neural Information Processing Systems (nips 2016), barcelona, spain. scale cubically). while previously Statistics, sect. ], reduction widely Machine learning. -bfgs proposed]), approximate inference fails real-world cases.  multi-stage likelihood, taylored intermittent bursty demand data (extension]), transfer function Poisson likelihood, robustifies Laplace approximation bursty data. demonstrate approach work novelties. structure paper follows. Section introduce intermittent demand likelihood function generalized linear model baseline. our latent state forecasting methodology detailed Section relate approach prior work Section Section evaluate methods publicly data large dataset real-world demand context ecommerce, comparing state art intermittent forecasting methods. Generalized Linear Models section, introduce likelihood function intermittent demand data, generalized linear model baseline. denote demand zit item, day). our goal predict distributions zit aggregates future. fitting probabilistic model maximize likelihood training demand data, drawing sample paths fitted model, represent forecast distributions. sequel, fix item write zit model defined likelihood latent function poisson: Ppoi) ?? )  ) rate ) depends transfer function. demand data large inventories intermittent (many bursty (occasional large represented poisson. choice multistage likelihood, generalizing proposal]. this likelihood decomposes stages, latent function) stage emit probability1 ) orwise, transfer stage emitted probability ) finally, stage draws Poisson) rate ) latent function functions) linear, generalized linear model (glm]. features include kernels anchored holidays (christmas, halloween), seasonality indicators (dayofweek, monthofyear), promotion price change indicators. weights learned maximizing training data likelihood. for multi-stage likelihood, amounts separate instances binary classification stages Poisson regression stage generalized linear forecasters work well, important drawbacks. lack temporal continuity: short term predictions, simple smoors outperform tuned glm. more important, GLM predicts overly narrow forecast distributions, widths grow time, neglects temporal correlations. both drawbacks alleviated Gaussian linear time series models, exponential smoothing]. major challenge combine technology general likelihood functions (poisson, multi-stage) enable intermittent demand forecasting. Latent State Forecasting section, develop latent state forecasting intermittent demand, combining glms, general likelihoods, exponential smoothing time series models. begin single likelihood Poisson), multi-stage extension. latent process ). ) here, GLM deterministic linear function, latent state. this innovation state space model (issm] defined prior  note ISSMs characterized single Gaussian innovation variable time step. experiments here, here,  logistic sigmoid. employ simple2 instance   meaning], latent state level component only. free parameters (weights),  collected vector  training. prediction. multiple Stages learn maximizing likelihood data compared GLM case, harder, latent (unobserved) variables   integrated out. likelihood gaussian, marginalization computed analytically Kalman smoothing]. with non-gaussian likelihood, problem analytically intractable, amenable LaplaceRapproximation, sect. ]. exact log likelihood log—?) log—?) log) aﬃne mapping). proceed steps. first, find mode posterior: argmax log—? optimization problem. second, replace log—?) quadratic Taylor approximation; mode. criterion replace negative log likelihood ?(?)  log;? . more precisely, denote log (?  posterior mode. log-concavity likelihood implies convex, ?00t quadratic Taylor approximation  log /?00t    now, laplace approximation log—?) written ?(?)  log )    ; ?). ) For log-concave3 optimization convex problem. newton-raphson algorithm solve. this algorithm iterates fitting current criterion local order approximation minimizing quadratic surrogate. for step, compute values forward pass), replace potentials  values determined order fit above  step amounts computing posterior (equal mode] resulting gaussian-linear model. this inference problem solved Kalman smoothing computation fully reduced Kalman smoothing. not finding mode effective optimization algorithm. This point crucial. find general, Newton step requires inversion Hessian matrix. reduce Kalman smoothing, robust algorithm scaling. shown Section newton-raphson reasonable time. essential here: commonly optimizers fail find Prediction samples obtained follows. denote observed demand unobserved demand prediction range  run newton-raphson time obtain Gaussian approximation posterior final state. for sample path draw  ), compute forward pass,  drawing prediction samples expensive glm. finally, generalize latent state forecasting multi-stage likelihood. glm, learn parameters ) separately stage Stages binary classification, stage count regression. say day active stage Recall glms) simply drop non-active days. here, ISSMs full range ) non-active considered unobserved: likelihood potential Both Kalman smoothing mode finding (laplace approximation) adapted missing observations, presents diﬃculties (see Section). More advanced variants include damping, linear trend, seasonality fac      tors]. unless orwise said, likelihoods paper log-concave. numerically robust implementation Kalman smoothing, detailed, sect. ].  Some Details section, sketch technical details, contributions. demonstrated experiments, details essential approach work robustly intended scale diﬃcult real-world data. full details supplemental report. -bfgs outer optimization ?(? encoding constrained parameters:    log add quadratic regularizer   criterion, shared items. finally, recall multi-stage likelihood, day unobserved stage item, observed days stage, skip training fall back fixed parameters   this high-dimensional Every single evaluation ?(?) requires finding posterior mode argmin;  log) optimizationp converge robustly iterations: log log). newton-raphson reduction linear-time Kalman smoothing noted above. algorithm extended line search procedure heuristic pick starting point (see supplemental report). compute gradient ?(? criterion). main diﬃculty  (?  (?). since computed indirect dependencies: (?, iterative algorithm, commonly automated differentiation tools sensibly apply here.  defined Maybe diﬃcult indirect term?  (??    first?  taking derivative. sides, obtain (??  ? ? ? ? ? ?). question compute invert?  but? ?  corresponds posterior ISSM Gaussian likelihood, depends?  this means indirect gradient part costs run Kalman smoothing, independent number parameters note reasoning underlies reduction newton-raphson Kalman smoothing. final contribution essential making Laplace approximation work real-world bursty demand data. recall transfer function ) Poisson likelihood) highest stage shown Section exponential choice fails short term forecasts. with glm, logistic transfer ) works well) log behaves grows linearly positive however, exhibits grave problems latent state forecasting. denote ) log) Poisson logistic transfer.    ? Recall laplace approximation Section: ?(?) fit quadratic ?(?)  /? ),    ). for large terms scale grow polynomially. real-world data, regularly exhibit sizable counts \\x0c(say, driving single point, huge values  arise, causing cancellation errors ?(? outer optimization terminates prematurely. root issues lies transfer function) large curvature behaves our remedy propose logistic transfer function: )),   ) log) transfer function, ) behaves similar ) small negative crucially (?? )  large This means Laplace approximation terms/?). setting  resolves problems above. importantly, resulting Poisson likelihood log-concave   conjecture similar problems arise ?local? variational expectation propagation inference approximations well. logistic transfer function refore wider applicability. Related Work Our work precursors Statistics Machine learning. maximum likelihood learning exponential smoothing models developed]. methods limited Gaussian likelihood, approximate Bayesian inference used. starting croston method, sect. ], sizable literature intermittent demand forecasting, reviewed]. best-performing method] negative binomial likelihood damped dynamic, parameters learned maximum likelihood. latent (random) state, neir non-gaussian inference Kalman smoothing required. combination glms. employ approximate Bayesian inference linear dynamical system, lot prior work Machine Learning]. while laplace technique frequently deterministic approximation statistics, publications automated inference systems], techniques expectation propagation applicable models interest]. robustness predictable running time laplace approximation key application, inference driving parameter learning, running parallel hundreds thousands items. expectation propagation guaranteed converge, Markov chain Monte Carlo methods lack automated convergence tests. work closely related]. target intermittent demand forecasting, Laplace approximation maximum likelihood learning, combination glms, work transferring information items hierarchical prior distribution. work evaluated small datasets short term scenarios only. contrast, system runs robustly hundreds thousands items millions item-days, orders magnitude larger scale report. explore featurebased deterministic part, real-world data essential medium term forecasts. find number choices] limiting robustness scalability. first, choose likelihood log-concave reasons: negative binomial distribution poisson, zero-inﬂation multi-stage setup This \\x0cmeans optimization problem non-convex, jeopardizing robustness eﬃciency nested learning process. moreover, multi-stage setup, conditional probability versus represented exactly, zero-inﬂation caters time-independent probability only. next, exponential transfer function negative binomial rate, propose logistic function newton lbfgs (section). experiments exponen10 tial choice data resulted total failure, short term forecasts. its huge curvature large results extremely large instable predictions holidays. fact, exponential function rapid growth predictions linear function ex10 tension, random process strongly damped. finally, standard-bfgs time] solver problem, evaluating criterion additional sparse matrix software. contrast, enable newton-raphson refigure Comparison NewtonRaphson. Lducing Kalman smoothing. Figure BFGS optimization. sampled evaluate usefulness-bfgs mode evaluation ?(?). shown median (p10, p90) finding setup-bfgs fails. 1500 items. -bfgs fails converge attain decent accuracy reasonable amount decent accuracy. time, NewtonRaphson converges reliably. such reliability key reaching goal fully automated learning industrial system. conclusion, lack public code] precludes direct comparison, approach, partly advanced, limited smaller problems, shorter forecast horizons, hard run industrial setting. gradient norm Experiments section, present experimental results, comparing variants approach related work.  Out Stock Treatment With large growing inventory, fraction items stock time, meaning order fulfillments delayed happen all. when stock, item sold zero-inﬂation}  destroys log-concavity problem convex, criterion eﬃciently implemented dependence foreign code). situation] diﬃcult. ), elicit considerable customer demand. probabilistic nature latent state forecasting renders easy stock information. item stock day data explained away, \\x0ccorresponding likelihood term dropped. noted Section, presents diﬃculty framework. Sep 2015 Jun 2015 Mar 2015 Dec 2014 Sep 2014 Jun 2014 Mar 2014 Dec 2013 Sep 2015 Jun 2015 Mar 2015 Dec 2014 Sep 2014 Jun 2014 unobserveddays Mar 2014 Dec 2013 unobserveddays Figure Demand forecast item partially stock. each panel: Training range left (green), prediction range (red), true targets black. color: median, P10 p90. bottom: Out stock % day) marked red. left: Out stock signal ignored. demand forecast drops zero, strong underbias prediction range. right: Out stock regions treated missing observations. demand uncertain stock region. underbias prediction range. Figure show demand forecasts item stock periods training range. obvious ignoring stock signal leads systematic underbias (since interpreted demand?). this underbias corrected treating stock regions unobserved targets. note item partially stock day, creating sales. cases, treat unobserved, lower-bounded sales, expectation maximization extension applied. however, situations comparatively rare data (compared full-day stock). rest section, latent state forecasting taking stock information account.  Comparative Study present experimental results obtained number datasets, intermittent counts time series. parts monthly demand spare parts automobile company, publicly available, previously]. furr results obtained internal daily-commerce sales data. eir case, subsampled sets stratified manner larger volume production setting. -sub medium size fast medium moving items. -all large dataset (more 500k items, 150m item-days), union-sub items slower moving. properties datasets Figure top left. demand highly intermittent bursty cases, witnessed large high proportion properties typical supply chain data. not-all larger public demand forecasting dataset aware, internal datasets consists longer series?) bursty parts. methods compared. ets exponential smoothing Gaussian additive errors automatic model selection, frequently package]. negbin implementation negative binomial damped dynamic variant]. variants latent state forecaster-pure features-feats feature vector (basic seasonality, kernels holidays, price changes, stock). predictive distributions represented 100 samples prediction range (length parts, length 365 ors). employ quadratic regularization methods ETS (see Section). hyperparameters consist regularization constants centers (full details supplemental report). tune7 parameters random% data, evaluating test results remaining%. for-pure-feats, sets tuned hyperparameters largest set-all: ECsub part, rest. our metrics quantify forecast accuracy quantiles predictive distributions. defined terms spans, prediction range, lead times. general, ignore days items stock (see Figure top left-stock ratios). found careful hyperparameter tuning important obtaining good results, negbin. contrast, regularization mentioned] (our implementation NegBin includes quadratic regularization methods).  stock define zit for  ), predicted ?-quantile ) denoted) predictions obtained sample paths summing span, estimating quantile sorting. ?-quantile loss8 defined ?)   }  ? } (?  100) risk metric, defined  )  left argument) computed test targets focus P50 risk ; absolute error) P90 risk -quantile relevant automated ordering). -sub-all 19874 month% 100% 656k 39700 day% 329 13m 534884 day% 293 157m) ETS NegBin-pure-feats true demand) ETS NegBin-pure-feats P90 risk ETS NegBin-pure-feats P50 risk) sum units Parts items Unit Median freq. -stock ratio avg. size series item-days  dataset. Figure table: Dataset properties. var measures burstiness. ): Sum weekly P50 point (median) forecast one-year prediction range methods (lines) sum true demand): Weekly P50 risk; (shaded area), dataset-sub. )], ): Same) P90 risk. plot P50 P90 risk dataset-sub, sum P50 point (median) forecast true demand, panels Figure all methods work week, considerable differences furr out. naturally, losses highest Christmas peak sales period. -feats strongly outperforms ors critical region (see Figure top right), means features (holidays, seasonality). Gaussian predictive distributions ETS exhibit growing errors time. with exception Christmas period, NegBin works rar P50 risk), uniformly outperformed-pure-feats particular. larger range results Table (parts-sub) Table-all), numbers relative negbin. note code ETS run large-all. parts, NegBin works best-pure close features dataset). -sub-feats outperforms ors scenarios. featureless NegBin-pure comparable dataset. largest set-all-feats generally outperforms ors, differences smaller. finally, report running times parameter learning (outer optimization-feats-sub. lbfgs run maxiters, gradtol our experimental cluster consists 150 nodes, Intel Xeon-2670 CPUs cores) 30gb ram. profiling separately stage.180s.30s.15s.143s.11s.79s.138s.29s.25s). here, quote median (p50% percentiles, p95). largest time recorded. narrow spread numbers witnesses robustness predictability nested optimization process, crucial properties context production systems running parallel compute clusters. ? ?)] minimized ?-quantile. also?)  ?—. More precisely, filter )]:  }. , ETS-pure-feats NegBin \\x0cparts P90 risk P50 risk   -sub) P90 risk) P50 risk  Table Results dataset Parts (left-sub (right). metric values relative NegBin (each column). ): Average )],   ): Average ; )],   . -pure-feats NegBin) P90 risk) P50 risk Table Results dataset-all. metric values relative NegBin (each column). ets run scale. conclusions. future Work paper, developed framework maximum likelihood learning probabilistic latent state forecasting models, principled time series extensions generalized linear models. pay special attention intermittent bursty statistics demand, characteristic vast inventories maintained large retailers-commerce platforms. show approximate Bayesian inference techniques implemented robust highly scalable way, enable forecasting system runs safely hundred thousands items hundreds millions item-days. draw conclusions comparative study range real-world datasets. our proposed method strongly outperforms competitors sales data \\x0cfrom fast medium moving items. besides good short term forecasts due temporal smoothness well-calibrated growth uncertainty, feature vector decisive medium term forecasts. slow moving items, simpler methods NegBin] competitive, lack signal models learned data. investigating directions future work. our current system time-independent issms, [?] means amount innovation variance applied day. this assumption violated data, lot variation weeks leading Christmas major holidays rest year. end, exploring learning parameters: high-variation periods, remaining days. plan augment state seasonality10 factors, sect. ] (both depend time). one important future directions learn exploit dependencies demand time series items. fact, strategy learn forecast item independently suitable items short demand history, slow moving items. one approach pursue couple latent processes shared (global) linear non-linear function. acknowledgements Maren Mahsereci determining running time figures, Wupper team hard work paper happened. currently, periodic seasonality dealt features',\n",
       " 'PP6338': 'bayesian inference powerful tool modeling complex data reasoning uncertainty, casts long standing challenge computing intractable posterior distributions. markov chain Monte Carlo (mcmc) widely draw approximate posterior samples, slow diﬃculty accessing convergence. variational inference frames Bayesian inference problem deterministic optimization approximates target distribution simpler distribution minimizing divergence. makes variational methods eﬃciently solvable off-shelf optimization techniques, easily applicable large datasets., ”big data”) stochastic gradient descent trick]. contrast, challenging scale MCMC big data settings [see]. meanwhile, accuracy computational cost variational inference critically depend set distributions approximation defined. simple approximation sets, traditional field methods, restrictive resemble true posterior distributions, advanced choices cast diﬃculties subsequent optimization tasks. reason, eﬃcient variational methods derived model-model basis, causing major barrier developing general purpose, user-friendly variational tools applicable kinds models, accessible non experts application domains. case contrast maximum posteriori (map) optimization tasks finding posterior mode (sometimes poor man Bayesian estimator, contrast full Bayesian inference approximating full posterior distribution), variants (stochastic) gradient descent serve simple, generic, extremely powerful toolbox. recent growth interest creating user-friendly variational inference tools], efforts needed develop eﬃcient general purpose algorithms. work, propose general purpose variational inference algorithm treated natural counterpart gradient descent full Bayesian inference (see Algorithm). 30th Conference Neural Information Processing Systems (nips 2016), barcelona, spain. algorithm set particles approximation, form (functional) gradient descent performed minimize divergence drive particles fit true posterior distribution. algorithm simple form, applied gradient descent applied. fact, reduces gradient descent MAP single particle, automatically turns full Bayesian sampling approach particles. underlying algorithm oretical result connects derivative divergence. smooth variable transforms recently introduced kernelized Stein discrepancy], derive closed form solution optimal smooth perturbation direction steepest descent divergence unit ball reproducing kernel Hilbert space (rkhs). result independent interest, find wide application machine learning statistics variational inference. background Preliminary Let continuous random variable parameter interest taking values  set. observation. prior), Bayesian inference involves reasoning posterior distribution)  ),  troublesome normalization constant. dropped conditioning data) convenience. ,  positive definite kernel. reproducing kernel Hilbert space (rkhs, closure linear span,   equipped products, gih, denote space vector functions  equipped product gihd hfi assume vectors column vectors.   stein Identity Kernelized Stein Discrepancy stein identity plays fundamental role framework. ) continuously differentiable (also called smooth) density supported  ),   )]¿ smooth vector function. stein identity states suﬃciently    regular )] ) log)? )¿ ) called Stein operator, acts function yields function ) this identity easily checked integration parts assuming mild zeroh boundary conditions eir)? , compact, limr?? )?  surface integral sphere radius centered origin) unit normal call stein class stein identity) holds. ) smooth density supported expectation )  )] longer equal general instead, magnitude )] relates are, leveraged define discrepancy measure, Stein discrepancy, ?maximum violation stein identity?  proper function set, max [trace ))] here choice function set critical, decides discriminative power computational tractability Stein discrepancy. traditionally, sets functions bounded Lipschitz norms, casts challenging functional optimization problem computationally intractable requires special considerations (see Gorham Mackey] reference rein). kernelized Stein discrepancy (ksd) bypasses diﬃculty maximizing unit ball reproducing kernel Hilbert space (rkhs) optimization closed form solution. ksd defined, max [trace .  )  assume kernel, RKHS Stein class function fixed  optimal solution) shown ) )/——? ],  (?) , , ——?  ) One furr show, equals (and equivalently ) , strictly positive definite proper sense [see], satisfied commonly kernels RBF kernel, exp(?   note RBF kernel Stein class smooth densities supported decaying property. Stein operator KSD depend score function log), calculated knowing normalization constant log) log . property makes stein identity powerful tool handling unnormalized distributions widely machine learning statistics. Variational Inference Using Smooth Transforms Variational inference approximates target distribution) simpler distribution ) found predefined set)} distributions minimizing divergence, arg min [log)] [log )] log calculate constant log solving optimization. choice set critical defines types variational inference methods. set strike balance accuracy, broad closely approximate large class target distributions) tractability, consisting simple distributions easy inference, iii) solvability subsequent minimization problem eﬃciently solved. work, focus sets consisting distributions obtained smooth transforms tractable reference distribution, set distributions random variables form) smooth one-one transform, drawn tractable reference distribution). change variables formula, density)) det))—, denotes inverse map Jacobian matrix distributions computationally tractable, sense expectation easily evaluated averaging  principle closely approximate arbitrary distributions: shown exists measurable transform distributions atoms. single point carries positive mass); addition, Lipschitz continuous densities exist transforms smooth refer readers Villani-depth discussion topic. practice, however, restrict set transforms properly make variational optimization) practically solvable. approach parametric form optimize parameters]. however, introduces diﬃcult problem selecting proper parametric family balance accuracy, tractability solvability, one-one map eﬃciently computable Jacobian matrix. instead, propose algorithm iteratively constructs incremental transforms effectively perform steepest descent rkhs. algorithm require explicitly parametric forms, calculate Jacobian matrix, simple form mimics typical gradient descent algorithm, making easily implementable non-experts variational inference.  Stein Operator Derivative Divergence explain minimize divergence), incremental transform formed small perturbation identity map) ), ) smooth function characterizes perturbation direction scalar represents perturbation magnitude. suﬃciently small, Jacobian \\x0cfull rank (close identity matrix), guaranteed one-one map inverse function orem. result, forms foundation method, draws insightful connection Stein operator derivative divergence. perturbation magnitude orem. ) ) density) ),  [trace ) ) log)? )¿ ) Stein operator. relating definition KSD), identify ) optimal perturbation direction steepest descent divergence zero-centered balls lemma. assume conditions orem. perturbation directions ball   )} vector-valued RKHS direction steepest descent maximizes negative gradient) .,  (?)  log, , ) equals square ksd, ). ) result Lemma) suggests iterative procedure transforms initial reference distribution target distribution start applying transform)  ) decreases divergence amount ), small step size; give distribution), furr transform)  ) furr decrease divergence ). repeating process constructs path distributions)  ). ) This eventually converge target suﬃciently small stepsize? )  reduces identity map. recall ? )  functional Gradient gain furr intuition process, reinterpret) functional gradient rkhs. functional (functional) gradient function gihd  orem. ),  density)  ), RKHS norm ——? ). suggests )  ) equivalent step functional gradient descent rkhs. however, critical iterative procedure) iteratively apply variable transform time evaluate functional gradient descent perturbation identity map) brings critical advantage gradient complex require calculate inverse Jacobian matrix casts computational implementation hurdles.  \\x0cstein Variational Gradient Descent implement iterative procedure) practice, approximate expectation calculating ). this, draw set particles {x0i initial distribution iteratively update particles empirical version transform) expectation ‘ approximated empirical particles iteration. procedure summarized Algorithm (deterministically) transport set points match target distribution), effectively providing Algorithm Bayesian Inference Variational Gradient Descent input: target distribution density function) set initial particles {x0i output: set particles approximates target distribution). iteration      log step size iteration. end sampling method). implementation procedure depend initial distribution all, practice start set arbitrary points possibly generated complex (randomly deterministic) black-box procedure. expect forms increasingly approximation increases. this, denote nonlinear map takes measure outputs‘ enters map ‘ updates Algorithm applying map empirical measure particles empirical measure particles iteration   converges increases, converge map ?continuous? proper sense. rigorous oretical results convergence established Pnin field ory interacting particle systems], general guarantee / bounded testing functions addition, distribution particle fixed independent finite subset particles phenomenon called propagation chaos]. leave concrete oretical analysis future work.  ) Algorithm mimics gradient dynamics particle level, terms play roles: term drives particles high probability areas) smood gradient direction, weighted sum gradients points weighted kernel function. term acts repulsive force prevents points collapse toger local modes); this, RBF kernel, exp(?   term reduces ), drives neighboring points large). bandwidth repulsive term vanishes, update) reduces set independent chains typical gradient ascent maximizing log., map) particles collapse local modes. anor interesting case single particle), case Algorithm reduces single chain typical gradient ascent MAP kernel satisfies, (for RBF holds). suggests algorithm generalize supervised learning tasks small number particles, gradient ascent MAP shown successful practice. property distinguishes particle method typical Monte Carlo methods requires average points. key difference deterministic repulsive force, Monte Carlo randomness, diverse points distributional approximation. complexity Eﬃcient Implementation major computation bottleneck) lies calculating gradient logq) points case big data settings) ) large conveniently address problem approximating log) subsampled mini-batches  ,   data log) log) log). ?? additional speedup obtained parallelizing gradient evaluation update) requires compute kernel matrix costs practice, cost small compared cost gradient evaluation, suﬃcient small., hundreds) practice. large particles. approximate summation) subsampling particles, random feature expansion kernel]. related Works Our work related Rezende Mohamed], considers variational inference set transformed random variables, focuses transforms parametric form)   )))) (?) predefined simple parametric transform predefined length; essentially creates feedforward neural network layers, invertibility requires furr conditions parameters established case case. similar idea discussed Marzouk. ], considers transforms parameterized special ways ensure invertible computational tractability Jacobian matrix. recently, Tran. ] constructed variational family achieves universal approximation based Gaussian process (equivalent single-layer, infinitely-wide neural network), Jacobian matrix calculate inverse kernel matrix Gaussian process. algorithm simpler form, require calculate matrix determinant inversion. works leverage variable transforms variational inference, limited forms; examples include aﬃne transforms], recently copula models correspond element-wise transforms individual variables]. algorithm maintains updates set particles, similar style Gaussian mixture variation inference methods]. optimizmean parameters treated set particles. ing mixture objectives requires approximation, recently Gershman. ] approximating entropy jensen inequality expectation term Taylor approximation. large set particle-based Monte Carlo methods, including variants sequential Monte Carlo], recent particle mirror descent optimizing variational objective function]; compared methods, method weight degeneration problem, ?particle-eﬃcient? reduce MAP single particle. experiments test algorithm toy real world examples, find method outperform variety baseline methods. code https://github.com/dartml/ stein-variational-gradient-descent. experiments, RBF kernel, exp(?   bandwidth med2 log med median pairwise distance current points based intuition exp(? med2 contribution gradient inﬂuence points balance. note way, bandwidth adaptively iterations. adagrad step size initialize particles prior distribution orwise specified. toy Example Gaussian Mixture set target distribution; ), initialize particles). creates challenging situation probability mass) (with overlap). figure shows distribution particles method evolve iterations. small overlap), method push particles target distribution, recover mode furr initial point. found particle based algorithms, Dai. ], tend experience weight degeneracy toy due ill choice). figure compares method Monte Carlo sampling obtained particles estimate expectation)) test functions(?). mse method perform similarly exact Monte Carlo sampling. particles spread. samples due repulsive force, give higher estimation accuracy. remains open question formally establish error rate method. 0th Iteration 50th Iteration 75th Iteration 100th Iteration 150th Iteration 500th Iteration 250 Sample Size) Estimating) Log10 MSE Log10 MSE Log10 MSE Figure Toy Gaussian mixture. red dashed lines target density function solid green lines densities particles iterations algorithm (estimated kernel density estimator) note initial distribution set overlap target distribution, method demonstrates ability escaping local mode left recover mode left furr away. 100 particles. 250 Sample Size) Estimating Monte Carlo Stein Variational Gradient Descent 250 Sample Size) Estimating(cos)) Figure setting Figure varying number particles. ) show square errors obtained particles estimate expectation) cos); cos), draw  , uniform?]) report average MSE random draws Bayesian Logistic Regression Bayesian logistic regression binary classification setting Gershman. ], assigns regression weights Gaussian prior—?) , (?) gamma). inference applied posterior, log ?]. compared algorithm-turn sampler (nuts] non-parametric variational inference (npv] datasets 500) Gershman. ], find tend give similar results (relatively simple) datasets; Appendix details. furr test binary Covertype dataset3 581,012 data points features. dataset large, stochastic gradient descent needed speed. NUTS NPV mini-batch option code, compare stochastic gradient Langevin dynamics (sgld) Welling Teh], particle mirror descent (pmd) Dai. ], doubly stochastic variational inference (dsvi) Titsias?zaro-gredilla compare parallel version SGLD runs parallel chains point chain result. parallel SGLD similar method step-size suggested Welling Teh] fair comparison?select validation set training set. pmd, step size /(100), RBF kernel, exp ) bandwidth.002 med2 based guidance Dai. ] find works eﬃciently pmd. figure) shows results initialize method versions SGLD prior —? find PMD unstable initialization generates weights large magnitudes, divided initialized weights pmd; shown Figure), advantage PMD initial stage. find method generally performs best, parallel sgld, sequential counterpart; comparison favor parallel sgld, iteration requires 100 times likelihood evaluations compared sequential sgld. however, leveraging matrix operation matlab, find iteration parallel SGLD times expensive sequential sgld. code: http://www.princeton.edu/ mdhoffma/ code: http://gershmanlab.webfactional.com/pubs/npv.zip https://www.csie.ntu.edu/cjlin/libsvmtools/datasets/binary.html code: http://www.aueb/users/mtitsias/code/dsvi matlabv1.zip. scale gradient SGLD factor make match scale gradient).  Testing Accuracy Testing Accuracy Number Epoches) Particle size 100 Stein Variational Gradient Descent (our method) Stochastic Langevin (parallel sgld) Particle Mirror Descent (pmd) Doubly Stochastic (dsvi) Stochastic Langevin (sequential sgld 250 Particle Size) Results 3000 iteration  epoches) Figure Results Bayesian logistic regression Covertype dataset. epochs particle size 100 particles method, parallel SGLD pmd, average 100 points sequential sgld. ?particle-based? methods (solid lines) principle require 100 times \\x0clikelihood evaluations compare DVSI sequential SGLD (dash lines) iteration, implemented eﬃciently Matlab matrix operation., iteration parallel SGLD times slower sequential sgld). partition data% training% testing average random trials. mini-batch size algorithms. Bayesian Neural Network compare algorithm probabilistic back-propagation (pbp) algorithm hern?ndez-lobato Adams] Bayesian neural networks. experiment settings identity, gamma) prior inverse covariances trick scaling input output layer. neural networks hidden layers, hidden units datasets, 100 units Protein Year large; datasets randomly partitioned% training% testing, results averaged random trials, Protein Year trials repeated, respectively. relu) max, active function, weak derivative (stein identity holds weak derivatives; Stein. ]). pbp repeated default setting authors? code6 algorithm, particles, AdaGrad momentum standard deep learning. mini-batch size 100 Year 1000. find algorithm consistently improves PBP terms accuracy speed; encouraging PBP specifically designed Bayesian neural network. find results comparable recent results reported datasets] leverage advanced techniques benefit from. dataset Boston Concrete Energy Kin8nm Naval Combined Protein Wine Yacht Year avg. test RMSE PBP Our Method.977 .093.957 .099.506 .103.324 .104.734 .051.374 .045.098 .001.090 .001.006 .000.004 .000.052 .031.033 .033.623 .009.606 .013.614 .008.609 .010.778 .042.864 .052.733 .684 avg. test PBP Our Method.579 .052.504 .029.137 .021.082 .018.981 .028.767 .024.901 .010.984 .008.735 .004.089 .012.819 .008.815 .008.950 .002.947 .003.931 .014.925 .014.211 .044.225 .042.586 .580 avg. time (secs) PBP Ours 118 173 136 682 7777 684 Conclusion propose simple general purpose variational inference algorithm fast scalable Bayesian inference. future directions include oretical understanding method, practical applications deep learning models, potential applications basic orem Section. Bayesian inference powerful tool modeling complex data reasoning uncertainty, casts long standing challenge computing intractable posterior distributions. markov chain Monte Carlo (mcmc) widely draw approximate posterior samples, slow diﬃculty accessing convergence. variational inference frames Bayesian inference problem deterministic optimization approximates target distribution simpler distribution minimizing divergence. this makes variational methods eﬃciently solvable off-shelf optimization techniques, easily applicable large datasets., ”big data”) stochastic gradient descent trick]. contrast, challenging scale MCMC big data settings [see]. meanwhile, accuracy computational cost variational inference critically depend set distributions approximation defined. simple approximation sets, traditional field methods, restrictive resemble true posterior distributions, advanced choices cast diﬃculties subsequent optimization tasks. for reason, eﬃcient variational methods derived model-model basis, causing major barrier developing general purpose, user-friendly variational tools applicable kinds models, accessible non experts application domains. this case contrast maximum posteriori (map) optimization tasks finding posterior mode (sometimes poor man Bayesian estimator, contrast full Bayesian inference approximating full posterior distribution), variants (stochastic) gradient descent serve simple, generic, extremely powerful toolbox. recent growth interest creating user-friendly variational inference tools], efforts needed develop eﬃcient general purpose algorithms. work, propose general purpose variational inference algorithm treated natural counterpart gradient descent full Bayesian inference (see Algorithm). our 30th Conference Neural Information Processing Systems (nips 2016), barcelona, spain. algorithm set particles approximation, form (functional) gradient descent performed minimize divergence drive particles fit true posterior distribution. our algorithm simple form, applied gradient descent applied. fact, reduces gradient descent MAP single particle, automatically turns full Bayesian sampling approach particles. underlying algorithm oretical result connects derivative divergence. smooth variable transforms recently introduced kernelized Stein discrepancy], derive closed form solution optimal smooth perturbation direction steepest descent divergence unit ball reproducing kernel Hilbert space (rkhs). this result independent interest, find wide application machine learning statistics variational inference. Background Preliminary Let continuous random variable parameter interest taking values  set. observation. with prior), Bayesian inference involves reasoning posterior distribution)  ),  troublesome normalization constant. dropped conditioning data) convenience. let,  positive definite kernel. reproducing kernel Hilbert space (rkhs, closure linear span,   equipped products, gih, denote space vector functions  equipped product gihd hfi assume vectors column vectors. let  stein Identity Kernelized Stein Discrepancy stein identity plays fundamental role framework. let) continuously differentiable (also called smooth) density supported  ),   )]¿ smooth vector function. stein identity states suﬃciently    regular )] ) log)? )¿ ) called Stein operator, acts function yields function ) This identity easily checked integration parts assuming mild zeroh boundary conditions eir)? , compact, limr?? )?  surface integral sphere radius centered origin) unit normal call Stein class stein identity) holds. now) smooth density supported expectation )  )] longer equal general instead, magnitude )] relates are, leveraged define discrepancy measure, Stein discrepancy, ?maximum violation stein identity?  proper function set, max [trace ))] Here choice function set critical, decides discriminative power computational tractability Stein discrepancy. traditionally, sets functions bounded Lipschitz norms, casts challenging functional optimization problem computationally intractable requires special considerations (see Gorham Mackey] reference rein). kernelized Stein discrepancy (ksd) bypasses diﬃculty maximizing unit ball reproducing kernel Hilbert space (rkhs) optimization closed form solution. ksd defined, max [trace .  )  assume kernel, RKHS Stein class function fixed  optimal solution) shown ) )/——? ],  (?) , , ——?  ) One furr show, equals (and equivalently ) , strictly positive definite proper sense [see], satisfied commonly kernels RBF kernel, exp(?   note RBF kernel Stein class smooth densities supported decaying property. both Stein operator KSD depend score function log), calculated knowing normalization constant log) log . this property makes stein identity powerful tool handling unnormalized distributions widely machine learning statistics. Variational Inference Using Smooth Transforms Variational inference approximates target distribution) simpler distribution ) found predefined set)} distributions minimizing divergence, arg min [log)] [log )] log calculate constant log solving optimization. choice set critical defines types variational inference methods. set strike balance accuracy, broad closely approximate large class target distributions) tractability, consisting simple distributions easy inference, iii) solvability subsequent minimization problem eﬃciently solved. work, focus sets consisting distributions obtained smooth transforms tractable reference distribution, set distributions random variables form) smooth one-one transform, drawn tractable reference distribution). change variables formula, density)) det))—, denotes inverse map Jacobian matrix such distributions computationally tractable, sense expectation easily evaluated averaging  such principle closely approximate arbitrary distributions: shown exists measurable transform distributions atoms. single point carries positive mass); addition, Lipschitz continuous densities exist transforms smooth refer readers Villani-depth discussion topic. practice, however, restrict set transforms properly make variational optimization) practically solvable. one approach parametric form optimize parameters]. however, introduces diﬃcult problem selecting proper parametric family balance accuracy, tractability solvability, one-one map eﬃciently computable Jacobian matrix. instead, propose algorithm iteratively constructs incremental transforms effectively perform steepest descent rkhs. our algorithm require explicitly parametric forms, calculate Jacobian matrix, simple form mimics typical gradient descent algorithm, making easily implementable non-experts variational inference.  Stein Operator Derivative Divergence explain minimize divergence), incremental transform formed small perturbation identity map) ), ) smooth function characterizes perturbation direction scalar represents perturbation magnitude. when suﬃciently small, Jacobian \\x0cfull rank (close identity matrix), guaranteed one-one map inverse function orem. result, forms foundation method, draws insightful connection Stein operator derivative divergence. perturbation magnitude orem. let) ) density) ),  [trace ) ) log)? )¿ ) Stein operator. relating definition KSD), identify ) optimal perturbation direction steepest descent divergence zero-centered balls lemma. assume conditions orem. consider perturbation directions ball   )} vector-valued RKHS direction steepest descent maximizes negative gradient) .,  (?)  log, , ) equals square ksd, ). ) result Lemma) suggests iterative procedure transforms initial reference distribution target distribution start applying transform)  ) decreases divergence amount ), small step size; give distribution), furr transform)  ) furr decrease divergence ). repeating process constructs path distributions)  ). ) This eventually converge target suﬃciently small stepsize? )  reduces identity map. recall ? )  functional Gradient gain furr intuition process, reinterpret) functional gradient rkhs. for functional (functional) gradient function gihd  orem. let),  density)  ), RKHS norm ——? ). this suggests )  ) equivalent step functional gradient descent rkhs. however, critical iterative procedure) iteratively apply variable transform time evaluate functional gradient descent perturbation identity map) this brings critical advantage gradient complex require calculate inverse Jacobian matrix casts computational implementation hurdles.  \\x0cstein Variational Gradient Descent implement iterative procedure) practice, approximate expectation calculating ). this, draw set particles {x0i initial distribution iteratively update particles empirical version transform) expectation ‘ approximated empirical particles iteration. this procedure summarized Algorithm (deterministically) transport set points match target distribution), effectively providing Algorithm Bayesian Inference Variational Gradient Descent input: target distribution density function) set initial particles {x0i output: set particles approximates target distribution). iteration      log step size iteration. end sampling method). implementation procedure depend initial distribution all, practice start set arbitrary points possibly generated complex (randomly deterministic) black-box procedure. expect forms increasingly approximation increases. this, denote nonlinear map takes measure outputs‘ enters map ‘ updates Algorithm applying map empirical measure particles empirical measure particles iteration  since converges increases, converge map ?continuous? proper sense. rigorous oretical results convergence established Pnin field ory interacting particle systems], general guarantee / bounded testing functions addition, distribution particle fixed independent finite subset particles phenomenon called propagation chaos]. leave concrete oretical analysis future work.  ) Algorithm mimics gradient dynamics particle level, terms play roles: term drives particles high probability areas) smood gradient direction, weighted sum gradients points weighted kernel function. term acts repulsive force prevents points collapse toger local modes); this, RBF kernel, exp(?   term reduces ), drives neighboring points large). bandwidth repulsive term vanishes, update) reduces set independent chains typical gradient ascent maximizing log., map) particles collapse local modes. anor interesting case single particle), case Algorithm reduces single chain typical gradient ascent MAP kernel satisfies, (for RBF holds). this suggests algorithm generalize supervised learning tasks small number particles, gradient ascent MAP shown successful practice. this property distinguishes particle method typical Monte Carlo methods requires average points. key difference deterministic repulsive force, Monte Carlo randomness, diverse points distributional approximation. complexity Eﬃcient Implementation major computation bottleneck) lies calculating gradient logq) points case big data settings) ) large conveniently address problem approximating log) subsampled mini-batches  ,   data log) log) log). ?? additional speedup obtained parallelizing gradient evaluation update) requires compute kernel matrix costs practice, cost small compared cost gradient evaluation, suﬃcient small., hundreds) practice. large particles. approximate summation) subsampling particles, random feature expansion kernel]. Related Works Our work related Rezende Mohamed], considers variational inference set transformed random variables, focuses transforms parametric form)   )))) (?) predefined simple parametric transform predefined length; essentially creates feedforward neural network layers, invertibility requires furr conditions parameters established case case. similar idea discussed Marzouk. ], considers transforms parameterized special ways ensure invertible computational tractability Jacobian matrix. recently, Tran. ] constructed variational family achieves universal approximation based Gaussian process (equivalent single-layer, infinitely-wide neural network), Jacobian matrix calculate inverse kernel matrix Gaussian process. our algorithm simpler form, require calculate matrix determinant inversion. several works leverage variable transforms variational inference, limited forms; examples include aﬃne transforms], recently copula models correspond element-wise transforms individual variables]. our algorithm maintains updates set particles, similar style Gaussian mixture variation inference methods]. optimizmean parameters treated set particles. ing mixture objectives requires approximation, recently Gershman. ] approximating entropy jensen inequality expectation term Taylor approximation. large set particle-based Monte Carlo methods, including variants sequential Monte Carlo], recent particle mirror descent optimizing variational objective function]; compared methods, method weight degeneration problem, ?particle-eﬃcient? reduce MAP single particle. Experiments test algorithm toy real world examples, find method outperform variety baseline methods. our code https://github.com/dartml/ stein-variational-gradient-descent. for experiments, RBF kernel, exp(?   bandwidth med2 log med median pairwise distance current points based intuition exp(? med2 contribution gradient inﬂuence points balance. note way, bandwidth adaptively iterations. AdaGrad step size initialize particles prior distribution orwise specified. toy Example Gaussian Mixture set target distribution; ), initialize particles). this creates challenging situation probability mass) (with overlap). figure shows distribution particles method evolve iterations. small overlap), method push particles target distribution, recover mode furr initial point. found particle based algorithms, Dai. ], tend experience weight degeneracy toy due ill choice). figure compares method Monte Carlo sampling obtained particles estimate expectation)) test functions(?). MSE method perform similarly exact Monte Carlo sampling. this particles spread. samples due repulsive force, give higher estimation accuracy. remains open question formally establish error rate method. 0th Iteration 50th Iteration 75th Iteration 100th Iteration 150th Iteration 500th Iteration 250 Sample Size) Estimating) Log10 MSE Log10 MSE Log10 MSE Figure Toy Gaussian mixture. red dashed lines target density function solid green lines densities particles iterations algorithm (estimated kernel density estimator) note initial distribution set overlap target distribution, method demonstrates ability escaping local mode left recover mode left furr away. 100 particles. 250 Sample Size) Estimating Monte Carlo Stein Variational Gradient Descent 250 Sample Size) Estimating(cos)) Figure setting Figure varying number particles. ) show square errors obtained particles estimate expectation) cos); cos), draw  , uniform?]) report average MSE random draws Bayesian Logistic Regression Bayesian logistic regression binary classification setting Gershman. ], assigns regression weights Gaussian prior—?) , (?) gamma). inference applied posterior, log ?]. compared algorithm-turn sampler (nuts] non-parametric variational inference (npv] datasets 500) Gershman. ], find tend give similar results (relatively simple) datasets; Appendix details. furr test binary Covertype dataset3 581,012 data points features. this dataset large, stochastic gradient descent needed speed. because NUTS NPV mini-batch option code, compare stochastic gradient Langevin dynamics (sgld) Welling Teh], particle mirror descent (pmd) Dai. ], doubly stochastic variational inference (dsvi) Titsias?zaro-gredilla compare parallel version SGLD runs parallel chains point chain result. this parallel SGLD similar method step-size suggested Welling Teh] fair comparison?select validation set training set. for pmd, step size /(100), RBF kernel, exp ) bandwidth.002 med2 based guidance Dai. ] find works eﬃciently pmd. figure) shows results initialize method versions SGLD prior —? find PMD unstable initialization generates weights large magnitudes, divided initialized weights pmd; shown Figure), advantage PMD initial stage. find method generally performs best, parallel sgld, sequential counterpart; comparison favor parallel sgld, iteration requires 100 times likelihood evaluations compared sequential sgld. however, leveraging matrix operation matlab, find iteration parallel SGLD times expensive sequential sgld. code: http://www.princeton.edu/ mdhoffma/ code: http://gershmanlab.webfactional.com/pubs/npv.zip https://www.csie.ntu.edu/cjlin/libsvmtools/datasets/binary.html code: http://www.aueb/users/mtitsias/code/dsvi matlabv1.zip. scale gradient SGLD factor make match scale gradient).  Testing Accuracy Testing Accuracy Number Epoches) Particle size 100 Stein Variational Gradient Descent (our method) Stochastic Langevin (parallel sgld) Particle Mirror Descent (pmd) Doubly Stochastic (dsvi) Stochastic Langevin (sequential sgld 250 Particle Size) Results 3000 iteration  epoches) Figure Results Bayesian logistic regression Covertype dataset. epochs particle size 100 particles method, parallel SGLD pmd, average 100 points sequential sgld. ?particle-based? methods (solid lines) principle require 100 times \\x0clikelihood evaluations compare DVSI sequential SGLD (dash lines) iteration, implemented eﬃciently Matlab matrix operation., iteration parallel SGLD times slower sequential sgld). partition data% training% testing average random trials. mini-batch size algorithms. Bayesian Neural Network compare algorithm probabilistic back-propagation (pbp) algorithm hern?ndez-lobato Adams] Bayesian neural networks. our experiment settings identity, gamma) prior inverse covariances trick scaling input output layer. neural networks hidden layers, hidden units datasets, 100 units Protein Year large; datasets randomly partitioned% training% testing, results averaged random trials, Protein Year trials repeated, respectively. relu) max, active function, weak derivative (stein identity holds weak derivatives; Stein. ]). pbp repeated default setting authors? code6 for algorithm, particles, AdaGrad momentum standard deep learning. mini-batch size 100 Year 1000. find algorithm consistently improves PBP terms accuracy speed; encouraging PBP specifically designed Bayesian neural network. find results comparable recent results reported datasets] leverage advanced techniques benefit from. dataset Boston Concrete Energy Kin8nm Naval Combined Protein Wine Yacht Year avg. test RMSE PBP Our Method.977 .093.957 .099.506 .103.324 .104.734 .051.374 .045.098 .001.090 .001.006 .000.004 .000.052 .031.033 .033.623 .009.606 .013.614 .008.609 .010.778 .042.864 .052.733 .684 avg. test PBP Our Method.579 .052.504 .029.137 .021.082 .018.981 .028.767 .024.901 .010.984 .008.735 .004.089 .012.819 .008.815 .008.950 .002.947 .003.931 .014.925 .014.211 .044.225 .042.586 .580 avg. time (secs) PBP Ours 118 173 136 682 7777 684 Conclusion propose simple general purpose variational inference algorithm fast scalable Bayesian inference. future directions include oretical understanding method, practical applications deep learning models, potential applications basic orem Section.',\n",
       " 'PP6397': 'minimizing convex function set positive semidefinite matrices unit trace, aka spectrahedron, important optimization task lies heart optimization, machine learning, signal processing tasks matrix completion], metric learning], kernel matrix learning], multiclass classification], more. modern applications large scale, first-order methods obvious choice deal optimization problem. however, notoriously diﬃcult apply, popular gradient schemes require computation orthogonal projection iteration enforce feasibility, spectraheron, amounts computing full eigen-decomposition 30th Conference Neural Information Processing Systems (nips 2016), barcelona, spain. real symmetric matrix. decomposition requires arithmetic operations matrix prohibitive high-dimensional problems. alternative first-order methods require expensive decompositions, rely computationally-cheap leading eigenvector computations. methods based conditional gradient method, frank-wolfe algorithm], generic method constrained convex optimization oracle minimizing linear functions feasible domain. indeed, linear minimization spectrahedron amounts single leading eigenvector computation. method discovered 1950], regained interest recent years machine learning optimization communities, due applications semidefinite optimization convex optimization nuclear norm constraint regularization1]. regained interest surprising: full eigendecomposition matrix requires arithmetic operations, leading eigenvecor computations carried out, roughly speaking, worst-case time linear number non-zeros input matrix multiplied eir popular Power Method  eﬃcient Lanczos method, target accuracy. running times improve exponentially depend log/?) eigenvalues input matrix distributed]. indeed, important machine learning applications, matrix completion, method requires eigenvector computations sparse matrices]. also, recently, eigenvector algorithms significantly improved performance guarantees introduced applicable matrices popular structure]. main drawback method convergence rate, general, inferior compared projection-based gradient methods. convergence rate minimizing smooth function, roughly speaking, scales. particular, rate improve function strongly convex. hand, convergence rate optimal projection-based methods, nesterov accelerated gradient method, scales smooth functions, improved exponentially exp( )) objective strongly convex. recently, successful attempts made devise natural modifications method retain low per-iteration complexity, enjoying provably faster convergence rates, strong-convexity assumption, slightly weaker one. results exhibit provablyfaster rates optimization polyhedral sets] \\x0cand strongly-convex sets], apply spectrahedron. specific setting considered work, heuristic improvements method suggested show promising empirical evidence, however, provably improve rate standard method]. work present non-trivial variant method, which, knowledge, exhibit provably faster convergence rates optimization spectrahedron, standard smoothness strong convexity assumptions. per-iteration complexity method essentially identical standard method., single leading eigenvector computation iteration required. method tailored optimization spectrahedron, hybridization standard method projected gradient method. high-level view, advantage fact solving -regularized linear problem set extreme points spectrahedron equivalent linear optimization set., amounts single eigenvector computation. show non-trivial analysis, includes decomposition concepts positive semidefinite matrices, algorithmically-cheap regularization suﬃcient, presence strong convexity, derive faster convergence rates. preliminaries Notation For vectors denote standard Euclidean norm, matrices denote spectral norm denote Frobenius norm,  denote nuclear norm. denote space real symmetric matrices, spectrahedron., }. (?) rank(?) denote trace rank matrix respectively.  denote standard inner-product matrices. matrix min) denote smallest non-zero eigenvalue minimizing convex function subject nuclear norm constraint eﬃciently reducible minimization function spectrahedron, fully detailed]. given matrix denote) eigenvector corresponds largest (signed) eigenvalue) arg maxv:kvk. scalar denote? ) ?-approximation largest terms eigenvalue) eigenvector? ) returns unit vector max Definition function) ?-strongly convex. norm  holds ) ) Yk2  Definition function) -smooth. norm  holds) ) ) Yk2 first-order optimality condition implies ?-strongly convex unique minimizer convex set  holds ?    Problem setting main focus work optimization problem: min), X2Sd) assume) ?-strongly convex -smooth.   denote (unique) minimizer  our Approach begin brieﬂy describing conditional gradient projected-gradient methods, pointing advantages short-comings solving Problem) Subsection. present method combination ideas methods Subsection.  Conditional gradient projected gradient descent standard conditional gradient algorithm detailed Algorithm input: sequence step-sizes arbitrary matrix algorithm Conditional Gradient¿ end , Let denote approximation error Algorithm iterations?  convergence result Algorithm based simple servations? ) ¿  kvt¿ k2f  ?   kvt¿ k2f  kvt¿ k2f inequality -smoothness), optimal choice convexity). unfortunately, expect error rapidly converge zero, term kvt¿ k2f. ), principal, remain large diameter which, proper choice step-size results well-known convergence rate]. consequence holds case) smooth, strongly-convex. however, case strongly convex, non-trivial modification Algorithm lead faster convergence rate. case. ), iteration kxt k2f   thus, replacing choice Algorithm update rule: arg min  V2Sd k2f) basically steps.  ?  ? k2f    ) proper choice linear convergence rate attained. issue now, computing longer computationallycheap leading eigenvalue problem rank-one), requires full eigen-decomposition expensive. fact, update rule. ) projected gradient decent method.  hybrid approach: rank one-regularized conditional gradient algorithm heart method combination ideas approaches: hand, solving regularized linear problem order avoid shortcomings method., slow convergence rate, hand, maintaining simple structure leading eigenvalue computation avoids shortcoming computationallyexpensive projected-gradient method. end, suppose explicit decomposition current iterate ..., probability distribution], unit vector. note standard method (algorithm naturally produces explicit decomposition (provided chosen rank-one). update rule. ), additional restriction rank one, arg minv2sd rank  note case unit trace rank-one matrix corresponds leading eigenvector matrix however, rank-one, regularization kvt k2f makes sense general, rank-one, expect (note however, rank one, modification result linear convergence rate). however, solving set decoupled component-wise regularized problems] arg min kvv kvk   equivalence line kvv¿ minimizer LHS. leading eigenvector matrix rhs. lines.    )¿   ? )¿ kvt )¿ kvt) inequality convexity squared Frobenius norm, equality ..., probability distribution]. approach. ) relies leading eigenvector computations, benefit terms potential convergence rates trivial, non-trivial)¿ bounds individual distances kvt indeed, main novelty analysis dedicated precisely issue. motivation, any, exists decomposition   ) )¿ close sense decomposition regularized problem. ), attempt push individual component) component decomposition result, bring iterate closer  note. ) implicitly describes randomized algorithm which, solving regularized problem rank-one matrix decomposition expensive decomposition grows large number iterations, pick single rank-one component weight decomposition, update. directly brings proposed algorithm, Algorithm below. algorithm Randomized Rank one-regularized Conditional Gradient input: sequence step-sizes sequence error tolerances arbitrary unit vector... suppose unit vector ..., probability distribution], integer pick] probability distribution set step-size follows: ait  xit ¿ xit: end guarantee Algorithm main result paper. orem [main orem] Consider sequence step-sizes defined), suppose holds iteration   rank min{ min?  iterates feasible rank?  ? @min{ min  important note step-size choice orem require knowledge parameters rank? min?  knowledge required computations. orem knowledge rank? min? needed set accuracy parameters practice, iterative eigenvector methods eﬃcient sensitive exact knowledge parameters choice stepsize instance. eigenvalue problem Algorithm Algorithm due additional term xit eﬃciency solving problems essentially eﬃcient procedures based iteratively multiplying input matrix vector. particular, multiplying vector rank-one matrix takes) time. thus, long nnz ), highly reasonable, computations run essentially time. finally, note computation gradient direction leading eigenvector computation, operations iteration carried additional time. analysis complete proof orem supporting lemmas full detail appendix. detail main ingredients analysis Algorithm section, matrix,? denote projection matrix eigenvectors correspond eigenvalues magnitude  similarly, ,? denote projection matrix eigenvectors correspond eigenvalues magnitude smaller (including eigenvectors correspond zero-valued eigenvalues).  decomposition positive semidefinite matrices locality prop erties analysis Algorithm relies heavily decomposition idea matrices suggests matrix form convex combination rank-one matrices anor matrix roughly speaking, decompose sum rank-one matrices, components decomposition close decomposition terms distance YkF decomposition property justifies idea solving rank-one regularized problems, suggested. ), applied Algorithm lemma  unit vector ..., distribution, scalars satisfy ykf written, unit vector ]   rank) kyp? ,?  ¿ k2f rank) kyp? ,?  kxi YkF YkF Bounding per-iteration improvement main step proof orem understanding per-iteration improvement, captured. ), achievable applying update rule. ), updates iteration rank-one components decomposition current iterate. lemma [full deterministic update] Fix scalar   unit vector ..., probability distribution]. )  holds ¿ )  min, kvi¿ rank? )  ? )  min? ? )}. proof sketch. proof divided parts, term min expression bound lemma. bound, high-level, standard conditional gradient analysis (see. )). continue derive bounds. Lemma write way: ? ? ? unit vector,  ) Using. ), optimality], bounds Lemma shown  ¿ kvi¿ ) ? ?    kyi       ) rank? ?     ) Now optimize bound terms option upper bound?     rank?  toger choice 2rank?  2rank? give: RHS) ?  ) rank?   ) Anor option, choose This results bound: RHS) min ?   min ?        min ) now, convexity upper bound?  ) ? . . ), parts bound lemma. preliminary Empirical Evaluation evaluate method, conditional gradient variants, task matrix completion]. setting underlying optimization problem matrix completion task following: min Z2N Bd1 (?) ) eil indicator matrix entry, Rd1    (?) denotes nuclear-norm ball radius rd1 (?)  kzk? min)  ) denote vector singular values , goal find matrix bounded nuclear norm (which serves convex surrogate bounded rank) matches partial observations order transform Problem) optimization spectrahedron, reduction full detail], Section appendix. objective function. ) smoothness parameter respect satisfies), instance]. objective function. ) strongly convex, conditions, matrix completion problem exhibit properties similar strong convexity, sense. ) (which consequence strong convexity analysis].  away ror away ror error error 100 120 140 160 180 200 220 #iterations 100 120 140 160 180 200 220 240 #iterations Figure Comparison conditional gradient variants solving matrix completion problem OVIE ENS 100 (left) OVIE ENS (right) datasets. modifications Algorithm implemented rank one-regularized conditional gradient variant, Algorithm (denoted ror figures) modifications. first, iteration picking index rank-one matrix decomposition current iterate random distribution ..., choose greedy way., choose rank-one component largest product current gradient direction. approach computationally expensive, easily parallelized dotproduct computations independent. second, computing eigenvector step-size (which close prescribed orem), apply line-search, detailed], order determine optimal step-size direction¿ xit baselines baselines comparison standard conditional gradient method exact line-search setting step-size (denoted figures], conditional gradient away-steps variant, recently studied] (denoted away figures). away-steps variant studied context optimization polyhedral sets, formal improved guarantees apply setting, concept away-steps makes sense convex feasible set. variant incorporation exact line-search procedure choose optimal step-size. datasets experimented datasets matrix completion task: OVIE ENS 100 dataset 943, 1682, 105 OVIE ENS dataset 6040, 3952, 106 ovie ENS dataset furr sub-sampled roughly half observations. set parameter problem) 10000 ML100 dataset, 35000 ML1M dataset. figure presents objective. number iterations executed. graph average independent experiments approach improves significantly baselines terms convergence rate, setting consideration. Minimizing convex function set positive semidefinite matrices unit trace, aka spectrahedron, important optimization task lies heart optimization, machine learning, signal processing tasks matrix completion], metric learning], kernel matrix learning], multiclass classification], more. since modern applications large scale, first-order methods obvious choice deal optimization problem. however, notoriously diﬃcult apply, popular gradient schemes require computation orthogonal projection iteration enforce feasibility, spectraheron, amounts computing full eigen-decomposition 30th Conference Neural Information Processing Systems (nips 2016), barcelona, spain. real symmetric matrix. such decomposition requires arithmetic operations matrix prohibitive high-dimensional problems. alternative first-order methods require expensive decompositions, rely computationally-cheap leading eigenvector computations. methods based conditional gradient method, frank-wolfe algorithm], generic method constrained convex optimization oracle minimizing linear functions feasible domain. indeed, linear minimization spectrahedron amounts single leading eigenvector computation. while method discovered 1950], regained interest recent years machine learning optimization communities, due applications semidefinite optimization convex optimization nuclear norm constraint regularization1]. this regained interest surprising: full eigendecomposition matrix requires arithmetic operations, leading eigenvecor computations carried out, roughly speaking, worst-case time linear number non-zeros input matrix multiplied eir popular Power Method  eﬃcient Lanczos method, target accuracy. running times improve exponentially depend log/?) eigenvalues input matrix distributed]. indeed, important machine learning applications, matrix completion, method requires eigenvector computations sparse matrices]. also, recently, eigenvector algorithms significantly improved performance guarantees introduced applicable matrices popular structure]. main drawback method convergence rate, general, inferior compared projection-based gradient methods. convergence rate minimizing smooth function, roughly speaking, scales. particular, rate improve function strongly convex. hand, convergence rate optimal projection-based methods, nesterov accelerated gradient method, scales smooth functions, improved exponentially exp( )) objective strongly convex. very recently, successful attempts made devise natural modifications method retain low per-iteration complexity, enjoying provably faster convergence rates, strong-convexity assumption, slightly weaker one. results exhibit provablyfaster rates optimization polyhedral sets] \\x0cand strongly-convex sets], apply spectrahedron. for specific setting considered work, heuristic improvements method suggested show promising empirical evidence, however, provably improve rate standard method]. work present non-trivial variant method, which, knowledge, exhibit provably faster convergence rates optimization spectrahedron, standard smoothness strong convexity assumptions. per-iteration complexity method essentially identical standard method., single leading eigenvector computation iteration required. our method tailored optimization spectrahedron, hybridization standard method projected gradient method. from high-level view, advantage fact solving -regularized linear problem set extreme points spectrahedron equivalent linear optimization set., amounts single eigenvector computation. show non-trivial analysis, includes decomposition concepts positive semidefinite matrices, algorithmically-cheap regularization suﬃcient, presence strong convexity, derive faster convergence rates. Preliminaries Notation For vectors denote standard Euclidean norm, matrices denote spectral norm denote Frobenius norm,  denote nuclear norm. denote space real symmetric matrices, spectrahedron., }. (?) rank(?) denote trace rank matrix respectively.  denote standard inner-product matrices. given matrix min) denote smallest non-zero eigenvalue minimizing convex function subject nuclear norm constraint eﬃciently reducible minimization function spectrahedron, fully detailed]. Given matrix denote) eigenvector corresponds largest (signed) eigenvalue) arg maxv:kvk. given scalar denote? ) ?-approximation largest terms eigenvalue) eigenvector? ) returns unit vector max Definition function) ?-strongly convex. norm  holds ) ) Yk2  Definition function) -smooth. norm  holds) ) ) Yk2 first-order optimality condition implies ?-strongly convex unique minimizer convex set  holds ?    Problem setting main focus work optimization problem: min), X2Sd) assume) ?-strongly convex -smooth.   denote (unique) minimizer  Our Approach begin brieﬂy describing conditional gradient projected-gradient methods, pointing advantages short-comings solving Problem) Subsection. present method combination ideas methods Subsection.  Conditional gradient projected gradient descent standard conditional gradient algorithm detailed Algorithm input: sequence step-sizes arbitrary matrix algorithm Conditional Gradient¿ end , Let denote approximation error Algorithm iterations?  convergence result Algorithm based simple servations? ) ¿  kvt¿ k2f  ?   kvt¿ k2f  kvt¿ k2f inequality -smoothness), optimal choice convexity). unfortunately, expect error rapidly converge zero, term kvt¿ k2f. ), principal, remain large diameter which, proper choice step-size results well-known convergence rate]. this consequence holds case) smooth, strongly-convex. however, case strongly convex, non-trivial modification Algorithm lead faster convergence rate. case. ), iteration kxt k2f   thus, replacing choice Algorithm update rule: arg min  V2Sd k2f) basically steps.  ?  ? k2f    ) proper choice linear convergence rate attained. issue now, computing longer computationallycheap leading eigenvalue problem rank-one), requires full eigen-decomposition expensive. fact, update rule. ) projected gradient decent method.  hybrid approach: rank one-regularized conditional gradient algorithm heart method combination ideas approaches: hand, solving regularized linear problem order avoid shortcomings method., slow convergence rate, hand, maintaining simple structure leading eigenvalue computation avoids shortcoming computationallyexpensive projected-gradient method. towards end, suppose explicit decomposition current iterate ..., probability distribution], unit vector. note standard method (algorithm naturally produces explicit decomposition (provided chosen rank-one). consider update rule. ), additional restriction rank one, arg minv2sd rank  note case unit trace rank-one matrix corresponds leading eigenvector matrix however, rank-one, regularization kvt k2f makes sense general, rank-one, expect (note however, rank one, modification result linear convergence rate). however, solving set decoupled component-wise regularized problems] arg min kvv kvk   equivalence line kvv¿ minimizer LHS. leading eigenvector matrix rhs. following lines.    )¿   ? )¿ kvt )¿ kvt) inequality convexity squared Frobenius norm, equality ..., probability distribution]. while approach. ) relies leading eigenvector computations, benefit terms potential convergence rates trivial, non-trivial)¿ bounds individual distances kvt indeed, main novelty analysis dedicated precisely issue. motivation, any, exists decomposition   ) )¿ close sense decomposition regularized problem. ), attempt push individual component) component decomposition result, bring iterate closer  note. ) implicitly describes randomized algorithm which, solving regularized problem rank-one matrix decomposition expensive decomposition grows large number iterations, pick single rank-one component weight decomposition, update. this directly brings proposed algorithm, Algorithm below. algorithm Randomized Rank one-regularized Conditional Gradient input: sequence step-sizes sequence error tolerances arbitrary unit vector... suppose unit vector ..., probability distribution], integer pick] probability distribution set step-size follows: ait  xit ¿ xit: end guarantee Algorithm main result paper. orem [main orem] Consider sequence step-sizes defined), suppose holds iteration   rank min{ min?  iterates feasible rank?  ? @min{ min  important note step-size choice orem require knowledge parameters rank? min?  knowledge required computations. while orem knowledge rank? min? needed set accuracy parameters practice, iterative eigenvector methods eﬃcient sensitive exact knowledge parameters choice stepsize instance. while eigenvalue problem Algorithm Algorithm due additional term xit eﬃciency solving problems essentially eﬃcient procedures based iteratively multiplying input matrix vector. particular, multiplying vector rank-one matrix takes) time. thus, long nnz ), highly reasonable, computations run essentially time. finally, note computation gradient direction leading eigenvector computation, operations iteration carried additional time. Analysis complete proof orem supporting lemmas full detail appendix. here detail main ingredients analysis Algorithm throughout section, matrix,? denote projection matrix eigenvectors correspond eigenvalues magnitude  similarly, ,? denote projection matrix eigenvectors correspond eigenvalues magnitude smaller (including eigenvectors correspond zero-valued eigenvalues).  decomposition positive semidefinite matrices locality prop erties analysis Algorithm relies heavily decomposition idea matrices suggests matrix form convex combination rank-one matrices anor matrix roughly speaking, decompose sum rank-one matrices, components decomposition close decomposition terms distance YkF this decomposition property justifies idea solving rank-one regularized problems, suggested. ), applied Algorithm Lemma let unit vector ..., distribution, scalars satisfy YkF written, unit vector ]   rank) kyp? ,?  ¿ k2f rank) kyp? ,?  kxi YkF YkF Bounding per-iteration improvement main step proof orem understanding per-iteration improvement, captured. ), achievable applying update rule. ), updates iteration rank-one components decomposition current iterate. Lemma [full deterministic update] Fix scalar  let unit vector ..., probability distribution]. for)  holds ¿ )  min, kvi¿ rank? )  ? )  min? ? )}. proof sketch. proof divided parts, term min expression bound lemma. bound, high-level, standard conditional gradient analysis (see. )). continue derive bounds. from Lemma write way: ? ? ? unit vector,  ) Using. ), optimality], bounds Lemma shown  ¿ kvi¿ ) ? ?    kyi       ) rank? ?     ) Now optimize bound terms one option upper bound?     rank?  toger choice 2rank?  2rank? give: RHS) ?  ) rank?   ) Anor option, choose This results bound: RHS) min ?   min ?        min ) now, convexity upper bound?  ) ? . . ), parts bound lemma. Preliminary Empirical Evaluation evaluate method, conditional gradient variants, task matrix completion]. setting underlying optimization problem matrix completion task following: min Z2N Bd1 (?) ) eil indicator matrix entry, Rd1    (?) denotes nuclear-norm ball radius Rd1 (?)  kzk? min)  ) denote vector singular values that, goal find matrix bounded nuclear norm (which serves convex surrogate bounded rank) matches partial observations order transform Problem) optimization spectrahedron, reduction full detail], Section appendix. objective function. ) smoothness parameter respect satisfies), instance]. while objective function. ) strongly convex, conditions, matrix completion problem exhibit properties similar strong convexity, sense. ) (which consequence strong convexity analysis].  away ror away ror error error 100 120 140 160 180 200 220 #iterations 100 120 140 160 180 200 220 240 #iterations Figure Comparison conditional gradient variants solving matrix completion problem OVIE ENS 100 (left) OVIE ENS (right) datasets. two modifications Algorithm implemented rank one-regularized conditional gradient variant, Algorithm (denoted ror figures) modifications. first, iteration picking index rank-one matrix decomposition current iterate random distribution ..., choose greedy way., choose rank-one component largest product current gradient direction. while approach computationally expensive, easily parallelized dotproduct computations independent. second, computing eigenvector step-size (which close prescribed orem), apply line-search, detailed], order determine optimal step-size direction¿ xit baselines baselines comparison standard conditional gradient method exact line-search setting step-size (denoted figures], conditional gradient away-steps variant, recently studied] (denoted away figures). while away-steps variant studied context optimization polyhedral sets, formal improved guarantees apply setting, concept away-steps makes sense convex feasible set. this variant incorporation exact line-search procedure choose optimal step-size. datasets experimented datasets matrix completion task: OVIE ENS 100 dataset 943, 1682, 105 OVIE ENS dataset 6040, 3952, 106 OVIE ENS dataset furr sub-sampled roughly half observations. set parameter Problem) 10000 ML100 dataset, 35000 ML1M dataset. figure presents objective. number iterations executed. each graph average independent experiments approach improves significantly baselines terms convergence rate, setting consideration.',\n",
       " 'PP6406': 'magnetic Resonance Imaging (mri) non-invasive imaging technique providing functional anatomical information clinical diagnosis. imaging speed fundamental challenge. fast MRI techniques essentially demanded accelerating data acquisition reconstructing high quality image. compressive sensing MRI-mri) effective approach allowing data sampling rate lower Nyquist rate significantly degrading image quality]. -mri methods sample data-space., Fourier space), reconstruct image compressive sensing ory. regularization related data prior key component CSMRI model reduce imaging artifacts improve imaging precision. sparse regularization explored specific transform domain general dictionary-based subspace]. total Variation) regularization gradient domain widely utilized MRI]. easy fast optimize, introduces staircase artifacts reconstructed image. methods, leverage sparse regularization wavelet domain. dictionary learning methods rely dictionary local patches improve reconstruction accuracy]. non-local method groups similar local patches joint patch-level reconstruction preserve image details]. performance, basic-mri methods run fast produce accurate reconstruction results. non-local dictionary learning-based methods generally output higher quality images, suffer slow reconstruction speed. -mri model, commonly challenging choose optimal image transform domain subspace sparse regularization. 30th Conference Neural Information Processing Systems (nips 2016), barcelona, spain. optimize-mri models, Alternating Direction Method Multipliers (admm) proven eﬃcient variable splitting algorithm convergence guarantee]. considers augmented Lagrangian function-mri model, splits variables subgroups, alternatively optimized solving simply subproblems. ADMM generally eﬃcient, trivial determine optimal parameters., update rates, penalty parameters) inﬂuencing accuracy-mri. work, aim design fast accurate method reconstruct high-quality images under-sampled-space data. propose deep architecture, dubbed admm-net, inspired ADMM iterative procedures optimizing general-mri model. deep architecture consists multiple stages, corresponds iteration ADMM algorithm. specifically, define deep architecture represented data ﬂow graph] ADMM procedures. operations ADMM represented graph nodes, data ﬂow operations ADMM represented directed edge. refore, ADMM iterative procedures naturally determine deep architecture data ﬂow graph. under-sampled data-space, ﬂows graph generates reconstructed image. parameters., transforms, shrinkage functions, penalty parameters, etc.) deep architecture discriminatively learned training pairs undersampled data-space reconstructed image fully sampled data backpropagation] data ﬂow graph. experiments demonstrate proposed deep admm-net effective reconstruction accuracy speed. compared baseline methods sparse regularization transform domain, achieves significantly higher accuracy takes comparable computational time. compared state--art methods dictionary learning non-local techniques, achieves high accuracy significantly faster computational speed. main contributions paper summarized follows. propose deep admm-net reformulating ADMM algorithm deep network-mri. achieved designing data ﬂow graph ADMM effectively build train admm-net. admmnet achieves high accuracy image reconstruction fast computational speed justified experiments. discriminative parameter learning \\x0capproach applied sparse coding Markov Random Filed]. but, knowledge, computational framework maps ADMM algorithm learnable deep architecture.  Deep admm-net Fast MRI Compressive Sensing MRI Model ADMM Algorithm general-mri model: Assume mri image reconstructed,  under-sampled-space data, ory, reconstructed image estimated solving optimization problem:   arg min   measurement matrix, undersampling matrix, Fourier transform. denotes transform matrix filtering operation., Discrete Wavelet Transform (dwt), Discrete Cosine Transform (dct), etc. (?) regularization function derived data prior., -norm  sparse prior. regularization parameter. ADMM solver] optimization problem solved eﬃciently ADMM algorithm. introducing auxiliary variables   eqn. ) equivalent: min .   ,   ].  Its augmented Lagrangian function         ,  ) Sampling data-space) Reconstructed image) stage Figure data ﬂow graph ADMM optimization general-mri model. graph consists types nodes: reconstruction), convolution), non-linear transform), multiplier update). under-sampled data-space successively processed graph, finally generates image. deep admm-net defined data ﬂow graph.   Lagrangian multipliers  penalty parameters. admm alternatively optimizes, solving subproblems: ) arg min          ) arg min   )    ) )  arg min )  ) ,   denotes iteration. simplicity,  ,   ]), substitute eqn. ). subproblems solutions: )     ) eﬃciently computed fast Fourier transform(?) nonlinear shrinkage function. soft hard thresholding function sparse regularization -norm -norm]. parameter update rate. -mri, commonly run ADMM algorithm dozens iterations satisfactory reconstruction result. however, challenging choose transform shrinkage function(?) general regularization function(?). moreover, trivial tune parameters-space data sampling ratios. overcome diﬃculties, design data ﬂow graph ADMM algorithm, define deep admm-net discriminatively learn transforms, functions, parameters.  Data Flow Graph ADMM Algorithm design deep admm-net, map ADMM iterative procedures eqn. ) data ﬂow graph]. shown fig. graph comprises nodes operations admm, directed edges data ﬂows operations. case iteration ADMM algorithm corresponds stage data ﬂow graph.  stage graph, types nodes mapped types operations admm., reconstruction operation) convolution operation) defined nonlinear transform operation) defined(? multiplier update operation) eqn. ). data ﬂow graph multiple repetition stages successive iterations admm. under-sampled data-space, ﬂows graph finally generates reconstructed image. way, map ADMM iterations data ﬂow graph, define train deep admm-net sections.  Deep admm-net Our deep admm-net defined data ﬂow graph. graph structure generalizes types operations learnable parameters network layers. operations generalized reconstruction layer, convolution layer, non-linear transform layer, multiplier update layer. discuss details. reconstruction layer) This layer reconstructs MRI image reconstruction) operation) eqn. ). output layer defined) ) )  filter penalty parameter,   input) under-sampled data-space. stage), initialized zeros, ) refore). convolution layer) performs convolution operation transform image transform domain. image., reconstructed image stage output) learnable filter matrix stage Different original admm) constrain filters increase network capacity. nonlinear transform layer) This layer performs nonlinear transform inspired shrinkage function(?) defined) eqn. ). setting shrinkage function determined regularization term(?) eqn. ), aim learn general function) piecewise linear function. output layer defined (?) piecewise linear function determined set control points )    ) pnc)    .  predefined positions uniformly located values positions filter stage. figure illustrative example. piecewise linear function approximate function, learn ﬂexible nonlinear transform function data off--shelf hard soft thresholding functions.   (?)  ??,? figure Illustration piecewise linear function determined set control points. multiplier update layer) This layer defined Lagrangian multiplier updating procedure) eqn. ). output layer stage defined) ) learnable parameters. network parameters: layers organized data ﬂow graph shown fig.  ) deep architecture, aim learn parameters: reconstruction layer) filters convolution layer multiplier update nonlinear transform layer, layer, ,    ,   indexes filters stages respectively. parameters network parameters learned. figure shows deep admm-net stages. under-sampled data-space ﬂows stages order circled number number, final reconstruction layer circled number generates reconstructed image. reconstruction result stage shown reconstruction layer. sampling data-space) ) reconstructed image)? )          figure deep admm-net stages. sampled data-space successively processed operations order, reconstruction layer) output final reconstructed image. reconstructed image stage shown reconstruction layer. network Training reconstructed image fully sampled data-space ground-truth image xgt under-sampled data-space input. training set constructed pairs under-sampled data ground-truth image. choose normalized square error \\x0c(nmse) loss function network training. pairs training data, loss network output ground truth defined:   ,  xgt (?) ) —?— ?xgt )??  , network output based network parameter under-sampled data) space. learn parameters     minimizing loss. -bfgs following, discuss initialization parameters compute gradients loss function(?) . parameters backpropagation] data ﬂow graph.  Initialization initialize network parameters admm solver baseline-mri model:  ) arg min   model, set DCT basis impose -norm regularization DCT transform space. function(?) admm algorithm (eqn. )) soft thresholding function sgn—  orwise.  stage deep) admm-net, filters convolution layers reconstruction layers initialized eqn. ). nonlinear transform layer, uniformly choose 101 positions located initialized parameters initialized values ADMM algorithm. case, initialized net realization ADMM optimizing eqn. ), refore outputs reconstructed image ADMM algorithm. optimization network parameters expected produce improved reconstruction result.  Gradient Computation Backpropagation Data Flow Graph challenging compute gradients loss. parameters backpropagation deep architecture fig. directed graph. forward pass, process data stage order) backward pass, gradients http://users.eecs.northwestern.edu/nocedal/lbfgsb.html) Multiplier update layer) non-linear transform layer) Reconstruction layer) Convolution layer Figure Illustration types graph nodes., layers network) data ﬂows stage solid arrow data ﬂow forward pass dashed arrow backward pass computing gradients backpropagation. computed inverse order. figure shows example, gradient computed backwardly layers circled number successively. stage fig. shows types nodes., network layers) data ﬂow node multiple inputs) outputs. brieﬂy introduce gradients computation layer typical stage refer supplementary material details. multiplier update layer) shown fig. ), layer sets inputs output input compute) parameters layer   gradients loss. parameters computed) ) ) ) )       ) summation gradients dashed blue arrows fig. ). compute ) gradients output layer. inputs) )  )  nonlinear transform layer) shown fig. ), layer sets inputs output input computing) stage. ) parameters layers   gradient loss. parameters computed ) ) ) compute gradients layer output inputs)  ) Convolution layer) parameters layer   ). represent) filter basis element set filter coeﬃcients learned. gradients loss. filter coeﬃcients computed) )  ) ) ) gradient layer output. input computed ) Reconstruction layer) parameters layer   ). similar) convolution layer, represent filter set filter coeﬃcients learned. gradients loss. parameters computed) ) )  )  xgt   —?— ?xgt) xgt) gradients layer output. inputs computed)  experiments train test admm-net brain chest images2 dataset, randomly 100 images training images testing. admm-net separately learned sampling ratio. reconstruction accuracies reported average NMSE Peak signalto-noise Ratio (psnr) test images. sampling pattern-space commonly pseudo radial sampling. experiments performed desktop Intel core-4790k cpu. table Performance comparisons brain data sampling ratios. % Method% Test time NMSE PSNR NMSE PSNR NMSE PSNR NMSE PSNR zero-filling] RecPF] SIDWT.1700.0929.0917.0885.1247.0673.0668.0620.0968.0534.0533.0484.0770.0440.0440.0393.0013s.7391s.3105s.8637s PBDW] PANO] FDLCP] bm3d-mri.0814.0800.0759.0674.0627.0592.0592.0515.0518.0477.0500.0426.0437.0390.0428.0359.3637s.4776s.2220s.9114s init-net13 admm-net13 admm-net14 admm-net15.1394.0752.0742.0739.1225.0553.0548.0544.1128.0456.0448.0447.1066.0395.0380.0379.6914s.6964s.7400s.7911s tab. compare method conventional compressive sensing MRI methods brain data. methods include zero-filling], RecPF], SIDWT state--art methods PBDW], PANO], FDLCP] bm3d-mri]. admm-net, initialize filters stage DCT basis average DCT basis discarded). compared baseline methods zero-filling, RecPF sidwt, proposed method produces quality comparable reconstruction speed. compared stateof--art methods pbdw, PANO fdlcp, admm-net accurate reconstruction results fastest computational speed. sampling ratio%, method (admmnet15 outperforms state--art methods PANO FDLCP. moreover, reconstruction speed times faster. bm3d-mri method relies designed BM3D denoiser, produces higher accuracy, runs times slower computational time ours. visual comparisons fig. show proposed network preserve fine image details obvious artifacts. fig. ), compare NMSEs average test time methods scatter plot. easy observe method reconstruction accuracy running time. examples learned nonlinear functions filters shown fig.  table Comparisons NMSE PSNR chest data% sampling ratio. method RecPF PANO \\x0cfdlcp admm-net15 admm-net15 admm-net17 NMSE PSNR.1019.1017.0858.0775.0790.0775.0768 Network generalization ability: test generalization ability ADMMNet applying learned net brain data chest data. table shows net learned brain data (admmnet15) achieves competitive reconstruction accuracy chest data, resulting remarkable generalization ability. due learned filters nonlinear transforms performed local patches, repetitive organs. moreover, ADMMNet17 learned chest data achieves reconstruction accuracy test chest data. effectiveness network training: tab. present results initialized network admm-net13 discussed Section, initialized network (init-net13 realization CAF project: https://masi.vuse.vanderbilt.edu/workshop2013/index.php/segmentation Challenge Details Rice Wavelet toolbox: http://dsp.rice.edu/software/rice-wavelet-toolbox nmse.0727; psnr nmse.0612; psnr nmse.0489; psnr Ground truth image nmse.0660; psnr nmse.0843; psnr nmse.0726; psnr nmse.0614; psnr Ground truth image nmse.0564; psnr NMSE Figure Examples reconstruction results% row% row) sampling ratios. left columns show results admm-net15 recpf, pano, bm3d-mri. test time seconds) Stage number) Figure) Scatter plot NMSEs average test time methods) NMSEs admm-net number stages% sampling ratio brain data). figure Examples learned filters convolution layer nonlinear transforms stage admm-net15% sampling ratio brain data). admm optimizing eqn. ). network training produces significantly improved accuracy., PNSR increased \\x0cfrom sampling ratio%. effect number stages: test effect number stages., greedily train deeper network adding stage time. fig. ) shows average testing NMSE values stages admm-net sampling ratio%. reconstruction error decreases fast marginally decreases furr increasing number stages. effect filter sizes: train admm-net initialized gradient filters size convolution reconstruction layers, trained net stages% sampling ratio achieves NMSE.0899 PSNR brain data, compared.0752 filters shown tab.  learn admm-net13 filters sized initialized DCT basis, performance significantly improved, training testing time significantly longer. conclusions proposed deep network compressive sensing mri. deep architecture defined data ﬂow graph determined ADMM algorithm. due ﬂexibility parameter learning, deep net achieved high reconstruction accuracy keeping computational eﬃciency ADMM algorithm. general framework, idea models ADMM algorithm deep network potentially applied applications future work. Magnetic Resonance Imaging (mri) non-invasive imaging technique providing functional anatomical information clinical diagnosis. imaging speed fundamental challenge. fast MRI techniques essentially demanded accelerating data acquisition reconstructing high quality image. compressive sensing MRI-mri) effective approach allowing data sampling rate lower Nyquist rate significantly degrading image quality]. -mri methods sample data-space., Fourier space), reconstruct image compressive sensing ory. regularization related data prior key component CSMRI model reduce imaging artifacts improve imaging precision. sparse regularization explored specific transform domain general dictionary-based subspace]. total Variation) regularization gradient domain widely utilized MRI]. although easy fast optimize, introduces staircase artifacts reconstructed image. methods, leverage sparse regularization wavelet domain. dictionary learning methods rely dictionary local patches improve reconstruction accuracy]. non-local method groups similar local patches joint patch-level reconstruction preserve image details]. performance, basic-mri methods run fast produce accurate reconstruction results. non-local dictionary learning-based methods generally output higher quality images, suffer slow reconstruction speed. -mri model, commonly challenging choose optimal image transform domain subspace sparse regularization. 30th Conference Neural Information Processing Systems (nips 2016), barcelona, spain. optimize-mri models, Alternating Direction Method Multipliers (admm) proven eﬃcient variable splitting algorithm convergence guarantee]. considers augmented Lagrangian function-mri model, splits variables subgroups, alternatively optimized solving simply subproblems. although ADMM generally eﬃcient, trivial determine optimal parameters., update rates, penalty parameters) inﬂuencing accuracy-mri. work, aim design fast accurate method reconstruct high-quality images under-sampled-space data. propose deep architecture, dubbed admm-net, inspired ADMM iterative procedures optimizing general-mri model. this deep architecture consists multiple stages, corresponds iteration ADMM algorithm. more specifically, define deep architecture represented data ﬂow graph] ADMM procedures. operations ADMM represented graph nodes, data ﬂow operations ADMM represented directed edge. refore, ADMM iterative procedures naturally determine deep architecture data ﬂow graph. given under-sampled data-space, ﬂows graph generates reconstructed image. all parameters., transforms, shrinkage functions, penalty parameters, etc.) deep architecture discriminatively learned training pairs undersampled data-space reconstructed image fully sampled data backpropagation] data ﬂow graph. our experiments demonstrate proposed deep admm-net effective reconstruction accuracy speed. compared baseline methods sparse regularization transform domain, achieves significantly higher accuracy takes comparable computational time. compared state--art methods dictionary learning non-local techniques, achieves high accuracy significantly faster computational speed. main contributions paper summarized follows. propose deep admm-net reformulating ADMM algorithm deep network-mri. this achieved designing data ﬂow graph ADMM effectively build train admm-net. admmnet achieves high accuracy image reconstruction fast computational speed justified experiments. discriminative parameter learning \\x0capproach applied sparse coding Markov Random Filed]. but, knowledge, computational framework maps ADMM algorithm learnable deep architecture.  Deep admm-net Fast MRI Compressive Sensing MRI Model ADMM Algorithm general-mri model: Assume MRI image reconstructed,  under-sampled-space data, ory, reconstructed image estimated solving optimization problem:   arg min   measurement matrix, undersampling matrix, Fourier transform. denotes transform matrix filtering operation., Discrete Wavelet Transform (dwt), Discrete Cosine Transform (dct), etc. (?) regularization function derived data prior., -norm  sparse prior. regularization parameter. ADMM solver] optimization problem solved eﬃciently ADMM algorithm. introducing auxiliary variables   eqn. ) equivalent: min .   ,   ].  Its augmented Lagrangian function         ,  ) Sampling data-space) Reconstructed image) stage Figure data ﬂow graph ADMM optimization general-mri model. this graph consists types nodes: reconstruction), convolution), non-linear transform), multiplier update). under-sampled data-space successively processed graph, finally generates image. our deep admm-net defined data ﬂow graph.   Lagrangian multipliers  penalty parameters. admm alternatively optimizes, solving subproblems: ) arg min          ) arg min   )    ) )  arg min )  ) ,   denotes iteration. for simplicity,  ,   ]), substitute eqn. ). subproblems solutions: )     ) eﬃciently computed fast Fourier transform(?) nonlinear shrinkage function. soft hard thresholding function sparse regularization -norm -norm]. parameter update rate. -mri, commonly run ADMM algorithm dozens iterations satisfactory reconstruction result. however, challenging choose transform shrinkage function(?) general regularization function(?). moreover, trivial tune parameters-space data sampling ratios. overcome diﬃculties, design data ﬂow graph ADMM algorithm, define deep admm-net discriminatively learn transforms, functions, parameters.  Data Flow Graph ADMM Algorithm design deep admm-net, map ADMM iterative procedures eqn. ) data ﬂow graph]. shown fig. graph comprises nodes operations admm, directed edges data ﬂows operations. case iteration ADMM algorithm corresponds stage data ﬂow graph.  stage graph, types nodes mapped types operations admm., reconstruction operation) convolution operation) defined nonlinear transform operation) defined(? multiplier update operation) eqn. ). data ﬂow graph multiple repetition stages successive iterations admm. given under-sampled data-space, ﬂows graph finally generates reconstructed image. way, map ADMM iterations data ﬂow graph, define train deep admm-net sections.  Deep admm-net Our deep admm-net defined data ﬂow graph. graph structure generalizes types operations learnable parameters network layers. operations generalized reconstruction layer, convolution layer, non-linear transform layer, multiplier update layer. discuss details. Reconstruction layer) This layer reconstructs MRI image reconstruction) operation) eqn. ). given output layer defined) ) )  filter penalty parameter,   input) under-sampled data-space. stage), initialized zeros, ) refore). convolution layer) performs convolution operation transform image transform domain. given image., reconstructed image stage output) learnable filter matrix stage Different original admm) constrain filters increase network capacity. nonlinear transform layer) This layer performs nonlinear transform inspired shrinkage function(?) defined) eqn. ). instead setting shrinkage function determined regularization term(?) eqn. ), aim learn general function) piecewise linear function. given output layer defined (?) piecewise linear function determined set control points )    ) pnc)    .  predefined positions uniformly located values positions filter stage. figure illustrative example. since piecewise linear function approximate function, learn ﬂexible nonlinear transform function data off--shelf hard soft thresholding functions.   (?)  ??,? Figure Illustration piecewise linear function determined set control points. multiplier update layer) This layer defined Lagrangian multiplier updating procedure) eqn. ). output layer stage defined) ) learnable parameters. network parameters: layers organized data ﬂow graph shown fig.  ) deep architecture, aim learn parameters: reconstruction layer) filters convolution layer multiplier update nonlinear transform layer, layer, ,    ,   indexes filters stages respectively. all parameters network parameters learned. figure shows deep admm-net stages. under-sampled data-space ﬂows stages order circled number number, final reconstruction layer circled number generates reconstructed image. immediate reconstruction result stage shown reconstruction layer. Sampling data-space) ) reconstructed image)? )          figure deep admm-net stages. sampled data-space successively processed operations order, reconstruction layer) output final reconstructed image. reconstructed image stage shown reconstruction layer. Network Training reconstructed image fully sampled data-space ground-truth image xgt under-sampled data-space input. training set constructed pairs under-sampled data ground-truth image. choose normalized square error \\x0c(nmse) loss function network training. given pairs training data, loss network output ground truth defined:   ,  xgt (?) ) —?— ?xgt )??  , network output based network parameter under-sampled data) space. learn parameters     minimizing loss. -bfgs following, discuss initialization parameters compute gradients loss function(?) . parameters backpropagation] data ﬂow graph.  Initialization initialize network parameters ADMM solver baseline-mri model:  ) arg min   model, set DCT basis impose -norm regularization DCT transform space. function(?) ADMM algorithm (eqn. )) soft thresholding function sgn—  orwise. for stage deep) admm-net, filters convolution layers reconstruction layers initialized eqn. ). nonlinear transform layer, uniformly choose 101 positions located initialized parameters initialized values ADMM algorithm. case, initialized net realization ADMM optimizing eqn. ), refore outputs reconstructed image ADMM algorithm. optimization network parameters expected produce improved reconstruction result.  Gradient Computation Backpropagation Data Flow Graph challenging compute gradients loss. parameters backpropagation deep architecture fig. directed graph. forward pass, process data stage order) backward pass, gradients http://users.eecs.northwestern.edu/nocedal/lbfgsb.html) Multiplier update layer) non-linear transform layer) Reconstruction layer) Convolution layer Figure Illustration types graph nodes., layers network) data ﬂows stage solid arrow data ﬂow forward pass dashed arrow backward pass computing gradients backpropagation. computed inverse order. figure shows example, gradient computed backwardly layers circled number successively. for stage fig. shows types nodes., network layers) data ﬂow each node multiple inputs) outputs. brieﬂy introduce gradients computation layer typical stage please refer supplementary material details. multiplier update layer) shown fig. ), layer sets inputs its output input compute) parameters layer   gradients loss. parameters computed) ) ) ) )       ) summation gradients dashed blue arrows fig. ). compute ) gradients output layer. inputs) )  )  nonlinear transform layer) shown fig. ), layer sets inputs output input computing) stage. ) parameters layers   gradient loss. parameters computed ) ) ) compute gradients layer output inputs)  ) Convolution layer) parameters layer   ). represent) filter basis element set filter coeﬃcients learned. gradients loss. filter coeﬃcients computed) )  ) ) ) gradient layer output. input computed ) Reconstruction layer) parameters layer   ). similar) convolution layer, represent filter set filter coeﬃcients learned. gradients loss. parameters computed) ) )  )  xgt   —?— ?xgt) xgt) gradients layer output. inputs computed)  Experiments train test admm-net brain chest images2 for dataset, randomly 100 images training images testing. admm-net separately learned sampling ratio. reconstruction accuracies reported average NMSE Peak signalto-noise Ratio (psnr) test images. sampling pattern-space commonly pseudo radial sampling. all experiments performed desktop Intel core-4790k cpu. table Performance comparisons brain data sampling ratios. % Method% Test time NMSE PSNR NMSE PSNR NMSE PSNR NMSE PSNR zero-filling] RecPF] SIDWT.1700.0929.0917.0885.1247.0673.0668.0620.0968.0534.0533.0484.0770.0440.0440.0393.0013s.7391s.3105s.8637s PBDW] PANO] FDLCP] bm3d-mri.0814.0800.0759.0674.0627.0592.0592.0515.0518.0477.0500.0426.0437.0390.0428.0359.3637s.4776s.2220s.9114s init-net13 admm-net13 admm-net14 admm-net15.1394.0752.0742.0739.1225.0553.0548.0544.1128.0456.0448.0447.1066.0395.0380.0379.6914s.6964s.7400s.7911s tab. compare method conventional compressive sensing MRI methods brain data. methods include zero-filling], RecPF], SIDWT state--art methods PBDW], PANO], FDLCP] bm3d-mri]. for admm-net, initialize filters stage DCT basis average DCT basis discarded). compared baseline methods zero-filling, RecPF sidwt, proposed method produces quality comparable reconstruction speed. compared stateof--art methods pbdw, PANO fdlcp, admm-net accurate reconstruction results fastest computational speed. for sampling ratio%, method (admmnet15 outperforms state--art methods PANO FDLCP. moreover, reconstruction speed times faster. bm3d-mri method relies designed BM3D denoiser, produces higher accuracy, runs times slower computational time ours. visual comparisons fig. show proposed network preserve fine image details obvious artifacts. fig. ), compare NMSEs average test time methods scatter plot. easy observe method reconstruction accuracy running time. examples learned nonlinear functions filters shown fig.  table Comparisons NMSE PSNR chest data% sampling ratio. method RecPF PANO \\x0cfdlcp admm-net15 admm-net15 admm-net17 NMSE PSNR.1019.1017.0858.0775.0790.0775.0768 Network generalization ability: test generalization ability ADMMNet applying learned net brain data chest data. table shows net learned brain data (admmnet15) achieves competitive reconstruction accuracy chest data, resulting remarkable generalization ability. this due learned filters nonlinear transforms performed local patches, repetitive organs. moreover, ADMMNet17 learned chest data achieves reconstruction accuracy test chest data. effectiveness network training: tab. present results initialized network admm-net13 discussed Section, initialized network (init-net13 realization CAF project: https://masi.vuse.vanderbilt.edu/workshop2013/index.php/segmentation Challenge Details Rice Wavelet toolbox: http://dsp.rice.edu/software/rice-wavelet-toolbox nmse.0727; psnr nmse.0612; psnr nmse.0489; psnr Ground truth image nmse.0660; psnr nmse.0843; psnr nmse.0726; psnr nmse.0614; psnr Ground truth image nmse.0564; psnr NMSE Figure Examples reconstruction results% row% row) sampling ratios. left columns show results admm-net15 recpf, pano, bm3d-mri. Test time seconds) Stage number) Figure) Scatter plot NMSEs average test time methods) NMSEs admm-net number stages% sampling ratio brain data). figure Examples learned filters convolution layer nonlinear transforms stage admm-net15% sampling ratio brain data). ADMM optimizing eqn. ). network training produces significantly improved accuracy., PNSR increased \\x0cfrom sampling ratio%. effect number stages: test effect number stages., greedily train deeper network adding stage time. fig. ) shows average testing NMSE values stages admm-net sampling ratio%. reconstruction error decreases fast marginally decreases furr increasing number stages. effect filter sizes: train admm-net initialized gradient filters size convolution reconstruction layers, trained net stages% sampling ratio achieves NMSE.0899 PSNR brain data, compared.0752 filters shown tab.  learn admm-net13 filters sized initialized DCT basis, performance significantly improved, training testing time significantly longer. Conclusions proposed deep network compressive sensing mri. deep architecture defined data ﬂow graph determined ADMM algorithm. due ﬂexibility parameter learning, deep net achieved high reconstruction accuracy keeping computational eﬃciency ADMM algorithm. general framework, idea models ADMM algorithm deep network potentially applied applications future work.',\n",
       " 'PP6487': 'correspondence estimation workhorse drives fundamental problems computer vision, reconstruction, image retrieval object recognition. applications structure motion panorama stitching demand sub-pixel accuracy rely sparse keypoint matches descriptors SIFT]. cases, dense correspondences form stereo disparities, optical ﬂow dense trajectories applications surface reconstruction, tracking, video analysis stabilization. scenarios, correspondences sought projections point images, semantic analogs instances category, beaks birds headlights cars. thus, general form, notion visual correspondence estimation spans range low-level feature matching high-level object scene understanding. traditionally, correspondence estimation relies hand-designed features domain-specific priors. recent years, increasing interest leveraging power convolutional neural networks (cnns) estimate visual correspondences. example, Siamese network pair image patches generate similiarity output]. intermediate convolution layer activations CNNs usable generic features. however, intermediate activations optimized visual correspondence task. features trained surrogate objective function (patch similarity) necessarily form metric space visual correspondence thus, metric operation distance 30th Conference Neural Information Processing Systems (nips 2016), barcelona, spain. figure Various types correspondence problems traditionally required specialized methods: example, SIFT SURF sparse structure motion, DAISY DSP dense matching, SIFT Flow FlowWeb semantic matching. universal Correspondence Network accurately eﬃciently learns metric space geometric correspondences, dense trajectories semantic correspondences. explicit interpretation. addition, patch similarity inherently ineﬃcient, features extracted overlapping regions patches. furr, requires feed-forward passes compare patches patches image. contrast, present Universal Correspondence Network (ucn), cnn-based generic discriminative framework learns geometric semantic visual correspondences. unlike previous CNNs patch similarity, deep metric learning directly learn mapping, feature, preserves similarity (eir geometric semantic) generic correspondences. mapping, thus, invariant projective transformations, intra-class shape appearance variations, variations irrelevant considered similarity. propose correspondence contrastive loss faster training eﬃciently sharing computations effectively encoding neighborhood relations feature space. test time, correspondence reduces nearest neighbor search feature space, eﬃcient evaluating pairwise patch similarities. ucn fully convolutional, allowing eﬃcient generation dense features. propose active hard-negative mining strategy faster training. addition, propose adaptation spatial transformer], called convolutional spatial transformer, desgined make features invariant families transformations. learning optimal feature space compensates aﬃne transformations, convolutional spatial transformer imparts ability mimic patch normalization descriptors sift. figure illustrates framework. capabilities UCN compared important prior approaches Table empirically, correspondences obtained UCN denser accurate prior approaches specialized task. demonstrate \\x0cthis experimentally showing state--art performances sparse SFM kitti, dense geometric semantic correspondences rigid non-rigid bodies kitti, PASCAL CUB datasets. summarize, propose end-end system optimizes general correspondence objective, independent domain, main contributions: deep metric learning eﬃcient correspondence constrastive loss learning feature representation optimized correspondence task.  fully convolutional network dense eﬃcient feature extraction, fast active hard negative mining.  fully convolutional spatial transformer patch normalization.  state--art correspondences sparse sfm, dense matching semantic matching, encompassing rigid bodies, non-rigid bodies intra-class shape appearance variations. related Works Correspondences Visual features form basic building blocks computer vision applications. carefully designed features kernel methods inﬂuenced fields structure Figure System overview: network fully convolutional, consisting series convolutions, pooling, nonlinearities convolutional spatial transformer, channel-wise normalization correspondence contrastive loss. inputs, network takes pair images coordinates points images (blue: positive, red: negative). features correspond positive points (from images) trained closer, features correspond negative points trained margin apart. normalization fcnn, convolutional spatial transformer normalize patches larger context account. features SIFT] DAISY] Conv4] DeepMatching] patch-cnn] LIFT] Ours Dense Geometric corr. semantic corr. trainable Eﬃcient Metric Space Table Comparison prior state--art methods UCN (ours). ucn generates dense accurate correspondences eir geometric semantic correspondence tasks. ucn directly learns feature space achieve high accuracy distinct eﬃciency advantages, discussed Section motion, object recognition image classification. handdesigned features, sift, hog, SURF DAISY found widespread applications]. recently, cnn-based similarity measures proposed. siamese network] measure patch similarity. driving dataset train CNN patch similarity] Siamese network measuring patch similarity stereo matching. cnn pretrained ImageNet analyzed visual semantic correspon \\x0cdence]. correspondences learned] appearance global shape deformation exploiting relationships fine-grained datasets. contrast, learn metric space metric operations direct interpretations, rar optimizing network patch similarity intermediate features. this, implement fully convolutional architecture correspondence contrastive loss faster training testing propose convolutional spatial transformer local patch normalization. metric learning neural networks Neural networks] learning mapping Euclidean distance space preserves semantic distance. loss function learning similarity metric Siamese networks subsequently formalized]. recently, triplet loss] fine-grained image ranking, triplet loss face recognition clustering]. mini-batches eﬃciently training network]. CNN invariances spatial transformations CNN invariant types transformations translation scale due convolution pooling layers. however, explicitly handling invariances forms data augmentation explicit network structure yields higher accuracy tasks]. recently, spatial transformer network proposed] learn zoom, rotate, apply arbitrary transformations object interest. Fully convolutional neural network Fully connected layers converted convolutional filters] propose fully convolutional framework segmentation. changing regular CNN fully convolutional network detection leads speed accuracy gains]. similar works, gain eﬃciency fully convolutional architecture reusing activations Methods Siamese Network Triplet Loss Contrastive Loss corres. contrast. loss examples image pair feed forwards test 103 Table Comparisons metric learning methods visual correspon dence. feature learning Figure Correspondence contrastive loss takes faster test times. correspondence contrastive loss alinputs: dense features extracted images lows correspondences pair correspondence table positive negative pairs. images methods. overlapping regions. furr, number training instances larger number images batch, variance gradient reduced, leading faster training convergence. universal Correspondence Network present details framework. recall Universal Correspondence Network trained directly learn mapping preserves similarity relying surrogate features. discuss fully convolutional nature architecture, correspondence contrastive loss \\x0cfor faster training testing, active hard negative mining, convolutional spatial transformer enables patch normalization. fully Convolutional Feature Learning speed training resources eﬃciently, implement fully convolutional feature learning, benefits. first, network reuse activations computed overlapping regions. second, train thousand correspondences image pair, network accurate gradient faster learning. third, hard negative mining eﬃcient straightforward, discussed subsequently. fourth, unlike patch-based methods, extract dense features eﬃciently images arbitrary sizes. testing, fully convolutional network faster well. patch similarity based networks] require feed forward passes, number keypoints image, compared) network. note extracting intermediate layer activations surrogate mapping comparatively suboptimal choice activations directly trained visual correspondence task. correspondence Contrastive Loss Learning metric space visual correspondence requires encoding points views) mapped neighboring points feature space. encode constraints, propose generalization contrastive loss], called correspondence contrastive loss. ) denote feature image location). loss function takes features images coordinates (see Figure). coordinates correspond point, pair positive pair encouraged close feature space, orwise negative pair encouraged margin apart. denote positive pair negative pair. full correspondence contrastive loss kfi  max, kfi) ) For image pair, sample correspondences training set. instance, KITTI dataset, laser scan point, train 100k points single image pair. practice, correspondences limit memory consumption. accurate gradient computations traditional contrastive loss, yields image pair. note number feed forward passes test time) compared Siamese network variants]. table summarizes advantages fully convolutional architecture correspondence contrastive loss. hard Negative Mining correspondence contrastive loss. ) consists terms. term minimizes distance positive pairs term pushes negative pairs margin. thus, term active distance features (x0i smaller margin Such boundary defines) SIFT) Spatial transformer) Convolutional spatial transformer \\x0cfigure) SIFT normalizes rotation scaling. ) spatial transformer takes image input estimate transformation. ) Our convolutional spatial transformer applies independent transformation features. metric space, crucial find negatives violate constraint train network push negatives away. however, random negative pairs contribute training generally embedding space. instead, actively mine negative pairs violate constraints dramatically speed training. extract features image find nearest neighbor image. location ground truth correspondence location, pair negative. compute nearest neighbor ground truth points image. mining process time consuming requires) comparisons feature points images, respectively. experiments thousand points features image, large 22000. gpu implementation speed search] embed Caffe layer actively mine hard negatives. convolutional Spatial Transformer CNNs handle degree scale rotation invariances. however, handling spatial transformations explicitly data-augmentation special network structure shown successful tasks]. visual correspondence, finding scale rotation crucial, traditionally achieved patch normalization]. series simple convolutions poolings mimic complex spatial transformations. mimic patch normalization, borrow idea spatial transformer layer]. however, global image transformation, keypoint image undergo independent transformation. thus, propose convolutional version generate transformed activations, called convolutional spatial transformer. demonstrated experiments, important correspondences large intra-class shape variations. proposed transformer takes input lower layer output feature, applies independent spatial transformation. transformation parameters extracted convolutionally. independent transformation, transformed activations inside larger activation overlap successive convolution stride combine transformed activations independently. stride size equal size spatial transformer kernel size. figure illustrates convolutional spatial transformer module. experiments Caffe] package implementation. support layers propose, implement correspondence contrastive loss layer convolutional spatial transformer layer layer based] channel-wise normalization layer. ﬂattening layer fully connected layer make network fully convolutional, generating features fourth pixel. accurate localization, extract \\x0cfeatures densely bilinear interpolation mitigate quantization error sparse correspondences. refer supplementary materials network implementation details visualization. experiment setup, train test variations networks. first, network hard negative mining spatial transformer (ours). second, network spatial transformer (ours). third, network spatial transformer hard negative mining, providing random negative samples pixels ground method sift] hog] sift-ﬂow] DaisyFF] DSP] ours ours mpi-sintel KITTI Table Matching performance pck@10px KITTI Flow 2015] mpi-sintel]. note daisyff, dsp, global optimization raw correspondences nearest neighbor matches. ) PCK performance dense features) PCK performance keypoints Figure Comparison PCK performance KITTI raw dataset) PCK performance densely extracted feature nearest neighbor) PCK performance keypoint features nearest neighbor dense CNN feature nearest neighbor) Original image pair keypoints) SIFT] matches) DAISY] matches) ours matches Figure Visualization nearest neighbor) matches KITTI images) top bottom, images FAST keypoints dense keypoints image) SIFT matches image. ) dense DAISY matches image. ) dense UCN matches image. truth correspondence location (ours). configuration networks, verify effectiveness component Universal Correspondence network. datasets Metrics evaluate UCN tasks: geometric correspondence, semantic correspondence accuracy correspondences camera localization. geometric correspondence (matching images point views), optical ﬂow datasets KITTI 2015 Flow benchmark MPI Sintel dataset split training set training validation set individually. exact splits project website. alidation For semantic correspondences (finding functional part instances), pascal-berkeley dataset keypoint annotations, subset FlowWeb]. compare prior state--art caltech-ucsd Bird dataset]. test accuracy correspondences camera motion estimation, raw KITTI driving sequences include Velodyne scans, GPS IMU measurements. velodyne points projected successive frames establish correspondences points moving objects removed. measure performance, percentage correct keypoints (pck) metric equivalently ?accuracy? ]). extract features densely set sparse keypoints (for semantic correspondence) query image find nearest neighboring feature image predicted correspondence. correspondence classified correct predicted keypoint closer pixels ground-truth short, pck unlike prior works, apply post-processing, global optimization mrf. capture performance raw correspondences ucn, surpasses previous methods. geometric Correspondence pick random 1000 correspondences KITTI MPI Sintel image training. correspondence hard negative nearest neighbor conv4 ﬂow SIFT ﬂow transfer Ours Ours Ours aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train Table per-class PCK pascal-berkeley correspondence dataset] , max)). Query Ground Truth Ours VGG conv4 Query Ground Truth Ours VGG conv4 Figure Qualitative semantic correspondence results PASCAL] correspondences Berkeley keypoint annotation] caltech-ucsd Bird dataset]. feature space pixels ground truth correspondence. architecture training scheme datasets. convention], measure PCK pixel threshold compare state--art methods Table sift-ﬂow], DaisyFF], DSP] additional global optimization generate accurate correspondences. hand, raw correspondences outperform state--art methods. note spatial transformer improve performance case, due overfitting smaller training set. show experiments, benefits apparent larger-scale dataset greater shape variations. note stereo datasets generate large number \\x0ccorrespondences, result directly comparable stereo methods global optimization epipolar geometry filter noise incorporate edges. kitti raw sequences generate large number correspondences, split sequences train test sets. details split supplementary material. plot PCK thresholds methods densely extracted features larger KITTI raw dataset Figure. accuracy features outperforms traditional features including SIFT], DAISY] KAZE]. due dense extraction original image scale rotation, SIFT perform well. , extract features sparsely SIFT keypoints plot PCK curves Figure. prior methods improve (sift dramatically), UCN features perform significantly dense extraction. note improved performance convolutional spatial transformer. pck curves geometric correspondences individual semantic classes road car supplementary material. semantic Correspondence UCN learn semantic correspondences invariant intraclass appearance shape variations. independently train PASCAL dataset] annotations] CUB dataset], network architecture. pck metric]. account variable image size, predicted keypoint correctly matched lies Euclidean distance  ground truth keypoint, size image variable control. comparison, definition varies depending baseline. intraclass correspondence alignment diﬃcult task, preceding works eir geometric] learned] spatial priors. however, raw correspondences, spatial priors, achieve stronger results previous works. shown Table approach outperforms Long. ] large margin PASCAL dataset Berkeley keypoint annotation, classes overall. note conv4 ﬂow] SIFT ﬂow fc7 ours ours ours   .025 Table Mean PCK pascal-berkeley correspondence dataset] max)). global optimization, nearest neighbor search outperforms methods large margin. figure PCK CUB dataset], compared proaches including WarpNet]  features SIFT] DAISY] SURF] KAZE] Agrawal. ] OursHN ours ang. dev. (deg.307.309.344.312.394.317.325 trans. dev. (deg.749.516.790.584.293.147.728 Table Essential matrix decomposition performance features. performance measured angular deviation ground truth rotation angle predicted translation ground truth translation. features generate accurate estimation. result purely nearest neighbor matching] global opti \\x0cmization too. train test UCN CUB dataset], cleaned test subset WarpNet]. shown Figure outperform WarpNet large margin. however, note WarpNet unsupervised method. Figure qualitative matches. results FlowWeb datasets supplementary material, similar trends. finally, observe significant performance improvement obtained convolutional spatial transformer, PASCAL CUB datasets. shows utility estimating optimal patch normalization presence large shape deformations. camera Motion Estimation KITTI raw sequences training examples task. augment data, randomly crop mirror images make effective fully convolutional structure, large images train thousands correspondences once. establish correspondences nearest neighbor matching, RANSAC estimate essential matrix decompose obtain camera motion. candidate rotations, choose inliers estimate Rpred angular deviation respect ground truth Rgt reported arccos (rpred Rgt  translation estimated scale, report angular deviation unit vectors estimated ground truth translation gps-imu. table list decomposition errors features. note sparse features SIFT designed perform setting, dense UCN features competitive. note intermediate features] learn optimize patch similarity, thus, UCN significantly outperforms trained directly correspondence task. conclusion proposed deep metric learning approach visual correspondence, shown advantageous approaches optimize surrogate patch similarity objective. propose innovations, correspondence contrastive loss fully convolutional architecture active hard negative mining convolutional spatial transformer. lend capabilities eﬃcient training, accurate gradient computations, faster testing local patch normalization, lead improved speed accuracy. demonstrate experiments features perform prior state--art geometric semantic correspondence tasks, spatial priors global optimization. future work, explore applications rigid non-rigid motion shape estimation applying global optimization applications optical ﬂow dense stereo. acknowledgments This work part choy internship NEC labs. acknowledge support Korea Foundation Advanced studies, Toyota Award #122282, ONR n00014-0761, MURI wf911nf-0479. Correspondence estimation workhorse drives fundamental problems computer vision, reconstruction, image retrieval object recognition. applications structure motion panorama stitching demand sub-pixel accuracy rely sparse keypoint matches descriptors SIFT]. cases, dense correspondences form stereo disparities, optical ﬂow dense trajectories applications surface reconstruction, tracking, video analysis stabilization. scenarios, correspondences sought projections point images, semantic analogs instances category, beaks birds headlights cars. thus, general form, notion visual correspondence estimation spans range low-level feature matching high-level object scene understanding. traditionally, correspondence estimation relies hand-designed features domain-specific priors. recent years, increasing interest leveraging power convolutional neural networks (cnns) estimate visual correspondences. for example, Siamese network pair image patches generate similiarity output]. intermediate convolution layer activations CNNs usable generic features. however, intermediate activations optimized visual correspondence task. such features trained surrogate objective function (patch similarity) necessarily form metric space visual correspondence thus, metric operation distance 30th Conference Neural Information Processing Systems (nips 2016), barcelona, spain. figure Various types correspondence problems traditionally required specialized methods: example, SIFT SURF sparse structure motion, DAISY DSP dense matching, SIFT Flow FlowWeb semantic matching. Universal Correspondence Network accurately eﬃciently learns metric space geometric correspondences, dense trajectories semantic correspondences. explicit interpretation. addition, patch similarity inherently ineﬃcient, features extracted overlapping regions patches. furr, requires feed-forward passes compare patches patches image. contrast, present Universal Correspondence Network (ucn), cnn-based generic discriminative framework learns geometric semantic visual correspondences. unlike previous CNNs patch similarity, deep metric learning directly learn mapping, feature, preserves similarity (eir geometric semantic) generic correspondences. mapping, thus, invariant projective transformations, intra-class shape appearance variations, variations irrelevant considered similarity. propose correspondence contrastive loss faster training eﬃciently sharing computations effectively encoding neighborhood relations feature space. test time, correspondence reduces nearest neighbor search feature space, eﬃcient evaluating pairwise patch similarities. UCN fully convolutional, allowing eﬃcient generation dense features. propose active hard-negative mining strategy faster training. addition, propose adaptation spatial transformer], called convolutional spatial transformer, desgined make features invariant families transformations. learning optimal feature space compensates aﬃne transformations, convolutional spatial transformer imparts ability mimic patch normalization descriptors sift. figure illustrates framework. capabilities UCN compared important prior approaches Table empirically, correspondences obtained UCN denser accurate prior approaches specialized task. demonstrate \\x0cthis experimentally showing state--art performances sparse SFM kitti, dense geometric semantic correspondences rigid non-rigid bodies kitti, PASCAL CUB datasets. summarize, propose end-end system optimizes general correspondence objective, independent domain, main contributions: deep metric learning eﬃcient correspondence constrastive loss learning feature representation optimized correspondence task.  fully convolutional network dense eﬃcient feature extraction, fast active hard negative mining.  fully convolutional spatial transformer patch normalization.  state--art correspondences sparse sfm, dense matching semantic matching, encompassing rigid bodies, non-rigid bodies intra-class shape appearance variations. Related Works Correspondences Visual features form basic building blocks computer vision applications. carefully designed features kernel methods inﬂuenced fields structure Figure System overview: network fully convolutional, consisting series convolutions, pooling, nonlinearities convolutional spatial transformer, channel-wise normalization correspondence contrastive loss. inputs, network takes pair images coordinates points images (blue: positive, red: negative). features correspond positive points (from images) trained closer, features correspond negative points trained margin apart. before normalization fcnn, convolutional spatial transformer normalize patches larger context account. features SIFT] DAISY] Conv4] DeepMatching] patch-cnn] LIFT] Ours Dense Geometric corr. Semantic corr. Trainable Eﬃcient Metric Space Table Comparison prior state--art methods UCN (ours). UCN generates dense accurate correspondences eir geometric semantic correspondence tasks. UCN directly learns feature space achieve high accuracy distinct eﬃciency advantages, discussed Section motion, object recognition image classification. several handdesigned features, sift, hog, SURF DAISY found widespread applications]. recently, cnn-based similarity measures proposed. Siamese network] measure patch similarity. driving dataset train CNN patch similarity] Siamese network measuring patch similarity stereo matching. CNN pretrained ImageNet analyzed visual semantic correspon \\x0cdence]. correspondences learned] appearance global shape deformation exploiting relationships fine-grained datasets. contrast, learn metric space metric operations direct interpretations, rar optimizing network patch similarity intermediate features. for this, implement fully convolutional architecture correspondence contrastive loss faster training testing propose convolutional spatial transformer local patch normalization. metric learning neural networks Neural networks] learning mapping Euclidean distance space preserves semantic distance. loss function learning similarity metric Siamese networks subsequently formalized]. recently, triplet loss] fine-grained image ranking, triplet loss face recognition clustering]. mini-batches eﬃciently training network]. CNN invariances spatial transformations CNN invariant types transformations translation scale due convolution pooling layers. however, explicitly handling invariances forms data augmentation explicit network structure yields higher accuracy tasks]. recently, spatial transformer network proposed] learn zoom, rotate, apply arbitrary transformations object interest. Fully convolutional neural network Fully connected layers converted convolutional filters] propose fully convolutional framework segmentation. changing regular CNN fully convolutional network detection leads speed accuracy gains]. similar works, gain eﬃciency fully convolutional architecture reusing activations Methods Siamese Network Triplet Loss Contrastive Loss corres. contrast. loss examples image pair feed forwards test 103 Table Comparisons metric learning methods visual correspon dence. feature learning Figure Correspondence contrastive loss takes faster test times. correspondence contrastive loss alinputs: dense features extracted images lows correspondences pair correspondence table positive negative pairs. images methods. overlapping regions. furr, number training instances larger number images batch, variance gradient reduced, leading faster training convergence. Universal Correspondence Network present details framework. recall Universal Correspondence Network trained directly learn mapping preserves similarity relying surrogate features. discuss fully convolutional nature architecture, correspondence contrastive loss \\x0cfor faster training testing, active hard negative mining, convolutional spatial transformer enables patch normalization. fully Convolutional Feature Learning speed training resources eﬃciently, implement fully convolutional feature learning, benefits. first, network reuse activations computed overlapping regions. second, train thousand correspondences image pair, network accurate gradient faster learning. third, hard negative mining eﬃcient straightforward, discussed subsequently. fourth, unlike patch-based methods, extract dense features eﬃciently images arbitrary sizes. during testing, fully convolutional network faster well. patch similarity based networks] require feed forward passes, number keypoints image, compared) network. note extracting intermediate layer activations surrogate mapping comparatively suboptimal choice activations directly trained visual correspondence task. correspondence Contrastive Loss Learning metric space visual correspondence requires encoding points views) mapped neighboring points feature space. encode constraints, propose generalization contrastive loss], called correspondence contrastive loss. let) denote feature image location). loss function takes features images coordinates (see Figure). coordinates correspond point, pair positive pair encouraged close feature space, orwise negative pair encouraged margin apart. denote positive pair negative pair. full correspondence contrastive loss kfi  max, kfi) ) For image pair, sample correspondences training set. for instance, KITTI dataset, laser scan point, train 100k points single image pair. however practice, correspondences limit memory consumption. this accurate gradient computations traditional contrastive loss, yields image pair. note number feed forward passes test time) compared Siamese network variants]. table summarizes advantages fully convolutional architecture correspondence contrastive loss. hard Negative Mining correspondence contrastive loss. ) consists terms. term minimizes distance positive pairs term pushes negative pairs margin. thus, term active distance features (x0i smaller margin Such boundary defines) SIFT) Spatial transformer) Convolutional spatial transformer \\x0cfigure) SIFT normalizes rotation scaling. ) spatial transformer takes image input estimate transformation. ) Our convolutional spatial transformer applies independent transformation features. metric space, crucial find negatives violate constraint train network push negatives away. however, random negative pairs contribute training generally embedding space. instead, actively mine negative pairs violate constraints dramatically speed training. extract features image find nearest neighbor image. location ground truth correspondence location, pair negative. compute nearest neighbor ground truth points image. such mining process time consuming requires) comparisons feature points images, respectively. our experiments thousand points features image, large 22000. GPU implementation speed search] embed Caffe layer actively mine hard negatives. convolutional Spatial Transformer CNNs handle degree scale rotation invariances. however, handling spatial transformations explicitly data-augmentation special network structure shown successful tasks]. for visual correspondence, finding scale rotation crucial, traditionally achieved patch normalization]. series simple convolutions poolings mimic complex spatial transformations. mimic patch normalization, borrow idea spatial transformer layer]. however, global image transformation, keypoint image undergo independent transformation. thus, propose convolutional version generate transformed activations, called convolutional spatial transformer. demonstrated experiments, important correspondences large intra-class shape variations. proposed transformer takes input lower layer output feature, applies independent spatial transformation. transformation parameters extracted convolutionally. since independent transformation, transformed activations inside larger activation overlap successive convolution stride combine transformed activations independently. stride size equal size spatial transformer kernel size. figure illustrates convolutional spatial transformer module. Experiments Caffe] package implementation. since support layers propose, implement correspondence contrastive loss layer convolutional spatial transformer layer layer based] channel-wise normalization layer. ﬂattening layer fully connected layer make network fully convolutional, generating features fourth pixel. for accurate localization, extract \\x0cfeatures densely bilinear interpolation mitigate quantization error sparse correspondences. please refer supplementary materials network implementation details visualization. for experiment setup, train test variations networks. first, network hard negative mining spatial transformer (ours). second, network spatial transformer (ours). third, network spatial transformer hard negative mining, providing random negative samples pixels ground method sift] hog] sift-ﬂow] DaisyFF] DSP] ours ours mpi-sintel KITTI Table Matching performance pck@10px KITTI Flow 2015] mpi-sintel]. note daisyff, dsp, global optimization raw correspondences nearest neighbor matches. ) PCK performance dense features) PCK performance keypoints Figure Comparison PCK performance KITTI raw dataset) PCK performance densely extracted feature nearest neighbor) PCK performance keypoint features nearest neighbor dense CNN feature nearest neighbor) Original image pair keypoints) SIFT] matches) DAISY] matches) ours matches Figure Visualization nearest neighbor) matches KITTI images) top bottom, images FAST keypoints dense keypoints image) SIFT matches image. ) dense DAISY matches image. ) dense UCN matches image. truth correspondence location (ours). with configuration networks, verify effectiveness component Universal Correspondence network. datasets Metrics evaluate UCN tasks: geometric correspondence, semantic correspondence accuracy correspondences camera localization. for geometric correspondence (matching images point views), optical ﬂow datasets KITTI 2015 Flow benchmark MPI Sintel dataset split training set training validation set individually. exact splits project website. alidation For semantic correspondences (finding functional part instances), pascal-berkeley dataset keypoint annotations, subset FlowWeb]. compare prior state--art caltech-ucsd Bird dataset]. test accuracy correspondences camera motion estimation, raw KITTI driving sequences include Velodyne scans, GPS IMU measurements. velodyne points projected successive frames establish correspondences points moving objects removed. measure performance, percentage correct keypoints (pck) metric equivalently ?accuracy? ]). extract features densely set sparse keypoints (for semantic correspondence) query image find nearest neighboring feature image predicted correspondence. correspondence classified correct predicted keypoint closer pixels ground-truth short, pck unlike prior works, apply post-processing, global optimization mrf. this capture performance raw correspondences ucn, surpasses previous methods. geometric Correspondence pick random 1000 correspondences KITTI MPI Sintel image training. correspondence hard negative nearest neighbor conv4 ﬂow SIFT ﬂow transfer Ours Ours Ours aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train Table per-class PCK pascal-berkeley correspondence dataset] , max)). Query Ground Truth Ours VGG conv4 Query Ground Truth Ours VGG conv4 Figure Qualitative semantic correspondence results PASCAL] correspondences Berkeley keypoint annotation] caltech-ucsd Bird dataset]. feature space pixels ground truth correspondence. architecture training scheme datasets. following convention], measure PCK pixel threshold compare state--art methods Table sift-ﬂow], DaisyFF], DSP] additional global optimization generate accurate correspondences. hand, raw correspondences outperform state--art methods. note spatial transformer improve performance case, due overfitting smaller training set. show experiments, benefits apparent larger-scale dataset greater shape variations. note stereo datasets generate large number \\x0ccorrespondences, result directly comparable stereo methods global optimization epipolar geometry filter noise incorporate edges. KITTI raw sequences generate large number correspondences, split sequences train test sets. details split supplementary material. plot PCK thresholds methods densely extracted features larger KITTI raw dataset Figure. accuracy features outperforms traditional features including SIFT], DAISY] KAZE]. due dense extraction original image scale rotation, SIFT perform well. , extract features sparsely SIFT keypoints plot PCK curves Figure. all prior methods improve (sift dramatically), UCN features perform significantly dense extraction. also note improved performance convolutional spatial transformer. pck curves geometric correspondences individual semantic classes road car supplementary material. semantic Correspondence UCN learn semantic correspondences invariant intraclass appearance shape variations. independently train PASCAL dataset] annotations] CUB dataset], network architecture. PCK metric]. account variable image size, predicted keypoint correctly matched lies Euclidean distance  ground truth keypoint, size image variable control. for comparison, definition varies depending baseline. since intraclass correspondence alignment diﬃcult task, preceding works eir geometric] learned] spatial priors. however, raw correspondences, spatial priors, achieve stronger results previous works. shown Table approach outperforms Long. ] large margin PASCAL dataset Berkeley keypoint annotation, classes overall. note conv4 ﬂow] SIFT ﬂow fc7 ours ours ours   .025 Table Mean PCK pascal-berkeley correspondence dataset] max)). even global optimization, nearest neighbor search outperforms methods large margin. figure PCK CUB dataset], compared proaches including WarpNet]  features SIFT] DAISY] SURF] KAZE] Agrawal. ] OursHN ours ang. dev. (deg.307.309.344.312.394.317.325 trans. dev. (deg.749.516.790.584.293.147.728 Table Essential matrix decomposition performance features. performance measured angular deviation ground truth rotation angle predicted translation ground truth translation. all features generate accurate estimation. result purely nearest neighbor matching] global opti \\x0cmization too. train test UCN CUB dataset], cleaned test subset WarpNet]. shown Figure outperform WarpNet large margin. however, note WarpNet unsupervised method. please Figure qualitative matches. results FlowWeb datasets supplementary material, similar trends. finally, observe significant performance improvement obtained convolutional spatial transformer, PASCAL CUB datasets. this shows utility estimating optimal patch normalization presence large shape deformations. camera Motion Estimation KITTI raw sequences training examples task. augment data, randomly crop mirror images make effective fully convolutional structure, large images train thousands correspondences once. establish correspondences nearest neighbor matching, RANSAC estimate essential matrix decompose obtain camera motion. among candidate rotations, choose inliers estimate Rpred angular deviation respect ground truth Rgt reported arccos (rpred Rgt  since translation estimated scale, report angular deviation unit vectors estimated ground truth translation gps-imu. Table list decomposition errors features. note sparse features SIFT designed perform setting, dense UCN features competitive. note intermediate features] learn optimize patch similarity, thus, UCN significantly outperforms trained directly correspondence task. Conclusion proposed deep metric learning approach visual correspondence, shown advantageous approaches optimize surrogate patch similarity objective. propose innovations, correspondence contrastive loss fully convolutional architecture active hard negative mining convolutional spatial transformer. lend capabilities eﬃcient training, accurate gradient computations, faster testing local patch normalization, lead improved speed accuracy. demonstrate experiments features perform prior state--art geometric semantic correspondence tasks, spatial priors global optimization. future work, explore applications rigid non-rigid motion shape estimation applying global optimization applications optical ﬂow dense stereo. acknowledgments This work part choy internship NEC labs. acknowledge support Korea Foundation Advanced studies, Toyota Award #122282, ONR n00014-0761, MURI wf911nf-0479.',\n",
       " 'PP6522': 'problem eﬃciently estimating coeﬃcients generalized linear models (glms) number observations larger dimension coeﬃcient vector). glms play crucial role numerous machine learning statistics problems, provide miscellaneous framework regression classification tasks. celebrated examples include ordinary squares, logistic regression, multinomial regression applications involving graphical models [mn89, wj08, kf09]. standard approach estimating regression coeﬃcients GLM maximum likelihood method. standard assumptions link function, maximum likelihood estimator (mle) written solution convex minimization problem [mn89]. due non-linear structure MLE problem, resulting optimization task requires iterative methods. commonly optimization technique computing MLE newton-raphson method, viewed reweighted squares algorithm [mn89]. method order approximation benefit curvature log-likelihood achieves locally quadratic convergence. drawback approach excessive per-iteration cost(np2 remedy this, hessian-free Krylov sub-space based methods conjugate gradient minimal residual used, resulting direction imprecise [hs52, ps75, mar10]. hand, order work conducted Rutgers University 30th Conference Neural Information Processing Systems (nips 2016), barcelona, spain. approximation yields gradient descent algorithm, attains linear convergence rate) per-iteration cost. convergence rate slow compared order methods, modest per-iteration cost makes practical large-scale problems. regime anor popular optimization technique class quasi-newton methods [bis95, nes04], attain per-iteration cost), convergence rate locally super-linear; well-known member class methods BFGS algorithm [nes04]. recent studies exploit special structure GLMs [erd15], achieve near-quadratic convergence per-iteration cost), additional cost covariance estimation. paper, alternative approach fitting glms, based identity wellknown areas statistics, appears received attention computational implications large scale problems. glm denote GLM regression coeﬃcients, ols denote ordinary squares (ols) coeﬃcients (this notation defined precisely Section). random predictor (design) models, glm ols ) For logistic regression Gaussian design (which equivalent fisher discriminant analysis) noted Fisher 1930s [fis36]; general formulation models Gaussian design [bri82]. relationship) suggests constant proportionality known, glm estimated computing OLS estimator, substantially simpler finding MLE original glm. work paper builds idea. contributions summarized follows.  show glm approximately proportional ols random design glms, predictor distribution. , prove glm ols  design computationally eﬃcient estimator glm estimating OLS coeﬃcients, estimating proportionality constant refer resulting estimator Scaled Least Squares (sls) estimator denote sls estimating OLS coeﬃcients, step algorithm involves finding root real valued function; \\x0cthis accomplished iterative methods cubic convergence rate) per-iteration cost. cheaper classical batch methods mentioned factor).  random design GLMs sub-gaussian predictors, show glm sls  max {log), This bound characterizes performance proposed estimator terms data dimensions, justifies algorithm regime  study statistical computational performance sls compare MLE (using well-known implementations), variety large-scale datasets. rest paper organized follows: Section surveys related work Section introduces required background notation. section provide intuition relationship), based exact calculations GLMs Gaussian design. section propose algorithm discuss computational properties. section comparison proposed algorithm existing methods. oretical results found Section finally, conclude discussion Section  Related work mentioned Section relationship) well-known forms statistics. brillinger [bri82] derived) models Gaussian predictors. duan [ld89] studied model misspecification problems statistics derived) predictor distribution linear conditional means (this slight generalization Gaussian predictors). recently, stein lemma [bem13] relationship) revisited context compressed sensing [pv15, tah15], shown standard lasso estimator effective models relationship expected response signal nonlinear, predictors. design sensing matrix) gaussian. common previous work focuses solely settings) holds predictors Gaussian, [ld89], gaussian). key novelties present paper) focus computational benefits) large scale problems) rigorous analysis models non-gaussian predictors) shown approximately valid. preliminaries notation assume random design setting, observed data consists random iid pairs   response variable (xi1 xip vector predictors covariates. focus problems fitting GLM desirable, assume drawn statistical model. model misspecification). mle GLMs canonical link defined  ?mle argmax hxi 2rp (hxi). ) \\x0cwhere?, denotes Euclidean inner-product suﬃciently smooth convex function. glm coeﬃcients glm defined taking population average): glm argmax hxi 2rp (hxi)] ) While make assumptions smoothness, note cumulant generating function recover standard GLM canonical link regression parameters glm [mn89]. examples GLMs form include logistic regression) log Poisson regression) linear regression (least squares. objective find computationally eﬃcient estimator glm alternative estimator glm proposed paper related OLS coefficient vector, defined ols xti OLS estimator ?ols    design matrix    additionally, text}, positive integers denote size set—.  derivative function denoted) vector matrix kukq kukq denote -vector -operator norms, respectively.  ], denote— matrix obtained extracting rows indexed For symmetric matrix max) min) denote maximum minimum eigenvalues, respectively. ) denotes condition number respect-norm. denote-variate normal distribution. ols equivalent GLM scalar factor motivate methodology, assume section covariates multivariate normal, [bri82]. distributional assumptions relaxed Section proposition.? assume covariates multivariate normal covariance matrix xti.  , ?). glm written glm ols  satisfies equation, ols proof Proposition optimal point optimization problem), satisfy normal equations) (hxi ) now, denote multivariate normal density covariance matrix recall well-known property Gaussian density  ?). Algorithm sls: Scaled Least Squares Estimator input: Data Step compute squares estimator: ?ols ?ols sub-sampling based OLS estimator, ] random subset ?ols Step solve equation newton root-finding method: Initialize/var Repeat convergence  output: sls ?ols integration parts hand side equation, obtain) (hxi,  ) (hxi) (this basically stein lemma). combining identity), conclude proof. proposition proof provide main intuition proposed method. observe derivation, worked hand side normal equations) depend response variable equivalence holds joint distribution [bri82], assumed follow single index model. section extend method non-gaussian predictors) generalized zero-bias transformations.  Regularization version Proposition incorporating regularization important tool datasets large relative predictors highly collinear possible, outlined brieﬂy section. focus -regularization (ridge regression) section; connections lasso -regularization) discussed Section Corollary define -regularized GLM coeﬃcients, glm argmax hxi glm ols (hxi)]  -regularized OLS coeﬃcients ols xti glm 0glm ols 0ols argument implies 2rp ) This suggests ordinary ridge regression linear model estimate -regularized GLM coeﬃcients glm furr pursuing ideas problems regularization critical issue interesting area future research. sls: Scaled Least Squares estimator GLMs Motivated results previous section, design computationally eﬃcient algorithm GLM task simple solving squares problem; Algorithm algorithm basic steps. first, estimate OLS coeﬃcients, step estimate proportionality constant simple root-finding algorithm. numerous fast optimization methods solve squares problem, superficial review page limits paper. emphasize step (finding OLS estimator) iterative main computational cost proposed algorithm. suggest sub-sampling based estimator ols subset observations estimate covariance matrix.  ] SLS MLE Computation Method SLS MLE Method SLS MLE   time(sec) SLS MLE Accuracy log10) log10) Figure Logistic regression general Gaussian design. left plot shows computational cost (time) finding MLE SLS grows 200. plot depicts accuracy estimators. regime MLE expensive compute, SLS found rapidly accuracy.  built functions find mle. random sub-sample denote sub-matrix formed rows sub-sampled OLS estimator ?ols— XTS properties estimator well-studied [ver10, dlfu13, em15]. sub-gaussian covariates, suﬃces subsample size log)) [ver10]. hence, step requires single time computational cost max log), approaches, refer reader [rt08, dlfu13] references rein. step Algorithm involves solving simple root-finding problem. step algorithm, numerous methods completing task. newton root-finding method quadratic convergence halley method cubic convergence choices. highlight step costs) per-iteration attain cubic rate convergence. resulting per-iteration cost cheaper commonly batch algorithms factor) indeed, cost computing gradient). simplicity, newton root-finding method initialized/var assuming GLM good approximation true conditional distribution, law total \\x0cvariance basic properties glms, Var [var Var var) (hxi ) initialization reasonable long [var smaller Var) (hxi experiments show SLS robust initialization. figure compare performance SLS estimator mle, analyze syntic data generated logistic regression model general Gaussian design randomly generated covariance matrix. left plot shows computational cost obtaining estimators increases fixed plot shows accuracy estimators. regime mle hard compute mle SLS achieve accuracy, SLS significantly smaller computation time. refer reader Section oretical results characterizing finite sample behavior sls. experiments This section results variety numerical studies, show Scaled Least Squares estimator reaches minimum achievable test error substantially faster commonly batch algorithms finding mle. logistic Poisson regression models (two types glms) utilized analyses, based syntic real datasets. below, brieﬂy describe optimization algorithms MLE experiments.  newton-raphson) achieves locally quadratic convergence scaling gradient inverse Hessian evaluated current iterate. computing Hessian per-iteration cost np2 makes impractical large-scale datasets.  newton-stein) recently proposed second-order batch algorithm specifically designed GLMs [erd16]. algorithm stein lemma sub-sampling eﬃciently estimate Hessian) per-iteration cost, achieving quadratic rates. logis0c Regression poi?reg Covariates ber( SLS BFGS LBFGS AGD SLS BFGS LBFGS AGD SLS BFGS LBFGS AGD Time (sec) Time (sec) log?reg Covariates {exp} Test Error log?reg Higgs dataset SLS BFGS LBFGS AGD Time (sec Time (sec poi?reg Covertype dataset SLS BFGS LBFGS AGD) poi?reg Covariates ber( SLS BFGS LBFGS AGD) Test Error SLS BFGS LBFGS AGD Time (sec) log(test error) log(test error) OLS start poi?reg Covertype dataset log(test error) Test Error Random start Poisson Regression log?reg Higgs dataset Test Error SLS BFGS LBFGS AGD log(test error) log?reg Covariates {exp Time (sec) Time (sec Time (sec) Figure Performance SLS compared MLE obtained optimization algorithms datasets. sls represented red straight line. details provided Table  broyden-fletcher-goldfarb-shanno (bfgs) popular stable quasi-newton method [nes04]. iteration, gradient scaled matrix formed accumulating information previous iterations gradient computations. convergence locally super-linear periteration cost).  limited memory BFGS (lbfgs) variant bfgs, recent iterates gradients approximate hessian, providing significant improvement terms memory usage. lbfgs variants; formulation [bis95].  gradient descent) takes step opposite direction gradient, evaluated current iterate. performance strongly depends condition number design matrix. assumptions, convergence linear) per-iteration cost.  accelerated gradient descent (agd) modified version gradient descent additional ?momentum? term [nes83]. iteration cost) performance strongly depends smoothness objective function. algorithms, step size iteration chosen backtracking line search [bv04]. recall proposed Algorithm composed steps; finds estimate OLS coeﬃcients. -front computation needed MLE algorithms above. hand, MLE algorithms requires initial initialization needed find OLS estimator Algorithm raises question MLE algorithms initialized, order compare fairly proposed method. scenarios experiments: first, OLS estimator computed Algorithm initialize MLE algorithms; second, random initial value. dataset, main criterion assessing performance estimators rapidly minimum test error achieved. test error measured squared error estimated current parameters iteration test dataset, randomly selected (and set-aside% portion entire dataset. noted previously, MLE accurate small (see Figure). however, regime considered), MLE SLS perform similarly terms error rates; instance, Higgs dataset, SLS MLE test error rates%, respectively. dataset, minimum achievable test error set maximum final test errors, maximum estimation methods.  ) ) randomly generated covariance matrices. datasets analyzed were) syntic dataset generated logistic regression model iid {exponential) predictors scaled ) Higgs dataset (logistic regression) [bsw14]; (iii) syntic dataset generated Poisson regression model iid binary) predictors scaled ) Covertype dataset (poisson regression) [bd99]. cases, SLS outperformed alternative algorithms finding MLE large margin, terms computation. detailed results found Figure Table provide additional experiments datasets Supplementary material. table Details experiments shown Figure odel DATASET IZE NITIALIZED LOT ETHOD FGS BFGS AGD OGISTIC REGRESSION OISSON REGRESSION } IGGS [bsw14] ) OVERTYPE [bd99 300 300?105) IME SECONDS NUMBER ITERATIONS REACH MIN TEST ERROR 301 170 130 282/216 148 660 701 125 6368/651 6946/670 224/106 357 669/138 134 100871/10101 141736/13808 1711/513 1364/374 218 2405/251 2879/277 103 102 oretical results section, zero-bias transformations [gr97] generalize equivalence OLS GLMs settings covariates non-gaussian. definition random variable variance exists random variable satisfies) )], differentiable functions distribution -zero-bias distribution. existence definition consequence Riesz representation orem [gr97]. normal distribution unique distribution zero-bias transformation. normal distribution fixed point operation mapping distribution  provide intuition usefulness zero-bias transformation, refer back proof Proposition simplicity, assume covariate vector iid entries variance zero-bias transformation applied normal equation) yields xij xij) xij xik xik  normal equation zero-bias transformation distribution xij -zero-bias distribution determined distribution xij general properties found, example, [cgs10]. spread, turns toger, right-hand side) behaves similar side), , behavior similar Gaussian case, proportionality relationship Proposition holds. argument leads approximate proportionality relationship non-gaussian predictors, which, carried rigorously, yields following.   orem suppose covariate vector covariance matrix and, fur1 rmore, random vector independent entries sub-gaussian norm bounded assume function) Lipschitz continuous constant Let assume-well-spread sense ].   ) (hxi glm  denoting condition number glm ols     ) orem proved Supplementary material. implies population parameters ols glm approximately equivalent scaling factor, error bound). assumption glm well-spread relaxed minor modifications. example, sparse coeﬃcient vector, supp( glm; jglm support set glm orem holds replaced size support set. interesting consequence orem remarks orem entry glm zero, entry ols small, conversely. define lasso coeﬃcients  lasso argmin hxi ) 2rp  corollary ?/—supp( glm xti lasso glm glm supp( supp( furr, satisfy supp( glm jglm ?/—supp( glm supp( lasso supp( glm section, discussed properties population parameters, glm remainder section, turn attention results estimators main focus paper; results ultimately build earlier results. orem order precisely describe performance sls bounds OLS estimator. ols estimator studied extensively literature; however, purposes, find convenient derive bound accuracy. exact bound elsewhere, similar orem [dlfu13].   proposition assume xti sub-gaussian norms respectively. min denoting smallest eigenvalue ols ?ols  min— probability depends proposition proved Supplementary material. main result performance sls next. orem assumptions orem Proposition hold? xk2   ) furr assume function, ols) satisfies    derivative interval?] change sign., absolute lower bounded — suﬃciently large, glm sls ) min/ log} probability constants defined?     =?? min min ols max/?    constant depending  ) Note convergence rate upper bound) depends sum terms, functions data dimensions term) orem bounds discrepancy ols glm term small large, depend number observations term upper bound) estimating ols term increasing reﬂects fact estimating glm challenging large. expected, term decreasing. larger sample sizepyields estimates. full OLS solution), term max{log), suﬃciently large. suggests order good performance. discussion paper, showed coeﬃcients GLMs OLS approximately proportional general random design setting. relation, proposed computationally eﬃcient algorithm large-scale problems achieves accuracy MLE estimating OLS coeﬃcients estimating proportionality constant iterations attain quadratic cubic convergence rate) per-iteration cost. brieﬂy mentioned proportionality coeﬃcients holds regularization Section. furr pursuing idea interesting large-scale problems regularization crucial. anor interesting line research find similar proportionality relations parameters large-scale optimization problems support vector machines. relations reduce problem complexity significantly. problem eﬃciently estimating coeﬃcients generalized linear models (glms) number observations larger dimension coeﬃcient vector). glms play crucial role numerous machine learning statistics problems, provide miscellaneous framework regression classification tasks. celebrated examples include ordinary squares, logistic regression, multinomial regression applications involving graphical models [mn89, wj08, kf09]. standard approach estimating regression coeﬃcients GLM maximum likelihood method. under standard assumptions link function, maximum likelihood estimator (mle) written solution convex minimization problem [mn89]. due non-linear structure MLE problem, resulting optimization task requires iterative methods. commonly optimization technique computing MLE newton-raphson method, viewed reweighted squares algorithm [mn89]. this method order approximation benefit curvature log-likelihood achieves locally quadratic convergence. drawback approach excessive per-iteration cost(np2 remedy this, hessian-free Krylov sub-space based methods conjugate gradient minimal residual used, resulting direction imprecise [hs52, ps75, mar10]. hand, order work conducted Rutgers University 30th Conference Neural Information Processing Systems (nips 2016), barcelona, spain. approximation yields gradient descent algorithm, attains linear convergence rate) per-iteration cost. although convergence rate slow compared order methods, modest per-iteration cost makes practical large-scale problems. regime anor popular optimization technique class quasi-newton methods [bis95, nes04], attain per-iteration cost), convergence rate locally super-linear; well-known member class methods BFGS algorithm [nes04]. recent studies exploit special structure GLMs [erd15], achieve near-quadratic convergence per-iteration cost), additional cost covariance estimation. paper, alternative approach fitting glms, based identity wellknown areas statistics, appears received attention computational implications large scale problems. let glm denote GLM regression coeﬃcients, ols denote ordinary squares (ols) coeﬃcients (this notation defined precisely Section). random predictor (design) models, glm ols ) For logistic regression Gaussian design (which equivalent fisher discriminant analysis) noted Fisher 1930s [fis36]; general formulation models Gaussian design [bri82]. relationship) suggests constant proportionality known, glm estimated computing OLS estimator, substantially simpler finding MLE original glm. our work paper builds idea. our contributions summarized follows.  show glm approximately proportional ols random design glms, predictor distribution. that, prove glm ols  design computationally eﬃcient estimator glm estimating OLS coeﬃcients, estimating proportionality constant refer resulting estimator Scaled Least Squares (sls) estimator denote sls after estimating OLS coeﬃcients, step algorithm involves finding root real valued function; \\x0cthis accomplished iterative methods cubic convergence rate) per-iteration cost. this cheaper classical batch methods mentioned factor).  for random design GLMs sub-gaussian predictors, show glm sls  max {log), This bound characterizes performance proposed estimator terms data dimensions, justifies algorithm regime  study statistical computational performance sls compare MLE (using well-known implementations), variety large-scale datasets. rest paper organized follows: Section surveys related work Section introduces required background notation. Section provide intuition relationship), based exact calculations GLMs Gaussian design. Section propose algorithm discuss computational properties. section comparison proposed algorithm existing methods. oretical results found Section finally, conclude discussion Section  Related work mentioned Section relationship) well-known forms statistics. brillinger [bri82] derived) models Gaussian predictors. Duan [ld89] studied model misspecification problems statistics derived) predictor distribution linear conditional means (this slight generalization Gaussian predictors). more recently, stein lemma [bem13] relationship) revisited context compressed sensing [pv15, tah15], shown standard lasso estimator effective models relationship expected response signal nonlinear, predictors. design sensing matrix) gaussian. common previous work focuses solely settings) holds predictors Gaussian, [ld89], gaussian). two key novelties present paper) focus computational benefits) large scale problems) rigorous analysis models non-gaussian predictors) shown approximately valid. Preliminaries notation assume random design setting, observed data consists random iid pairs   response variable (xi1 xip vector predictors covariates. focus problems fitting GLM desirable, assume drawn statistical model. model misspecification). MLE GLMs canonical link defined  ?mle argmax hxi 2rp (hxi). ) \\x0cwhere?, denotes Euclidean inner-product suﬃciently smooth convex function. GLM coeﬃcients glm defined taking population average): glm argmax hxi 2rp (hxi)] ) While make assumptions smoothness, note cumulant generating function recover standard GLM canonical link regression parameters glm [mn89]. examples GLMs form include logistic regression) log Poisson regression) linear regression (least squares. our objective find computationally eﬃcient estimator glm alternative estimator glm proposed paper related OLS coefficient vector, defined ols xti OLS estimator ?ols    design matrix    additionally, text}, positive integers denote size set—.  derivative function denoted) for vector matrix kukq kukq denote -vector -operator norms, respectively.  ], denote— matrix obtained extracting rows indexed For symmetric matrix max) min) denote maximum minimum eigenvalues, respectively. ) denotes condition number respect-norm. denote-variate normal distribution. OLS equivalent GLM scalar factor motivate methodology, assume section covariates multivariate normal, [bri82]. distributional assumptions relaxed Section proposition.? assume covariates multivariate normal covariance matrix xti.  , ?). glm written glm ols  satisfies equation, ols proof Proposition optimal point optimization problem), satisfy normal equations) (hxi ) now, denote multivariate normal density covariance matrix recall well-known property Gaussian density  ?). using Algorithm sls: Scaled Least Squares Estimator input: Data Step compute squares estimator: ?ols ?ols for sub-sampling based OLS estimator, ] random subset ?ols Step solve equation use newton root-finding method: Initialize/var Repeat convergence  output: sls ?ols integration parts hand side equation, obtain) (hxi,  ) (hxi) (this basically stein lemma). combining identity), conclude proof. proposition proof provide main intuition proposed method. observe derivation, worked hand side normal equations) depend response variable equivalence holds joint distribution [bri82], assumed follow single index model. Section extend method non-gaussian predictors) generalized zero-bias transformations.  Regularization version Proposition incorporating regularization important tool datasets large relative predictors highly collinear possible, outlined brieﬂy section. focus -regularization (ridge regression) section; connections lasso -regularization) discussed Section Corollary for define -regularized GLM coeﬃcients, glm argmax hxi glm ols (hxi)]  -regularized OLS coeﬃcients ols xti glm 0glm ols 0ols argument implies 2rp ) This suggests ordinary ridge regression linear model estimate -regularized GLM coeﬃcients glm furr pursuing ideas problems regularization critical issue interesting area future research. sls: Scaled Least Squares estimator GLMs Motivated results previous section, design computationally eﬃcient algorithm GLM task simple solving squares problem; Algorithm algorithm basic steps. first, estimate OLS coeﬃcients, step estimate proportionality constant simple root-finding algorithm. numerous fast optimization methods solve squares problem, superficial review page limits paper. emphasize step (finding OLS estimator) iterative main computational cost proposed algorithm. suggest sub-sampling based estimator ols subset observations estimate covariance matrix. let ] SLS MLE Computation Method SLS MLE Method SLS MLE   time(sec) SLS MLE Accuracy log10) log10) Figure Logistic regression general Gaussian design. left plot shows computational cost (time) finding MLE SLS grows 200. plot depicts accuracy estimators. regime MLE expensive compute, SLS found rapidly accuracy.  built functions find mle. random sub-sample denote sub-matrix formed rows sub-sampled OLS estimator ?ols— XTS properties estimator well-studied [ver10, dlfu13, em15]. for sub-gaussian covariates, suﬃces subsample size log)) [ver10]. hence, step requires single time computational cost max log), for approaches, refer reader [rt08, dlfu13] references rein. step Algorithm involves solving simple root-finding problem. step algorithm, numerous methods completing task. newton root-finding method quadratic convergence halley method cubic convergence choices. highlight step costs) per-iteration attain cubic rate convergence. resulting per-iteration cost cheaper commonly batch algorithms factor) indeed, cost computing gradient). for simplicity, newton root-finding method initialized/var assuming GLM good approximation true conditional distribution, law total \\x0cvariance basic properties glms, Var [var Var Var) (hxi ) initialization reasonable long [var smaller Var) (hxi our experiments show SLS robust initialization. Figure compare performance SLS estimator mle, analyze syntic data generated logistic regression model general Gaussian design randomly generated covariance matrix. left plot shows computational cost obtaining estimators increases fixed plot shows accuracy estimators. regime MLE hard compute MLE SLS achieve accuracy, SLS significantly smaller computation time. refer reader Section oretical results characterizing finite sample behavior sls. Experiments This section results variety numerical studies, show Scaled Least Squares estimator reaches minimum achievable test error substantially faster commonly batch algorithms finding mle. both logistic Poisson regression models (two types glms) utilized analyses, based syntic real datasets. below, brieﬂy describe optimization algorithms MLE experiments.  newton-raphson) achieves locally quadratic convergence scaling gradient inverse Hessian evaluated current iterate. computing Hessian per-iteration cost np2 makes impractical large-scale datasets.  newton-stein) recently proposed second-order batch algorithm specifically designed GLMs [erd16]. algorithm stein lemma sub-sampling eﬃciently estimate Hessian) per-iteration cost, achieving quadratic rates. Logis0c Regression poi?reg Covariates ber( SLS BFGS LBFGS AGD SLS BFGS LBFGS AGD SLS BFGS LBFGS AGD Time (sec) Time (sec) log?reg Covariates {exp} Test Error log?reg Higgs dataset SLS BFGS LBFGS AGD Time (sec Time (sec poi?reg Covertype dataset SLS BFGS LBFGS AGD) poi?reg Covariates ber( SLS BFGS LBFGS AGD) Test Error SLS BFGS LBFGS AGD Time (sec) log(test error) log(test error) OLS start poi?reg Covertype dataset log(test error) Test Error Random start Poisson Regression log?reg Higgs dataset Test Error SLS BFGS LBFGS AGD log(test error) log?reg Covariates {exp Time (sec) Time (sec Time (sec) Figure Performance SLS compared MLE obtained optimization algorithms datasets. sls represented red straight line. details provided Table  broyden-fletcher-goldfarb-shanno (bfgs) popular stable quasi-newton method [nes04]. iteration, gradient scaled matrix formed accumulating information previous iterations gradient computations. convergence locally super-linear periteration cost).  limited memory BFGS (lbfgs) variant bfgs, recent iterates gradients approximate hessian, providing significant improvement terms memory usage. lbfgs variants; formulation [bis95].  gradient descent) takes step opposite direction gradient, evaluated current iterate. its performance strongly depends condition number design matrix. under assumptions, convergence linear) per-iteration cost.  accelerated gradient descent (agd) modified version gradient descent additional ?momentum? term [nes83]. its iteration cost) performance strongly depends smoothness objective function. for algorithms, step size iteration chosen backtracking line search [bv04]. recall proposed Algorithm composed steps; finds estimate OLS coeﬃcients. this-front computation needed MLE algorithms above. hand, MLE algorithms requires initial initialization needed find OLS estimator Algorithm this raises question MLE algorithms initialized, order compare fairly proposed method. scenarios experiments: first, OLS estimator computed Algorithm initialize MLE algorithms; second, random initial value. dataset, main criterion assessing performance estimators rapidly minimum test error achieved. test error measured squared error estimated current parameters iteration test dataset, randomly selected (and set-aside% portion entire dataset. noted previously, MLE accurate small (see Figure). however, regime considered), MLE SLS perform similarly terms error rates; instance, Higgs dataset, SLS MLE test error rates%, respectively. for dataset, minimum achievable test error set maximum final test errors, maximum estimation methods. let ) ) randomly generated covariance matrices. datasets analyzed were) syntic dataset generated logistic regression model iid {exponential) predictors scaled ) Higgs dataset (logistic regression) [bsw14]; (iii) syntic dataset generated Poisson regression model iid binary) predictors scaled ) Covertype dataset (poisson regression) [bd99]. cases, SLS outperformed alternative algorithms finding MLE large margin, terms computation. detailed results found Figure Table provide additional experiments datasets Supplementary material. Table Details experiments shown Figure ODEL DATASET IZE NITIALIZED LOT ETHOD FGS BFGS AGD OGISTIC REGRESSION OISSON REGRESSION } IGGS [bsw14] ) OVERTYPE [bd99 300 300?105) IME SECONDS NUMBER ITERATIONS REACH MIN TEST ERROR 301 170 130 282/216 148 660 701 125 6368/651 6946/670 224/106 357 669/138 134 100871/10101 141736/13808 1711/513 1364/374 218 2405/251 2879/277 103 102 oretical results section, zero-bias transformations [gr97] generalize equivalence OLS GLMs settings covariates non-gaussian. definition let random variable variance exists random variable satisfies) )], differentiable functions distribution -zero-bias distribution. existence Definition consequence Riesz representation orem [gr97]. normal distribution unique distribution zero-bias transformation. normal distribution fixed point operation mapping distribution  provide intuition usefulness zero-bias transformation, refer back proof Proposition for simplicity, assume covariate vector iid entries variance zero-bias transformation applied normal equation) yields xij xij) xij xik xik  normal equation zero-bias transformation distribution xij -zero-bias distribution determined distribution xij general properties found, example, [cgs10]. spread, turns toger, right-hand side) behaves similar side), , behavior similar Gaussian case, proportionality relationship Proposition holds. this argument leads approximate proportionality relationship non-gaussian predictors, which, carried rigorously, yields following.   orem suppose covariate vector covariance matrix and, fur1 rmore, random vector independent entries sub-gaussian norm bounded assume function) Lipschitz continuous constant Let assume-well-spread sense ].   ) (hxi glm  denoting condition number glm ols     ) orem proved Supplementary material. implies population parameters ols glm approximately equivalent scaling factor, error bound). assumption glm well-spread relaxed minor modifications. for example, sparse coeﬃcient vector, supp( glm; jglm support set glm orem holds replaced size support set. interesting consequence orem remarks orem entry glm zero, entry ols small, conversely. for define lasso coeﬃcients  lasso argmin hxi ) 2rp  corollary for ?/—supp( glm xti lasso glm glm supp( supp( furr, satisfy supp( glm jglm ?/—supp( glm supp( lasso supp( glm section, discussed properties population parameters, glm remainder section, turn attention results estimators main focus paper; results ultimately build earlier results. orem order precisely describe performance sls bounds OLS estimator. OLS estimator studied extensively literature; however, purposes, find convenient derive bound accuracy. while exact bound elsewhere, similar orem [dlfu13].   Proposition assume xti sub-gaussian norms respectively. for min denoting smallest eigenvalue ols ?ols  min— probability depends proposition proved Supplementary material. our main result performance sls next. orem let assumptions orem Proposition hold? xk2   ) Furr assume function, ols) satisfies    derivative interval?] change sign., absolute lower bounded and— suﬃciently large, glm sls ) min/ log} probability constants defined?     =?? min min ols max/?    constant depending  ) Note convergence rate upper bound) depends sum terms, functions data dimensions term) orem bounds discrepancy ols glm this term small large, depend number observations term upper bound) estimating ols this term increasing reﬂects fact estimating glm challenging large. expected, term decreasing. larger sample sizepyields estimates. when full OLS solution), term max{log), suﬃciently large. this suggests order good performance. Discussion paper, showed coeﬃcients GLMs OLS approximately proportional general random design setting. using relation, proposed computationally eﬃcient algorithm large-scale problems achieves accuracy MLE estimating OLS coeﬃcients estimating proportionality constant iterations attain quadratic cubic convergence rate) per-iteration cost. brieﬂy mentioned proportionality coeﬃcients holds regularization Section. furr pursuing idea interesting large-scale problems regularization crucial. anor interesting line research find similar proportionality relations parameters large-scale optimization problems support vector machines. such relations reduce problem complexity significantly.',\n",
       " 'PP6581': 'stochastic variational inference (blei., 2012; Hoffman., 2013) method scalable posterior inference large datasets stochastic gradient ascent. made eﬃcient continuous latent variables latent-variable reparameterization inference networks, amortizing cost, resulting highly scalable learning procedure (kingma welling, 2013; Rezende., 2014; Salimans., 2014). neural networks inference network generative model, results class models called variational autoencoders (kingma welling, 2013) (vaes). general strategy building ﬂexible inference networks, framework normalizing ﬂows (rezende mohamed, 2015). paper propose type ﬂow, inverse autoregressive ﬂow (iaf), scales highdimensional latent space. core proposed method lie Gaussian \\x0cfuncautoregressive functions density estimation: tions input variable ordering multidimensional tensors, output standard deviation element input variable conditioned previous elements. examples functions autoregressive neural density estimators rnns, MADE (germain., 2015), PixelCNN (van den Oord., 2016b) WaveNet (van den Oord., 2016a) models. show functions turned invertible nonlinear transformations input, simple Jacobian determinant. transformation ﬂexible determinant known, normalizing ﬂow, transforming tensor simple density, tensor complicated density cheaply computable. contrast previous work University amsterdam, University California irvine, Canadian Institute Advanced Research (cifar). 30th Conference Neural Information Processing Systems (nips 2016), barcelona, spain. ) Prior distribution) Posteriors standard VAE) Posteriors VAE IAF Figure Best viewed color. fitted variational auto-encoder (vae) spherical Gaussian prior, factorized Gaussian posteriors) inverse autoregressive ﬂow (iaf) posteriors) toy dataset datapoints. colored cluster corresponds posterior distribution datapoint. iaf greatly improves ﬂexibility posterior distributions, fit posteriors prior. improving inference models including previously normalizing ﬂows, transformation suited high-dimensional tensor variables, spatio-temporally organized variables. demonstrate method improving inference networks deep variational auto-encoders. particular, train deep variational auto-encoders latent variables multiple levels hierarchy, stochastic variable three-dimensional tensor stack featuremaps), demonstrate improved performance. variational Inference Learning Let (set) observed variable), (set) latent variable, parametric model joint distribution, called generative model defined variables. dataset ..., typically perform maximum marginal likelihood learning parameters. maximize log) log general marginal likelihood intractable compute differentiate directly ﬂexible generative models. components generative model parameterized neural networks. solution introduce), parametric inference model defined latent variables, \\x0cand optimize variational lower bound marginal log-likelihood observation log) [log, log; ) parameters models. keeping mind kullback-leibler divergences DKL (.) non-negative clear; lower bound log) written; log) DKL) ways optimize lower bound; continuous eﬃciently-parameterization. (kingma welling, 2013; Rezende., 2014). equation), maximizing; .  concurrently maximize log) minimize DKL)). closer DKL)) closer; log), approximation optimization objective; true objective log). also, minimization DKL)) goal itself interested) inference optimization. case, divergence DKL)) function parameters inference model generative model, increasing ﬂexibility eir generally helpful objective. note models multiple latent variables, inference model typically factorized partial inference models ordering. ).  write, denote partial inference models, conditioned data furr context includes previous latent variables ordering.  Requirements Computational Tractability Requirements inference model, order eﬃciently optimize bound) computationally eﬃcient compute differentiate probability density) computationally eﬃcient sample from, operations performed datapoint minibatch iteration optimization. high-dimensional make eﬃcient parallel computational resources gpus, parallelizability operations dimensions large factor eﬃciency. requirement restrict class approximate posteriors) practical use. practice leads diagonal posteriors. )  )), ) nonlinear functions parameterized neural networks. however, explained above, density) suﬃciently ﬂexible match true posterior).  Normalizing Flow Normalizing Flow), introduced (rezende mohamed, 2015) context stochastic gradient variational inference, powerful framework building ﬂexible posterior distributions iterative procedure. general idea start initial random variable simple distribution (and computationally cheap) probability density function, apply chain invertible parameterized transformations iterate ﬂexible distribution2 ) dzt dzt) long Jacobian determinant transformations computed, compute probability density function iterate: log) log) log det however, (rezende mohamed, 2015) experiment limited family invertible transformation Jacobian determinant, namely) vectors, transposed, scalar(.) nonlinearity interpreted MLP bottleneck hidden layer single unit. information single bottleneck, long chain transformations required capture high-dimensional dependencies. inverse Autoregressive Transformations order find type normalizing ﬂow scales high-dimensional space, Gaussian versions autoregressive autoencoders MADE (germain., 2015) PixelCNN (van den Oord., 2016b). variable modeled model, chosen ordering elements  )] denote function vector vectors  due autoregressive structure, Jacobian lower triangular zeros diagonal, elements predicted standard deviation element functions previous elements sampling model sequential transformation noise vector  , vector  context, datapoint. case models multiple levels latent variables, context includes previously sampled latent variables. algorithm pseudo-code approximate posterior Inverse Autoregressive Flow (iaf) data: datapoint, optionally conditioning information neural network parameters encodernn; encoder neural network, additional output autoregressivenn[? , autoregressive neural networks, additional input sum(. sum vector elements sigmoid(. element-wise sigmoid function result: random sample), approximate posterior distribution scalar log), evaluated sample? [?, encodernn;   , ?+? sum(log log?)) , autoregressivenn, sigmoid) sum(log end computation involved transformation proportional dimensionality Since variational inference requires sampling posterior, models interesting direct applications. however, inverse transformation interesting normalizing ﬂows, show. long sampling transformation one-one transformation, inverted:   make key observations, important normalizing ﬂows. inverse transformation parallelized case autoregressive autoencoders) computations individual elements depend eachor. vectorized transformation: ) ) subtraction division elementwise. key observation, inverse autoregressive operation simple Jacobian determinant. note due autoregressive structure, result, transformation lower triangular Jacobian), simple diagonal determinant lower triangular matrix equals product diagonal terms. result, log-determinant Jacobian transformation remarkably simple straightforward compute: log det  log) combination model ﬂexibility, parallelizability dimensions, simple log-determinant, make transformation interesting normalizing ﬂow high-dimensional latent space. inverse Autoregressive Flow (iaf) propose type normalizing ﬂow. )), based transformations equivalent inverse autoregressive transformation. ) reparameterization. algorithm pseudo-code appproximate \\x0cposterior proposed ﬂow. initial encoder neural network output addition extra output serves additional input subsequent step ﬂow. draw random sample  ), initialize chain with:  ) Approximate Posterior Inverse Autoregressive Flow (iaf) Encoder    iaf Step ??? IAF step IAF step Autoregressive ??? ???    figure Like normalizing ﬂows, drawing samples approximate posterior Inverse Autoregressive Flow (iaf) consists initial sample Inverse drawn aflow simple distribution, Approximate Posterior Autoregressive (iaf) Gaussian diagonal covariance, chain nonlinear invertible transformations simple Jacobian determinants. encoder   autoregressive ﬂow consists chain transformations:   autoregressive   ) normalizing Flow initial Network IAF step ??? iaf Step step ﬂow, autoregressive neural network inputs Generative outputs  neural network structured autoregressive. modelt choice parameters, Jacobians dzd? dzd triangular zeros Inference dzt diagonal. result, model (note dzt triangular diagonal, determinant Jacobian. constraints.) . ), density final Posterior Inverse Autoregressive Flow (iaf: iterate encoder? IAF IAF ??? step step log) log?) log ﬂexibility distribution final iterate ability closely fit true posterior, increases expressivity autoregressive models depth chain. figure illustration. numerically stable version, inspired lstm-type update, autoregressive network output unconstrained real-valued vectors autoregressivenn ) \\x0cand compute: sigmoid) This version shown algorithm note version update. ), simple computation final log-density. ) applies. found beneficial results parameterize initialize parameters autoregressivenn] outputs are, optimization, suﬃciently positive, close. leads initial behaviour updates slightly step iaf. parameterization ?forget gate bias? lstms, investigated Jozefowicz. (2015). simplest special version IAF simple step, linear autoregressive model. transforms Gaussian variable diagonal covariance, linear dependencies. gaussian distribution full covariance. appendix explanation. autoregressive neural networks form rich family nonlinear transformations iaf. nonconvolutional models, family masked autoregressive networks introduced (germain., 2015) autoregressive neural networks. cifar experiments, benefits scaling high dimensional latent space, family convolutional autoregressive autoencoders introduced (van den Oord., 2016b). found results improved reversing ordering variables step IAF chain. volume-preserving transformation, simple form. ) remains unchanged. iaf Step Context. encoder) Autoregress Related work Inverse autoregressive ﬂow (iaf) member family normalizing ﬂows, discussed (rezende mohamed, 2015) context stochastic variational inference. (rezende mohamed, 2015) specific types ﬂows introduced: planar ﬂows radial ﬂows. ﬂows shown effective problems low-dimensional latent space hundred dimensions). clear, however, scale ﬂows higher-dimensional latent spaces, latent spaces generative models /larger images, planar radial ﬂows leverage topology latent space, iaf. volume-conserving neural architectures presented (deco brauer, 1995), form nonlinear independent component analysis. anor type normalizing ﬂow, introduced (dinh., 2014) (nice), similar transformations iaf. contrast iaf, type transformations updates half latent variables step, adding vector neural network based function remaining latent variables large blocks advantage computationally cheap inverse transformation, disadvantage typically requiring longer chains. experiments, (rezende mohamed, 2015) found type transformation generally powerful types normalizing ﬂow, experiments low-dimensional latent space. concurrently work, NICE extended high-dimensional spaces (dinh., 2016) (real nvp). empirical comparison interesting subject future research. potentially powerful transformation Hamiltonian ﬂow Hamiltonian Variational Inference (salimans., 2014). here, transformation generated simulating ﬂow Hamiltonian system consisting latent variables set auxiliary momentum variables. type transformation additional benefit guided exact posterior distribution, leaves distribution invariant small step sizes. transformation arbitrarily close exact posterior distribution apply suﬃcient number times. practice, however, Hamiltonian Variational Inference demanding computationally. also, requires auxiliary variational bound account auxiliary variables, impede progress bound suﬃciently tight. alternative method increasing ﬂexiblity variational inference, introduction auxiliary latent variables (salimans., 2014; Ranganath., 2015; Tran., 2015) auxiliary inference models. latent variable models multiple layers stochastic variables, experiments, equivalent auxiliary-variable methods. combine deep latent variable models IAF experiments, benefiting techniques. experiments empirically evaluate IAF applying idea improve variational autoencoders. appendix details architectures generative model inference models. code reproducing key empirical results online3  MNIST expermiment follow similar implementation convolutional VAE (salimans., 2014) ResNet., 2015) blocks. single layer Gaussian stochastic units dimension used. investigate expressiveness approximate posterior affects performance, report results IAF posteriors varying degrees expressiveness. -layer MADE (germain., 2015) implement IAF transformation, stack multiple IAF transformations ordering reversed transformation. results: Table shows results MNIST types posteriors. results approximate posterior expressive, generative modeling performance better. worth noting expressive approximate posterior tightens variational lower bounds expected, making gap variational lower bounds marginal likelihoods smaller. making IAF deep wide enough, achieve published log-likelihood dynamically https://github.com/openai/iaf Table Generative modeling results dynamically sampled binarized MNIST version previous publications (burda., 2015). shown averages; number brackets standard deviations optimization runs. column shows importance sampled estimate marginal likelihood model 128 samples. previous results reproduced segment]: (salimans., 2014]: (burda., 2015]: (kaae?nderby., 2016]: (tran., 2015) Model VLB log) convolutional VAE HVI] DLGM 2hl IWAE] LVAE] DRAW VGP Diagonal covariance IAF (depth Width 320) IAF (depth Width 1920) IAF (depth Width 1920) IAF (depth Width 1920     )         ) bottom ResNet Block top-down ResNet Block Layer prior:? ) ELU ELU Layer posterior:? ) ELU ELU Deep generative model bidirectional? inference model VAE bidirectional inference Identity Identity Convolution ELU Nonlinearity Figure Overview ResNet VAE bidirectional inference. posterior layer parameterized iaf. binarized mnist. hugo larochelle statically binarized mnist, VAE deep IAF achieves log-likelihood, slightly worse reported result, PixelCNN (van den Oord., 2016b).  cifar evaluated IAF cifar dataset natural images. natural images greater variety patterns structure MNIST images; order capture structure well, experiment architecture, ResNet vae, layers stochastic variables, based residual convolutional networks (resnets., 2015, 2016). appendix details. log-likelihood. table comparison previously reported results. architecture IAF achieves bits dimension, published latent-variable models, par reported result pixelcnn. appendix experimental results. suspect results furr improved steps ﬂow, leave future work. synsis speed. sampling seconds/image ResNet VAE model, versus seconds/image PixelCNN model, NVIDIA Titan gpu. sampled PixelCNN?vely sequentially generating pixel time, full generative model iteration. custom code evaluates relevant part network, PixelCNN sampling sped significantly; speedup limited parallel hardware due Table Our results ResNet VAEs cifar images, compared earlier results, average number bits data dimension test set. number convolutional DRAW upper bound, ResNet VAE log-likelihood estimated importance sampling. method bits/dim Results tractable likelihood models: Uniform distribution (van den Oord., 2016b) Multivariate Gaussian (van den Oord., 2016b) NICE (dinh., 2014) Deep GMMs (van den Oord schrauwen, 2014) Real NVP (dinh., 2016) PixelRNN (van den Oord., 2016b) Gated PixelCNN (van den Oord., 2016c Results variationally trained latent-variable models: Deep Diffusion (sohl-dickstein., 2015) Convolutional DRAW (gregor., 2016) ResNet VAE IAF (ours \\x0csequential nature sampling operation. eﬃcient sampling ResNet VAE parallel computation require custom code. conclusion presented inverse autoregressive ﬂow (iaf), type normalizing ﬂow scales high-dimensional latent space. experiments demonstrated autoregressive ﬂow leads significant performance gains compared similar models factorized Gaussian approximate posteriors, report close state--art log-likelihood results cifar, model faster sampling. acknowledgements Jascha sohl-dickstein, Karol gregor, ors Google Deepmind interesting discussions. harri Valpola referring Gustavo deco relevant pioneering work form inverse autoregressive ﬂow applied nonlinear independent component analysis. Stochastic variational inference (blei., 2012; Hoffman., 2013) method scalable posterior inference large datasets stochastic gradient ascent. made eﬃcient continuous latent variables latent-variable reparameterization inference networks, amortizing cost, resulting highly scalable learning procedure (kingma welling, 2013; Rezende., 2014; Salimans., 2014). when neural networks inference network generative model, results class models called variational autoencoders (kingma welling, 2013) (vaes). general strategy building ﬂexible inference networks, framework normalizing ﬂows (rezende mohamed, 2015). paper propose type ﬂow, inverse autoregressive ﬂow (iaf), scales highdimensional latent space. core proposed method lie Gaussian \\x0cfuncautoregressive functions density estimation: tions input variable ordering multidimensional tensors, output standard deviation element input variable conditioned previous elements. examples functions autoregressive neural density estimators rnns, MADE (germain., 2015), PixelCNN (van den Oord., 2016b) WaveNet (van den Oord., 2016a) models. show functions turned invertible nonlinear transformations input, simple Jacobian determinant. since transformation ﬂexible determinant known, normalizing ﬂow, transforming tensor simple density, tensor complicated density cheaply computable. contrast previous work University amsterdam, University California irvine, Canadian Institute Advanced Research (cifar). 30th Conference Neural Information Processing Systems (nips 2016), barcelona, spain. ) Prior distribution) Posteriors standard VAE) Posteriors VAE IAF Figure Best viewed color. fitted variational auto-encoder (vae) spherical Gaussian prior, factorized Gaussian posteriors) inverse autoregressive ﬂow (iaf) posteriors) toy dataset datapoints. each colored cluster corresponds posterior distribution datapoint. iaf greatly improves ﬂexibility posterior distributions, fit posteriors prior. improving inference models including previously normalizing ﬂows, transformation suited high-dimensional tensor variables, spatio-temporally organized variables. demonstrate method improving inference networks deep variational auto-encoders. particular, train deep variational auto-encoders latent variables multiple levels hierarchy, stochastic variable three-dimensional tensor stack featuremaps), demonstrate improved performance. Variational Inference Learning Let (set) observed variable), (set) latent variable, parametric model joint distribution, called generative model defined variables. given dataset ..., typically perform maximum marginal likelihood learning parameters. maximize log) log general marginal likelihood intractable compute differentiate directly ﬂexible generative models. components generative model parameterized neural networks. solution introduce), parametric inference model defined latent variables, \\x0cand optimize variational lower bound marginal log-likelihood observation log) [log, log; ) parameters models. keeping mind kullback-leibler divergences DKL (.) non-negative clear; lower bound log) written; log) DKL) ways optimize lower bound; continuous eﬃciently-parameterization. (kingma welling, 2013; Rezende., 2014). equation), maximizing; .  concurrently maximize log) minimize DKL)). closer DKL)) closer; log), approximation optimization objective; true objective log). also, minimization DKL)) goal itself interested) inference optimization. case, divergence DKL)) function parameters inference model generative model, increasing ﬂexibility eir generally helpful objective. Note models multiple latent variables, inference model typically factorized partial inference models ordering. ).  write, denote partial inference models, conditioned data furr context includes previous latent variables ordering.  Requirements Computational Tractability Requirements inference model, order eﬃciently optimize bound) computationally eﬃcient compute differentiate probability density) computationally eﬃcient sample from, operations performed datapoint minibatch iteration optimization. high-dimensional make eﬃcient parallel computational resources gpus, parallelizability operations dimensions large factor eﬃciency. this requirement restrict class approximate posteriors) practical use. practice leads diagonal posteriors. )  )), ) nonlinear functions parameterized neural networks. however, explained above, density) suﬃciently ﬂexible match true posterior).  Normalizing Flow Normalizing Flow), introduced (rezende mohamed, 2015) context stochastic gradient variational inference, powerful framework building ﬂexible posterior distributions iterative procedure. general idea start initial random variable simple distribution (and computationally cheap) probability density function, apply chain invertible parameterized transformations iterate ﬂexible distribution2 ) dzt dzt) long Jacobian determinant transformations computed, compute probability density function iterate: log) log) log det however, (rezende mohamed, 2015) experiment limited family invertible transformation Jacobian determinant, namely) vectors, transposed, scalar(.) nonlinearity interpreted MLP bottleneck hidden layer single unit. since information single bottleneck, long chain transformations required capture high-dimensional dependencies. Inverse Autoregressive Transformations order find type normalizing ﬂow scales high-dimensional space, Gaussian versions autoregressive autoencoders MADE (germain., 2015) PixelCNN (van den Oord., 2016b). let variable modeled model, chosen ordering elements  )] denote function vector vectors  due autoregressive structure, Jacobian lower triangular zeros diagonal, elements predicted standard deviation element functions previous elements sampling model sequential transformation noise vector  , vector  where context, datapoint. case models multiple levels latent variables, context includes previously sampled latent variables. Algorithm pseudo-code approximate posterior Inverse Autoregressive Flow (iaf) data: datapoint, optionally conditioning information neural network parameters encodernn; encoder neural network, additional output autoregressivenn[? , autoregressive neural networks, additional input sum(. sum vector elements sigmoid(. element-wise sigmoid function result: random sample), approximate posterior distribution scalar log), evaluated sample? [?, encodernn;   , ?+? sum(log log?)) , autoregressivenn, sigmoid) sum(log end computation involved transformation proportional dimensionality Since variational inference requires sampling posterior, models interesting direct applications. however, inverse transformation interesting normalizing ﬂows, show. long sampling transformation one-one transformation, inverted:   make key observations, important normalizing ﬂows. inverse transformation parallelized case autoregressive autoencoders) computations individual elements depend eachor. vectorized transformation: ) ) subtraction division elementwise. key observation, inverse autoregressive operation simple Jacobian determinant. note due autoregressive structure, result, transformation lower triangular Jacobian), simple diagonal determinant lower triangular matrix equals product diagonal terms. result, log-determinant Jacobian transformation remarkably simple straightforward compute: log det  log) combination model ﬂexibility, parallelizability dimensions, simple log-determinant, make transformation interesting normalizing ﬂow high-dimensional latent space. Inverse Autoregressive Flow (iaf) propose type normalizing ﬂow. )), based transformations equivalent inverse autoregressive transformation. ) reparameterization. see algorithm pseudo-code appproximate \\x0cposterior proposed ﬂow. initial encoder neural network output addition extra output serves additional input subsequent step ﬂow. draw random sample  ), initialize chain with:  ) Approximate Posterior Inverse Autoregressive Flow (iaf) Encoder    IAF Step ??? IAF step IAF step Autoregressive ??? ???    Figure Like normalizing ﬂows, drawing samples approximate posterior Inverse Autoregressive Flow (iaf) consists initial sample Inverse drawn aflow simple distribution, Approximate Posterior Autoregressive (iaf) Gaussian diagonal covariance, chain nonlinear invertible transformations simple Jacobian determinants. Encoder   Autoregressive ﬂow consists chain transformations:   Autoregressive   ) normalizing Flow Initial Network IAF step ??? IAF Step step ﬂow, autoregressive neural network inputs Generative outputs  neural network structured autoregressive. modelt choice parameters, Jacobians dzd? dzd triangular zeros Inference dzt diagonal. result, model (note dzt triangular diagonal, determinant Jacobian. constraints.) following. ), density final Posterior Inverse Autoregressive Flow (iaf: iterate encoder? IAF IAF ??? step step log) log?) log ﬂexibility distribution final iterate ability closely fit true posterior, increases expressivity autoregressive models depth chain. see figure illustration. numerically stable version, inspired lstm-type update, autoregressive network output unconstrained real-valued vectors autoregressivenn ) \\x0cand compute: sigmoid) This version shown algorithm note version update. ), simple computation final log-density. ) applies. found beneficial results parameterize initialize parameters autoregressivenn] outputs are, optimization, suﬃciently positive, close. this leads initial behaviour updates slightly step iaf. such parameterization ?forget gate bias? lstms, investigated Jozefowicz. (2015). perhaps simplest special version IAF simple step, linear autoregressive model. this transforms Gaussian variable diagonal covariance, linear dependencies. Gaussian distribution full covariance. see appendix explanation. autoregressive neural networks form rich family nonlinear transformations iaf. for nonconvolutional models, family masked autoregressive networks introduced (germain., 2015) autoregressive neural networks. for cifar experiments, benefits scaling high dimensional latent space, family convolutional autoregressive autoencoders introduced (van den Oord., 2016b). found results improved reversing ordering variables step IAF chain. this volume-preserving transformation, simple form. ) remains unchanged. IAF Step Context. encoder) Autoregress Related work Inverse autoregressive ﬂow (iaf) member family normalizing ﬂows, discussed (rezende mohamed, 2015) context stochastic variational inference. (rezende mohamed, 2015) specific types ﬂows introduced: planar ﬂows radial ﬂows. ﬂows shown effective problems low-dimensional latent space hundred dimensions). clear, however, scale ﬂows higher-dimensional latent spaces, latent spaces generative models /larger images, planar radial ﬂows leverage topology latent space, iaf. volume-conserving neural architectures presented (deco brauer, 1995), form nonlinear independent component analysis. anor type normalizing ﬂow, introduced (dinh., 2014) (nice), similar transformations iaf. contrast iaf, type transformations updates half latent variables step, adding vector neural network based function remaining latent variables such large blocks advantage computationally cheap inverse transformation, disadvantage typically requiring longer chains. experiments, (rezende mohamed, 2015) found type transformation generally powerful types normalizing ﬂow, experiments low-dimensional latent space. concurrently work, NICE extended high-dimensional spaces (dinh., 2016) (real nvp). empirical comparison interesting subject future research. potentially powerful transformation Hamiltonian ﬂow Hamiltonian Variational Inference (salimans., 2014). here, transformation generated simulating ﬂow Hamiltonian system consisting latent variables set auxiliary momentum variables. this type transformation additional benefit guided exact posterior distribution, leaves distribution invariant small step sizes. such transformation arbitrarily close exact posterior distribution apply suﬃcient number times. practice, however, Hamiltonian Variational Inference demanding computationally. also, requires auxiliary variational bound account auxiliary variables, impede progress bound suﬃciently tight. alternative method increasing ﬂexiblity variational inference, introduction auxiliary latent variables (salimans., 2014; Ranganath., 2015; Tran., 2015) auxiliary inference models. latent variable models multiple layers stochastic variables, experiments, equivalent auxiliary-variable methods. combine deep latent variable models IAF experiments, benefiting techniques. Experiments empirically evaluate IAF applying idea improve variational autoencoders. please appendix details architectures generative model inference models. code reproducing key empirical results online3  MNIST expermiment follow similar implementation convolutional VAE (salimans., 2014) ResNet., 2015) blocks. single layer Gaussian stochastic units dimension used. investigate expressiveness approximate posterior affects performance, report results IAF posteriors varying degrees expressiveness. -layer MADE (germain., 2015) implement IAF transformation, stack multiple IAF transformations ordering reversed transformation. results: Table shows results MNIST types posteriors. results approximate posterior expressive, generative modeling performance better. also worth noting expressive approximate posterior tightens variational lower bounds expected, making gap variational lower bounds marginal likelihoods smaller. making IAF deep wide enough, achieve published log-likelihood dynamically https://github.com/openai/iaf Table Generative modeling results dynamically sampled binarized MNIST version previous publications (burda., 2015). shown averages; number brackets standard deviations optimization runs. column shows importance sampled estimate marginal likelihood model 128 samples. best previous results reproduced segment]: (salimans., 2014]: (burda., 2015]: (kaae?nderby., 2016]: (tran., 2015) Model VLB log) convolutional VAE HVI] DLGM 2hl IWAE] LVAE] DRAW VGP Diagonal covariance IAF (depth Width 320) IAF (depth Width 1920) IAF (depth Width 1920) IAF (depth Width 1920     )         ) bottom ResNet Block top-down ResNet Block Layer prior:? ) ELU ELU Layer posterior:? ) ELU ELU Deep generative model bidirectional? inference model VAE bidirectional inference Identity Identity Convolution ELU Nonlinearity Figure Overview ResNet VAE bidirectional inference. posterior layer parameterized iaf. binarized mnist. Hugo larochelle statically binarized mnist, VAE deep IAF achieves log-likelihood, slightly worse reported result, PixelCNN (van den Oord., 2016b).  cifar evaluated IAF cifar dataset natural images. natural images greater variety patterns structure MNIST images; order capture structure well, experiment architecture, ResNet vae, layers stochastic variables, based residual convolutional networks (resnets., 2015, 2016). please appendix details. log-likelihood. see table comparison previously reported results. our architecture IAF achieves bits dimension, published latent-variable models, par reported result pixelcnn. see appendix experimental results. suspect results furr improved steps ﬂow, leave future work. synsis speed. sampling seconds/image ResNet VAE model, versus seconds/image PixelCNN model, NVIDIA Titan gpu. sampled PixelCNN?vely sequentially generating pixel time, full generative model iteration. with custom code evaluates relevant part network, PixelCNN sampling sped significantly; speedup limited parallel hardware due Table Our results ResNet VAEs cifar images, compared earlier results, average number bits data dimension test set. number convolutional DRAW upper bound, ResNet VAE log-likelihood estimated importance sampling. method bits/dim Results tractable likelihood models: Uniform distribution (van den Oord., 2016b) Multivariate Gaussian (van den Oord., 2016b) NICE (dinh., 2014) Deep GMMs (van den Oord schrauwen, 2014) Real NVP (dinh., 2016) PixelRNN (van den Oord., 2016b) Gated PixelCNN (van den Oord., 2016c Results variationally trained latent-variable models: Deep Diffusion (sohl-dickstein., 2015) Convolutional DRAW (gregor., 2016) ResNet VAE IAF (ours \\x0csequential nature sampling operation. eﬃcient sampling ResNet VAE parallel computation require custom code. Conclusion presented inverse autoregressive ﬂow (iaf), type normalizing ﬂow scales high-dimensional latent space. experiments demonstrated autoregressive ﬂow leads significant performance gains compared similar models factorized Gaussian approximate posteriors, report close state--art log-likelihood results cifar, model faster sampling. acknowledgements Jascha sohl-dickstein, Karol gregor, ors Google Deepmind interesting discussions. Harri Valpola referring Gustavo deco relevant pioneering work form inverse autoregressive ﬂow applied nonlinear independent component analysis.',\n",
       " 'PP6595': 'mapping neuroanatomy, pursuit linking hyposized computational models consistent observed functions actual physical structures, long-standing fundamental problem neuroscience. primary interest mapping network structure neural circuits identifying morphology neuron locations synaptic connections neurons, field called connectomics. currently, promising approach obtaining maps neural circuit structure volume electron microscopy stained fixed block tissue. ] This technique \\x0csuccessfully decades ago mapping structure complete nervous system 302-neuron Caenorhabditis elegans; due manually cut, image, align, trace neuronal processes 8000 serial sections, small circuit required years labor, spent image analysis. ] time, scaling approach larger circuits practical. recent advances volume electron microscopy] make feasible imaging large circuits, potentially hundreds thousands neurons, suﬃcient resolution discern smallest neuronal processes. ] high image quality near-isotropic resolution achievable methods enables resultant data treated true volume, significantly aids reconstruction processes run parallel sectioning axis, potentially amenable automated image processing. 30th Conference Neural Information Processing Systems (nips 2016), barcelona, spain. fully-connected layer Convolutional neural network; Image) voxel positions Boundary classification Global energy Shape descriptors Candidate segmentation) Initial oversegmentation Agglomeration Local energy Figure Illustration computation global energy single candidate segmentation local energy; ], computed deep neural network, summed shape descriptor types voxel positions image analysis remains key challenge, however. primary bottleneck segmenting full volume, filled heavily intertwined neuronal processes, volumes occupied individual neuron. cell boundaries shown stain provide strong visual cue cases, neurons extend tens centimeters path length places narrow; single mistake path render connectivity information neuron largely inaccurate. existing automated semi-automated segmentation methods suﬃciently reduce amount human labor required: recent reconstruction 950 neurons mouse retina required 20000 hours human labor, eﬃcient method tracing skeleton neuron]; recent reconstruction 379 neurons Drosophila medulla column (part visual pathway) required 12940 hours manual proof-reading/correction automated segmentation]. related work: Algorithmic approaches image segmentation formulated variations pipeline: boundary detection step establishes local hyposes object boundaries, region formation step integrates boundary evidence local regions. superpixels supervoxels), region agglomeration step merges adjacent regions based image object features. , Although extensive integration machine learning pipelines begun yield promising segmentation results], argue pipelines, previously formulated, fundamentally neglect potentially important aspects achieving accurate segmentation) combinatorial nature reasoning dense image segmentation structure) fundamental importance shape criterion segmentation quality. contributions: propose method attempts overcome deficiencies. particular, propose energybased model scores segmentation quality deep neural network ﬂexibly integrates shape image information: Combinatorial Energy Learning Image Segmentation (celis). pursuit model paper makes specific contributions: connectivity region data structure eﬃciently computing energy configurations objects; binary shape descriptor eﬃcient representation shape configurations; neural network architecture splices intermediate unit output trained convolutional network input deep fully-connected neural network architecture scores segmentation image; training procedure pairwise object relations segmentation learn energy-based model. experimental evaluation proposed baseline automated reconstruction methods massive knowledge) unprecedented scale reﬂects true size connectomic datasets required biological analysis (many billions voxels). conditional energy modeling segmentations images define global, translation-invariant energy model predicting cost complete segmentation image cost analogous negative While prior work, recognized importance combinatorial reasoning, previously proposed global optimization methods local decisions interact limited way. log-likelihood segmentation image, treat probabilistically. goal define model true segmentation image found minimizing cost; energy reﬂect prior object configurations alone, compatibility object configurations image. shown fig. define global energy; sum local energy models (defined deep neural network; scales computed sliding-window fashion centered position volume); )) ; local energy; depends local image context centered position vector representation ; computed deep convolutional neural network local shape/object configuration scale local binary shape descriptor), defined Section find (locally) minimal-cost segmentations model, local search space agglomerations starting initial supervoxel segmentation. simple greedy policy, step agglomeration actions. merges adjacent segments, pick action results lowest energy. ?vely, computing energy single segmentation requires computing shape descriptors evaluating energy model voxel position volume; small volume tens hundreds millions voxels. stage agglomeration, thousands, tens thousands, potential agglomeration steps, results unique segmentation. order choose step, energy potential segmentations. computational cost perform computations directly tremendous, supplement, prove collection orems eﬃcient implementation computes energy terms incrementally. representing Shape Configurations Local Binary Descriptors propose binary shape descriptor based subsampled pairwise connectivity information: specification pairs position offsets    relative center fixed-size bounding box size-bit binary shape descriptor segmentation bounding box defined connected ]. orwise. shown fig. , bit descriptor specifies wher pair positions part segment, determined constant time suitable data structure. limit case, list pairs positions-voxel bounding box, information lost Hamming distance descriptors precisely equal Rand index. ] general sample subset pairs possible; sample uniformly random, retain property expected Hamming distance descriptors equal Rand index. found picking 512 bits reasonable tradeoff fidelity representation size. pairs randomly sampled initially, naturally obtain consistent results learning models based descriptors fixed list positions defining descriptor training test time. note descriptor serves general type sketch full segmentation bounding box. restricting positions pair center position bounding box, obtain sketch single segment center position. refer descriptor case center-based, general case pairwise, shown fig. . shape descriptors represent local sub-regions segmentation. represent shape information large volume, compute shape descriptors densely positions sliding \\x0cwindow fashion, shown fig. . brief descriptor] similarly defined binary descriptor based subset pairs points patch, bit based intensity difference, rar connectivity, pair. ... 100000000110   10000000011000000110100000101001) Sequence showing computation shape descriptor. 00001000001011100111100100001000 00000000000101110000010000110010 10001001101100010100000010000111) Shape descriptors computed multiple scales. pairwise descriptors (shown left center) arbitrary pairwise connectivity, centerbased shape descriptors (shown right) restrict position pair center point. 10000001110010100110100001011001 11000011110011100100100011011011 10000011100111100100110011011111) Shape descriptors computed densely position volume. figure Illustration shape descriptors. connected components bounding box descriptor computed shown distinct colors. pairwise connectivity relationships define descriptor dashed lines; connected pairs shown white, disconnected pairs shown black. connectivity determined based connected components underlying segmentation, geometry line itself. illustration, experiments shape descriptors computed fully. Connectivity Regions defined, single shape descriptor represents segmentation fixed-size bounding box; shifting position bounding box obtain descriptors local regions larger segmentation. size bounding box determines scale local representation. raises question connectivity defined local regions. voxels connected long path descriptor bounding box. shape descriptors consistent local topology, pairs considered disconnected. shape descriptors are, refore, defined respect connectivity larger connectivity region, necessarily descriptor bounding boxes general significantly smaller full segmentation; conceptually, shape descriptor bounding box slides positions contained connectivity region. (this sliding necessarily results minor inconsistency context positions, reduces computational memory costs.) obtain shape descriptors positions, simply tile space overlapping rectangular connectivity regions uniform size stride, \\x0cshown supplement. connectivity region size determines degree locality connectivity information captured shape descriptor (independent descriptor bounding box size). affects computational costs, supplement. energy model learning; shape descriptor type/scale learned neural define local energy model network model computes real-valued score, shape descriptor image feature vector simplify presentation, define notation forward discrete derivative respect) ). based notation, discrete derivative energy function ), denotes result merging supervoxels existing segmentation agglomerate, greedy policy simply chooses step action minimizes), denotes current segmentation step prior work], treat classification problem, goal matching sign error change segmentation error respect ground truth segmentation measured Variation Information].  Local training procedure Because term simply sum change energies position; descriptor type heuristic optimize parameters energy model independently shape descriptor type/scale seek minimize expectation); )))+ ‘(?esii error ; ))) ‘(? ?esii error indexes training examples correspond sam, pled position merge action applied segmentation denotes binary classification loss function, , predicted probability true label positive, weighted—. note ?esii error action improved score refore low predicted score post-merge descriptor high predicted score pre-merge descriptor ?esii error opposite applies. tested standard log loss—  log log )], signed linear loss, closely matches; terms contribute; scores. stochastic gradient descent (sgd) perform optimization. obtain training examples agglomerating expert policy greedily optimizes error  segmentation state agglomeration step (including initial state), agglomeration action position volume, compute shape descriptor pair) reﬂecting pre-merge post-merge states, respectively. ), emit training descriptor pair. reby obtain \\x0cconceptual stream examples error  . stream examples billions examples (and highly correlated), required learn parameters reduce resource requirements, priority sampling], based error, )—, obtain fixed number weighted samples replacement descriptor type equalize total weight true merge examples error, false merge examples error, order avoid learning degenerate models Experiments tested approach large, publicly electron microscopy dataset, called Janelia fib25, portion Drosophila melangaster optic lobe. dataset collected  for example, weight false merge examples, occur balancing, model simply learn assign score increases number bits shape descriptor.  Split error  Rand CELIS (this paper-cnn+gala-cnn+watershed 7colseg1.672.069.143.981.691.597.629.099 Oracle.428.901 Merge error)) Figure Segmentation accuracy-gigavoxel fib test set. left: Pareto frontiers information-oretic split/merge error, previously evaluate segmentation accuracy. ] right: Comparison Variation Information (lower better) Rand score (higher better). celis, 3dcnn+gala-cnn+watershed, hyperparameters optimized metric training set. resolution Focused Ion Beam Scanning Electron Microscopy (fib-sem); labor-intensive semi-automated approach segment larger neuronal processes ,000 cubic \\x0cmicron volume (comprising billion voxels). ] knowledge, challenging dataset largest publicly electron microscopy dataset neuropil ?ground truth? segmentation. experiments, split dataset separate training testing portions axis: training portion comprises-sections 2005?5005, testing portion comprises-sections 5005?8000 (about billion voxels).  Boundary classification oversegmentation obtain image features oversegmentation input agglomeration, trained convolutional neural networks predict, based  voxel image context region, wher center voxel part neurite adjacent voxel directions, prior work. ] optimized parameters network stochastic gradient descent log loss. trained networks, varying hyperparameters amount dilation boundaries training data order increase extracellular space) voxels wher components smaller 10000 voxels excluded. supplementary information description network architecture. connection aﬃnities, applied watershed algorithm] obtain (approximate) oversegmentation. parameters, 1000 voxels.  Energy model architecture types 512-dimensional shape descriptors: pairwise descriptor types 173 333 bounding boxes, center-based descriptor types 173 333 bounding boxes, respectively. connectivity positions bounding boxes descriptor type sampled uniformly random. 512-dimensional fully-connected penultimate layer output low-level classification convolutional neural network image feature vector ). shape descriptor type): concatenated shape architecture local energy model descriptor vector image feature vector obtain 1024-dimensional input vector. 2048-dimensional fully-connected rectified linear hidden layers, logistic output unit, applied dropout (with) hidden layer. effectively computes score raw image patch shape descriptor, segregating expensive convolutional image processing depend shape descriptor, architecture benefit pre-training precomputation intermediate image feature vector ; position training energy models boundary classifier performed asynchronous SGD distributed architecture.  Evaluation compared method state--art agglomeration method GALA], trains random forest classifier predict merge decisions image features derived boundary probabilities. obtain probabilities low-level convolutional neural network classifier, predicts edge aﬃnities adjacent voxels rar per-voxel predic \\x0ctions, compute voxel minimum connection probability voxel-connectivity neighborhood, treat probability/score cell interior. comparison, evaluated watershed procedure applied CNN aﬃnity graph output, varying parameter choices, measure accuracy deep CNN boundary classification agglomeration procedure. finally, evaluated accuracy publicly released automated segmentation fib (referred 7colseg1] basis proofreading process obtain ground truth; produced applying watershed segmentation variant GALA agglomeration predictions made Ilastik]-trained voxel classifier. tested GALA CELIS initial oversegmentations training test regions. compare accuracy reconstructions, computed measures segmentation consistency relative ground truth: Variation Information] Rand score, defined classification score connectivity voxel pairs volumes; primary metrics prior work. ] advantage weighing segments linearly size rar quadratically. agglomeration method ultimately limited quality initial oversegmentation, computed accuracy oracle agglomeration policy greedily optimizes error metric directly. (computing true globally-optimal agglomeration eir metric intractable.) serves (approximate) upper bound separating error due agglomeration error due initial oversegmentation. results Figure shows Pareto optimal trade-offs test set split merge error method obtained varying choice hyperparameters agglomeration thresholds, Variation Information Rand scores obtained training set-optimal hyperparameters. celis consistently outperforms methods significant margin metrics. large gap Oracle results automated reconstruction indicates, however, large room improvement agglomeration. evaluations single dataset, single large dataset; verify improvement due CELIS broad general (rar localized specific part image volume), evaluated accuracy independently non-overlapping 5003 -voxel subvolumes evenly spaced test region. subvolumes CELIS outperformed existing method metrics, median reduction Variation Information error% Rand error%. suggests CELIS improving accuracy parts volume span significant variations shape image characteristics. gala supports multi-channel image features, potentially representing predicted probabilities additional classes, mitochondria, make functionality training data additional classes. discussion introduced celis, framework modeling image segmentations learned energy function specifically exploits combinatorial nature dense segmentation. approach model conditional energy segmentation image, resulting model guide supervoxel agglomeration decisions. experiments challenging microscopy reconstruction problem, CELIS improved volumetric reconstruction accuracy% existing method, offered strictly trade-off split merge errors, wide margin, compared existing methods. experimental results unique scale evaluations-gigavoxel test region orders magnitude larger evaluation prior work, large scale evaluation critically important; found evaluations smaller volumes, short neurite fragments, unreliable predicting accuracy larger volumes (where propagation merge errors major challenge). computationally expensive prior methods, CELIS noneless practical: successfully run CELIS volumes approaching teravoxel matter hours, albeit thousands CPU cores. addition advancing state art learning-based image segmentation, work significant implications application area studied, connectomic reconstruction. fib dataset reﬂects stateof--art techniques sample preparation imaging large-scale neuron reconstruction, highly representative larger datasets actively collected. full adult brain). expect, refore, significant improvements automated reconstruction accuracy made CELIS dataset directly translate decrease human proof-reading effort required reconstruct volume tissue, increase total size neural circuit reconstructed. future work specific areas fruitful: end-end training CELIS energy modeling pipeline, including CNN model computing image feature representation aggregation local energies position scale. existing pipeline fully differentiable, directly amenable end-end training.  integration CELIS energy model discriminative training neural networkbased agglomeration policy. policy depend distribution local energy changes, rar sum, per-object peraction features proposed prior work. , CELIS energy model fixing undersegmentation errors. energy minimization procedure proposed paper based greedy local search limited performing merges, CELIS energy model capable evaluating arbitrary segmentation. evaluation candidate splits (based hierarchical initial segmentation heuristic criteria) potentially robust simulated annealing energy minimization procedure capable splits merges. recent works, integrated deep neu \\x0cral networks pairwise-potential conditional random field models. similar celis, approaches combine deep learning structured prediction, differ CELIS key ways: restriction models factored pairwise potentials, approaches field pseudomarginal approximations perform eﬃcient approximate inference. celis energy model, contrast, sacrifices factorization richer combinatorial modeling provided proposed shape descriptors.  generally, prior CRF methods focused refining predictions. improving boundary localization/detail semantic segmentation) made feed-forward neural network correct high level. contrast, CELIS designed correct fundamental inaccuracy feed-forward convolutional neural network critical cases ambiguity, reﬂected greater complexity structured model. acknowledgments This material based work supported National Science Foun dation Grant. 1118055. Mapping neuroanatomy, pursuit linking hyposized computational models consistent observed functions actual physical structures, long-standing fundamental problem neuroscience. one primary interest mapping network structure neural circuits identifying morphology neuron locations synaptic connections neurons, field called connectomics. currently, promising approach obtaining maps neural circuit structure volume electron microscopy stained fixed block tissue. ] This technique \\x0csuccessfully decades ago mapping structure complete nervous system 302-neuron Caenorhabditis elegans; due manually cut, image, align, trace neuronal processes 8000 serial sections, small circuit required years labor, spent image analysis. ] time, scaling approach larger circuits practical. recent advances volume electron microscopy] make feasible imaging large circuits, potentially hundreds thousands neurons, suﬃcient resolution discern smallest neuronal processes. ] high image quality near-isotropic resolution achievable methods enables resultant data treated true volume, significantly aids reconstruction processes run parallel sectioning axis, potentially amenable automated image processing. 30th Conference Neural Information Processing Systems (nips 2016), barcelona, spain. fully-connected layer Convolutional neural network; Image) voxel positions Boundary classification Global energy Shape descriptors Candidate segmentation) Initial oversegmentation Agglomeration Local energy Figure Illustration computation global energy single candidate segmentation local energy; ], computed deep neural network, summed shape descriptor types voxel positions image analysis remains key challenge, however. primary bottleneck segmenting full volume, filled heavily intertwined neuronal processes, volumes occupied individual neuron. while cell boundaries shown stain provide strong visual cue cases, neurons extend tens centimeters path length places narrow; single mistake path render connectivity information neuron largely inaccurate. existing automated semi-automated segmentation methods suﬃciently reduce amount human labor required: recent reconstruction 950 neurons mouse retina required 20000 hours human labor, eﬃcient method tracing skeleton neuron]; recent reconstruction 379 neurons Drosophila medulla column (part visual pathway) required 12940 hours manual proof-reading/correction automated segmentation]. related work: Algorithmic approaches image segmentation formulated variations pipeline: boundary detection step establishes local hyposes object boundaries, region formation step integrates boundary evidence local regions. superpixels supervoxels), region agglomeration step merges adjacent regions based image object features. , Although extensive integration machine learning pipelines begun yield promising segmentation results], argue pipelines, previously formulated, fundamentally neglect potentially important aspects achieving accurate segmentation) combinatorial nature reasoning dense image segmentation structure) fundamental importance shape criterion segmentation quality. contributions: propose method attempts overcome deficiencies. particular, propose energybased model scores segmentation quality deep neural network ﬂexibly integrates shape image information: Combinatorial Energy Learning Image Segmentation (celis). pursuit model paper makes specific contributions: connectivity region data structure eﬃciently computing energy configurations objects; binary shape descriptor eﬃcient representation shape configurations; neural network architecture splices intermediate unit output trained convolutional network input deep fully-connected neural network architecture scores segmentation image; training procedure pairwise object relations segmentation learn energy-based model. experimental evaluation proposed baseline automated reconstruction methods massive knowledge) unprecedented scale reﬂects true size connectomic datasets required biological analysis (many billions voxels). Conditional energy modeling segmentations images define global, translation-invariant energy model predicting cost complete segmentation image this cost analogous negative While prior work, recognized importance combinatorial reasoning, previously proposed global optimization methods local decisions interact limited way. log-likelihood segmentation image, treat probabilistically. our goal define model true segmentation image found minimizing cost; energy reﬂect prior object configurations alone, compatibility object configurations image. shown fig. define global energy; sum local energy models (defined deep neural network; scales computed sliding-window fashion centered position volume); )) ; local energy; depends local image context centered position vector representation ; computed deep convolutional neural network local shape/object configuration scale local binary shape descriptor), defined Section find (locally) minimal-cost segmentations model, local search space agglomerations starting initial supervoxel segmentation. using simple greedy policy, step agglomeration actions. merges adjacent segments, pick action results lowest energy. ?vely, computing energy single segmentation requires computing shape descriptors evaluating energy model voxel position volume; small volume tens hundreds millions voxels. stage agglomeration, thousands, tens thousands, potential agglomeration steps, results unique segmentation. order choose step, energy potential segmentations. computational cost perform computations directly tremendous, supplement, prove collection orems eﬃcient implementation computes energy terms incrementally. Representing Shape Configurations Local Binary Descriptors propose binary shape descriptor based subsampled pairwise connectivity information: specification pairs position offsets    relative center fixed-size bounding box size-bit binary shape descriptor segmentation bounding box defined connected ]. orwise. shown fig. , bit descriptor specifies wher pair positions part segment, determined constant time suitable data structure. limit case, list pairs positions-voxel bounding box, information lost Hamming distance descriptors precisely equal Rand index. ] general sample subset pairs possible; sample uniformly random, retain property expected Hamming distance descriptors equal Rand index. found picking 512 bits reasonable tradeoff fidelity representation size. while pairs randomly sampled initially, naturally obtain consistent results learning models based descriptors fixed list positions defining descriptor training test time. Note descriptor serves general type sketch full segmentation bounding box. restricting positions pair center position bounding box, obtain sketch single segment center position. refer descriptor case center-based, general case pairwise, shown fig. . shape descriptors represent local sub-regions segmentation. represent shape information large volume, compute shape descriptors densely positions sliding \\x0cwindow fashion, shown fig. . BRIEF descriptor] similarly defined binary descriptor based subset pairs points patch, bit based intensity difference, rar connectivity, pair. ... 100000000110   10000000011000000110100000101001) Sequence showing computation shape descriptor. 00001000001011100111100100001000 00000000000101110000010000110010 10001001101100010100000010000111) Shape descriptors computed multiple scales. pairwise descriptors (shown left center) arbitrary pairwise connectivity, centerbased shape descriptors (shown right) restrict position pair center point. 10000001110010100110100001011001 11000011110011100100100011011011 10000011100111100100110011011111) Shape descriptors computed densely position volume. figure Illustration shape descriptors. connected components bounding box descriptor computed shown distinct colors. pairwise connectivity relationships define descriptor dashed lines; connected pairs shown white, disconnected pairs shown black. connectivity determined based connected components underlying segmentation, geometry line itself. while illustration, experiments shape descriptors computed fully. Connectivity Regions defined, single shape descriptor represents segmentation fixed-size bounding box; shifting position bounding box obtain descriptors local regions larger segmentation. size bounding box determines scale local representation. this raises question connectivity defined local regions. two voxels connected long path descriptor bounding box. shape descriptors consistent local topology, pairs considered disconnected. shape descriptors are, refore, defined respect connectivity larger connectivity region, necessarily descriptor bounding boxes general significantly smaller full segmentation; conceptually, shape descriptor bounding box slides positions contained connectivity region. (this sliding necessarily results minor inconsistency context positions, reduces computational memory costs.) obtain shape descriptors positions, simply tile space overlapping rectangular connectivity regions uniform size stride, \\x0cshown supplement. connectivity region size determines degree locality connectivity information captured shape descriptor (independent descriptor bounding box size). affects computational costs, supplement. Energy model learning; shape descriptor type/scale learned neural define local energy model network model computes real-valued score, shape descriptor image feature vector simplify presentation, define notation forward discrete derivative respect) ). based notation, discrete derivative energy function ), denotes result merging supervoxels existing segmentation agglomerate, greedy policy simply chooses step action minimizes), denotes current segmentation step prior work], treat classification problem, goal matching sign error change segmentation error respect ground truth segmentation measured Variation Information].  Local training procedure Because term simply sum change energies position; descriptor type heuristic optimize parameters energy model independently shape descriptor type/scale seek minimize expectation); )))+ ‘(?esii error ; ))) ‘(? ?esii error indexes training examples correspond sam, pled position merge action applied segmentation denotes binary classification loss function, , predicted probability true label positive, weighted—. note ?esii error action improved score refore low predicted score post-merge descriptor high predicted score pre-merge descriptor ?esii error opposite applies. tested standard log loss—  log log )], signed linear loss, closely matches; terms contribute; scores. stochastic gradient descent (sgd) perform optimization. obtain training examples agglomerating expert policy greedily optimizes error  segmentation state agglomeration step (including initial state), agglomeration action position volume, compute shape descriptor pair) reﬂecting pre-merge post-merge states, respectively. ), emit training descriptor pair. reby obtain \\x0cconceptual stream examples error  . this stream examples billions examples (and highly correlated), required learn parameters reduce resource requirements, priority sampling], based error, )—, obtain fixed number weighted samples replacement descriptor type equalize total weight true merge examples error, false merge examples error, order avoid learning degenerate models Experiments tested approach large, publicly electron microscopy dataset, called Janelia fib25, portion Drosophila melangaster optic lobe. dataset collected  For example, weight false merge examples, occur balancing, model simply learn assign score increases number bits shape descriptor.  Split error  Rand CELIS (this paper-cnn+gala-cnn+watershed 7colseg1.672.069.143.981.691.597.629.099 Oracle.428.901 Merge error)) Figure Segmentation accuracy-gigavoxel fib test set. left: Pareto frontiers information-oretic split/merge error, previously evaluate segmentation accuracy. ] right: Comparison Variation Information (lower better) Rand score (higher better). for celis, 3dcnn+gala-cnn+watershed, hyperparameters optimized metric training set. resolution Focused Ion Beam Scanning Electron Microscopy (fib-sem); labor-intensive semi-automated approach segment larger neuronal processes ,000 cubic \\x0cmicron volume (comprising billion voxels). ] knowledge, challenging dataset largest publicly electron microscopy dataset neuropil ?ground truth? segmentation. for experiments, split dataset separate training testing portions axis: training portion comprises-sections 2005?5005, testing portion comprises-sections 5005?8000 (about billion voxels).  Boundary classification oversegmentation obtain image features oversegmentation input agglomeration, trained convolutional neural networks predict, based  voxel image context region, wher center voxel part neurite adjacent voxel directions, prior work. ] optimized parameters network stochastic gradient descent log loss. trained networks, varying hyperparameters amount dilation boundaries training data order increase extracellular space) voxels wher components smaller 10000 voxels excluded. see supplementary information description network architecture. using connection aﬃnities, applied watershed algorithm] obtain (approximate) oversegmentation. parameters, 1000 voxels.  Energy model architecture types 512-dimensional shape descriptors: pairwise descriptor types 173 333 bounding boxes, center-based descriptor types 173 333 bounding boxes, respectively. connectivity positions bounding boxes descriptor type sampled uniformly random. 512-dimensional fully-connected penultimate layer output low-level classification convolutional neural network image feature vector ). for shape descriptor type): concatenated shape architecture local energy model descriptor vector image feature vector obtain 1024-dimensional input vector. 2048-dimensional fully-connected rectified linear hidden layers, logistic output unit, applied dropout (with) hidden layer. while effectively computes score raw image patch shape descriptor, segregating expensive convolutional image processing depend shape descriptor, architecture benefit pre-training precomputation intermediate image feature vector ; position training energy models boundary classifier performed asynchronous SGD distributed architecture.  Evaluation compared method state--art agglomeration method GALA], trains random forest classifier predict merge decisions image features derived boundary probabilities. obtain probabilities low-level convolutional neural network classifier, predicts edge aﬃnities adjacent voxels rar per-voxel predic \\x0ctions, compute voxel minimum connection probability voxel-connectivity neighborhood, treat probability/score cell interior. for comparison, evaluated watershed procedure applied CNN aﬃnity graph output, varying parameter choices, measure accuracy deep CNN boundary classification agglomeration procedure. finally, evaluated accuracy publicly released automated segmentation fib (referred 7colseg1] basis proofreading process obtain ground truth; produced applying watershed segmentation variant GALA agglomeration predictions made Ilastik]-trained voxel classifier. tested GALA CELIS initial oversegmentations training test regions. compare accuracy reconstructions, computed measures segmentation consistency relative ground truth: Variation Information] Rand score, defined classification score connectivity voxel pairs volumes; primary metrics prior work. ] advantage weighing segments linearly size rar quadratically. because agglomeration method ultimately limited quality initial oversegmentation, computed accuracy oracle agglomeration policy greedily optimizes error metric directly. (computing true globally-optimal agglomeration eir metric intractable.) this serves (approximate) upper bound separating error due agglomeration error due initial oversegmentation. Results Figure shows Pareto optimal trade-offs test set split merge error method obtained varying choice hyperparameters agglomeration thresholds, Variation Information Rand scores obtained training set-optimal hyperparameters. celis consistently outperforms methods significant margin metrics. large gap Oracle results automated reconstruction indicates, however, large room improvement agglomeration. while evaluations single dataset, single large dataset; verify improvement due CELIS broad general (rar localized specific part image volume), evaluated accuracy independently non-overlapping 5003 -voxel subvolumes evenly spaced test region. subvolumes CELIS outperformed existing method metrics, median reduction Variation Information error% Rand error%. this suggests CELIS improving accuracy parts volume span significant variations shape image characteristics. GALA supports multi-channel image features, potentially representing predicted probabilities additional classes, mitochondria, make functionality training data additional classes. Discussion introduced celis, framework modeling image segmentations learned energy function specifically exploits combinatorial nature dense segmentation. approach model conditional energy segmentation image, resulting model guide supervoxel agglomeration decisions. experiments challenging microscopy reconstruction problem, CELIS improved volumetric reconstruction accuracy% existing method, offered strictly trade-off split merge errors, wide margin, compared existing methods. experimental results unique scale evaluations-gigavoxel test region orders magnitude larger evaluation prior work, large scale evaluation critically important; found evaluations smaller volumes, short neurite fragments, unreliable predicting accuracy larger volumes (where propagation merge errors major challenge). while computationally expensive prior methods, CELIS noneless practical: successfully run CELIS volumes approaching teravoxel matter hours, albeit thousands CPU cores. addition advancing state art learning-based image segmentation, work significant implications application area studied, connectomic reconstruction. fib dataset reﬂects stateof--art techniques sample preparation imaging large-scale neuron reconstruction, highly representative larger datasets actively collected. full adult brain). expect, refore, significant improvements automated reconstruction accuracy made CELIS dataset directly translate decrease human proof-reading effort required reconstruct volume tissue, increase total size neural circuit reconstructed. future work specific areas fruitful: end-end training CELIS energy modeling pipeline, including CNN model computing image feature representation aggregation local energies position scale. because existing pipeline fully differentiable, directly amenable end-end training.  integration CELIS energy model discriminative training neural networkbased agglomeration policy. such policy depend distribution local energy changes, rar sum, per-object peraction features proposed prior work. , use CELIS energy model fixing undersegmentation errors. while energy minimization procedure proposed paper based greedy local search limited performing merges, CELIS energy model capable evaluating arbitrary segmentation. evaluation candidate splits (based hierarchical initial segmentation heuristic criteria) potentially robust simulated annealing energy minimization procedure capable splits merges. several recent works, integrated deep neu \\x0cral networks pairwise-potential conditional random field models. similar celis, approaches combine deep learning structured prediction, differ CELIS key ways: through restriction models factored pairwise potentials, approaches field pseudomarginal approximations perform eﬃcient approximate inference. CELIS energy model, contrast, sacrifices factorization richer combinatorial modeling provided proposed shape descriptors.  more generally, prior CRF methods focused refining predictions. improving boundary localization/detail semantic segmentation) made feed-forward neural network correct high level. contrast, CELIS designed correct fundamental inaccuracy feed-forward convolutional neural network critical cases ambiguity, reﬂected greater complexity structured model. acknowledgments This material based work supported National Science Foun dation Grant. 1118055.',\n",
       " 'PP6685': 'label distribution learning (ldl] learning framework deal problems label ambiguity. unlike single-label learning (sll) multi-label learning (mll], assume instance assigned single label multiple labels, LDL aims learning relative importance label involved description instance., distribution set labels. learning strategy suitable real-world problems, label ambiguity. facial age estimation]. humans predict precise age single facial image. person age group anor. natural assign distribution age labels facial image \\x0c(fig. )) single age label. anor movie rating prediction]. famous movie review web sites, netﬂix, IMDb douban, provide crowd opinion movie distribution ratings collected users (fig. )). system precisely predict rating distribution movie released, movie producers reduce investment risk audience choose movies watch. LDL methods assume label distribution represented maximum entropy model] learn optimizing energy function based model]. but, exponential part model restricts generality distribution form., diﬃculty representing mixture distributions. LDL methods extend existing learning algorithms, boosting support vector regression, deal label distributions], avoid making assumption, limitations representation learning., learn deep features end-end manner. 31st Conference Neural Information Processing Systems (nips 2017), Long beach, usa. figure real-world data suitable modeled label distribution learning. ) Estimated facial ages unimodal distribution). ) Rating distribution crowd opinion movie multimodal distribution). paper, present label distribution learning forests (ldlfs) label distribution learning algorithm inspired differentiable decision trees]. extending differentiable decision trees deal LDL task advantages. decision trees potential model general form label distributions mixture leaf node predictions, avoid making strong assumption form label distributions. split node parameters differentiable decision trees learned back-propagation, enables combination tree learning representation learning end-end manner. define distributionbased loss function tree kullback-leibler divergence) ground truth label distribution distribution predicted tree. fixing split nodes, show optimization leaf node predictions minimize loss function tree addressed variational bounding], original loss function minimized iteratively replaced decreasing sequence upper bounds. optimization strategy, derive discrete iterative function update leaf node predictions. learn forest, average losses individual trees loss forest split nodes trees connected output unit feature learning function. way, split node parameters individual trees learned jointly. LDLFs (shallow) stand-alone model, integrated deep networks., feature learning function linear transformation deep network, respectively. fig. illustrates sketch chart ldlfs, forest consists trees shown. verify effectiveness model LDL tasks, crowd opinion prediction movies disease prediction based human genes, computer vision application., facial age estimation, showing significant improvements \\x0cstate--art LDL methods. label distributions tasks include unimodal distributions., age distribution fig. )) mixture distributions rating distribution movie fig. )). superiority model verifies ability model general form label distributions Figure Illustration label distribution learning forest. top circles denote output units function parameterized feature vector fully-connected layer deep network. blue green circles split nodes leaf nodes, respectively. index function assigned trees respectively. black dash arrows correspondence split nodes trees output units function note that, output unit correspond split nodes belonging trees. tree independent leaf node predictions (denoted histograms leaf nodes). output forest mixture tree predictions.   learned jointly end-end manner. related Work Since LDL algorithm inspired differentiable decision trees, review typical techniques decision trees. discuss current LDL methods. decision trees. random forests randomized decision trees], popular ensemble predictive model suitable machine learning tasks. past, learning decision tree based heuristics greedy algorithm locally-optimal hard decisions made split node], thus, integrated deep learning framework., combined representation learning endto-end manner. newly proposed deep neural decision forests (dndfs] overcomes problem introducing soft differentiable decision function split nodes global loss function defined tree. ensures split node parameters learned back-propagation leaf node predictions updated discrete iterative function. method extends dndfs address LDL problems, extension non-trivial, learning leaf node predictions constrained convex optimization problem. step-size free update function dndfs update leaf node predictions, proved converge classification loss. consequently, unclear obtain update function losses. observed, however, update function dndfs derived variational bounding, extend LDL loss. addition, strategies LDLFs dndfs learning ensemble multiple trees (forests) different: explicitly define loss function forests, loss function single tree defined dndfs; split nodes trees connected output unit feature learning function, dndfs not; trees LDLFs learned jointly, trees dndfs learned alternatively. ensemble learning important, shown experiments (sec. ), LDLFs results trees, ensemble strategy proposed dndfs, results forests worse single tree. sum. dndfs], contributions LDLFs are: first, extend classification] distribution learning proposing distribution-based loss forests derive gradient learn splits nodes. loss; second, derived update function leaf nodes variational bounding (having observed update function] special case variational bounding); least, propose strategies learning ensemble multiple trees], show effective. label distribution learning. number specialized algorithms proposed address LDL task, shown effectiveness computer vision applications, facial age estimation], expression recognition] hand orientation estimation]. geng. ] defined label distribution instance vector probabilities instance label. gave strategy assign proper label distribution instance single label., assigning Gaussian Triangle distribution peak single label, proposed algorithm called iis-lld, iterative optimization process based two-layer energy based model. yang. ] defined three-layer energy based model, called sce-ldl, ability perform feature learning improved adding extra hidden layer sparsity constraints incorporated ameliorate model. geng] developed accelerated version iis-lld, called bfgsldl, quasi-newton optimization. LDL methods assume label distribution represented maximum entropy model], exponential part model restricts generality distribution form. anor address LDL task, extend existing learning algorithms deal label distributions. geng Hou] proposed ldsvr, LDL method extending support vector regressor, fit sigmoid function component distribution simultaneously support vector machine. xing. ] extended boosting address LDL task additive weighted regressors. showed vector tree model weak regressor lead performance named method aoso-ldllogitboost. learning tree model based locallyoptimal hard data partition functions split node, aoso-ldllogitboost unable combined representation learning. extending current deep learning algorithms address LDL task interesting topic. but, existing method, called DLDL], focuses maximum entropy model based ldl. method, ldlfs, extends differentiable decision trees address LDL tasks, predicted label distribution sample expressed linear combination label distributions training data, restrictions distributions., requirement maximum entropy model). addition, introduction differentiable decision functions, LDLFs combined representation learning., learn deep features end-end manner. label Distribution Learning Forests forest ensemble decision trees. introduce learn single decision tree label distribution learning, describe learning forest.     problem Formulation Let denote input space denote complete set labels, number label values. label distribution learning (ldl) problem, input sample label distribution (dxy1 dyx2 dyxc  dyxc expresses probability sample label constraints dyxc  dyxc goal LDL problem learn mapping function input sample label distribution here, learn mapping function) decision tree based model decision tree consists set split nodes set leaf nodes Each split node defines split function   , parameterized determine wher sample left subtree. leaf node holds distribution     build differentiable decision tree], probabilistic split function; ? ; )), ?(?) sigmoid function, ?(?) index function bring  output function; correspondence split node real-valued feature learning function depending sample parameter form. simple form, linear transformation transformation matrix; For complex form, deep network perform representation learning end-end manner, network parameter. correspondence split nodes output units function ?(?) randomly generated tree learning., output units constructing tree determined randomly. demonstrate ?(?) shown fig.  probability sample falling leaf node; ;  ; (?) indicator function Lln Lrn denote sets leaf nodes held left subtrees node Tnl Tnr respectively. output tree. ., mapping function defined; ‘  Tree Optimization Given training set goal learn decision tree sec.  output distribution similar sample end, straightforward minimize kullback-leibler) divergence equivalently minimize cross-entropy loss,  dxi log dxi log  denote distributions held leaf nodes output unit learning tree requires estimation parameters: split node parameter distributions held leaf nodes. parameters (??  determined (??  arg min, ).  solve eqn. alternating optimization strategy: first, fix optimize fix optimize learning steps alternatively performed, convergence maximum number iterations reached (defined experiments).  Learning Split Nodes section, describe learn parameter split nodes, distributions held leaf nodes fixed. compute gradient loss, .  chain rule, , ?  ) ?    term depends tree term depends specific type function ) term Tnl Tnr, dxi    ?   Tnl ‘?lln  Tnr ‘?lrn  note that, tree rooted node Tnl Tnr means gradient computation eqn. started leaf nodes carried bottom manner. thus, split node parameters learned standard back-propagation.  Learning Leaf Nodes now, fixing parameter show learn distributions held leaf nodes constrained optimization problem: min,   here, propose address constrained convex optimization problem variational bounding], leads step-size free fast-converged update rule variational bounding, original objective function minimized replaced bound iterative manner. upper bound loss function, obtained jensen inequality,  dxi log    ,  dyxci  log  ) define dxi  log   upper bound, ), property , ,  , ), , ). assume point iteration, ) upper bound, ). iteration) chosen ) ) ), implies)  ) ). consequently, minimize , , ensuring) )  .,  ) arg min  leads minimizing Lagrangian defined )  Lagrange multiplier. setting) Note that) , distributions held leaf nodes. starting) distribution    satisfies) dxi ) ) eqn. update scheme) point simply initialized uniform Learning Forest forest ensemble decision trees  training stage, trees forest parameters feature learning function  (but correspond output units assigned fig. ), tree independent leaf node predictions loss function forest averaging loss functions individual trees RTk RTk loss function tree defined eqn.  learn fixing leaf node predictions trees forest based derivation sec.  referring fig.   ?rtk      (?) split node set index function respectively. note that, index function (?) tree randomly assigned tree learning, split nodes correspond subset output units strategy similar random subspace method], increases randomness training reduce risk overfitting. tree forest leaf node predictions update independently eqn. , implementational convenience, conduct update scheme dataset set mini-batches training procedure LDLF shown algorithm.  algorithm training procedure ldlf. require: training set, number mini-batches update Initialize randomly uniformly, set {?} not converge— Randomly select mini-batch UpdateS computing gradient (eqn.  end Update iterating eqn. {?} end testing stage, output forest averaging predictions individual trees; experimental Results Our realization LDLFs based ?caffe? ]. modular implemented standard neural network layer. eir shallow stand-alone model (sldlfs) integrate deep networks (dldlfs). evaluate sldlfs LDL tasks compare standalone LDL methods. dldlfs learned raw image data end-end manner, verify dldlfs computer vision application., facial age estimation. default settings parameters forests are: tree number), tree depth), output unit number feature learning function), iteration times update leaf node predictions), number mini-batches update leaf node predictions (100), maximum iteration (25000).  \\x0ccomparison sldlfs stand-alone LDL Methods compare shallow model sldlfs state--art standalone LDL methods. sldlfs, feature learning function, linear transformation output unit column transformation matrix popular LDL datasets], movie, Human Gene Natural Scene1 samples datasets represented numerical descriptors, ground truths rating distributions crowd opinion movies, diseases distributions related human genes label distributions scenes, plant, sky cloud, respectively. label distributions datasets mixture distributions, rating distribution shown fig. ). ], measures evaluate performances LDL methods, compute average similarity/distance predicted rating distributions real rating distributions, including distance measures, euclidean?rensen, Squared similarity measures (fidelity, intersection). evaluate shallow model sldlfs datasets compare state--art stand-alone LDL methods. results sldlfs competitors summarized Table Movie quote results reported], code] publicly available. results ors two, run code authors made available. case], split dataset fixed folds standard ten-fold cross validation, represents result ?mean?standard deviation? matters training testing data divided. table sldlfs perform measures. table Comparison results LDL datasets]. ??? ??? larger smaller better, respectively. dataset Method euclidean ?rensen squared fidelity intersection movie sldlf (ours) aoso-ldlogitboost] LDLogitBoost] LDSVR] BFGS LDL] iis-ldl.073.005.086.004.090.004.092.005.099.004.129.007.133.003.155.003.159.003.158.004.167.004.187.004.130.003.152.003.155.003.156.004.164.003.183.004.070.004.084.003.088.003.088.004.096.004.120.005.981.001.978.001.977.001.977.001.974.001.967.001.870.003.848.003.845.003.844.004.836.003.817.004 sldlf (ours) LDSVR] bfgs-ldl] iis-ldl.228.006.245.019.231.021.239.018.085.002.099.005.076.006.089.006.212.002.229.015.231.012.253.009.179.004.189.021.211.018.205.012.948.001.940.006.938.008.944.003.788.002.771.015.769.012.747.009 sldlf (ours) LDSVR] bfgs-ldl] iis-ldl.534.013.852.023.856.061.879.023.317.014.511.021.475.029.458.014.336.010.492.016.508.026.539.011.448.017.595.026.716.041.792.019.824.008.813.008.722.021.686.009.664.010.509.016.492.026.461.011 Human Gene Natural Scene Evaluation dldlfs Facial Age Estimation literature], age estimation formulated LDL problem. conduct facial age estimation experiments Morph,000 facial images,000 people races. facial image annotated chronological age. generate age distribution face image, follow strategy], Gaussian distribution chronological age face image (fig. )). predicted age face image simply age highest probability predicted download datasets http://cse.seu.edu/people/xgeng/ldl/index.htm. label distribution. performance age estimation evaluated absolute error (mae) predicted ages chronological ages. current state--art result Morph obtain fine-tuning DLDL] vgg-face], build dldlf vgg-face, replacing softmax layer VGGNet ldlf. ], standard tenfold cross validation results summarized table. shows dldlf achieve state--art performance morph. note that, significant performance gain deep LDL models (dldl dldlf) non-deep LDL models (iis-ldl, cpnn, bfgs-ldl) superiority dldlf compared DLDL verifies effectiveness end-end learning tree-based model ldl, respectively. table MAE age estimation comparison Morph]. method iis-ldl] CPNN] bfgs-ldl] dldl+vgg-face] dldlf+vgg-face (ours) MAE distribution gender ethnicity unbalanced morph, age estimation methods] evaluated subset morph, called Morph Sub short, consists,160 selected facial images avoid inﬂuence unbalanced distribution. performance reported Morph Sub D2LDL], data-dependent LDL method. d2ldl output ?fc7? layer AlexNet] face image features, integrate LDLF alexnet. experiment setting d2ldl, evaluate dldlf competitors, including SLL LDL based methods, training set ratios%). competitors trained deep features d2ldl. table dldlfs significantly outperform ors training set ratios. note that, generated age distrifigure MAE age estimation comparison butions unimodal distributions Morph sub. label distributions Training set ratio Method sec.  mixture distributions. % proposed method LDLFs achieve AAS.9081.7616.6507.5553.4690.4061 state--art results LARR.7501.6112.5131.4273.3500.2949 iis-aldl.1791.1683.1228.1107.1024.0902 verifies model D2LDL.1080.9857.9204.8712.8560.8385 ability model general dldlf (ours.8495.6220.3991.2401.1917.1224 form label distributions.  Time Complexity Let tree depth batch size, respectively. tree split nodes leaf nodes.   tree sample, complexity forward pass backward pass), respectively. trees batches, complexity forward backward pass complexity iteration update leaf nodes thus, complexity training procedure (one epoch, batches) testing procedure (one sample), respectively. ldlfs eﬃcient: Morph Sub (12636 training images, 8424 testing images), model takes 5250s training (25000 iterations) testing 8424 images.  Parameter Discussion Now discuss inﬂuence parameter settings performance. report results rating prediction Movie (measured) age estimation Morph Sub% training set ratio (measured mae) parameter settings section. tree number. forest ensemble model, investigate performances change varying tree number forest. note that, discussed sec. ensemble strategy learn forest proposed dndfs] ours. refore, ensemble strategy learn forest. end, replace ensemble strategy dldlfs dndfs, method dndfs-ldl. shallow \\x0cmodel named sndfs-ldl. fix parameters., tree depth output unit number feature learning function, default setting. shown fig. ), ensemble strategy improve performance trees, dndfs leads worse performance single tree. observed fig. performance LDLFs improved trees, improvement increasingly smaller smaller. refore, larger ensembles yield big improvement movie, number trees 100.070.071). note that, random forests based methods large number trees., Shotton. ] obtained good pose estimation results depth images decision trees. tree depth. tree depth anor important parameter decision trees. ldlfs, implicit constraint tree depth output unit number feature learning function     discuss inﬂuence tree depth performance dldlfs, set  fix tree number performance change varying tree depth shown fig. ). performance improves decreases increase tree depth. reason tree depth increases, dimension learned features increases exponentially, greatly increases training diﬃculty. larger depths lead bad performance movie, tree depth.1162.0831). figure performance change age estimation Morph Sub rating prediction Movie varying) tree number) tree depth. approach (dldlfs/sldlfs) improve performance trees, ensemble strategy proposed dndfs (dndfsldl/sndfsldl) leads worse performance single tree. conclusion present label distribution learning forests, label distribution learning algorithm inspired differentiable decision trees. defined distributionbased loss function forests found leaf node predictions optimized variational bounding, enables trees feature learned jointly end-end manner. experimental results showed superiority algorithm LDL tasks related computer vision application, verified model ability model general form label distributions. acknowledgement. work supported part National Natural Science Foundation China. 61672336, part ?chen guang? project supported Shanghai Municipal Education Commission Shanghai Education Development Foundation. 15cg43 part ONR n00014-2356. Label distribution learning (ldl] learning framework deal problems label ambiguity. unlike single-label learning (sll) multi-label learning (mll], assume instance assigned single label multiple labels, LDL aims learning relative importance label involved description instance., distribution set labels. such learning strategy suitable real-world problems, label ambiguity. facial age estimation]. even humans predict precise age single facial image. person age group anor. hence natural assign distribution age labels facial image \\x0c(fig. )) single age label. anor movie rating prediction]. many famous movie review web sites, netﬂix, IMDb douban, provide crowd opinion movie distribution ratings collected users (fig. )). system precisely predict rating distribution movie released, movie producers reduce investment risk audience choose movies watch. many LDL methods assume label distribution represented maximum entropy model] learn optimizing energy function based model]. but, exponential part model restricts generality distribution form., diﬃculty representing mixture distributions. some LDL methods extend existing learning algorithms, boosting support vector regression, deal label distributions], avoid making assumption, limitations representation learning., learn deep features end-end manner. 31st Conference Neural Information Processing Systems (nips 2017), Long beach, usa. figure real-world data suitable modeled label distribution learning. ) Estimated facial ages unimodal distribution). ) Rating distribution crowd opinion movie multimodal distribution). paper, present label distribution learning forests (ldlfs) label distribution learning algorithm inspired differentiable decision trees]. extending differentiable decision trees deal LDL task advantages. one decision trees potential model general form label distributions mixture leaf node predictions, avoid making strong assumption form label distributions. split node parameters differentiable decision trees learned back-propagation, enables combination tree learning representation learning end-end manner. define distributionbased loss function tree kullback-leibler divergence) ground truth label distribution distribution predicted tree. fixing split nodes, show optimization leaf node predictions minimize loss function tree addressed variational bounding], original loss function minimized iteratively replaced decreasing sequence upper bounds. following optimization strategy, derive discrete iterative function update leaf node predictions. learn forest, average losses individual trees loss forest split nodes trees connected output unit feature learning function. way, split node parameters individual trees learned jointly. our LDLFs (shallow) stand-alone model, integrated deep networks., feature learning function linear transformation deep network, respectively. fig. illustrates sketch chart ldlfs, forest consists trees shown. verify effectiveness model LDL tasks, crowd opinion prediction movies disease prediction based human genes, computer vision application., facial age estimation, showing significant improvements \\x0cstate--art LDL methods. label distributions tasks include unimodal distributions., age distribution fig. )) mixture distributions rating distribution movie fig. )). superiority model verifies ability model general form label distributions Figure Illustration label distribution learning forest. top circles denote output units function parameterized feature vector fully-connected layer deep network. blue green circles split nodes leaf nodes, respectively. two index function assigned trees respectively. black dash arrows correspondence split nodes trees output units function note that, output unit correspond split nodes belonging trees. each tree independent leaf node predictions (denoted histograms leaf nodes). output forest mixture tree predictions.   learned jointly end-end manner. Related Work Since LDL algorithm inspired differentiable decision trees, review typical techniques decision trees. discuss current LDL methods. decision trees. random forests randomized decision trees], popular ensemble predictive model suitable machine learning tasks. past, learning decision tree based heuristics greedy algorithm locally-optimal hard decisions made split node], thus, integrated deep learning framework., combined representation learning endto-end manner. newly proposed deep neural decision forests (dndfs] overcomes problem introducing soft differentiable decision function split nodes global loss function defined tree. this ensures split node parameters learned back-propagation leaf node predictions updated discrete iterative function. our method extends dndfs address LDL problems, extension non-trivial, learning leaf node predictions constrained convex optimization problem. although step-size free update function dndfs update leaf node predictions, proved converge classification loss. consequently, unclear obtain update function losses. observed, however, update function dndfs derived variational bounding, extend LDL loss. addition, strategies LDLFs dndfs learning ensemble multiple trees (forests) different: explicitly define loss function forests, loss function single tree defined dndfs; split nodes trees connected output unit feature learning function, dndfs not; trees LDLFs learned jointly, trees dndfs learned alternatively. ensemble learning important, shown experiments (sec. ), LDLFs results trees, ensemble strategy proposed dndfs, results forests worse single tree. sum. dndfs], contributions LDLFs are: first, extend classification] distribution learning proposing distribution-based loss forests derive gradient learn splits nodes. loss; second, derived update function leaf nodes variational bounding (having observed update function] special case variational bounding); least, propose strategies learning ensemble multiple trees], show effective. label distribution learning. number specialized algorithms proposed address LDL task, shown effectiveness computer vision applications, facial age estimation], expression recognition] hand orientation estimation]. geng. ] defined label distribution instance vector probabilities instance label. gave strategy assign proper label distribution instance single label., assigning Gaussian Triangle distribution peak single label, proposed algorithm called iis-lld, iterative optimization process based two-layer energy based model. yang. ] defined three-layer energy based model, called sce-ldl, ability perform feature learning improved adding extra hidden layer sparsity constraints incorporated ameliorate model. geng] developed accelerated version iis-lld, called bfgsldl, quasi-newton optimization. all LDL methods assume label distribution represented maximum entropy model], exponential part model restricts generality distribution form. anor address LDL task, extend existing learning algorithms deal label distributions. geng Hou] proposed ldsvr, LDL method extending support vector regressor, fit sigmoid function component distribution simultaneously support vector machine. xing. ] extended boosting address LDL task additive weighted regressors. showed vector tree model weak regressor lead performance named method aoso-ldllogitboost. learning tree model based locallyoptimal hard data partition functions split node, aoso-ldllogitboost unable combined representation learning. extending current deep learning algorithms address LDL task interesting topic. but, existing method, called DLDL], focuses maximum entropy model based ldl. our method, ldlfs, extends differentiable decision trees address LDL tasks, predicted label distribution sample expressed linear combination label distributions training data, restrictions distributions., requirement maximum entropy model). addition, introduction differentiable decision functions, LDLFs combined representation learning., learn deep features end-end manner. label Distribution Learning Forests forest ensemble decision trees. introduce learn single decision tree label distribution learning, describe learning forest.     problem Formulation Let denote input space denote complete set labels, number label values. label distribution learning (ldl) problem, input sample label distribution (dxy1 dyx2 dyxc  here dyxc expresses probability sample label constraints dyxc  dyxc goal LDL problem learn mapping function input sample label distribution here, learn mapping function) decision tree based model decision tree consists set split nodes set leaf nodes Each split node defines split function   , parameterized determine wher sample left subtree. each leaf node holds distribution     build differentiable decision tree], probabilistic split function; ? ; )), ?(?) sigmoid function, ?(?) index function bring  output function; correspondence split node real-valued feature learning function depending sample parameter form. for simple form, linear transformation transformation matrix; For complex form, deep network perform representation learning end-end manner, network parameter. correspondence split nodes output units function ?(?) randomly generated tree learning., output units constructing tree determined randomly. demonstrate ?(?) shown fig.  probability sample falling leaf node; ;  ; (?) indicator function Lln Lrn denote sets leaf nodes held left subtrees node Tnl Tnr respectively. output tree. ., mapping function defined; ‘  Tree Optimization Given training set goal learn decision tree sec.  output distribution similar sample end, straightforward minimize kullback-leibler) divergence equivalently minimize cross-entropy loss,  dxi log dxi log  denote distributions held leaf nodes output unit learning tree requires estimation parameters: split node parameter distributions held leaf nodes. parameters (??  determined (??  arg min, ).  solve eqn. alternating optimization strategy: first, fix optimize fix optimize learning steps alternatively performed, convergence maximum number iterations reached (defined experiments).  Learning Split Nodes section, describe learn parameter split nodes, distributions held leaf nodes fixed. compute gradient loss, .  chain rule, , ?  ) ?    term depends tree term depends specific type function ) term Tnl Tnr, dxi    ?   Tnl ‘?lln  Tnr ‘?lrn  note that, tree rooted node Tnl Tnr this means gradient computation eqn. started leaf nodes carried bottom manner. thus, split node parameters learned standard back-propagation.  Learning Leaf Nodes now, fixing parameter show learn distributions held leaf nodes constrained optimization problem: min,   here, propose address constrained convex optimization problem variational bounding], leads step-size free fast-converged update rule variational bounding, original objective function minimized replaced bound iterative manner. upper bound loss function, obtained jensen inequality,  dxi log    ,  dyxci  log  ) define dxi  log   upper bound, ), property , ,  , ), , ). assume point iteration, ) upper bound, ). iteration) chosen ) ) ), implies)  ) ). consequently, minimize , , ensuring) )  .,  ) arg min  leads minimizing Lagrangian defined )  Lagrange multiplier. setting) Note that) , distributions held leaf nodes. starting) distribution    satisfies) dxi ) ) eqn. update scheme) point simply initialized uniform Learning Forest forest ensemble decision trees  training stage, trees forest parameters feature learning function  (but correspond output units assigned fig. ), tree independent leaf node predictions loss function forest averaging loss functions individual trees RTk RTk loss function tree defined eqn.  learn fixing leaf node predictions trees forest based derivation sec.  referring fig.   ?rtk      (?) split node set index function respectively. note that, index function (?) tree randomly assigned tree learning, split nodes correspond subset output units this strategy similar random subspace method], increases randomness training reduce risk overfitting. tree forest leaf node predictions update independently eqn. , for implementational convenience, conduct update scheme dataset set mini-batches training procedure LDLF shown algorithm.  algorithm training procedure ldlf. require: training set, number mini-batches update Initialize randomly uniformly, set {?} Not converge— Randomly select mini-batch UpdateS computing gradient (eqn.  end Update iterating eqn. {?} end testing stage, output forest averaging predictions individual trees; Experimental Results Our realization LDLFs based ?caffe? ]. modular implemented standard neural network layer. eir shallow stand-alone model (sldlfs) integrate deep networks (dldlfs). evaluate sldlfs LDL tasks compare standalone LDL methods. dldlfs learned raw image data end-end manner, verify dldlfs computer vision application., facial age estimation. default settings parameters forests are: tree number), tree depth), output unit number feature learning function), iteration times update leaf node predictions), number mini-batches update leaf node predictions (100), maximum iteration (25000).  \\x0ccomparison sldlfs stand-alone LDL Methods compare shallow model sldlfs state--art standalone LDL methods. for sldlfs, feature learning function, linear transformation output unit column transformation matrix popular LDL datasets], movie, Human Gene Natural Scene1 samples datasets represented numerical descriptors, ground truths rating distributions crowd opinion movies, diseases distributions related human genes label distributions scenes, plant, sky cloud, respectively. label distributions datasets mixture distributions, rating distribution shown fig. ). following], measures evaluate performances LDL methods, compute average similarity/distance predicted rating distributions real rating distributions, including distance measures, euclidean?rensen, Squared similarity measures (fidelity, intersection). evaluate shallow model sldlfs datasets compare state--art stand-alone LDL methods. results sldlfs competitors summarized Table for Movie quote results reported], code] publicly available. for results ors two, run code authors made available. case], split dataset fixed folds standard ten-fold cross validation, represents result ?mean?standard deviation? matters training testing data divided. Table sldlfs perform measures. table Comparison results LDL datasets]. ??? ??? larger smaller better, respectively. dataset Method euclidean ?rensen squared fidelity intersection movie sldlf (ours) aoso-ldlogitboost] LDLogitBoost] LDSVR] BFGS LDL] iis-ldl.073.005.086.004.090.004.092.005.099.004.129.007.133.003.155.003.159.003.158.004.167.004.187.004.130.003.152.003.155.003.156.004.164.003.183.004.070.004.084.003.088.003.088.004.096.004.120.005.981.001.978.001.977.001.977.001.974.001.967.001.870.003.848.003.845.003.844.004.836.003.817.004 sldlf (ours) LDSVR] bfgs-ldl] iis-ldl.228.006.245.019.231.021.239.018.085.002.099.005.076.006.089.006.212.002.229.015.231.012.253.009.179.004.189.021.211.018.205.012.948.001.940.006.938.008.944.003.788.002.771.015.769.012.747.009 sldlf (ours) LDSVR] bfgs-ldl] iis-ldl.534.013.852.023.856.061.879.023.317.014.511.021.475.029.458.014.336.010.492.016.508.026.539.011.448.017.595.026.716.041.792.019.824.008.813.008.722.021.686.009.664.010.509.016.492.026.461.011 Human Gene Natural Scene Evaluation dldlfs Facial Age Estimation literature], age estimation formulated LDL problem. conduct facial age estimation experiments Morph,000 facial images,000 people races. each facial image annotated chronological age. generate age distribution face image, follow strategy], Gaussian distribution chronological age face image (fig. )). predicted age face image simply age highest probability predicted download datasets http://cse.seu.edu/people/xgeng/ldl/index.htm. label distribution. performance age estimation evaluated absolute error (mae) predicted ages chronological ages. current state--art result Morph obtain fine-tuning DLDL] vgg-face], build dldlf vgg-face, replacing softmax layer VGGNet ldlf. following], standard tenfold cross validation results summarized table. shows dldlf achieve state--art performance morph. note that, significant performance gain deep LDL models (dldl dldlf) non-deep LDL models (iis-ldl, cpnn, bfgs-ldl) superiority dldlf compared DLDL verifies effectiveness end-end learning tree-based model ldl, respectively. table MAE age estimation comparison Morph]. method iis-ldl] CPNN] bfgs-ldl] dldl+vgg-face] dldlf+vgg-face (ours) MAE distribution gender ethnicity unbalanced morph, age estimation methods] evaluated subset morph, called Morph Sub short, consists,160 selected facial images avoid inﬂuence unbalanced distribution. performance reported Morph Sub D2LDL], data-dependent LDL method. D2LDL output ?fc7? layer AlexNet] face image features, integrate LDLF alexnet. following experiment setting d2ldl, evaluate dldlf competitors, including SLL LDL based methods, training set ratios%). all competitors trained deep features d2ldl. Table dldlfs significantly outperform ors training set ratios. note that, generated age distrifigure MAE age estimation comparison butions unimodal distributions Morph sub. label distributions Training set ratio Method sec.  mixture distributions. % proposed method LDLFs achieve AAS.9081.7616.6507.5553.4690.4061 state--art results LARR.7501.6112.5131.4273.3500.2949 iis-aldl.1791.1683.1228.1107.1024.0902 verifies model D2LDL.1080.9857.9204.8712.8560.8385 ability model general dldlf (ours.8495.6220.3991.2401.1917.1224 form label distributions.  Time Complexity Let tree depth batch size, respectively. each tree split nodes leaf nodes. let  for tree sample, complexity forward pass backward pass), respectively. trees batches, complexity forward backward pass complexity iteration update leaf nodes thus, complexity training procedure (one epoch, batches) testing procedure (one sample), respectively. ldlfs eﬃcient: Morph Sub (12636 training images, 8424 testing images), model takes 5250s training (25000 iterations) testing 8424 images.  Parameter Discussion Now discuss inﬂuence parameter settings performance. report results rating prediction Movie (measured) age estimation Morph Sub% training set ratio (measured mae) parameter settings section. tree number. forest ensemble model, investigate performances change varying tree number forest. note that, discussed sec. ensemble strategy learn forest proposed dndfs] ours. refore, ensemble strategy learn forest. towards end, replace ensemble strategy dldlfs dndfs, method dndfs-ldl. shallow \\x0cmodel named sndfs-ldl. fix parameters., tree depth output unit number feature learning function, default setting. shown fig. ), ensemble strategy improve performance trees, dndfs leads worse performance single tree. observed fig. performance LDLFs improved trees, improvement increasingly smaller smaller. refore, larger ensembles yield big improvement movie, number trees 100.070.071). note that, random forests based methods large number trees., Shotton. ] obtained good pose estimation results depth images decision trees. tree depth. tree depth anor important parameter decision trees. ldlfs, implicit constraint tree depth output unit number feature learning function     discuss inﬂuence tree depth performance dldlfs, set  fix tree number performance change varying tree depth shown fig. ). performance improves decreases increase tree depth. reason tree depth increases, dimension learned features increases exponentially, greatly increases training diﬃculty. larger depths lead bad performance movie, tree depth.1162.0831). figure performance change age estimation Morph Sub rating prediction Movie varying) tree number) tree depth. our approach (dldlfs/sldlfs) improve performance trees, ensemble strategy proposed dndfs (dndfsldl/sndfsldl) leads worse performance single tree. Conclusion present label distribution learning forests, label distribution learning algorithm inspired differentiable decision trees. defined distributionbased loss function forests found leaf node predictions optimized variational bounding, enables trees feature learned jointly end-end manner. experimental results showed superiority algorithm LDL tasks related computer vision application, verified model ability model general form label distributions. acknowledgement. this work supported part National Natural Science Foundation China. 61672336, part ?chen guang? project supported Shanghai Municipal Education Commission Shanghai Education Development Foundation. 15cg43 part ONR n00014-2356.',\n",
       " 'PP6689': 'key application predictive learning, generating images conditioned consecutive frames received growing interests machine learning computer vision communities. learn representations spatiotemporal sequences, recurrent neural networks (rnn] Long short-term Memory (lstm] recently extended supervised sequence learning tasks, machine translation], speech recognition], action recognition, video captioning], spatiotemporal predictive learning scenario].  Why spatiotemporal memory? spatiotemporal predictive learning, crucial aspects: spatial correlations temporal dynamics. performance prediction system depends wher memorize relevant structures. however, knowledge, state--art rnn/lstm predictive learning methods] focus modeling temporal variations (such object moving trajectories), memory states updated repeatedly time inside LSTM unit. admittedly, stacked LSTM architecture proved powerful supervised spatiotemporal learning author: Mingsheng Long 31st Conference Neural Information Processing Systems (nips 2017), Long beach, usa. (such video action recognition]). conditions met scenario) Temporal features strong classification tasks. contrast, fine-grained spatial appearances prove significant) complex visual structures modeled expected outputs spatial representations highly abstracted. however, spatiotemporal predictive leaning satisfy conditions. here, spatial deformations temporal dynamics equally significant generating future frames. straightforward idea hope foretell future, memorize historical details possible. recall happened before, recall object movements, recollect visual appearances coarse fine. motivated this, present recurrent architecture called Predictive RNN (predrnn), memory states belonging LSTMs interact layers conventional rnns, mutually independent). key component predrnn, design Spatiotemporal LSTM-lstm) unit. models spatial temporal representations unified memory cell convey memory vertically layers horizontally states. predrnn achieves state-art prediction results video datasets. general modular framework predictive learning limited video prediction.  Related work Recent advances recurrent neural network models provide insights predict future visual sequences based historical observations. ranzato. ] defined RNN architecture inspired language modeling, predicting frames discrete space patch clusters. srivastava. ] adapted sequence sequence LSTM framework. shi. ] extended model furr extract visual representations exploiting convolutions input-state state-state transitions. Convolutional LSTM (convlstm) model seminal work area. subsequently, Finn. ] constructed network based ConvLSTMs predicts transformations input pixels next-frame prediction. lotter. ] presented deep predictive coding network ConvLSTM layer outputs layer-specific prediction time step produces error term, propagated laterally vertically network. however, settings, predicted frame bases previous ground truth sequence. contrast, predict sequence sequence] Villegas. ] challenging. patraucean. brought optical ﬂow RNNs model short-term temporal dynamics, inspired two-stream CNNs] designed action recognition. however, optical ﬂow images hard bring high additional computational costs reduce prediction eﬃciency. kalchbrenner. ] proposed Video Pixel Network (vpn) estimates discrete joint distribution raw pixel values video well-established PixelCNNs]. suffers high computational complexity. RNN architectures, deep architectures involved solve visual predictive learning problem. . ] defined cnn-based action conditional autoencoder model predict frames Atari games. mathieu. ] successfully employed generative adversarial networks, preserve sharpness predicted frames. summary, existing visual prediction models yield shortcomings due causes. rnn-based architectures] model temporal structures lstms, predicted images tend blur due loss fine-grained visual appearances. contrast, cnn-based networks] predict frame time generate future images recursively, prone focus spatial appearances weak capturing long-term motions. paper, explore RNN framework predictive learning present LSTM unit memorizing spatiotemporal information simultaneously.  Preliminaries Spatiotemporal predictive learning Suppose monitoring dynamical system. video clip) measurements time, measurement. rgb channel) recorded locations spatial region represented grid. video frames). spatial view, observation measurements time represented tensor  temporal view, observations time steps form sequence tensors  spatiotemporal predictive learning problem predict probable length sequence future  previous length sequence including current observation: xbt   xbt arg max        Spatiotemporal predictive learning important problem, find crucial high-impact applications domains: video prediction surveillance, meteorological environmental forecasting, energy smart grid management, economics finance prediction, etc. taking video prediction example, measurements RGB channels, observation time step video frame RGB image. anor radar-based precipitation forecasting, measurement radar echo values observation time step radar echo map visualized RGB image.  Convolutional LSTM  compared standard lstms, convolutional LSTM (convlstm] model spatiotemporal structures simultaneously explicitly encoding spatial information tensors, overcoming limitation vector-variate representations standard LSTM spatial information lost. convlstm, inputs   cell outputs   hidden state gates tensors dimension eir number measurement (for inputs) number feature maps (for intermediate representations), dimensions spatial dimensions (rows columns picture inputs states, imagine vectors standing spatial grid. convlstm determines future state cell grid inputs past states local neighbors. easily achieved convolution operators state-state input-state transitions. key equations ConvLSTM shown follows: tanh(wxg whg  (wxi whi  Wci (wxf whf  Wcf (wxo who  Wco tanh) sigmoid activation function, denote convolution operator Hadamard product respectively. states viewed hidden representations moving objects, ConvLSTM larger transitional kernel capture faster motions smaller kernel capture slower motions]. input gate forget gate output gate input-modulation gate controls information ﬂow memory cell way, gradient prevented vanishing quickly trapped memory. convlstm network adopts encoder-decoder RNN architecture proposed] extended video prediction]. -layer ConvLSTM encoder-decoder network, input frames fed layer future video sequence generated fourth one. process, spatial representations encoded layer-layer, hidden states delivered bottom top. however, memory cells belong layers mutually independent updated time domain. circumstances, bottom layer totally ignore memorized top layer previous time step. overcoming drawbacks layer-independent memory mechanism important predictive learning video sequences. predrnn section, give detailed descriptions predictive recurrent neural network (predrnn). initially, architecture enlightened idea predictive learning system memorize spatial appearances temporal variations unified memory pool. this, make memory states ﬂow network zigzag direction. step furr make spatiotemporal memory interact original long short-term memory. make \\x0cexplorations memory cell, memory gate memory fusion mechanisms inside lstms/convlstms. finally derive Spatiotemporal LSTM-lstm) unit predrnn, deliver memory states vertically horizontally.  Spatiotemporal memory ﬂow     \\x0cctl Figure left: convolutional LSTM network spatiotemporal memory ﬂow. right: conventional ConvLSTM architecture. orange arrows denote memory ﬂow direction memory cells. generating spatiotemporal predictions, PredRNN initially exploits convolutional LSTMs (convlstm] basic building blocks. stacked ConvLSTMs extract highly abstract features layer-bylayer make predictions mapping back pixel space. conventional ConvLSTM architecture, illustrated Figure (right), cell states constrained inside ConvLSTM layer updated horizontally. information conveyed upwards hidden states. temporal memory ﬂow reasonable supervised learning, study stacked convolutional layers, hidden representations abstract classspecific bottom layer upwards. however, suppose predictive learning, detailed information raw input sequence maintained. future, learn representations extracted different-level convolutional layers. thus, apply unified spatiotemporal memory pool alter RNN connections illustrated Figure (left). orange arrows denote feed-forward directions LSTM memory cells. left figure, unified memory shared LSTMs updated zigzag direction. key equations convolutional LSTM unit spatiotemporal memory ﬂow shown follows: tanh(wxg } Whg htl (wxi } Whi htl Wci  (wxf } Whf htl Wcf  Mlt) (wxo } Who htl Wco mlt Htl tanh(mlt input gate, input modulation gate, forget gate output gate longer depend hidden states cell states previous time step layer. instead, illustrated Figure (left), rely hidden states htl cell states }) updated previous layer current time step. specifically, bottom LSTM unit receives state values top layer previous time step: htl layers figure sets input-state state-state convolutional parameters, maintain spatiotemporal memory cell update states separately repeatedly information ﬂows current node. note that, replace notation memory cell emphasize ﬂows zigzag direction predrnn, horizontal direction standard recurrent networks. ConvLSTM Hadamard product state transitions gates, adopt convolution operators finer-grained memory transitions.  \\x0cspatiotemporal LSTM Input Gate Output Gate ctl Input Modulation Gate  Forget Gate Original Temporal Memory Spatiotemporal Memory   ? ctl ctl? ctl ?   ctl Figure-lstm (left) PredRNN (right). orange circles-lstm unit denotes differences compared conventional convlstm. orange arrows PredRNN denote spatiotemporal memory ﬂow, transition path spatiotemporal memory Mtl left. section, present predictive recurrent neural network (predrnn), replacing convolutional LSTMs spatiotemporal long short-term memory-lstm) unit (see Figure). architecture presented previous sub-section, spatiotemporal memory cells updated zigzag direction, information delivered upwards layers forwards time. happen memory cell states passed directions simultaneously. aid-lstms, PredRNN model spatiotemporal memory ﬂow evolves ultimate architecture. equations-lstm shown follows: tanh(wxg whg  (wxi whi  (wxf whf  Ctl gt0 tanh(wxg wmg  b0g i0t (wxi wmi  b0i) ft0 (wxf wmf  b0f Mlt ft0 i0t gt0 (wxo who  Wco ctl Wmo mlt Htl tanh [ctl Mlt ]). memory cells maintained: Ctl standard temporal cell delivered previous node current time step LSTM unit. mtl spatiotemporal memory current section, conveyed vertically layer current node time step. bottom-lstm layer previous subsection. construct anor set gate structures Mtl maintaining original gates Ctl standard lstms. last, final hidden states node rely fused spatiotemporal memory. concatenate memory derived directions toger apply convolution layer dimension reduction, makes hidden state Htl dimensions memory cells. simple memory concatenation-lstm unit shared output gate memory types enable seamless memory fusion, effectively model shape deformations motion trajectories spatiotemporal sequences. experiments \\x0cour model demonstrated achieve state--art performance video prediction datasets including syntic natural video sequences. PredRNN model optimized loss losses tried, loss works best). models trained ADAM optimizer] starting learning rate training process stopped, 000 iterations. orwise specified, batch size iteration set experiments implemented TensorFlow] conducted NVIDIA titan gpus.  Moving MNIST dataset Implementation generate Moving MNIST sequences method]. sequence consists consecutive frames, input prediction. frame handwritten digits bouncing inside grid image. digits chosen randomly MNIST training set initially random locations. digit, assign velocity direction randomly chosen uniform distribution unit circle, amplitude chosen randomly). digits bounce-off edges image occlude reaching location. properties make hard model give accurate predictions learning dynamics movement. digits generated quickly, infinite samples size training set. test set fixed, consisting,000 sequences. sample digits MNIST test set, assuring trained model before. also, model trained digits tested anor Moving MNIST dataset digits. test setup measure predrnn generalization transfer ability, frames digits training period. strong competitor, include latest state--art VPN model]. find hard reproduce vpn experimental results Moving MNIST open source, adopt baseline version CNNs PixelCNNs decoder generate frame pass. observe total number hidden states strong impact final accuracy predrnn. number trials, present-layer architecture 128 hidden states layer, yields high prediction accuracy reasonable training time memory footprint. table Results PredRNN spatiotemporal memory PredRNN-lstms, state-art models. report per-frame MSE cross-entropy) generated sequences averaged Moving MNIST test sets. lower MSE denotes prediction accuracy. model-lstm] ConvLSTM enc.dec. (128 ] CDNA] DFN] VPN baseline] PredRNN spatiotemporal memory PredRNN-lstm (128 mnist/frame) mnist (mse/frame) mnist (mse/frame) 483 367 346 285 110 118 118 103 162 142 138 130 125 118 \\x0cresults ablation study, PredRNN zigzag memory ﬂow reduces per-frame MSE Moving mnist test set (see Table). replacing convolutional LSTMs-lstms, furr decline sequence MSE. frame-frame quantitative comparisons presented Figure compared vpn, model turns accurate long-term predictions, Moving mnist. per-frame cross-entropy likelihood anor evaluation metric Moving mnist. PredRNN-lstms significantly outperforms previous methods, PredRNN spatiotemporal memory performs comparably VPN baseline. qualitative comparison predicted video sequences Figure vpn generated frames bit sharper, predictions gradually deviate correct trajectories, illustrated example. moreover, sequences digits overlapped entangled, VPN diﬃculties separating digits maintaining individual shapes. example, 140 180 120 160 Mean Square Error Mean Square Error figure, digit? loses left-side pixels predicted? overlapping. baseline models suffer severer blur effect, longer future time steps. contrast, predrnn results sharp accurate long-term motion predictions. 100 ConvLSTM enc.-dec. CDNA DFN VPN baseline PredRNN-lstm time steps) mnist 140 120 100 ConvLSTM enc.-dec. CDNA DFN VPN baseline PredRNN-lstm time steps) mnist Figure frame-wise MSE comparisons models Moving MNIST test sets. input frames Ground truth PredRNN VPN baseline CDNA ConvLSTM enc.-dec. figure Prediction examples Moving mnist test set.  KTH action dataset Implementation KTH action dataset] types human actions (walking, jogging, running, boxing, hand waving hand clapping) performed times subjects scenarios: outdoors, outdoors scale variations, outdoors clos indoors. video clips homogeneous backgrounds static camera 25fps frame rate length seconds average. make results comparable, adopt experiment setup] video frames resized 128 128 pixels videos divided respect subjects training set (persons) test set (persons). models, including PredRNN baselines, trained training set action categories generating subsequent frames observations, presented prediction results Figure Figure obtained test set predicting time steps future. sample sub-clips-frame-wide sliding window stride training set. evaluation, broaden sliding window-frame-wide set stride running jogging, categories. sub-clips running, jogging, walking manually trimmed ensure humans present frame sequences. end, split database training set 108,717 sequences test set,086 sequences. results Peak Signal Noise Ratio (psnr) Structural Similarity Index Measure (ssim] metrics evaluate prediction results provide frame-wise quantitative comparisons Figure higher denotes prediction performance. ssim ranges larger score means greater similarity images. predrnn consistently outperforms comparison models. specifically, Predictive Coding Network] exploits ground truth sequence current time step predict frame. thus, make sequence predictions. here, make predict frames feeding ground truth frames recursively generated frames previous time steps. performance MCnet] deteriorates quickly long-term predictions. residual connections MCnet convey CNN features frame decoder ignore previous frames, emphasizes spatial appearances weakens temporal variations. contrast, results PredRNN metrics remain stable time, slow reasonable decline. figure visualizes sample video sequence KTH test set. convlstm network] generates blurred future frames, fails memorize detailed spatial representations. mcnet] produces sharper images forecast movement trajectory accurately. -lstms, PredRNN memorizes detailed visual appearances long-term motions. outperforms baseline models shows superior predicting power spatially temporally. convlstm enc.-dec. MCnet res. predictive Coding Network PredRNN-lstms Structural Similarity Peak Signal Noise Ratio ConvLSTM enc.-dec. MCnet res. predictive Coding Network PredRNN-lstms time steps) frame-wise PSNR time steps) frame-wise SSIM Figure frame-wise PSNR SSIM comparisons models KTH action test set. higher score denotes prediction accuracy. input sequence Ground truth predictions PredRNN MCnet res. convlstm enc.-dec. Predictive Coding Network Figure KTH prediction samples. predict frames future observing frames.  Radar echo dataset Predicting shape movement future radar echoes real application predictive learning foundation precipitation nowcasting. challenging task radar echoes rigid. also, speeds fixed moving digits, trajectories periodical KTH actions, shapes accumulate, dissipate change rapidly \\x0cdue complex atmospheric environment. modeling spatial deformation significant prediction data. implementation collect radar echo dataset adapting data handling method]. dataset consists,000 consecutive radar observations, recorded minutes guangzhou, china. preprocessing, map radar intensities pixel values, represent 100 100 gray-scale images. slice consecutive images-frame-wide sliding window. thus, sequence consists frames, input, forecasting. total,600 sequences split training set,800 samples test set,800 samples. predrnn model consists-lstm layers 128 hidden states each. convolution filters inside-lstms set  prediction, transform resulted echo intensities colored radar maps, shown Figure calculate amount precipitation grid cell radar maps relationships. bring additional systematic error rainfall prediction makes final results misleading, account paper, compare predicted echo intensity ground truth. results Two baseline models considered. convlstm network] architecture models sequential radar maps convolutional lstms, predictions tend blur inaccurate (see Figure). strong competitor, include latest state--art VPN model]. pixelcnn-based VPN predicts image pixel pixel recursively, takes minutes generate radar map. precipitation nowcasting high demand real-time computing, trade prediction accuracy computation eﬃciency adopt vpn baseline model CNNs decoders generates frame pass. table shows prediction error PredRNN significantly lower VPN baseline. VPN generates accurate radar maps future, suffers rapid decay long term. phenomenon results lack strong LSTM layers model spatiotemporal variations. furrmore, PredRNN takes memory space training time VPN baseline. table Quantitative results methods radar echo dataset. model ConvLSTM enc.-dec] VPN baseline] PredRNN mse/frame Training time/100 batches Memory usage 105 539 117 1756 11513 2367 Input frames Ground truth PredRNN ConvLSTM enc.-dec. VPN baseline Figure prediction radar echo test set. conclusions paper, propose end-end recurrent network named PredRNN spatiotemporal predictive learning models spatial deformations temporal variations simultaneously. memory states zigzag stacked LSTM layers vertically time states horizontally. furrmore, introduce spatiotemporal LSTM-lstm) unit gate-controlled dual memory structure key building block predrnn. model achieves state--art performance video prediction datasets including syntic natural video sequences. acknowledgments This work supported National Key Program China (2016yfb1000701), National Natural Science Foundation China (61772299, 61325008, 61502265, 61672313) TNList fund. key application predictive learning, generating images conditioned consecutive frames received growing interests machine learning computer vision communities. learn representations spatiotemporal sequences, recurrent neural networks (rnn] Long short-term Memory (lstm] recently extended supervised sequence learning tasks, machine translation], speech recognition], action recognition, video captioning], spatiotemporal predictive learning scenario].  Why spatiotemporal memory?  spatiotemporal predictive learning, crucial aspects: spatial correlations temporal dynamics. performance prediction system depends wher memorize relevant structures. however, knowledge, state--art rnn/lstm predictive learning methods] focus modeling temporal variations (such object moving trajectories), memory states updated repeatedly time inside LSTM unit. admittedly, stacked LSTM architecture proved powerful supervised spatiotemporal learning corresponding author: Mingsheng Long 31st Conference Neural Information Processing Systems (nips 2017), Long beach, usa. (such video action recognition]). two conditions met scenario) Temporal features strong classification tasks. contrast, fine-grained spatial appearances prove significant) complex visual structures modeled expected outputs spatial representations highly abstracted. however, spatiotemporal predictive leaning satisfy conditions. here, spatial deformations temporal dynamics equally significant generating future frames. straightforward idea hope foretell future, memorize historical details possible. when recall happened before, recall object movements, recollect visual appearances coarse fine. motivated this, present recurrent architecture called Predictive RNN (predrnn), memory states belonging LSTMs interact layers conventional rnns, mutually independent). key component predrnn, design Spatiotemporal LSTM-lstm) unit. models spatial temporal representations unified memory cell convey memory vertically layers horizontally states. predrnn achieves state-art prediction results video datasets. general modular framework predictive learning limited video prediction.  Related work Recent advances recurrent neural network models provide insights predict future visual sequences based historical observations. ranzato. ] defined RNN architecture inspired language modeling, predicting frames discrete space patch clusters. srivastava. ] adapted sequence sequence LSTM framework. shi. ] extended model furr extract visual representations exploiting convolutions input-state state-state transitions. this Convolutional LSTM (convlstm) model seminal work area. subsequently, Finn. ] constructed network based ConvLSTMs predicts transformations input pixels next-frame prediction. lotter. ] presented deep predictive coding network ConvLSTM layer outputs layer-specific prediction time step produces error term, propagated laterally vertically network. however, settings, predicted frame bases previous ground truth sequence. contrast, predict sequence sequence] Villegas. ] challenging. patraucean. brought optical ﬂow RNNs model short-term temporal dynamics, inspired two-stream CNNs] designed action recognition. however, optical ﬂow images hard bring high additional computational costs reduce prediction eﬃciency. kalchbrenner. ] proposed Video Pixel Network (vpn) estimates discrete joint distribution raw pixel values video well-established PixelCNNs]. but suffers high computational complexity. besides RNN architectures, deep architectures involved solve visual predictive learning problem. . ] defined cnn-based action conditional autoencoder model predict frames Atari games. mathieu. ] successfully employed generative adversarial networks, preserve sharpness predicted frames. summary, existing visual prediction models yield shortcomings due causes. rnn-based architectures] model temporal structures lstms, predicted images tend blur due loss fine-grained visual appearances. contrast, cnn-based networks] predict frame time generate future images recursively, prone focus spatial appearances weak capturing long-term motions. paper, explore RNN framework predictive learning present LSTM unit memorizing spatiotemporal information simultaneously.  Preliminaries Spatiotemporal predictive learning Suppose monitoring dynamical system. video clip) measurements time, measurement. RGB channel) recorded locations spatial region represented grid. video frames). from spatial view, observation measurements time represented tensor  from temporal view, observations time steps form sequence tensors  spatiotemporal predictive learning problem predict probable length sequence future  previous length sequence including current observation: xbt   xbt arg max        Spatiotemporal predictive learning important problem, find crucial high-impact applications domains: video prediction surveillance, meteorological environmental forecasting, energy smart grid management, economics finance prediction, etc. taking video prediction example, measurements RGB channels, observation time step video frame RGB image. anor radar-based precipitation forecasting, measurement radar echo values observation time step radar echo map visualized RGB image.  Convolutional LSTM  compared standard lstms, convolutional LSTM (convlstm] model spatiotemporal structures simultaneously explicitly encoding spatial information tensors, overcoming limitation vector-variate representations standard LSTM spatial information lost. convlstm, inputs   cell outputs   hidden state gates tensors dimension eir number measurement (for inputs) number feature maps (for intermediate representations), dimensions spatial dimensions (rows columns picture inputs states, imagine vectors standing spatial grid. convlstm determines future state cell grid inputs past states local neighbors. this easily achieved convolution operators state-state input-state transitions. key equations ConvLSTM shown follows: tanh(wxg Whg  (wxi Whi  Wci (wxf Whf  Wcf (wxo Who  Wco tanh) sigmoid activation function, denote convolution operator Hadamard product respectively. states viewed hidden representations moving objects, ConvLSTM larger transitional kernel capture faster motions smaller kernel capture slower motions]. input gate forget gate output gate input-modulation gate controls information ﬂow memory cell way, gradient prevented vanishing quickly trapped memory. ConvLSTM network adopts encoder-decoder RNN architecture proposed] extended video prediction]. for-layer ConvLSTM encoder-decoder network, input frames fed layer future video sequence generated fourth one. process, spatial representations encoded layer-layer, hidden states delivered bottom top. however, memory cells belong layers mutually independent updated time domain. under circumstances, bottom layer totally ignore memorized top layer previous time step. overcoming drawbacks layer-independent memory mechanism important predictive learning video sequences. PredRNN section, give detailed descriptions predictive recurrent neural network (predrnn). initially, architecture enlightened idea predictive learning system memorize spatial appearances temporal variations unified memory pool. this, make memory states ﬂow network zigzag direction. step furr make spatiotemporal memory interact original long short-term memory. thus make \\x0cexplorations memory cell, memory gate memory fusion mechanisms inside lstms/convlstms. finally derive Spatiotemporal LSTM-lstm) unit predrnn, deliver memory states vertically horizontally.  Spatiotemporal memory ﬂow     \\x0cctl Figure left: convolutional LSTM network spatiotemporal memory ﬂow. right: conventional ConvLSTM architecture. orange arrows denote memory ﬂow direction memory cells. for generating spatiotemporal predictions, PredRNN initially exploits convolutional LSTMs (convlstm] basic building blocks. stacked ConvLSTMs extract highly abstract features layer-bylayer make predictions mapping back pixel space. conventional ConvLSTM architecture, illustrated Figure (right), cell states constrained inside ConvLSTM layer updated horizontally. information conveyed upwards hidden states. such temporal memory ﬂow reasonable supervised learning, study stacked convolutional layers, hidden representations abstract classspecific bottom layer upwards. however, suppose predictive learning, detailed information raw input sequence maintained. future, learn representations extracted different-level convolutional layers. thus, apply unified spatiotemporal memory pool alter RNN connections illustrated Figure (left). orange arrows denote feed-forward directions LSTM memory cells. left figure, unified memory shared LSTMs updated zigzag direction. key equations convolutional LSTM unit spatiotemporal memory ﬂow shown follows: tanh(wxg } Whg htl (wxi } Whi htl Wci  (wxf } Whf htl Wcf  Mlt) (wxo } Who htl Wco Mlt Htl tanh(mlt input gate, input modulation gate, forget gate output gate longer depend hidden states cell states previous time step layer. instead, illustrated Figure (left), rely hidden states htl cell states }) updated previous layer current time step. specifically, bottom LSTM unit receives state values top layer previous time step: htl layers figure sets input-state state-state convolutional parameters, maintain spatiotemporal memory cell update states separately repeatedly information ﬂows current node. note that, replace notation memory cell emphasize ﬂows zigzag direction predrnn, horizontal direction standard recurrent networks. different ConvLSTM Hadamard product state transitions gates, adopt convolution operators finer-grained memory transitions.  \\x0cspatiotemporal LSTM Input Gate Output Gate Ctl Input Modulation Gate  Forget Gate Original Temporal Memory Spatiotemporal Memory   ? ctl ctl? ctl ?   ctl Figure-lstm (left) PredRNN (right). orange circles-lstm unit denotes differences compared conventional convlstm. orange arrows PredRNN denote spatiotemporal memory ﬂow, transition path spatiotemporal memory Mtl left. section, present predictive recurrent neural network (predrnn), replacing convolutional LSTMs spatiotemporal long short-term memory-lstm) unit (see Figure). architecture presented previous sub-section, spatiotemporal memory cells updated zigzag direction, information delivered upwards layers forwards time. happen memory cell states passed directions simultaneously. with aid-lstms, PredRNN model spatiotemporal memory ﬂow evolves ultimate architecture. equations-lstm shown follows: tanh(wxg Whg  (wxi Whi  (wxf Whf  Ctl gt0 tanh(wxg Wmg  b0g i0t (wxi Wmi  b0i) ft0 (wxf Wmf  b0f Mlt ft0 i0t gt0 (wxo Who  Wco ctl Wmo mlt Htl tanh [ctl Mlt ]). two memory cells maintained: Ctl standard temporal cell delivered previous node current time step LSTM unit. mtl spatiotemporal memory current section, conveyed vertically layer current node time step. for bottom-lstm layer previous subsection. construct anor set gate structures Mtl maintaining original gates Ctl standard lstms. last, final hidden states node rely fused spatiotemporal memory. concatenate memory derived directions toger apply convolution layer dimension reduction, makes hidden state Htl dimensions memory cells. different simple memory concatenation-lstm unit shared output gate memory types enable seamless memory fusion, effectively model shape deformations motion trajectories spatiotemporal sequences. Experiments \\x0cour model demonstrated achieve state--art performance video prediction datasets including syntic natural video sequences. our PredRNN model optimized loss losses tried, loss works best). all models trained ADAM optimizer] starting learning rate training process stopped, 000 iterations. unless orwise specified, batch size iteration set all experiments implemented TensorFlow] conducted NVIDIA titan gpus.  Moving MNIST dataset Implementation generate Moving MNIST sequences method]. each sequence consists consecutive frames, input prediction. each frame handwritten digits bouncing inside grid image. digits chosen randomly MNIST training set initially random locations. for digit, assign velocity direction randomly chosen uniform distribution unit circle, amplitude chosen randomly). digits bounce-off edges image occlude reaching location. properties make hard model give accurate predictions learning dynamics movement. with digits generated quickly, infinite samples size training set. test set fixed, consisting,000 sequences. sample digits MNIST test set, assuring trained model before. also, model trained digits tested anor Moving MNIST dataset digits. such test setup measure predrnn generalization transfer ability, frames digits training period. strong competitor, include latest state--art VPN model]. find hard reproduce vpn experimental results Moving MNIST open source, adopt baseline version CNNs PixelCNNs decoder generate frame pass. observe total number hidden states strong impact final accuracy predrnn. after number trials, present-layer architecture 128 hidden states layer, yields high prediction accuracy reasonable training time memory footprint. table Results PredRNN spatiotemporal memory PredRNN-lstms, state-art models. report per-frame MSE cross-entropy) generated sequences averaged Moving MNIST test sets. lower MSE denotes prediction accuracy. model-lstm] ConvLSTM enc.dec. (128 ] CDNA] DFN] VPN baseline] PredRNN spatiotemporal memory PredRNN-lstm (128 mnist/frame) mnist (mse/frame) mnist (mse/frame) 483 367 346 285 110 118 118 103 162 142 138 130 125 118 \\x0cresults ablation study, PredRNN zigzag memory ﬂow reduces per-frame MSE Moving mnist test set (see Table). replacing convolutional LSTMs-lstms, furr decline sequence MSE. frame-frame quantitative comparisons presented Figure compared vpn, model turns accurate long-term predictions, Moving mnist. per-frame cross-entropy likelihood anor evaluation metric Moving mnist. PredRNN-lstms significantly outperforms previous methods, PredRNN spatiotemporal memory performs comparably VPN baseline. qualitative comparison predicted video sequences Figure though vpn generated frames bit sharper, predictions gradually deviate correct trajectories, illustrated example. moreover, sequences digits overlapped entangled, VPN diﬃculties separating digits maintaining individual shapes. for example, 140 180 120 160 Mean Square Error Mean Square Error figure, digit? loses left-side pixels predicted? overlapping. baseline models suffer severer blur effect, longer future time steps. contrast, predrnn results sharp accurate long-term motion predictions. 100 ConvLSTM enc.-dec. CDNA DFN VPN baseline PredRNN-lstm time steps) mnist 140 120 100 ConvLSTM enc.-dec. CDNA DFN VPN baseline PredRNN-lstm time steps) mnist Figure frame-wise MSE comparisons models Moving MNIST test sets. input frames Ground truth PredRNN VPN baseline CDNA ConvLSTM enc.-dec. figure Prediction examples Moving mnist test set.  KTH action dataset Implementation KTH action dataset] types human actions (walking, jogging, running, boxing, hand waving hand clapping) performed times subjects scenarios: outdoors, outdoors scale variations, outdoors clos indoors. all video clips homogeneous backgrounds static camera 25fps frame rate length seconds average. make results comparable, adopt experiment setup] video frames resized 128 128 pixels videos divided respect subjects training set (persons) test set (persons). all models, including PredRNN baselines, trained training set action categories generating subsequent frames observations, presented prediction results Figure Figure obtained test set predicting time steps future. sample sub-clips-frame-wide sliding window stride training set. evaluation, broaden sliding window-frame-wide set stride running jogging, categories. sub-clips running, jogging, walking manually trimmed ensure humans present frame sequences. end, split database training set 108,717 sequences test set,086 sequences. results Peak Signal Noise Ratio (psnr) Structural Similarity Index Measure (ssim] metrics evaluate prediction results provide frame-wise quantitative comparisons Figure higher denotes prediction performance. SSIM ranges larger score means greater similarity images. predrnn consistently outperforms comparison models. specifically, Predictive Coding Network] exploits ground truth sequence current time step predict frame. thus, make sequence predictions. here, make predict frames feeding ground truth frames recursively generated frames previous time steps. performance MCnet] deteriorates quickly long-term predictions. residual connections MCnet convey CNN features frame decoder ignore previous frames, emphasizes spatial appearances weakens temporal variations. contrast, results PredRNN metrics remain stable time, slow reasonable decline. figure visualizes sample video sequence KTH test set. ConvLSTM network] generates blurred future frames, fails memorize detailed spatial representations. mcnet] produces sharper images forecast movement trajectory accurately. thanks-lstms, PredRNN memorizes detailed visual appearances long-term motions. outperforms baseline models shows superior predicting power spatially temporally. convlstm enc.-dec. MCnet res. predictive Coding Network PredRNN-lstms Structural Similarity Peak Signal Noise Ratio ConvLSTM enc.-dec. MCnet res. predictive Coding Network PredRNN-lstms time steps) frame-wise PSNR time steps) frame-wise SSIM Figure frame-wise PSNR SSIM comparisons models KTH action test set. higher score denotes prediction accuracy. input sequence Ground truth predictions PredRNN MCnet res. convlstm enc.-dec. Predictive Coding Network Figure KTH prediction samples. predict frames future observing frames.  Radar echo dataset Predicting shape movement future radar echoes real application predictive learning foundation precipitation nowcasting. challenging task radar echoes rigid. also, speeds fixed moving digits, trajectories periodical KTH actions, shapes accumulate, dissipate change rapidly \\x0cdue complex atmospheric environment. modeling spatial deformation significant prediction data. Implementation collect radar echo dataset adapting data handling method]. our dataset consists,000 consecutive radar observations, recorded minutes guangzhou, china. for preprocessing, map radar intensities pixel values, represent 100 100 gray-scale images. slice consecutive images-frame-wide sliding window. thus, sequence consists frames, input, forecasting. total,600 sequences split training set,800 samples test set,800 samples. PredRNN model consists-lstm layers 128 hidden states each. convolution filters inside-lstms set  after prediction, transform resulted echo intensities colored radar maps, shown Figure calculate amount precipitation grid cell radar maps relationships. since bring additional systematic error rainfall prediction makes final results misleading, account paper, compare predicted echo intensity ground truth. results Two baseline models considered. ConvLSTM network] architecture models sequential radar maps convolutional lstms, predictions tend blur inaccurate (see Figure). strong competitor, include latest state--art VPN model]. pixelcnn-based VPN predicts image pixel pixel recursively, takes minutes generate radar map. given precipitation nowcasting high demand real-time computing, trade prediction accuracy computation eﬃciency adopt vpn baseline model CNNs decoders generates frame pass. table shows prediction error PredRNN significantly lower VPN baseline. though VPN generates accurate radar maps future, suffers rapid decay long term. such phenomenon results lack strong LSTM layers model spatiotemporal variations. furrmore, PredRNN takes memory space training time VPN baseline. table Quantitative results methods radar echo dataset. model ConvLSTM enc.-dec] VPN baseline] PredRNN mse/frame Training time/100 batches Memory usage 105 539 117 1756 11513 2367 Input frames Ground truth PredRNN ConvLSTM enc.-dec. VPN baseline Figure prediction radar echo test set. Conclusions paper, propose end-end recurrent network named PredRNN spatiotemporal predictive learning models spatial deformations temporal variations simultaneously. memory states zigzag stacked LSTM layers vertically time states horizontally. furrmore, introduce spatiotemporal LSTM-lstm) unit gate-controlled dual memory structure key building block predrnn. our model achieves state--art performance video prediction datasets including syntic natural video sequences. Acknowledgments This work supported National Key Program China (2016yfb1000701), National Natural Science Foundation China (61772299, 61325008, 61502265, 61672313) TNList fund.',\n",
       " 'PP6690': 'brain connectivity crucial understanding healthy diseased brain states]. recent years, investigators pursued construction human connectomes made large datasets public domain]. functional Magnetic Resonance Imaging (fmri) widely examine complex processes perception cognition. particular, functional connectivity derived fmri signals proven effective delineating biomarkers neuropsychiatric conditions]. challenges encountered functional connectivity analysis precise definition nodes edges connected brain regions]. functional nodes defined based activation maps functional anatomical atlases. nodes defined, step estimate weights edges. traditionally, functional connectivity weights measured correlation-based metrics. previous simulation studies shown successful, outperforming higher-order statistics. linear nongaussian acyclic causal models) lag-based approaches. granger causality]. hand, studies investigated power-law cross-correlation properties (equivalent multi-time scale measures) brain connectivity. recent research suggested fmri author: Department psychiatry, Park. s110. Haven 06519. 31st Conference Neural Information Processing Systems (nips 2017), Long beach, usa. signals power-law properties. power-spectrum power law, deviations typical range power-exponents noted neuropsychiatric disorders]. instance], wavelet-based multivariate methods, authors observed scale-free properties characteristic univariate fmri signals pairwise cross-temporal dynamics. moreover, found association magnitude scale-free dynamics task performance. hyposize power-law correlation measures capture additional dimensions brain connectivity conventional analyses enhance clinical prediction. paper, aim answer key open questions) wher brain networks cross-correlated time scales long-range dependencies (?long-memory? process, equivalent power-law frequency domain) extract intrinsic association regions controlling inﬂuence interconnected regions; (iii) wher multi-time scale connectivity measures improve clinical prediction. address questions detrended partial cross-correlation analsyis (dpcca) coeﬃcient], measure quantifies correlations multiple time scales non-stationary time series, typically case task-related fmri signals. dpcca extension detrended cross-correlation analysis], successfully applied analyses complex systems, including climatological] financial] data. unlike methods based filtering frequency bands, DPCCA directly informs correlations multiple time scales, unlike wavelet-based approaches. cross wavelet transformation wavelet transform coherence]), DPCCA advantage estimating pairwise correlations controlling inﬂuence regions. critical brain regions fmri signals reof highly interconnected. answer question, correlation profiles, generated dpcca, input features machine learning methods classification tasks compare performance dpcca-based features competing features. section describe simulated real data sets study, show features classification task extracted fmri signals. section provide furr details DPCCA (section), present proposed multi-time scale functional connectivity measure (section). section describe core experiments designed validate effectiveness DPCCA brain connectivity analysis clinical prediction. demonstrate DPCCA) detects connectivity multiple-time scales controlling covariates (sections) accurately identifies functional connectivity well-known gold-standard simulated data (section), (iii) improves classification accuracy cocaine dependence fmri data seventy-five cocaine dependent eighty-eight healthy control individuals (section). section conclude highlighting significance study limitations future work.  Material Methods Simulated dataset: NetSim fmri data fmri simulation data NetSim] previously developed evaluation network modeling methods. simulating rich realistic fmri time series, NetSim comprised twentyeight brain networks, levels complexity. signals generated dynamic causal modeling (dcm]), generative network model aimed quantify neuronal interactions neurovascular dynamics, measured fmri signals. netsim graphs nodes organized ?small-world? topology, order reﬂect real brain networks. netsim signals 200 time points (mostly) sampled repetition time) seconds. network, separate realizations (?subjects?) generated. thus, total 1400 syntic dataset testing. finally, signals generated, white noise standard deviation% added reproduce scan rmal noise.  real-world dataset: Cocaine dependence prediction seventy-five cocaine dependent) eighty-eight healthy control) individuals matched age gender participated study. recruited local, greater New Haven area prospective study met criteria current cocaine dependence, diagnosed Structured Clinical Interview dsm. drug-free staying inpatient treatment unit. human Investigation committee Yale University School Medicine approved study, subjects signed informed consent prior participation. scanner, performed simple cognitive control paradigm called stop-signal task]. fmri data collected Siemens Trio scanner. scan comprised-min runs stop signal task. functional blood oxygenation level dependent (bold) signals acquired single-shot gradient echo echo-planar imaging (epi) sequence, axial slices parallel line covering brain=2000, bandwidth=2004/pixel, ﬂip angle? fov=220?220 mm2 matrix, slice thickness gap. high-resolution structural image (mprage; resolution) obtained anatomical coregistration. hundred images acquired session. functional MRI data pre-processed standard pipeline Statistical Parametric Mapping (spm12) (wellcome Department Imaging neuroscience, University College london.).  Brain activation constructed general linear models localized brain regions responding conﬂict (stop signal) anticipation (encoded probability(stop)) group level]. regions responding(stop) comprised bilateral parietal cortex, inferior frontal gyrus (ifg) middle frontal gyrus (mfg); regions responding motor slowing bilateral insula, left precentral cortex), supplementary motor area (sma) (fig.  regions interest (rois) masks extract average activation time courses functional connectivity analyses.  Functional connectivity analyzed frontoparietal circuit involved conﬂict anticipation response adjustment standard Pearson correlation analysis multivariate Granger causality analysis mgca]. fig. ), illustrate fifteen correlation coeﬃcients derived ROIs individual shown fig. ). mgca, connectivities bilateral parietal SMA disrupted (fig. )). findings offer circuit-level evidence altered cognitive control cocaine addiction. figure Disrupted frontoparietal circuit cocaine addicts. frontoparietal circuit included regions responding Bayesian conﬂict anticipation?) regions motor slowing? ) shared connections (orange arrows). ) Connectivity strengths nodes frontoparietal circuit. show connectivity strengths nodes individual subject (red line) (blue line) groups. ) Novel Measure Brain Functional Connectivity Detrended partial cross-correlation analysis (dpcca) Detrended partial cross-correlation measure recently proposed]. dpcca combines advantages detrended cross-correlation analysis (dcca] standard partial correlation. time series)  irm ..., time points, DPCCA Equation CCA) Peak MNI coordinates ifg], mfg],bilateral insula], sma. time scale term) obtained inverting matrix . ) ). coeﬃcient  ) called DCCA coeﬃcient]. dcca coeﬃcient extension detrented cross correlation analysis] combined detrended ﬂuctuation analysis (dfa]. time series} (indices omitted sake simplicity) time points time scale DCCA coeﬃcient Equation FDCCA) ) FDF)fdf) numerator denominator average detrended covariances variances windows (partial sums), respectively, Equations FDCCA FDF) fdcca, fdf ) partial sums (profiles) obtained sliding windows integrated time series time window size detrended covariances variances computed Equations fdcca   fdf  polynomial fits time trends. linear fit originally proposed], higher order fits]. dcca measure power-law cross-correlations. however, focus DCCA coeﬃcient robust measure detect pairwise cross-correlation multiple time scales, controlling covariates. importantly, DPCCA quantifies correlations time series varying levels non-stationarity].  DPCCA functional connectivity analysis section, propose DPCCA measure brain functional connectivity. first, show simulation experiments measure satisfies desired connectivity properties. furr, define proposed connectivity measure. properties expected mamatical \\x0cdefinition dpcca, critical confirm validity real fmri data. additionally, establish statistical significance computed measures group level.  Desired properties Given real fmri signals, measure accurately detect time scale pairwise connections occur, controlling covariates. verify this, create syntic data combining real fmri signals sinusoidal waves (fig. ). simplify, assume additive property signals sinusoidal waves reﬂecting time onset connections. simulation, randomly sample 100 sets time series ?subjects?. distinction short long memory connections. fmri signals derive pairs connectivity profiles: short-memory sin sin}, long-memory sin sin mixed sin sin sin sin}, Gaussian signal simulate measurement noise. hyposize nodes functionally connected time scales control covariates. fmri signals derive signals connectivity {xac +sin}, {xbc +sin}, measurement noise. hyposize nodes functionally connected scale mutual inﬂuence node controlled. figure Illustration syntic fmri signals generated combining real fmri signals sinusoidal waves. ) Original fmri signals) original signals sin 10s) sin 30s) waves added. ) Statistical significance Given nodes time series, assume functionally connected max CCA time range srange significantly greater null distribution. empirical null distributions estimated original data randomly shuﬄing time series subjects nodes, proposed]. way, generate realistic distributions connectivity weights occurring chance. multivariate measure, null dataset generated number nodes tested network. multiple comparisons controlled estimating false discovery rate. importantly, null distribution computed max CCA time range srange srange seconds, assuming functional connections transpire range. thus, connections time-scales. binary definition functional connectivity current approach comparable methods, work temporal profile CCA), classification experiment (section). statistical criteria, generate null distributions connectivity measures.  DPCCA Canonical correlation analysis furr demonstrated simulation results (table), DPCCA \\x0clower true positive rate (tpr) compared competing methods, restrictive statistical thresholds. order increase sensitivity dpcca, augmented method including additional canonical correlation analysis (cca]. cca previously fmri contexts detect brain activations], functional connectivity], multimodal information fusion]. short, sets multivariate time series) irm) irn ..., respective dimensions sets number time points, CCA seeks linear transformations correlation linear combinations maximized. work, propose CCA define existence true connection, addition DPCCA connectivity results. proposed method summarized Algorithm CCA (lines), identify nodes strongly connected linear transformations. line, CCA inform DPCCA terms positive connections.  Experiments Results Connectivity properties: Controlling time scales covariates Figure observe DPCCA successfully captured time scales correlations time series noisy nature fmri signals. instance, distinguished short long-memory connections, represented 10s 30s, (figs. 3ac). importantly, detected peak connection 10s controlling inﬂuence covariate signal (fig. ). furr, unlike dpcca, original DCCA method rule mutual inﬂuence peak 30s (fig. ). algorithm dpcca+cca input: Time series irm ..., number vectors number time points; time range srange values output: Connectivity matrix matrices step: dpcca compute pairwise DPCCA pair vectors) srange Compute coeﬃcient CCA, equation, max CCA srange, statistical significance, null empirical distribution return matrix connection weights-values step: cca compute CCA connectivity) ) : rcca, ? cca) effect excluding node: indexcon -means(rcca]) split connections binary groups: cca, indexcon : return CCA cca binary connectivity matrix: step: dpcca+cca,cca) augment DPCCA CCA results: pair nodes: ,  dpcca significant connections fill missing connections: , max ], cca: return   binary matrix Figure DPCCA temporal profiles syntic signals (details Section). ): DPCCA peak=10s=30s, mixed. ) DPCCA original fmri signals generate syntics signals. ) Temporal profile obtained DCCA \\x0cwithout partial correlation. ) DPCCA peak=10s controlling dashed lines% confidence interval DPCCA empirical null distribution.  Simulated networks: Improved connectivity accuracy goal experiment validate proposed methods extensive dataset designed test functional connectivity methods. dataset, ground truth networks architectures aimed reﬂect real brain networks. full NetSim dataset comprised brain circuits subjects. sample time series, compute partial correlation (parcorr) regularized inverse covariance (icov), reported performers], proposed DPCCA dpcca+cca methods. measure, construct empirical null distributions, Section, generate binary connectivity matrix threshold . evaluate connectivity accuracy, ground truth networks, compute true positive negative rates (tpr tnr, respectively) balanced accuracy bacc using NetSim fmri data testing benchmark, observed proposed dpcca+cca method provided accurate functional connectivity results methods reported original paper]. results summarized Table balanced accuracy (bacc) evaluation metric, straightforward quantify true positive negative connections. table Comparison functional connectivity methods NetSim dataset. standard deviation balanced accuracy (bacc), true positive rate (tpr) true negative rate (tnr) reported. parcorr: partial correlation, icov: regularized inverse covariance, dpcca: detrended cross correlation analysis, dpcca+cca: DPCCA augmented cca. dpcca+cca balanced accuracy significantly higher competing method ICOV (wilcoxon signed paired test). metrics Mean Std ParCorr BAcc TPR TNR.834.866.804.096.129.188 Functional connectivity measures ICOV DPCCA BAcc TPR TNR BAcc TPR TNR.841.866.817.846.835.855.095.131.181.095.150.177 dpcca+cca BAcc TPR TNR.859.893.824.091.081.169 real-world dataset: Learning connectivity temporal profiles unsupervised methods) learn representative temporal profiles connectivity DPCCAF ull) perform dimensionality reduction. temporal profiles capture additional information (such shortand long-memory connectivity). however, increases feature set dimensionality, imposing additional challenges classifier training, small dataset. natural choice task principal component analysis (pca), represent original features linear combination. additionally, popular non-linear dimensionality reduction methods Isomap] autoencoders]. isomap, attempt learn \\x0ctrinsic geometry (manifold) temporal profile data. autoencoders, seek represent data restricted Boltzmann machines stacked layers. figure show representative correlation profiles obtained computing DPPCA frontoparietal regions (circuit presented fig. ), principal components. interestingly, PCA learn characteristic temporal profiles. instance, expected, components captured main trend, components captured short (task-related) long (resting-state) memory connectivity trends (figs). figure Illustration DPCCA profiles principal components. ifg: inferior frontal gyrus, sma: supplementary motor area: premotor cortex. explained variances components reported.  real-world dataset: Cocaine dependence prediction classification task consists predicting class membership, cocaine dependence) healthy control), individual fmri data. initial preprocessing (section), extract average time series frontoparietal circuit regions (figure), compute cross-correlation measures. coeﬃcients features train test (leave-one-out cross-validation) set popular classifiers scikitlearn toolbox] (version), including-nearest neighbors (knn), support vector machine (svm), multilayer perceptron (mlp), Gaussian processes), naive Bayes) ensemble method Adaboost (ada). DPCCA coeﬃcients, test peak values DPCCAmax rich temporal profiles DPCCAF ull finally, include brain activation maps (section) feature set, allowing comparison popular fmri classification softwares PRONTO (http://www.mlnl.ucl/pronto/). features summarized Table although regions obtained whole-group, class infor mation avoid inﬂated classification rates. table Features cocaine dependence classification task. type Name(stop) UPE Corr ParCorr ICOV DPCCAmax DPCCAF ull DPC CAIso DPCCAAutoE DPCCAP Activation Connectivity Size 1042 1042 270 135-180 135-180 Description Brain regions responding anticipation stop signals Brain regions responding unsigned prediction error(stop) Pearson cross-correlation frontoparietal regions Partial cross-correlation frontoparietal regions Regularized inverse covariance frontoparietal regions Maximum DPCCA range seconds Temporal profile DPCCA range seconds Isomap components neighbors Autoencoders hidden layers neurons, batch=100, epoch=1000 PCA components \\x0cclassification results summarized Table Figure area curve (auc) evaluation metric order sensitivity specificity classifiers, balanced accuracy (bacc). tested features Table including DPCCA full profiles dimensionality reduction (isomap, autoencoders pca). activation maps produced poor classification results(stop.525.048 upe.509.032), comparable results obtained PRONTO software features (accuracy.556). features Corr ParCorr ICOV DPCCAmax DPCCAF ull DPCCAIso DPC CAAutoE DPCCAP Mean AUC std.757 .041.901 .034.900 .030.906 .019.899 .028.902 .030.815 .149.928 .035) Mean BAcc std.674 .037.848 .025.838 .023.831 .022.820 .052.827 .068.813 .106.844 .064) Top classifier (auc bacc) Ada SVM Ada MLP SVM knn5 Ada Accuracy (auc bacc.794.710.948.875.948.858.929.857.957.874.954.894.939.863.963.911 Table Comparison classification results features. dpcca features combined PCA produced top classifiers criteria.963.911). however, DPCCAP statistically ParCorr ICOV (wilcoxon signed paired test). Figure accuracy classification methods. figure Comparison classification results features meth ods (described Section). conclusions summary, multi-time scale approach characterize brain connectivity, proposed method (dpcca+cca) identified connectivity peak-times (fig. ) produced higher connectivity accuracy competing method ICOV (table), (iii) distinguished short/long memory connections brain regions involved cognitive control (ifc&sma sma) (fig. ). second, connectivity weights features, DPCCA measures combined PCA produced highest individual accuracies (table). however, statistically feature (parcorr) classifiers. furr separate test set identify classifiers. performed extensive experiments large simulated fmri dataset validate DPCCA promising functional connectivity analytic. hand, conclusions clinical prediction (classification task) limited case. finally, furr optimization Isomap autoencoders methods improve learning connectivity temporal profiles produced dpcca. acknowledgments Supported FAPESP (2016/21591), CNPq (408919/2016), NSF (bcs1309260) NIH (aa021449, da023248). Brain connectivity crucial understanding healthy diseased brain states]. recent years, investigators pursued construction human connectomes made large datasets public domain]. functional Magnetic Resonance Imaging (fmri) widely examine complex processes perception cognition. particular, functional connectivity derived fmri signals proven effective delineating biomarkers neuropsychiatric conditions]. one challenges encountered functional connectivity analysis precise definition nodes edges connected brain regions]. functional nodes defined based activation maps functional anatomical atlases. once \\x0cnodes defined, step estimate weights edges. traditionally, functional connectivity weights measured correlation-based metrics. previous simulation studies shown successful, outperforming higher-order statistics. linear nongaussian acyclic causal models) lag-based approaches. granger causality]. hand, studies investigated power-law cross-correlation properties (equivalent multi-time scale measures) brain connectivity. recent research suggested fmri corresponding author: Department psychiatry, Park. s110. new Haven 06519. 31st Conference Neural Information Processing Systems (nips 2017), Long beach, usa. signals power-law properties. power-spectrum power law, deviations typical range power-exponents noted neuropsychiatric disorders]. for instance], wavelet-based multivariate methods, authors observed scale-free properties characteristic univariate fmri signals pairwise cross-temporal dynamics. moreover, found association magnitude scale-free dynamics task performance. hyposize power-law correlation measures capture additional dimensions brain connectivity conventional analyses enhance clinical prediction. paper, aim answer key open questions) wher brain networks cross-correlated time scales long-range dependencies (?long-memory? process, equivalent power-law frequency domain) extract intrinsic association regions controlling inﬂuence interconnected regions; (iii) wher multi-time scale connectivity measures improve clinical prediction. address questions detrended partial cross-correlation analsyis (dpcca) coeﬃcient], measure quantifies correlations multiple time scales non-stationary time series, typically case task-related fmri signals. dpcca extension detrended cross-correlation analysis], successfully applied analyses complex systems, including climatological] financial] data. unlike methods based filtering frequency bands, DPCCA directly informs correlations multiple time scales, unlike wavelet-based approaches. cross wavelet transformation wavelet transform coherence]), DPCCA advantage estimating pairwise correlations controlling inﬂuence regions. this critical brain regions fmri signals reof highly interconnected. answer question, correlation profiles, generated dpcca, input features machine learning methods classification tasks compare performance dpcca-based features competing features. Section describe simulated real data sets study, show features classification task extracted fmri signals. Section provide furr details DPCCA (section), present proposed multi-time scale functional connectivity measure (section). Section describe core experiments designed validate effectiveness DPCCA brain connectivity analysis clinical prediction. demonstrate DPCCA) detects connectivity multiple-time scales controlling covariates (sections) accurately identifies functional connectivity well-known gold-standard simulated data (section), (iii) improves classification accuracy cocaine dependence fmri data seventy-five cocaine dependent eighty-eight healthy control individuals (section). Section conclude highlighting significance study limitations future work.  Material Methods Simulated dataset: NetSim fmri data fmri simulation data NetSim] previously developed evaluation network modeling methods. simulating rich realistic fmri time series, NetSim comprised twentyeight brain networks, levels complexity. signals generated dynamic causal modeling (dcm]), generative network model aimed quantify neuronal interactions neurovascular dynamics, measured fmri signals. netsim graphs nodes organized ?small-world? topology, order reﬂect real brain networks. netsim signals 200 time points (mostly) sampled repetition time) seconds. for network, separate realizations (?subjects?) generated. thus, total 1400 syntic dataset testing. finally, signals generated, white noise standard deviation% added reproduce scan rmal noise.  real-world dataset: Cocaine dependence prediction seventy-five cocaine dependent) eighty-eight healthy control) individuals matched age gender participated study. recruited local, greater New Haven area prospective study met criteria current cocaine dependence, diagnosed Structured Clinical Interview dsm. drug-free staying inpatient treatment unit. Human Investigation committee Yale University School Medicine approved study, subjects signed informed consent prior participation. scanner, performed simple cognitive control paradigm called stop-signal task]. fmri data collected Siemens Trio scanner. each scan comprised-min runs stop signal task. functional blood oxygenation level dependent (bold) signals acquired single-shot gradient echo echo-planar imaging (epi) sequence, axial slices parallel line covering brain=2000, bandwidth=2004/pixel, ﬂip angle? fov=220?220 mm2 matrix, slice thickness gap. high-resolution structural image (mprage; resolution) obtained anatomical coregistration. three hundred images acquired session. functional MRI data pre-processed standard pipeline Statistical Parametric Mapping (spm12) (wellcome Department Imaging neuroscience, University College london.).  Brain activation constructed general linear models localized brain regions responding conﬂict (stop signal) anticipation (encoded probability(stop)) group level]. regions responding(stop) comprised bilateral parietal cortex, inferior frontal gyrus (ifg) middle frontal gyrus (mfg); regions responding motor slowing bilateral insula, left precentral cortex), supplementary motor area (sma) (fig.  regions interest (rois) masks extract average activation time courses functional connectivity analyses.  Functional connectivity analyzed frontoparietal circuit involved conﬂict anticipation response adjustment standard Pearson correlation analysis multivariate Granger causality analysis mgca]. fig. ), illustrate fifteen correlation coeﬃcients derived ROIs individual shown fig. ). according mgca, connectivities bilateral parietal SMA disrupted (fig. )). findings offer circuit-level evidence altered cognitive control cocaine addiction. figure Disrupted frontoparietal circuit cocaine addicts. frontoparietal circuit included regions responding Bayesian conﬂict anticipation?) regions motor slowing? ) shared connections (orange arrows). ) Connectivity strengths nodes frontoparietal circuit. show connectivity strengths nodes individual subject (red line) (blue line) groups. ) Novel Measure Brain Functional Connectivity Detrended partial cross-correlation analysis (dpcca) Detrended partial cross-correlation measure recently proposed]. dpcca combines advantages detrended cross-correlation analysis (dcca] standard partial correlation. given time series)  irm ..., time points, DPCCA Equation CCA) Peak MNI coordinates ifg], mfg],bilateral insula], sma. time scale term) obtained inverting matrix . ) ). coeﬃcient  ) called DCCA coeﬃcient]. DCCA coeﬃcient extension detrented cross correlation analysis] combined detrended ﬂuctuation analysis (dfa]. given time series} (indices omitted sake simplicity) time points time scale DCCA coeﬃcient Equation FDCCA) ) FDF)fdf) numerator denominator average detrended covariances variances windows (partial sums), respectively, Equations FDCCA FDF) fdcca, fdf ) partial sums (profiles) obtained sliding windows integrated time series for time window size detrended covariances variances computed Equations fdcca   fdf  polynomial fits time trends. linear fit originally proposed], higher order fits]. dcca measure power-law cross-correlations. however, focus DCCA coeﬃcient robust measure detect pairwise cross-correlation multiple time scales, controlling covariates. importantly, DPCCA quantifies correlations time series varying levels non-stationarity].  DPCCA functional connectivity analysis section, propose DPCCA measure brain functional connectivity. first, show simulation experiments measure satisfies desired connectivity properties. furr, define proposed connectivity measure. although properties expected mamatical \\x0cdefinition dpcca, critical confirm validity real fmri data. additionally, establish statistical significance computed measures group level.  Desired properties Given real fmri signals, measure accurately detect time scale pairwise connections occur, controlling covariates. verify this, create syntic data combining real fmri signals sinusoidal waves (fig. ). simplify, assume additive property signals sinusoidal waves reﬂecting time onset connections. for simulation, randomly sample 100 sets time series ?subjects?. Distinction short long memory connections. given fmri signals derive pairs connectivity profiles: short-memory sin sin}, long-memory sin sin mixed sin sin sin sin}, Gaussian signal simulate measurement noise. hyposize nodes functionally connected time scales Control covariates. given fmri signals derive signals connectivity {xac +sin}, {xbc +sin}, measurement noise. hyposize nodes functionally connected scale mutual inﬂuence node controlled. figure Illustration syntic fmri signals generated combining real fmri signals sinusoidal waves. ) Original fmri signals) original signals sin 10s) sin 30s) waves added. ) Statistical significance Given nodes time series, assume functionally connected max CCA time range srange significantly greater null distribution. empirical null distributions estimated original data randomly shuﬄing time series subjects nodes, proposed]. way, generate realistic distributions connectivity weights occurring chance. since multivariate measure, null dataset generated number nodes tested network. multiple comparisons controlled estimating false discovery rate. importantly, null distribution computed max CCA time range srange srange seconds, assuming functional connections transpire range. thus, connections time-scales. binary definition functional connectivity current approach comparable methods, work temporal profile CCA), classification experiment (section). statistical criteria, generate null distributions connectivity measures.  DPCCA Canonical correlation analysis furr demonstrated simulation results (table), DPCCA \\x0clower true positive rate (tpr) compared competing methods, restrictive statistical thresholds. order increase sensitivity dpcca, augmented method including additional canonical correlation analysis (cca]. cca previously fmri contexts detect brain activations], functional connectivity], multimodal information fusion]. short, sets multivariate time series) irm) irn ..., respective dimensions sets number time points, CCA seeks linear transformations correlation linear combinations maximized. work, propose CCA define existence true connection, addition DPCCA connectivity results. proposed method summarized Algorithm with CCA (lines), identify nodes strongly connected linear transformations. Line, CCA inform DPCCA terms positive connections.  Experiments Results Connectivity properties: Controlling time scales covariates Figure observe DPCCA successfully captured time scales correlations time series noisy nature fmri signals. for instance, distinguished short long-memory connections, represented 10s 30s, (figs. 3ac). importantly, detected peak connection 10s controlling inﬂuence covariate signal (fig. ). furr, unlike dpcca, original DCCA method rule mutual inﬂuence peak 30s (fig. ). Algorithm dpcca+cca input: Time series irm ..., number vectors number time points; time range srange values output: Connectivity matrix matrices step: dpcca compute pairwise DPCCA pair vectors) srange Compute coeﬃcient CCA, equation, max CCA srange, statistical significance, null empirical distribution return matrix connection weights-values step: cca compute CCA connectivity) ) : rcca, ? cca) effect excluding node: indexcon -means(rcca]) split connections binary groups: cca, indexcon : return CCA CCA binary connectivity matrix: step: dpcca+cca,cca) augment DPCCA CCA results: pair nodes: ,  dpcca significant connections fill missing connections: , max ], cca: return   binary matrix Figure DPCCA temporal profiles syntic signals (details Section). ): DPCCA peak=10s=30s, mixed. ) DPCCA original fmri signals generate syntics signals. ) Temporal profile obtained DCCA \\x0cwithout partial correlation. ) DPCCA peak=10s controlling dashed lines% confidence interval DPCCA empirical null distribution.  Simulated networks: Improved connectivity accuracy goal experiment validate proposed methods extensive dataset designed test functional connectivity methods. dataset, ground truth networks architectures aimed reﬂect real brain networks. full NetSim dataset comprised brain circuits subjects. for sample time series, compute partial correlation (parcorr) regularized inverse covariance (icov), reported performers], proposed DPCCA dpcca+cca methods. for measure, construct empirical null distributions, Section, generate binary connectivity matrix threshold . evaluate connectivity accuracy, ground truth networks, compute true positive negative rates (tpr tnr, respectively) balanced accuracy bacc Using NetSim fmri data testing benchmark, observed proposed dpcca+cca method provided accurate functional connectivity results methods reported original paper]. results summarized Table here balanced accuracy (bacc) evaluation metric, straightforward quantify true positive negative connections. table Comparison functional connectivity methods NetSim dataset. mean standard deviation balanced accuracy (bacc), true positive rate (tpr) true negative rate (tnr) reported. parcorr: partial correlation, icov: regularized inverse covariance, dpcca: detrended cross correlation analysis, dpcca+cca: DPCCA augmented cca. dpcca+cca balanced accuracy significantly higher competing method ICOV (wilcoxon signed paired test). metrics Mean Std ParCorr BAcc TPR TNR.834.866.804.096.129.188 Functional connectivity measures ICOV DPCCA BAcc TPR TNR BAcc TPR TNR.841.866.817.846.835.855.095.131.181.095.150.177 dpcca+cca BAcc TPR TNR.859.893.824.091.081.169 real-world dataset: Learning connectivity temporal profiles unsupervised methods) learn representative temporal profiles connectivity DPCCAF ull) perform dimensionality reduction. temporal profiles capture additional information (such shortand long-memory connectivity). however, increases feature set dimensionality, imposing additional challenges classifier training, small dataset. natural choice task principal component analysis (pca), represent original features linear combination. additionally, popular non-linear dimensionality reduction methods Isomap] autoencoders]. with isomap, attempt learn \\x0ctrinsic geometry (manifold) temporal profile data. with autoencoders, seek represent data restricted Boltzmann machines stacked layers. Figure show representative correlation profiles obtained computing DPPCA frontoparietal regions (circuit presented fig. ), principal components. interestingly, PCA learn characteristic temporal profiles. for instance, expected, components captured main trend, components captured short (task-related) long (resting-state) memory connectivity trends (figs). figure Illustration DPCCA profiles principal components. ifg: inferior frontal gyrus, sma: supplementary motor area: premotor cortex. explained variances components reported.  real-world dataset: Cocaine dependence prediction classification task consists predicting class membership, cocaine dependence) healthy control), individual fmri data. after initial preprocessing (section), extract average time series frontoparietal circuit regions (figure), compute cross-correlation measures. coeﬃcients features train test (leave-one-out cross-validation) set popular classifiers scikitlearn toolbox] (version), including-nearest neighbors (knn), support vector machine (svm), multilayer perceptron (mlp), Gaussian processes), naive Bayes) ensemble method Adaboost (ada). for DPCCA coeﬃcients, test peak values DPCCAmax rich temporal profiles DPCCAF ull finally, include brain activation maps (section) feature set, allowing comparison popular fmri classification softwares PRONTO (http://www.mlnl.ucl/pronto/). features summarized Table Although regions obtained whole-group, class infor mation avoid inﬂated classification rates. Table Features cocaine dependence classification task. type Name(stop) UPE Corr ParCorr ICOV DPCCAmax DPCCAF ull DPC CAIso DPCCAAutoE DPCCAP Activation Connectivity Size 1042 1042 270 135-180 135-180 Description Brain regions responding anticipation stop signals Brain regions responding unsigned prediction error(stop) Pearson cross-correlation frontoparietal regions Partial cross-correlation frontoparietal regions Regularized inverse covariance frontoparietal regions Maximum DPCCA range seconds Temporal profile DPCCA range seconds Isomap components neighbors Autoencoders hidden layers neurons, batch=100, epoch=1000 PCA components \\x0cclassification results summarized Table Figure area curve (auc) evaluation metric order sensitivity specificity classifiers, balanced accuracy (bacc). here tested features Table including DPCCA full profiles dimensionality reduction (isomap, autoencoders pca). activation maps produced poor classification results(stop.525.048 upe.509.032), comparable results obtained PRONTO software features (accuracy.556). features Corr ParCorr ICOV DPCCAmax DPCCAF ull DPCCAIso DPC CAAutoE DPCCAP Mean AUC std.757 .041.901 .034.900 .030.906 .019.899 .028.902 .030.815 .149.928 .035) Mean BAcc std.674 .037.848 .025.838 .023.831 .022.820 .052.827 .068.813 .106.844 .064) Top classifier (auc bacc) Ada SVM Ada MLP SVM knn5 Ada Accuracy (auc bacc.794.710.948.875.948.858.929.857.957.874.954.894.939.863.963.911 Table Comparison classification results features. DPCCA features combined PCA produced top classifiers criteria.963.911). however, DPCCAP statistically ParCorr ICOV (wilcoxon signed paired test). see Figure accuracy classification methods. Figure Comparison classification results features meth ods (described Section). conclusions summary, multi-time scale approach characterize brain connectivity, proposed method (dpcca+cca) identified connectivity peak-times (fig. ) produced higher connectivity accuracy competing method ICOV (table), (iii) distinguished short/long memory connections brain regions involved cognitive control (ifc&sma sma) (fig. ). second, connectivity weights features, DPCCA measures combined PCA produced highest individual accuracies (table). however, statistically feature (parcorr) classifiers. furr separate test set identify classifiers. performed extensive experiments large simulated fmri dataset validate DPCCA promising functional connectivity analytic. hand, conclusions clinical prediction (classification task) limited case. finally, furr optimization Isomap autoencoders methods improve learning connectivity temporal profiles produced dpcca. acknowledgments Supported FAPESP (2016/21591), CNPq (408919/2016), NSF (bcs1309260) NIH (aa021449, da023248).',\n",
       " 'PP6764': 'growing interest analyzing sequentially observed count vectors    data appears real world applications, recommend systems, text analysis, network analysis time series analysis. analyzing data conquer computational statistical challenges, high-dimensional, sparse, complex dependence time steps. example, analyzing dynamic word count matrix research papers, amount words large words times. trend topic encourage researchers write papers related topics year, relationship time step topic hard analyze completely. bayesian factor analysis model recently reached success modeling sequentially observed count matrix. assume data Poisson distributed, model data Poisson Factorize Analysis (pfa). pfa factorizes count matrix,  + loading matrix   factor score matrix. assumption gamma included, smooth transition time. property gamma-poisson distribution gamma process, inference MCMC models. lack ability capture relationship factors, transition matrix included poisson-gamma Dynamical System (pgds]. however, models shortcomings exploring long-time dependence time steps, independence assumption made   given. text analysis problem, temporal Dirichlet process] catch time dependence topic decay rate. method weak points analyzing data pattern long-time dependence, fanatical data disaster data]. deep models, called hierarchical models Bayesian learning field, widely Bayesian models fit deep relationship latent variables. examples include nested Chinese Restaurant Process], nest hierarchical Dirichlet process], deep Gaussian process. models based neural network structure recurrent structure used, Deep Exponential Families], Deep Poisson Factor Analysis based RBM SBN], Neural Autoregressive Density Estimator based neural networks], Deep Poisson 31st Conference Neural Information Processing Systems (nips 2017), Long beach, usa.          )   )            ) Figure visual representation model. ), structure onelayer model shown. ) shows transmission posterior information. prior posterior distributions interfacing layers shown). factor Modeling recurrent structure based PFA bernoulli-poisson link], Deep Latent Dirichlet Allocation stochastic gradient MCMC]. models capture deep relationship shallow models, outperform shallow models. paper, present Deep Dynamic Poisson Factor Analysis (ddpfa) model. based pfa, model includes recurrent neural networks represent implicit distributions, order learn complicated relationship factors short time. deep structure included order capture long-time dependence. inference algorithm based variational inference inferring latent variables. parameters neural networks learnt loss function based variational distributions. finally, DDPFA model syntic real-world datasets, excellent results obtained prediction fitting tasks. deep Dynamic Poisson Factorization Model Assume -dimensional sequentially observed count data   represented count matrix count data xvt ,   generated proposed DDPFA model follows) xvt oisson latent variables positive variables. represents strength component treated factor.  represents strength component tth time step. feature-wise variable captures sparsity feature recognizes importance component. regular setting], factorization regarded oisson(?    absorbed paper, order extract sparsity feature component impose feature-wise temporal smoothness constraint,  included model. additive property Poisson distribution decompose observed count xvt latent counts xvtk , way, model rewritten) xvt xvtk xvtk oisson Capturing complicated temporal dependence major purpose paper. previous work, transition gamma-gamma-poisson distribution structure used, gamma]. non-homogeneous Poisson process}.   time model stochastic transition features exploited Poisson process models]. models trained MCMC variational inference. however, rough models catch complicated time dependence weak points shallow structure time dimension. order capture complex time dependence deep long-time dependence model dynamic structure time steps proposed. layer follows) ) size window analysis, latent variables nth layer, follows)  ..., )  implicit probability distribution —?) modeled recurrent neural network. proba) bility AutoEncoder auxiliary posterior distribution  modeled) neural network, exploited training phase. -dimensional latent variable nth layer tth time step. specially, nth layer, generated Gamma distribution prior information. structure illustrated Figure finally, prior parameters latent variables Bayesian inference. variables generated gamma(??  and gamma(??   gamma(??   Dirichlet distribution prior distribution previous works, Gamma distribution exploited model due including feature-wise parameter purpose obtaining feasible factor strength real world applications, recommendation systems, observed binary count data formulated proposed DDPFA model bernoulli-poisson link]. distribution called bernoulli-poisson distribution), oisson(?) linking distribution rewritten, ?? ) ??  conditional posterior distribution,   oisson+ oisson+ (?) truncated Poisson distribution, MCMC methods inference. non-count real-valued matrix linked latent count matrix compound Poisson distribution Gamma belief network]. inference classical inference approaches Bayesian probabilistic model, Monte Carlo methods variational inference. proposed method, variational inference exploited implicit distribution regarded prior distribution stages inference model adopted: stage updates latent variables coordinate-ascent method fixed implicit distribution, parameters neural networks learned one. mean-field approximation: order obtain mean-field, variables independent governed variational distribution. joint distribution variational distribution \\x0cwritten)?   ?  (htk —htk represents prior variational parameter variable variational parameters fitted minimize divergence: argmin?  —?)) ) variational distribution(?—?  proxy posterior. objective equal maximize evidence low bound (elbo]. optimization performed coordinate-ascent method variational method. result, variational parameter optimized iteratively remaining parameters model set fixed value. due. conditional distribution (xvt1   xvtk multinomial parameter normalized set rates] formulated: (xvt1 xvtk ult(xvt?  ) Given auxiliary variables xvtk Poisson factorization model conditional conjugate model. complete conditional latent variables Gamma distribution shown:   —?, gamma(??   —?, gamma(??     —?, gamma(?? ??   ) generally, distributions derived conjugate properties Poisson Gamma distribution. posterior distribution. gamma distribution) prior) gamma(?  )  calculated recurrent neural network ..., inputs. ) posterior distribution htk. ) htk) gamma) prior information layer) posterior information)  layer. here, notation htk) recurrent neural network ..., equal ) calculated inputs. ) calculated recurrent neural network ..., inputs. refore, distribution) mentioned. regarded implicit conditional distribution ..., ) And \\x0cdistribution. implicit distribution ..., variational inference: Mean field variational inference approximate latent variables parameters neural network given. observed data satisfies xvt auxiliary variables xvtk updated: shp rte rte xvtk exp{?  log (?shp log) rte shp rte (?shp log  log (?) digamma function. variables superscript ?shp? shape parameter Gamma distribution, superscript ?rte? rate parameter. update expectation logarithm Gamma variable hlog (?shp log(?rte here, generated Gamma distribution represents expectation variable. calculation expectation variable, obeyed Gamma distribution, noted ?shp /?rte variables updated mean-field method gamma(?? hxv ?  gamma(?? ?    gamma(?? hxv??  ? ) latent variables deep structure updated mean-field method gamma(?  hxv ) htk gamma) eed (hhn eed (hhn) fback fback (hhn). eed (?) neural network constructing prior distribution fback (?) neural network constructing posterior distribution. probability autoencoder: This stage inference update parameters neural networks. bottom layer example. latent variables, parameters) approximated ..., ) gamma(? modeled RNN inputs ..., outputs,   ..., modeled RNN inputs ..., outputs) posterior distribution ) prior distribution) probability maximized. loss function neural networks follows: max) ) represents parameters neural networks. integration. intractable, loss function include auxiliary variational variables sume generated optimization regarded maximizing probability minimal difference) max) min ) approximating variables generated distribution expectation, loss function, similar variational AutoEncoder], simplified: min{khp  ) )ik2  ) )ik2) Since samples drawn distrbution, means sampling latent variables high-cost useless, differentiable variational Bayes suitable. result, focus fitting data generating data. objective, term, regularization, encourages data reconstructed latent variables, term encourages decoder fit data. parameters networks nth min{khp) layer trained loss function: ) )ik2) ) )ik2) order make convergence stable, term layer collapsed fixed latent variables approximated mean-field, loss function follows: min{khp  ) )ik2 ?ihp) )ik2) After layer-wise training, parameters neural networks jointly trained fine-tuning trick stacked AutoEncoder]. experiments section, multi-dimensional syntic datasets real-world datasets exploited examine performance proposed model. besides, results existed methods, pgds, lstm, pfa, compared results model. pgds dynamic poisson-gamma system mentioned Section LSTM classical time sequence model. order prove deep relationship learnt deep structure improve performance, simple PFA model included baseline. hyperparameters PGDS set] paper. 1000 times gibbs sampling iterations PGDS performed, 100 iterations mean-field PFA performed, 400 epochs executed lstm. parameters proposed DDPFA model set follows:?(?,?,?) ?(?,?,?)  ) ) iterations set 100. stochastic gradient descent neural networks executed epochs iteration. size window hyperparameters PFA set model. data time step exploited predicting target prediction task. squared error (mse) ground truth estimated predicted squared error (pmse) ground truth predicted time step exploited evaluate performance model.  Syntic Datasets multi-dimensional syntic datasets obtained functions subscript stands index dimension: Data SDS1 SDS2 SDS3 SDS4 Table result syntic data Measure DDPFA PGDS LSTM MSE PMSE MSE PMSE MSE PMSE MSE PMSE                         PFA     sds1) interval]. sds2) (mod) (mod) interval]. sds3) interval], indicator function. sds4) (mod) (mod) (mod 100]. ) interval number factor set number layers fitting predicting tasks performed model. hidden layer LSTM size layer. table DDPFA performance fitting prediction task datasets. note complex relationship learnt time steps helps model catch time patterns results ddpfa, PGDS pfa. lstm performs worse SDS4 noise syntic data long time steps make neural network diﬃcult memorize information.  real-world Datasets Five real-world datasets follows: Integrated Crisis Early Warning System (icews): ICEWS international relations event data set extracted \\x0cfrom news corpora]. refore treated undirected pairs countries features created count matrix year 2003. number events pair day time step counted, pairs fewer twenty-five total events discarded, leaving 365, 6197, 475646 events matrix. NIPS corpus (nips): NIPS corpus text NIPS conference paper year 1987 2003. created single count matrix column year. dataset downloaded gal page, 14036, 3280697 events matrix. Ebola corpus (ebola EBOLA corpus data 2014 Ebola outbreak West Africa day Mar 22th, 2014 Jan 5th 2015, column represents cases deaths West Africa country. data cleaning, dataset 122. International disaster International Disaster dataset essential core data occurrence effects,000 mass disasters world 1900 present day. count matrix 115 built events disasters occurred Europe year 1902 2016, classified disaster types. Annual Sheep population(asp Annual Sheep Population sheep population England Wales year 1867 1939 yearly. data matrix, http.stanford.edu/gal/data.html https://github.com/cmrivers/ebola/blob/master/country timeseries.csv http://www.emdat/ https://datamarket.com/data/list=provider:tsdl Data ICEWS NIPS EBOLA ASP Measure MSE PMSE MSE PMSE MSE PMSE MSE PMSE MSE PMSE Table result real-world data DDPFA PGDS LSTM    289  381  490         337  516  1071    2128  760 ) PGDS   1053  1728  4892  5839    17962  21324  PFA   1493   388 ) DDPFA Figure visual factor strength time step ICEWS data, data normalized time step. ), result PGDS shows factors shrunk local time steps. ), result DDPFA shows factors taking effects locally. set ASP datasets, set ors. size hidden layers LSTM. settings remainder parameters experiment. results experiment shown Table table shows results models datasets, proposed model DDPFA satisfying performance experiments ddpfa result ICEWS prediction task good enough. smood data obtained transition \\x0cmatrix PGDS performs prediction task. however, EBOLA ASP datasets, PGDS fails catching complicated time dependence. tough challenge LSTM network memorize patterns input data includes long-time patterns dimension data high. observation Figure shown factors learnt model activated locally compared pgds. natrually, real-world data, impossible factor time step. example, ICEWS dataset, connection Israel Occupied Palestinian Territory remains strong Iraq War accidents. figure) reveals factors time step captured pgds. figure meaningful factors ICEWS shown. factor, respectively, israel-palestinian conﬂict six-party talks. long-time activation factors shown thi figure, DDPFA model capture weak strength time. table show performance model sizes. table, performance improved distinctly adding layers adding variables upper layer. noticed expanding dimension bottom layer upper layers. results reveal problems proposed ddpfa: ”pruning” uselessness adding network layers. ) Figure visual top factors ICEWS data generated DDPFA method. ), ?japan?russian federation?, ?north korea?united states?, ?russian federation?united states?, ?south korea?united states?, ?china?russian federation? largest features due loading weights. factor stands six-party talks accidents. ), ?israel?occupied Palestinian territory?, ?israel?united states?, ?occupied Palestinian territory?united states? largest features stands israeli-palestinian conﬂict. table MSE real datasets sizes. size ICEWS NIPS EBOLA (ladder structure (ladder structure 382 379 381 379 377 380 378] notices hierarchical latent variable models advantage structure, conclusion bottom latent layer hierarchical variational autoencoders enough. order solve problem, ladder-like architecture, layer combines independent variables latent variables depend upper layers, model. noticed ladder architecture reach results Table anor problem, ”pruning”, phenomenon optimizer severs connections latent variables data]. experiments, noticed dimenisions latent layers data noise. problem found differentiable variational Bayes solved auxiliary MCMC strcuture]. refore, \\x0cproblem caused-variational inference model hope solved inference methods. summary model, called ddpfa, proposed obtain long-time complicated dependence time series count data. inference DDPFA based variational method estimating latent variables approximating parameters neural networks. order show performance proposed model, multi-dimensional syntic datasets real-world datasets, icews, NIPS corpus, ebola, International Disaster Annual Sheep population, used, performance existed methods, pgds, lstm, pfa, compared. experimental results, DDPFA effectivity interpretability sequential count analysis. growing interest analyzing sequentially observed count vectors    such data appears real world applications, recommend systems, text analysis, network analysis time series analysis. analyzing data conquer computational statistical challenges, high-dimensional, sparse, complex dependence time steps. for example, analyzing dynamic word count matrix research papers, amount words large words times. although trend topic encourage researchers write papers related topics year, relationship time step topic hard analyze completely. bayesian factor analysis model recently reached success modeling sequentially observed count matrix. assume data Poisson distributed, model data Poisson Factorize Analysis (pfa). pfa factorizes count matrix,  + loading matrix   factor score matrix. assumption gamma included, smooth transition time. with property gamma-poisson distribution gamma process, inference MCMC models. considering lack ability capture relationship factors, transition matrix included poisson-gamma Dynamical System (pgds]. however, models shortcomings exploring long-time dependence time steps, independence assumption made   given. text analysis problem, temporal Dirichlet process] catch time dependence topic decay rate. this method weak points analyzing data pattern long-time dependence, fanatical data disaster data]. deep models, called hierarchical models Bayesian learning field, widely Bayesian models fit deep relationship latent variables. examples include nested Chinese Restaurant Process], nest hierarchical Dirichlet process], deep Gaussian process. some models based neural network structure recurrent structure used, Deep Exponential Families], Deep Poisson Factor Analysis based RBM SBN], Neural Autoregressive Density Estimator based neural networks], Deep Poisson 31st Conference Neural Information Processing Systems (nips 2017), Long beach, usa.          )   )            ) Figure visual representation model. ), structure onelayer model shown. ) shows transmission posterior information. prior posterior distributions interfacing layers shown). factor Modeling recurrent structure based PFA bernoulli-poisson link], Deep Latent Dirichlet Allocation stochastic gradient MCMC]. models capture deep relationship shallow models, outperform shallow models. paper, present Deep Dynamic Poisson Factor Analysis (ddpfa) model. based pfa, model includes recurrent neural networks represent implicit distributions, order learn complicated relationship factors short time. deep structure included order capture long-time dependence. inference algorithm based variational inference inferring latent variables. parameters neural networks learnt loss function based variational distributions. finally, DDPFA model syntic real-world datasets, excellent results obtained prediction fitting tasks. Deep Dynamic Poisson Factorization Model Assume -dimensional sequentially observed count data   represented count matrix count data xvt ,   generated proposed DDPFA model follows) xvt oisson latent variables positive variables. represents strength component treated factor.  represents strength component tth time step. feature-wise variable captures sparsity feature recognizes importance component. according regular setting], factorization regarded oisson(?    absorbed paper, order extract sparsity feature component impose feature-wise temporal smoothness constraint,  included model. additive property Poisson distribution decompose observed count xvt latent counts xvtk , way, model rewritten) xvt xvtk xvtk oisson Capturing complicated temporal dependence major purpose paper. previous work, transition gamma-gamma-poisson distribution structure used, gamma]. non-homogeneous Poisson process}.   over time model stochastic transition features exploited Poisson process models]. models trained MCMC variational inference. however, rough models catch complicated time dependence weak points shallow structure time dimension. order capture complex time dependence deep long-time dependence model dynamic structure time steps proposed. layer follows) ) size window analysis, latent variables nth layer, follows)  ..., )  implicit probability distribution —?) modeled recurrent neural network. proba) bility AutoEncoder auxiliary posterior distribution  modeled) neural network, exploited training phase. -dimensional latent variable nth layer tth time step. specially, nth layer, generated Gamma distribution prior information. this structure illustrated Figure finally, prior parameters latent variables Bayesian inference. variables generated gamma(??  and gamma(??   gamma(??   although Dirichlet distribution prior distribution previous works, Gamma distribution exploited model due including feature-wise parameter purpose obtaining feasible factor strength real world applications, recommendation systems, observed binary count data formulated proposed DDPFA model bernoulli-poisson link]. distribution called bernoulli-poisson distribution), oisson(?) linking distribution rewritten, ?? ) ??  conditional posterior distribution,   oisson+ oisson+ (?) truncated Poisson distribution, MCMC methods inference. non-count real-valued matrix linked latent count matrix compound Poisson distribution Gamma belief network]. Inference classical inference approaches Bayesian probabilistic model, Monte Carlo methods variational inference. proposed method, variational inference exploited implicit distribution regarded prior distribution two stages inference model adopted: stage updates latent variables coordinate-ascent method fixed implicit distribution, parameters neural networks learned one. mean-field approximation: order obtain mean-field, variables independent governed variational distribution. joint distribution variational distribution \\x0cwritten)?   ?  (htk —htk represents prior variational parameter variable variational parameters fitted minimize divergence: argmin?  —?)) ) variational distribution(?—?  proxy posterior. objective equal maximize evidence low bound (elbo]. optimization performed coordinate-ascent method variational method. result, variational parameter optimized iteratively remaining parameters model set fixed value. due. conditional distribution (xvt1   xvtk multinomial parameter normalized set rates] formulated: (xvt1 xvtk ult(xvt?  ) Given auxiliary variables xvtk Poisson factorization model conditional conjugate model. complete conditional latent variables Gamma distribution shown:   —?, gamma(??   —?, gamma(??     —?, gamma(?? ??   ) generally, distributions derived conjugate properties Poisson Gamma distribution. posterior distribution. Gamma distribution) prior) gamma(?  )  calculated recurrent neural network ..., inputs. ) posterior distribution htk. ) htk) gamma) prior information layer) posterior information)  layer. here, notation htk) recurrent neural network ..., equal ) calculated inputs. ) calculated recurrent neural network ..., inputs. refore, distribution) mentioned. regarded implicit conditional distribution ..., ) And \\x0cdistribution. implicit distribution ..., variational inference: Mean field variational inference approximate latent variables parameters neural network given. observed data satisfies xvt auxiliary variables xvtk updated: shp rte rte xvtk exp{?  log (?shp log) rte shp rte (?shp log  log (?) digamma function. variables superscript ?shp? shape parameter Gamma distribution, superscript ?rte? rate parameter. this update expectation logarithm Gamma variable hlog (?shp log(?rte here, generated Gamma distribution represents expectation variable. calculation expectation variable, obeyed Gamma distribution, noted ?shp /?rte variables updated mean-field method gamma(?? hxv ?  gamma(?? ?    gamma(?? hxv??  ? ) latent variables deep structure updated mean-field method gamma(?  hxv ) htk gamma) eed (hhn eed (hhn) fback fback (hhn). eed (?) neural network constructing prior distribution fback (?) neural network constructing posterior distribution. probability autoencoder: This stage inference update parameters neural networks. bottom layer example. given latent variables, parameters) approximated ..., ) gamma(? modeled RNN inputs ..., outputs,   ..., modeled RNN inputs ..., outputs) with posterior distribution ) prior distribution) probability maximized. loss function neural networks follows: max) ) represents parameters neural networks. because integration. intractable, loss function include auxiliary variational variables sume generated optimization regarded maximizing probability minimal difference) max) min ) approximating variables generated distribution expectation, loss function, similar variational AutoEncoder], simplified: min{khp  ) )ik2  ) )ik2) Since samples drawn distrbution, means sampling latent variables high-cost useless, differentiable variational Bayes suitable. result, focus fitting data generating data. objective, term, regularization, encourages data reconstructed latent variables, term encourages decoder fit data. parameters networks nth min{khp) layer trained loss function: ) )ik2) ) )ik2) order make convergence stable, term layer collapsed fixed latent variables approximated mean-field, loss function follows: min{khp  ) )ik2 ?ihp) )ik2) After layer-wise training, parameters neural networks jointly trained fine-tuning trick stacked AutoEncoder]. Experiments section, multi-dimensional syntic datasets real-world datasets exploited examine performance proposed model. besides, results existed methods, pgds, lstm, pfa, compared results model. pgds dynamic poisson-gamma system mentioned Section LSTM classical time sequence model. order prove deep relationship learnt deep structure improve performance, simple PFA model included baseline. all hyperparameters PGDS set] paper. 1000 times gibbs sampling iterations PGDS performed, 100 iterations mean-field PFA performed, 400 epochs executed lstm. parameters proposed DDPFA model set follows:?(?,?,?) ?(?,?,?)  ) ) iterations set 100. stochastic gradient descent neural networks executed epochs iteration. size window hyperparameters PFA set model. data time step exploited predicting target prediction task. mean squared error (mse) ground truth estimated predicted squared error (pmse) ground truth predicted time step exploited evaluate performance model.  Syntic Datasets multi-dimensional syntic datasets obtained functions subscript stands index dimension: Data SDS1 SDS2 SDS3 SDS4 Table result syntic data Measure DDPFA PGDS LSTM MSE PMSE MSE PMSE MSE PMSE MSE PMSE                         PFA     sds1) interval]. sds2) (mod) (mod) interval]. sds3) interval], indicator function. sds4) (mod) (mod) (mod 100]. ) interval number factor set number layers both fitting predicting tasks performed model. hidden layer LSTM size layer. Table DDPFA performance fitting prediction task datasets. note complex relationship learnt time steps helps model catch time patterns results ddpfa, PGDS pfa. lstm performs worse SDS4 noise syntic data long time steps make neural network diﬃcult memorize information.  real-world Datasets Five real-world datasets follows: Integrated Crisis Early Warning System (icews): ICEWS international relations event data set extracted \\x0cfrom news corpora]. refore treated undirected pairs countries features created count matrix year 2003. number events pair day time step counted, pairs fewer twenty-five total events discarded, leaving 365, 6197, 475646 events matrix. NIPS corpus (nips): NIPS corpus text NIPS conference paper year 1987 2003. created single count matrix column year. dataset downloaded gal page, 14036, 3280697 events matrix. Ebola corpus (ebola EBOLA corpus data 2014 Ebola outbreak West Africa day Mar 22th, 2014 Jan 5th 2015, column represents cases deaths West Africa country. after data cleaning, dataset 122. International disaster International Disaster dataset essential core data occurrence effects,000 mass disasters world 1900 present day. count matrix 115 built events disasters occurred Europe year 1902 2016, classified disaster types. Annual Sheep population(asp Annual Sheep Population sheep population England Wales year 1867 1939 yearly. data matrix, http.stanford.edu/gal/data.html https://github.com/cmrivers/ebola/blob/master/country timeseries.csv http://www.emdat/ https://datamarket.com/data/list=provider:tsdl Data ICEWS NIPS EBOLA ASP Measure MSE PMSE MSE PMSE MSE PMSE MSE PMSE MSE PMSE Table result real-world data DDPFA PGDS LSTM    289  381  490         337  516  1071    2128  760 ) PGDS   1053  1728  4892  5839    17962  21324  PFA   1493   388 ) DDPFA Figure visual factor strength time step ICEWS data, data normalized time step. ), result PGDS shows factors shrunk local time steps. ), result DDPFA shows factors taking effects locally. set ASP datasets, set ors. size hidden layers LSTM. settings remainder parameters experiment. results experiment shown Table table shows results models datasets, proposed model DDPFA satisfying performance experiments ddpfa result ICEWS prediction task good enough. while smood data obtained transition \\x0cmatrix PGDS performs prediction task. however, EBOLA ASP datasets, PGDS fails catching complicated time dependence. and tough challenge LSTM network memorize patterns input data includes long-time patterns dimension data high. according observation Figure shown factors learnt model activated locally compared pgds. natrually, real-world data, impossible factor time step. for example, ICEWS dataset, connection Israel Occupied Palestinian Territory remains strong Iraq War accidents. figure) reveals factors time step captured pgds. Figure meaningful factors ICEWS shown. factor, respectively, israel-palestinian conﬂict six-party talks. long-time activation factors shown thi figure, DDPFA model capture weak strength time. Table show performance model sizes. from table, performance improved distinctly adding layers adding variables upper layer. noticed expanding dimension bottom layer upper layers. results reveal problems proposed ddpfa: ”pruning” uselessness adding network layers. ) Figure visual top factors ICEWS data generated DDPFA method. ), ?japan?russian federation?, ?north korea?united states?, ?russian federation?united states?, ?south korea?united states?, ?china?russian federation? largest features due loading weights. this factor stands six-party talks accidents. ), ?israel?occupied Palestinian territory?, ?israel?united states?, ?occupied Palestinian territory?united states? largest features stands israeli-palestinian conﬂict. table MSE real datasets sizes. size ICEWS NIPS EBOLA (ladder structure (ladder structure 382 379 381 379 377 380 378] notices hierarchical latent variable models advantage structure, conclusion bottom latent layer hierarchical variational autoencoders enough. order solve problem, ladder-like architecture, layer combines independent variables latent variables depend upper layers, model. noticed ladder architecture reach results Table anor problem, ”pruning”, phenomenon optimizer severs connections latent variables data]. experiments, noticed dimenisions latent layers data noise. this problem found differentiable variational Bayes solved auxiliary MCMC strcuture]. refore, \\x0cproblem caused-variational inference model hope solved inference methods. Summary model, called ddpfa, proposed obtain long-time complicated dependence time series count data. inference DDPFA based variational method estimating latent variables approximating parameters neural networks. order show performance proposed model, multi-dimensional syntic datasets real-world datasets, icews, NIPS corpus, ebola, International Disaster Annual Sheep population, used, performance existed methods, pgds, lstm, pfa, compared. according experimental results, DDPFA effectivity interpretability sequential count analysis.',\n",
       " 'PP6768': 'surge massive data led significant interest distributed algorithms scaling computations context machine learning optimization. context, attention devoted scaling large-scale stochastic gradient descent (sgd) algorithms], brieﬂy defined follows.  function minimize. access stochastic gradients). standard instance SGD converge minimum iterating procedure ) current candidate, variable step-size parameter. notably, arises. data points   generated unknown distribution loss function, measures loss model data point find model minimizes (?) , )], expected loss data. framework captures fundamental tasks, neural network training. 31st Conference Neural Information Processing Systems (nips 2017), Long beach, usa. paper, focus parallel SGD methods, received considerable attention recently due high scalability]. specifically, setting large dataset partitioned processors, collectively minimize function processor maintains local copy parameter vector iteration, obtains stochastic gradient update (corresponding local data). processors broadcast gradient updates peers, aggregate gradients compute iterate current implementations parallel sgd, iteration, processor communicate entire gradient update processors. gradient vector dense, processor send receive ﬂoating-point numbers iteration/from peer communicate gradients maintain parameter vector practical applications, communicating gradients iteration observed significant performance bottleneck]. popular reduce cost perform lossy compression gradients]. simple implementation simply reduce precision representation, shown converge convexity sparsity assumptions]. drastic quantization technique 1bitsgd], reduces component gradient sign (one bit), scaled average coordinates, accumulating errors locally. 1bitsgd experimentally observed preserve convergence], conditions; reduction communication, enabled state--art scaling deep neural networks (dnns) acoustic modelling]. however, 1bitsgd guarantees, strong assumptions, clear higher compression achievable. contributions. focus understanding trade-offs communication cost dataparallel sgd, convergence guarantees. propose family algorithms allowing lossy compression gradients called Quantized SGD (qsgd), processors trade-off number bits communicated iteration variance added process. qsgd built algorithmic ideas. intuitive stochastic quantization scheme: gradient vector processor, quantize component randomized rounding discrete set values, principled preserves statistical properties original. step eﬃcient lossless code \\x0cfor quantized gradients, exploits statistical properties generate eﬃcient encodings. analysis tight bounds precision-variance trade-off induced qsgd.  extreme trade-off, guarantee processor transmits (log)) expected bits iteration, increasing variance multiplicative factor. extreme, show processor transmit  bits iteration expectation, increasing variance factor particular, regime, compared full precision sgd,  bits communication iteration opposed 32n bits, guarantee iterations, leading bandwidth savings ?. qsgd fairly general: shown converge, assumptions, local minima nonconvex objectives, asynchronous iterations. non-trivial extension develop stochastic variance-reduced] variant qsgd, called qsvrg, exponential convergence rate. key question wher qsgd compression-variance trade-off inherent: instance, algorithm guaranteeing constant variance blowup transmit ) bits iteration? answer positive: improving asymptotically trade-off break communication complexity lower bound distributed estimation (see, Proposition]). experiments. crucial question wher, practice, QSGD reduce communication cost offset overhead additional iterations convergence. answer yes. explore practicality QSGD variety state--art datasets machine learning models: examine performance training networks image classification tasks (alexnet, inception, resnet, vgg) ImageNet] cifar] datasets, LSTMs] speech recognition. implement QSGD Microsoft CNTK]. experiments show models significantly benefit reduced communication multi-gpu training, virtually accuracy loss, standard parameters. example, training AlexNet GPUs standard parameters, reduction communication time?, reduction training network top accuracy?. training LSTM gpus, reduction communication time?, reduction training time target accuracy?. furr, computationallyheavy architectures Inception ResNet benefit reduction communication: 16gpus, QSGD reduces end-end convergence time ResNet152 approximately?. networks trained QSGD converge virtually accuracy full-precision variants, gradient quantization slightly improve accuracy settings. related work. line related research studies communication complexity convex optimization. particular] studied two-processor convex minimization model, provided lower bound (log log/))) bits communication cost-dimensional convex problems, proposed nonstochastic algorithm strongly convex problems, communication cost log factor lower bound. contrast, focus stochastic \\x0cgradient methods. recent work] focused round complexity lower bounds number communication rounds convex learning. buckwild! ] convergence guarantees low-precision sgd. gave upper bounds error probability sgd, assuming unbiased stochastic quantization, convexity, gradient sparsity, showed significant speedup solving convex problems cpus. qsgd refines results focusing trade-off communication convergence. view quantization independent source variance sgd, employ standard convergence results]. main differences buckwild! focus variance-precision trade-off; results apply quantized non-convex case; validate practicality scheme neural network training gpus. concurrent work proposes TernGrad], starts similar stochastic quantization, focuses case individual gradient components values. show significant speedups achieved TensorFlow], maintaining accuracy percentage points relative full precision. main differences work are: implementation guarantees convergence standard assumptions; strive provide black-box compression technique, additional hyperparameters tune; experimentally, QSGD maintains accuracy target number epochs; this, gradients larger bit width; experiments focus single-machine multi-gpu case. note QSGD applied solve distributed estimation problem] optimal error-communication trade-off regimes. contrast elegant random rotation solution presented], QSGD employs quantization Elias coding. case federated learning application], advantage eﬃcient compute gpu. extremely rich area studying algorithms systems eﬃcient distributed large-scale learning. ]. significant interest recently dedicated quantized frameworks, inference] training]. context] proposed 1bitsgd, heuristic compressing gradients sgd, inspired delta-sigma modulation]. implemented Microsoft cntk, cost bits ﬂoats iteration. variants shown perform large-scale Amazon datasets]. compared 1bitsgd, QSGD achieve asymptotically higher compression, provably converges standard assumptions, shows superior practical performance cases. preliminaries SGD variants, preconditions guarantees. techniques rar portable, applied black-box fashion top sgd. conciseness, focus basic SGD setup. assumptions standard. ].  convex set, differentiable, convex, smooth, unknown. assume repeated access stochastic gradients (possibly random) input outputs direction expectation correct direction \\x0cmove. formally: Definition. fix stochastic gradient random function). stochastic gradient moment k22   variance )     observe stochastic gradient moment bound automatically stochastic gradient variance bound )   long). second, convex optimization, assumes moment bound data: Local copy parameter vector iteration Let eti independent stochastic gradient encode)) //encode gradients broadcast peers; peer receive peer decode //decode gradients end Figure illustration generalized stochastic quantization levels. end   Algorithm Parallel SGD algorithm. dealing non-smooth convex optimization, variance bound dealing smooth convex optimization. however, convenient consistently assume moment bound. major distinction ory practice]. access stochastic gradients, starting point SGD builds iterates Equation), projected sequence step sizes. setting, show: orem], orem).  convex, unknown, convex-smooth.  given, supx  fixed. repeated, independent access stochastic gradients variance bound sgd initial point constant step sizes/?    min)  achieves LR2  ) Minibatched sgd. modification SGD scheme presented observed practice technique minibatching. minibatched sgd, updates form get get  independent stochastic  gradient hard get stochastic gradients variance bound stochastic gradient variance bound . inspection orem, long term) dominates, minibatched SGD requires fewer iterations converge. data-parallel sgd. synchronous data-parallel sgd, modelling real-world multi-gpu systems, focus communication cost SGD setting. set processors proceed synchronous steps, communicate point-point messages. processor maintains local copy vector dimension representing current estimate minimizer, access private, independent stochastic gradients synchronous iteration, Algorithm processor aggregates obtains random gradient updates component communicates updates peers, finally aggregates received updates applies locally. importantly, add encoding decoding steps gradients send/receive lines respectively. following, describing variant sgd, assume general pattern, encode/decode functions. notice decoding step necessarily recover original gradient‘ instead, apply approximate version. encoding decoding steps identity., encoding decoding), refer algorithm parallel sgd. case, simple calculation processor, processors held iteration updated end iteration ‘ stochatic gradient. particular, update minibatched update size thus, discussion above, rephrasing orem, corollary: Corollary. orem. fix suppose run parallel SGD processors, access independent stochastic gradients moment bound step size/?  orem.  max min)  ) reasonable regimes, term max) dominate number iterations necessary. specifically, number iterations depend linearly moment bound quantized Stochastic Gradient Descent (qsgd) section, present main results stochastically quantized sgd. throughout, log denotes base logarithm, number bits represent ﬂoat. vector kvk0 denote number nonzeros For string  }? —?— denote length. scalar sgn) } denote sign, sgn)  \\x0cgeneralized Stochastic Quantization Coding Stochastic quantization. general, parametrizable lossycompression scheme stochastic gradient vectors. quantization function denoted), tuning parameter, number quantization levels implement. intuitively, define uniformly distributed levels quantized preserves expectation, introduces minimal variance. Figure  ) defined kvk2 sgn  independent random variables defined follows.  integer —/kvk2 ]. ] quantization interval —/kvk2    probability kvk orwise. here,  ]. define, distribution, minimal variance distributions support, }, expectation satisfies —/kvk2 formally, show: Lemma.  vector )] (unbiasedness),? [kqs)? vk22 min)kvk22 (variance bound), (iii[kqs  (sparsity). eﬃcient Coding gradients. observe vector output) naturally expressible tuple (kvk2  vector signs vector integer values ). key idea coding scheme integer values , equally likely: particular, larger integers frequent. exploit specialized Elias integer encoding], presented full full version paper]. intuitively, positive integer code, denoted elias), starts binary representation prepends length representation. recursively encodes prefix. show positive integer length resulting code —elias)— log log log   )) log encoding decoding eﬃciently. gradient vector represented triple (kvk2 quantization levels, coding outputs string defined follows. first, bits encode kvk2 proceeds encode Elias recursive coding position nonzero entry appends bit denoting elias )). iteratively, proceeds encode distance current coordinate nonzero, encodes coordinate way. decoding scheme straightforward: read bits construct kvk2 iteratively decoding scheme Elias recursive coding read positions values nonzeros  properties quantization encoding imply following. orem.  fixed, arbitrary. fix quantization levels. ) stochastic gradient moment bound)) stochastic gradient variance bound min moreover, encoding scheme expectation, number bits communicate)) upper bounded  . ) log Sparse levels, gradient density regime. case., quantization ), second-moment blowup intuitively, means?that employ log bits iteration, convergence time increased).  dense regime. variance blowup minimized quantization levels; case, devise eﬃcient encoding yields order magnitude shorter codes compared full-precision variant. proof statement obvious, exploits statistical properties quantization guarantees Elias coding. corollary. ) orem. encoding scheme)) expectation length.  QSGD Guarantees Putting bounds communication variance guarantees SGD algorithms smooth, convex functions yield results: orem (smooth Convex qsgd). orem. fix suppose run parallel QSGD quantization levels processors accessing indepen? dent stochastic gradients moment bound stepsize/?   orem min sn2  max minx)  moreover, QSGD re2, ) quires) log bits communication round.  special case reduced. qsgd portable, applied stochastic gradient method. illustration, quantization] communication-eﬃcient non-convex sgd. orem (qsgd smooth nonconvex optimization).  -smooth (possibly nonconvex) function, arbitrary initial point. fixed, random stopping time supported, qsgd quantization level constant stepsizes ) access   stochastic gradients min moment bound satisfies)k22  moreover, communication cost orem.  Quantized variance-reduced SGD \\x0cassume processors, parameter processor access functions {fim    goal approximately minimize  processor portion knows natural question wher apply stochastic quantization reduce communication parallel svrg. inspection, notice resulting update break standard svrg. resolve technical issue, proving quantize SVRG updates techniques obtain convergence bounds.  , defined Section. Algorithm description. ) arbitrary starting point) beginning epoch processor broadcasts),mthat, unquantized full gradient, processors aggregate) epoch, iteration  uniformly random integer] completely independent else. iteration epochp, processor broadcasts update vector) processor      )  Table Description networks, final top accuracy, end-end training speedup 8gpus. network AlexNet ResNet152 ResNet50 ResNet110-inception VGG19 LSTM Dataset ImageNet ImageNet ImageNet cifar ImageNet ImageNet AN4 params. 62m 60m 25m 11m 143m 13m init. rate top (32bit% top (qsgd% (4bit% (8bit% (4bit% (4bit (4bit) Speedup gpus    ? (projected? (projected) gpus) Each processor computes total update sets  ) end epoch processor sets prove following. orem. ), ‘-strongly convex, convex -smooth, unique minimizer  /‘), QSVRG initial point) ensures) ?  ) ? epoch  moreover, QSVRG iterations epoch requires  bits communication epoch. discussion. particular, largely decouple dependence condition number communication.  /‘ denote condition number observe term subsumed epoch communication dominated). specifically, fixed attain accuracy(log/). long log ?(? true instance case  poly log) poly), communication \\x0cper epoch(? (log)). gradient descent. full version paper] application QSGD gradient descent. roughly, case, QSGD simply truncate gradient top components, sorted magnitude. qsgd Variants Our experiments stretch ory, deep networks, nonconvex objectives.  tested QSGD convex objectives. results closely follow ory, refore omitted.) implementations depart previous algorithm description follows. first, notice control variance quantization quantizing buckets fixed size view gradient one-dimensional vector reshaping tensors necessary, bucket defined set consecutive vector values. . ith bucket sub-vector  ].) quantize bucket independently, qsgd. setting corresponds quantization (vanilla sgd), corresponds full quantization, previous section. easy that, bucketing, guarantees Lemma expressed terms opposed full dimension This knob control variance, cost storing extra scaling factor bucket values. example, bucket size 512, bits, variance increase due quantization upper bounded 512. oretical justification similar convergence rates observe practice. difference ory scale maximum vector opposed-norm). intuitively, normalizing max preserves values, slightly higher accuracy number iterations. methods baseline bandwidth reduction lower bit width. bits bits dimension), normalizing max longer sparsity guarantees. note affect bounds regime quantization levels component, employ sparsity case. (however, note practice max normalization generates non-trivial sparsity.) experiments setup. performed experiments Amazon EC2.16xlarge instances, NVIDIA K80 gpus. instances GPUDirect peer-peer communication, support NVIDIA Figure Breakdown communication versus computation neu ral networks, gpus Training loss faster 2bit QSGD=128) 4bit QSGD=8192) 8bit QSGD=8192) SGD) AlexNet Accuracy versus time. 300 600 900 Time (sec) 1200) LSTM error time. 1500 Test accuracy (%) full-bit precision versus QSGD-bit. bar represents total time epoch standard parameters. epoch time broken communication (bottom, solid) computation (top, transparent). epoch time diminishes parallelize, proportion communication increases. 1bitsgd* 32bit QSGD 4bit QSGD 8bit Epoch 100 120) ResNet50 accuracy. figure Accuracy numbers networks. light blue lines represent-bit accuracy. nccl extensions. implemented QSGD GPUs Microsoft Cognitive Toolkit (cntk]. package eﬃcient (mpibased) gpu-gpu communication, implements optimized version 1bit-sgd]. code released open-source]. execute types tasks: image classification ILSVRC 2015 (imagenet], CIFAR10], MNIST], speech recognition CMU AN4 dataset]. vision, experimented AlexNet], VGG], ResNet], Inception Batch Normalization] deep networks. speech, trained LSTM network]. Table details. protocol. methodology emphasizes error tolerance, sense aim preserve accuracy networks trained. standard sizes networks, hyperparameters optimized 32bit precision variant. (unless orwise stated, default networks hyper-parameters optimized full-precision CNTK.) increased batch size balance communication computation larger GPU counts, past point lose accuracy. employed double buffering] perform communication quantization concurrently computation. quantization benefits lowering learning rates; yet, run 32bit learning rate, decrease bucket size reduce variance. quantize small gradient matrices 10k elements), computational cost quantizing significantly exceeds reduction communication. however, experiments% parameters transmitted quantized form. reshape matrices fit bucket sizes, receptive field split buckets. communication. computation. set experiments, examine ratio computation communication costs training, increased parallelism. image classification networks trained imagenet, LSTM trained an4. examine cost breakdown networks pass dataset (epoch). figure results networks image classification. variance epoch times practically negligible%), omit confidence intervals. figure leads interesting observations. first, based ratio communication computation, roughly split networks communication-intensive (alexnet, vgg, lstm), computation-intensive (inception, resnet). network types, relative impact communication increases significantly increase number gpus. examining breakdown-bit version, networks significantly benefit reduced communication. example, AlexNet GPUs batch size 1024% training time spent communication, LSTM GPUs batch size 256, ratio%.  ratios slightly changed increasing batch size, decrease accuracy. ].) next, examine impact QSGD communication training time. (communication time includes time spent compressing uncompressing gradients.) measured QSGD-bit quantization 128 bucket size-bit-bit quantization 512 bucket size. results variants similar, bucket sizes 4bit version sends% data-bit version (but  -bit). bucket sizes chosen ensure good convergence, carefully tuned. 16gpu AlexNet batch size 1024-bit QSGD reduces communication time?, epoch time?. lstm, reduces communication time?, epoch time?. runtime improvements non-trivial architectures considered. accuracy. examine QSGD inﬂuences accuracy convergence rate. ran AlexNet ResNet full convergence imagenet, LSTM an4, ResNet110 cifar, two-layer perceptron mnist. results Figure exact numbers Table qsgd tests performed 8gpu setup, compared full-precision accuracy networks. general, notice 4bit 8bit gradient quantization suﬃcient recover slightly improve full accuracy, ensuring non-trivial speedup. experiments-bit gradients 512 bucket size suﬃcient recover improve full-precision accuracy. results consistent recent work] noting benefits adding noise gradients training deep networks. thus, quantization source zero-mean noise, render communication eﬃcient. time, note aggressive quantization hurt accuracy. particular-bit QSGD 8192 bucket size (not shown) loses% top accuracy% top, versus full precision AlexNet trained number epochs. also, QSGD-bit bucket size gap% top% top. issue exam \\x0cined detail layers sensitive quantization. appears quantizing convolutional layers aggressively-bit precision) lead accuracy loss trained period time full precision variant. however, increasing precision-bit-bit recovers accuracy. finding suggests modern architectures vision tasks, ResNet inception, convolutional, benefit quantization recurrent deep networks lstms. additional experiments. full version paper additional experiments, including full comparison 1bitsgd. brief, QSGD outperforms matches performance final accuracy 1bitsgd networks parameter values consider. conclusions Future Work presented qsgd, family SGD algorithms smooth trade amount communication iteration running time. experiments suggest QSGD highly competitive fullprecision variant variety tasks. number optimizations explore. significant leveraging sparsity created qsgd. current implementations MPI provide support sparse types, plan explore support future work. furr, plan examine potential QSGD larger-scale applications, supercomputing. oretical side, interesting applications quantization sgd. full version paper] complete proofs, additional applications. acknowledgments authors Martin jaggi, zhang, Frank Seide CNTK team support development project, anonymous NIPS reviewers careful consideration excellent suggestions. dan Alistarh supported Swiss National Fund Ambizione fellowship. jerry supported NSF CAREER Award ccf-1453261, ccf-1565235, Google Faculty Research award, NSF Graduate Research fellowship. work developed part Dan alistarh, Jerri Milan Vojnovic Microsoft Research cambridge. surge massive data led significant interest distributed algorithms scaling computations context machine learning optimization. context, attention devoted scaling large-scale stochastic gradient descent (sgd) algorithms], brieﬂy defined follows. let function minimize. access stochastic gradients). standard instance SGD converge minimum iterating procedure ) current candidate, variable step-size parameter. notably, arises. data points   generated unknown distribution loss function, measures loss model data point find model minimizes (?) , )], expected loss data. this framework captures fundamental tasks, neural network training. 31st Conference Neural Information Processing Systems (nips 2017), Long beach, usa. paper, focus parallel SGD methods, received considerable attention recently due high scalability]. specifically, setting large dataset partitioned processors, collectively minimize function each processor maintains local copy parameter vector iteration, obtains stochastic gradient update (corresponding local data). processors broadcast gradient updates peers, aggregate gradients compute iterate current implementations parallel sgd, iteration, processor communicate entire gradient update processors. gradient vector dense, processor send receive ﬂoating-point numbers iteration/from peer communicate gradients maintain parameter vector practical applications, communicating gradients iteration observed significant performance bottleneck]. one popular reduce cost perform lossy compression gradients]. simple implementation simply reduce precision representation, shown converge convexity sparsity assumptions]. drastic quantization technique 1bitsgd], reduces component gradient sign (one bit), scaled average coordinates, accumulating errors locally. 1bitsgd experimentally observed preserve convergence], conditions; reduction communication, enabled state--art scaling deep neural networks (dnns) acoustic modelling]. however, 1bitsgd guarantees, strong assumptions, clear higher compression achievable. contributions. our focus understanding trade-offs communication cost dataparallel sgd, convergence guarantees. propose family algorithms allowing lossy compression gradients called Quantized SGD (qsgd), processors trade-off number bits communicated iteration variance added process. qsgd built algorithmic ideas. intuitive stochastic quantization scheme: gradient vector processor, quantize component randomized rounding discrete set values, principled preserves statistical properties original. step eﬃcient lossless code \\x0cfor quantized gradients, exploits statistical properties generate eﬃcient encodings. our analysis tight bounds precision-variance trade-off induced qsgd.  extreme trade-off, guarantee processor transmits (log)) expected bits iteration, increasing variance multiplicative factor. extreme, show processor transmit  bits iteration expectation, increasing variance factor particular, regime, compared full precision sgd,  bits communication iteration opposed 32n bits, guarantee iterations, leading bandwidth savings ?. qsgd fairly general: shown converge, assumptions, local minima nonconvex objectives, asynchronous iterations. one non-trivial extension develop stochastic variance-reduced] variant qsgd, called qsvrg, exponential convergence rate. one key question wher qsgd compression-variance trade-off inherent: instance, algorithm guaranteeing constant variance blowup transmit ) bits iteration? answer positive: improving asymptotically trade-off break communication complexity lower bound distributed estimation (see, Proposition]). experiments. crucial question wher, practice, QSGD reduce communication cost offset overhead additional iterations convergence. answer yes. explore practicality QSGD variety state--art datasets machine learning models: examine performance training networks image classification tasks (alexnet, inception, resnet, vgg) ImageNet] cifar] datasets, LSTMs] speech recognition. implement QSGD Microsoft CNTK]. experiments show models significantly benefit reduced communication multi-gpu training, virtually accuracy loss, standard parameters. for example, training AlexNet GPUs standard parameters, reduction communication time?, reduction training network top accuracy?. when training LSTM gpus, reduction communication time?, reduction training time target accuracy?. furr, computationallyheavy architectures Inception ResNet benefit reduction communication: 16gpus, QSGD reduces end-end convergence time ResNet152 approximately?. networks trained QSGD converge virtually accuracy full-precision variants, gradient quantization slightly improve accuracy settings. related work. one line related research studies communication complexity convex optimization. particular] studied two-processor convex minimization model, provided lower bound (log log/))) bits communication cost-dimensional convex problems, proposed nonstochastic algorithm strongly convex problems, communication cost log factor lower bound. contrast, focus stochastic \\x0cgradient methods. recent work] focused round complexity lower bounds number communication rounds convex learning. buckwild! ] convergence guarantees low-precision sgd. gave upper bounds error probability sgd, assuming unbiased stochastic quantization, convexity, gradient sparsity, showed significant speedup solving convex problems cpus. qsgd refines results focusing trade-off communication convergence. view quantization independent source variance sgd, employ standard convergence results]. main differences buckwild! focus variance-precision trade-off; results apply quantized non-convex case; validate practicality scheme neural network training gpus. concurrent work proposes TernGrad], starts similar stochastic quantization, focuses case individual gradient components values. show significant speedups achieved TensorFlow], maintaining accuracy percentage points relative full precision. main differences work are: implementation guarantees convergence standard assumptions; strive provide black-box compression technique, additional hyperparameters tune; experimentally, QSGD maintains accuracy target number epochs; this, gradients larger bit width; experiments focus single-machine multi-gpu case. note QSGD applied solve distributed estimation problem] optimal error-communication trade-off regimes. contrast elegant random rotation solution presented], QSGD employs quantization Elias coding. our case federated learning application], advantage eﬃcient compute gpu. extremely rich area studying algorithms systems eﬃcient distributed large-scale learning. ]. significant interest recently dedicated quantized frameworks, inference] training]. context] proposed 1bitsgd, heuristic compressing gradients sgd, inspired delta-sigma modulation]. implemented Microsoft cntk, cost bits ﬂoats iteration. variants shown perform large-scale Amazon datasets]. compared 1bitsgd, QSGD achieve asymptotically higher compression, provably converges standard assumptions, shows superior practical performance cases. Preliminaries SGD variants, preconditions guarantees. our techniques rar portable, applied black-box fashion top sgd. for conciseness, focus basic SGD setup. assumptions standard. ]. let convex set, differentiable, convex, smooth, unknown. assume repeated access stochastic gradients (possibly random) input outputs direction expectation correct direction \\x0cmove. formally: Definition. fix stochastic gradient random function). stochastic gradient moment k22   variance )     observe stochastic gradient moment bound automatically stochastic gradient variance bound )   long). second, convex optimization, assumes moment bound data: Local copy parameter vector iteration Let eti independent stochastic gradient encode)) //encode gradients broadcast peers; peer receive peer decode //decode gradients end Figure illustration generalized stochastic quantization levels. end   Algorithm Parallel SGD algorithm. dealing non-smooth convex optimization, variance bound dealing smooth convex optimization. however, convenient consistently assume moment bound. this major distinction ory practice]. given access stochastic gradients, starting point SGD builds iterates Equation), projected sequence step sizes. setting, show: orem], orem). let convex, unknown, convex-smooth. let given, supx  let fixed. given repeated, independent access stochastic gradients variance bound SGD initial point constant step sizes/?    min)  achieves LR2  ) Minibatched sgd. modification SGD scheme presented observed practice technique minibatching. minibatched sgd, updates form get get  independent stochastic  gradient hard get stochastic gradients variance bound stochastic gradient variance bound . inspection orem, long term) dominates, minibatched SGD requires fewer iterations converge. data-parallel sgd. synchronous data-parallel sgd, modelling real-world multi-gpu systems, focus communication cost SGD setting. set processors proceed synchronous steps, communicate point-point messages. each processor maintains local copy vector dimension representing current estimate minimizer, access private, independent stochastic gradients synchronous iteration, Algorithm processor aggregates obtains random gradient updates component communicates updates peers, finally aggregates received updates applies locally. importantly, add encoding decoding steps gradients send/receive lines respectively. following, describing variant sgd, assume general pattern, encode/decode functions. notice decoding step necessarily recover original gradient‘ instead, apply approximate version. when encoding decoding steps identity., encoding decoding), refer algorithm parallel sgd. case, simple calculation processor, processors held iteration updated end iteration ‘ stochatic gradient. particular, update minibatched update size thus, discussion above, rephrasing orem, corollary: Corollary. let orem. fix suppose run parallel SGD processors, access independent stochastic gradients moment bound step size/?  orem.  max min)  ) reasonable regimes, term max) dominate number iterations necessary. specifically, number iterations depend linearly moment bound Quantized Stochastic Gradient Descent (qsgd) section, present main results stochastically quantized sgd. throughout, log denotes base logarithm, number bits represent ﬂoat. for vector kvk0 denote number nonzeros For string  }? —?— denote length. for scalar sgn) } denote sign, sgn)  \\x0cgeneralized Stochastic Quantization Coding Stochastic quantization. general, parametrizable lossycompression scheme stochastic gradient vectors. quantization function denoted), tuning parameter, number quantization levels implement. intuitively, define uniformly distributed levels quantized preserves expectation, introduces minimal variance. please Figure for ) defined kvk2 sgn  independent random variables defined follows. let integer —/kvk2 ]. that] quantization interval —/kvk2    probability kvk orwise. here,  ]. define, distribution, minimal variance distributions support, }, expectation satisfies —/kvk2 formally, show: Lemma. for vector )] (unbiasedness),? [kqs)? vk22 min)kvk22 (variance bound), (iii[kqs  (sparsity). eﬃcient Coding gradients. observe vector output) naturally expressible tuple (kvk2  vector signs vector integer values ). key idea coding scheme integer values , equally likely: particular, larger integers frequent. exploit specialized Elias integer encoding], presented full full version paper]. intuitively, positive integer code, denoted elias), starts binary representation prepends length representation. recursively encodes prefix. show positive integer length resulting code —elias)— log log log   )) log encoding decoding eﬃciently. given gradient vector represented triple (kvk2 quantization levels, coding outputs string defined follows. first, bits encode kvk2 proceeds encode Elias recursive coding position nonzero entry appends bit denoting elias )). iteratively, proceeds encode distance current coordinate nonzero, encodes coordinate way. decoding scheme straightforward: read bits construct kvk2 iteratively decoding scheme Elias recursive coding read positions values nonzeros  properties quantization encoding imply following. orem. let fixed, arbitrary. fix quantization levels. ) stochastic gradient moment bound)) stochastic gradient variance bound min moreover, encoding scheme expectation, number bits communicate)) upper bounded  . ) log Sparse levels, gradient density regime. for case., quantization ), second-moment blowup intuitively, means?that employ log bits iteration, convergence time increased).  dense regime. variance blowup minimized quantization levels; case, devise eﬃcient encoding yields order magnitude shorter codes compared full-precision variant. proof statement obvious, exploits statistical properties quantization guarantees Elias coding. corollary. let) orem. encoding scheme)) expectation length.  QSGD Guarantees Putting bounds communication variance guarantees SGD algorithms smooth, convex functions yield results: orem (smooth Convex qsgd). let orem. fix suppose run parallel QSGD quantization levels processors accessing indepen? dent stochastic gradients moment bound stepsize/?   orem min sn2  max minx)  moreover, QSGD re2, ) quires) log bits communication round.  special case reduced. qsgd portable, applied stochastic gradient method. for illustration, quantization] communication-eﬃcient non-convex sgd. orem (qsgd smooth nonconvex optimization). let -smooth (possibly nonconvex) function, arbitrary initial point. let fixed, random stopping time supported, QSGD quantization level constant stepsizes ) access   stochastic gradients min moment bound satisfies)k22  moreover, communication cost orem.  Quantized variance-reduced SGD \\x0cassume processors, parameter processor access functions {fim    goal approximately minimize for processor portion knows natural question wher apply stochastic quantization reduce communication parallel svrg. upon inspection, notice resulting update break standard svrg. resolve technical issue, proving quantize SVRG updates techniques obtain convergence bounds.  , defined Section. given Algorithm description. let) arbitrary starting point) beginning epoch processor broadcasts),mthat, unquantized full gradient, processors aggregate) within epoch, iteration  uniformly random integer] completely independent else. iteration epochp, processor broadcasts update vector) processor      )  Table Description networks, final top accuracy, end-end training speedup 8gpus. network AlexNet ResNet152 ResNet50 ResNet110-inception VGG19 LSTM Dataset ImageNet ImageNet ImageNet cifar ImageNet ImageNet AN4 params. 62m 60m 25m 11m 143m 13m init. rate top (32bit% top (qsgd% (4bit% (8bit% (4bit% (4bit (4bit) Speedup gpus    ? (projected? (projected) gpus) Each processor computes total update sets  ) end epoch processor sets prove following. orem. let), ‘-strongly convex, convex -smooth, let unique minimizer  /‘), QSVRG initial point) ensures) ?  ) ? epoch  moreover, QSVRG iterations epoch requires  bits communication epoch. discussion. particular, largely decouple dependence condition number communication. let /‘ denote condition number observe term subsumed epoch communication dominated). specifically, fixed attain accuracy(log/). long log ?(? true instance case  poly log) poly), communication \\x0cper epoch(? (log)). gradient descent. full version paper] application QSGD gradient descent. roughly, case, QSGD simply truncate gradient top components, sorted magnitude. QSGD Variants Our experiments stretch ory, deep networks, nonconvex objectives.  tested QSGD convex objectives. results closely follow ory, refore omitted.) our implementations depart previous algorithm description follows. first, notice control variance quantization quantizing buckets fixed size view gradient one-dimensional vector reshaping tensors necessary, bucket defined set consecutive vector values. . ith bucket sub-vector  ].) quantize bucket independently, qsgd. setting corresponds quantization (vanilla sgd), corresponds full quantization, previous section. easy that, bucketing, guarantees Lemma expressed terms opposed full dimension This knob control variance, cost storing extra scaling factor bucket values. example, bucket size 512, bits, variance increase due quantization upper bounded 512. this oretical justification similar convergence rates observe practice. difference ory scale maximum vector opposed-norm). intuitively, normalizing max preserves values, slightly higher accuracy number iterations. both methods baseline bandwidth reduction lower bit width. bits bits dimension), normalizing max longer sparsity guarantees. note affect bounds regime quantization levels component, employ sparsity case. (however, note practice max normalization generates non-trivial sparsity.) Experiments setup. performed experiments Amazon EC2.16xlarge instances, NVIDIA K80 gpus. instances GPUDirect peer-peer communication, support NVIDIA Figure Breakdown communication versus computation neu ral networks, gpus Training loss faster 2bit QSGD=128) 4bit QSGD=8192) 8bit QSGD=8192) SGD) AlexNet Accuracy versus time. 300 600 900 Time (sec) 1200) LSTM error time. 1500 Test accuracy (%) full-bit precision versus QSGD-bit. each bar represents total time epoch standard parameters. epoch time broken communication (bottom, solid) computation (top, transparent). although epoch time diminishes parallelize, proportion communication increases. 1bitsgd* 32bit QSGD 4bit QSGD 8bit Epoch 100 120) ResNet50 accuracy. figure Accuracy numbers networks. light blue lines represent-bit accuracy. nccl extensions. implemented QSGD GPUs Microsoft Cognitive Toolkit (cntk]. this package eﬃcient (mpibased) gpu-gpu communication, implements optimized version 1bit-sgd]. our code released open-source]. execute types tasks: image classification ILSVRC 2015 (imagenet], CIFAR10], MNIST], speech recognition CMU AN4 dataset]. for vision, experimented AlexNet], VGG], ResNet], Inception Batch Normalization] deep networks. for speech, trained LSTM network]. see Table details. protocol. our methodology emphasizes error tolerance, sense aim preserve accuracy networks trained. standard sizes networks, hyperparameters optimized 32bit precision variant. (unless orwise stated, default networks hyper-parameters optimized full-precision CNTK.) increased batch size balance communication computation larger GPU counts, past point lose accuracy. employed double buffering] perform communication quantization concurrently computation. quantization benefits lowering learning rates; yet, run 32bit learning rate, decrease bucket size reduce variance. quantize small gradient matrices 10k elements), computational cost quantizing significantly exceeds reduction communication. however, experiments% parameters transmitted quantized form. reshape matrices fit bucket sizes, receptive field split buckets. communication. computation. set experiments, examine ratio computation communication costs training, increased parallelism. image classification networks trained imagenet, LSTM trained an4. examine cost breakdown networks pass dataset (epoch). figure results networks image classification. variance epoch times practically negligible%), omit confidence intervals. figure leads interesting observations. first, based ratio communication computation, roughly split networks communication-intensive (alexnet, vgg, lstm), computation-intensive (inception, resnet). for network types, relative impact communication increases significantly increase number gpus. examining breakdown-bit version, networks significantly benefit reduced communication. for example, AlexNet GPUs batch size 1024% training time spent communication, LSTM GPUs batch size 256, ratio%.  ratios slightly changed increasing batch size, decrease accuracy. ].) next, examine impact QSGD communication training time. (communication time includes time spent compressing uncompressing gradients.) measured QSGD-bit quantization 128 bucket size-bit-bit quantization 512 bucket size. results variants similar, bucket sizes 4bit version sends% data-bit version (but  -bit). bucket sizes chosen ensure good convergence, carefully tuned. 16gpu AlexNet batch size 1024-bit QSGD reduces communication time?, epoch time?. lstm, reduces communication time?, epoch time?. runtime improvements non-trivial architectures considered. accuracy. examine QSGD inﬂuences accuracy convergence rate. ran AlexNet ResNet full convergence imagenet, LSTM an4, ResNet110 cifar, two-layer perceptron mnist. results Figure exact numbers Table qsgd tests performed 8gpu setup, compared full-precision accuracy networks. general, notice 4bit 8bit gradient quantization suﬃcient recover slightly improve full accuracy, ensuring non-trivial speedup. across experiments-bit gradients 512 bucket size suﬃcient recover improve full-precision accuracy. our results consistent recent work] noting benefits adding noise gradients training deep networks. thus, quantization source zero-mean noise, render communication eﬃcient. time, note aggressive quantization hurt accuracy. particular-bit QSGD 8192 bucket size (not shown) loses% top accuracy% top, versus full precision AlexNet trained number epochs. also, QSGD-bit bucket size gap% top% top. one issue exam \\x0cined detail layers sensitive quantization. appears quantizing convolutional layers aggressively-bit precision) lead accuracy loss trained period time full precision variant. however, increasing precision-bit-bit recovers accuracy. this finding suggests modern architectures vision tasks, ResNet inception, convolutional, benefit quantization recurrent deep networks lstms. additional experiments. full version paper additional experiments, including full comparison 1bitsgd. brief, QSGD outperforms matches performance final accuracy 1bitsgd networks parameter values consider. Conclusions Future Work presented qsgd, family SGD algorithms smooth trade amount communication iteration running time. experiments suggest QSGD highly competitive fullprecision variant variety tasks. number optimizations explore. significant leveraging sparsity created qsgd. current implementations MPI provide support sparse types, plan explore support future work. furr, plan examine potential QSGD larger-scale applications, supercomputing. oretical side, interesting applications quantization sgd. full version paper] complete proofs, additional applications. Acknowledgments authors Martin jaggi, zhang, Frank Seide CNTK team support development project, anonymous NIPS reviewers careful consideration excellent suggestions. dan Alistarh supported Swiss National Fund Ambizione fellowship. jerry supported NSF CAREER Award ccf-1453261, ccf-1565235, Google Faculty Research award, NSF Graduate Research fellowship. this work developed part Dan alistarh, Jerri Milan Vojnovic Microsoft Research cambridge.',\n",
       " 'PP6828': 'joint density) set variables central object interest machine learning. access manipulate) enables wide range tasks performed, inference, prediction, data completion data generation. such, problem estimating) set examples core probabilistic unsupervised learning generative modelling. recent years, neural networks density estimation successful. combining ﬂexibility learning capacity neural networks prior knowledge structure data modelled led impressive results modelling natural images] audio data]. state--art neural density estimators likelihood-free inference simulated data], variational inference], surrogates maximum entropy models]. neural density estimators differ approaches generative modelling?such variational autoencoders] generative adversarial networks readily provide exact density evaluations. such, suitable applications focus explicitly evaluating densities, rar \\x0cgenerating syntic data. instance, density estimators learn suitable priors data large unlabelled datasets, standard Bayesian inference]. simulation-based likelihood-free inference, conditional density estimators learn models likelihood] posterior] simulated data. density estimators learn effective proposals importance sampling] sequential Monte Carlo]; proposals probabilistic programming environments speed inference]. finally, conditional density estimators ﬂexible inference networks amortized variational inference part variational autoencoders]. challenge neural density estimation construct models ﬂexible represent complex densities, tractable density functions learning algorithms. families neural density estimators ﬂexible tractable: autoregressive models] normalizing ﬂows]. autoregressive models decompose joint density product conditionals, model conditional turn. normalizing ﬂows transform base density. standard gaussian) target density invertible transformation tractable jacobian. 31st Conference Neural Information Processing Systems (nips 2017), Long beach, usa. starting point realization pointed Kingma. ]) autoregressive models, generate data, correspond differentiable transformation external source randomness (typically obtained random number generators). transformation tractable Jacobian design, autoregressive models invertible, precisely corresponds normalizing ﬂow. viewing autoregressive model normalizing ﬂow opens possibility increasing ﬂexibility stacking multiple models type, model provide source randomness model stack. resulting stack models normalizing ﬂow ﬂexible original model, remains tractable. paper present Masked Autoregressive Flow (maf), implementation normalizing ﬂow Masked Autoencoder Distribution Estimation (made] building block. made enables density evaluations sequential loop typical autoregressive models, makes MAF fast evaluate train parallel computing architectures Graphics Processing Units (gpus). show close oretical connection MAF Inverse Autoregressive Flow (iaf], designed variational inference density estimation, show correspond generalizations successful Real NVP]. experimentally evaluate MAF wide range datasets, demonstrate) MAF outperforms Real NVP general-purpose density estimation) conditional version MAF achieves close state--art performance conditional image modelling general-purpose architecture.  Background Autoregressive density estimation Using chain rule probability, joint density) decomposed product one-dimensional conditionals autoregressive density estimators] model conditional parametric density, parameters function hidden state recurrent architectures, function previous hidden state ith input variable real-valued Neural Autoregressive Density Estimator (rnade] mixtures Gaussian Laplace densities modelling conditionals, simple linear rule updating hidden state. ﬂexible approaches updating hidden state based Long short-term Memory recurrent neural networks]. drawback autoregressive models sensitive order variables. example, order variables matters learning density Figure assume model Gaussian conditionals. figure shows, model order learn density, model order represent perfectly. practice hard factorially orders suitable task hand. autoregressive models trained work order chosen random developed, predictions orders combined ensemble]. approach (section order layer, random orders possible. straightforward recurrent autoregressive models update hidden state sequentially variable, requiring sequential computations compute probability-dimensional vector, well-suited computation parallel architectures gpus. enable parallel computation start fully-connected model inputs outputs, drop connections order ensure output connected inputs . output interpreted computing parameters ith conditional construction, resulting model satisfy autoregressive property, time calculate) eﬃciently gpu. approach Masked Autoencoder Distribution Estimation (made], drops connections multiplying weight matrices fully-connected autoencoder binary masks. mechanisms dropping connections include masked convolutions] causal convolutions].   normalizing ﬂows normalizing ﬂow] represents) invertible differentiable transformation base density). ) ). base density) chosen easily evaluated input common choice) standard gaussian). ) Target density) MADE Gaussian conditionals) MAF layers Figure) density learnt, defined x22 ) density learnt MADE order Gaussian conditionals. scatter plot shows train data transformed random numbers non-gaussian distribution model poor fit. ) Learnt density transformed train data layer MAF order invertibility assumption density) \\x0ccan calculated ) det) order Equation) tractable, transformation constructed) easy invert) determinant Jacobian easy compute. important point transformations properties, composition properties. words, transformation made deeper composing multiple instances, result valid normalizing ﬂow. approaches developing normalizing ﬂows. early Gaussianization], based successive application independent component analysis. enforcing invertibility nonsingular weight matrices proposed], approaches calculating determinant Jacobian scales cubicly data dimensionality general. planar/radial ﬂows] Inverse Autoregressive Flow (iaf] models Jacobian tractable design. however, developed primarily variational inference well-suited density estimation, eﬃciently calculate density samples externally provided datapoints. non-linear Independent Components Estimator (nice] successor Real NVP] tractable Jacobian suitable density estimation. iaf, NICE Real NVP discussed detail Section  Masked Autoregressive Flow Autoregressive models normalizing ﬂows Consider autoregressive model conditionals parameterized single gaussians. , ith conditional (exp ) above unconstrained scalar functions compute log standard deviation ith conditional previous variables. generate data model recursion: exp ). ) above   vector random numbers model internally generate data, typically making calls random number generator called randn(). equation) alternative characterization autoregressive model transformation space random numbers space data , express model) ). construction, easily invertible. datapoint random numbers generate obtained recursion exp(?  ) Due autoregressive structure, Jacobian triangular design, absolute determinant easily obtained follows: det exp ) autoregressive model equivalently interpreted normalizing ﬂow, density) obtained substituting Equations) Equation). observation pointed Kingma. ]. diagnostic assessing wher autoregressive model type fits target density transform train data random numbers Equation), assess wher independent standard normals. independent standard normals, evidence model bad fit. instance, Figure shows scatter plot random numbers train data significantly non-gaussian model fits target density poorly. interpret autoregressive models ﬂow, improve model fit stacking multiple instances model deeper ﬂow. autoregressive models   model density random numbers model random numbers, finally modelling random numbers standard gaussian. stacking adds ﬂexibility: example, Figure demonstrates ﬂow autoregressive models learn multimodal conditionals, model unimodal conditionals. stacking previously similar improve model fit deep belief nets] deep mixtures factor analyzers]. choose implement set functions masking, approach MADE]. made feedforward network takes input outputs single forward pass. autoregressive property enforced multiplying weight matrices MADE suitably constructed binary masks. words, MADE Gaussian conditionals building layer ﬂow. benefit masking enables transforming data random numbers calculating) forward pass ﬂow, eliminating sequential recursion Equation). call implementation stacking MADEs ﬂow Masked Autoregressive Flow (maf).  Relationship Inverse Autoregressive Flow Like maf, Inverse Autoregressive Flow (iaf] normalizing ﬂow MADE component layer. layer IAF defined recursion: exp ) Similarly maf, functions computed MADE Gaussian conditionals. difference architectural: MAF directly computed previous data variables IAF directly computed previous random numbers consequence MAF IAF models computational trade-offs. maf capable calculating density) datapoint pass model, sampling requires performing sequential passes (where dimensionality). contrast, IAF generate samples calculate density pass, calculating density) externally provided datapoint requires \\x0cpasses find random numbers hence, design choice wher connect directly (obtaining maf (obtaining iaf) depends intended usage. iaf suitable recognition model stochastic variational inference], calculate density samples. contrast, MAF suitable density estimation, requires pass model IAF requires oretical equivalence MAF IAF training MAF maximum likelihood corresponds fitting implicit IAF base density stochastic variational inference. ) data density learn) base density, transformation implemented maf. density defined MAF (with added subscript disambiguation) ) det) inverse transformation describing implicit IAF base density), defines implicit density space)) det ) Training MAF maximizing total log likelihood log train data corresponds fitting) stochastically minimizing DKL)). section supplementary material, show DKL)) DKL)). ) hence, stochastically minimizing DKL)) equivalent fitting) minimizing DKL)). loss function variational inference) IAF base density) transformation training MAF density estimator) equivalent performing stochastic variational inference implicit iaf, posterior base density) transformation implements reparameterization trick]. argument presented detail Section supplementary material.  Relationship Real NVP Real NVP] (nvp stands Non Volume preserving) normalizing ﬂow obtained stacking coupling layers. coupling layer invertible transformation random numbers data tractable jacobian, defined exp       ) above, denotes elementwise multiplication, exp applied element transformation copies elements, scales shifts remaining elements, amount scaling shifting function elements. stacking coupling layers ﬂow, elements permuted layers set elements copied time. special case coupling layer nice]. coupling layer special case \\x0cautoregressive transformation MAF Equation), autoregressive transformation IAF Equation). indeed, recover coupling layer autoregressive transformation MAF setting making functions (for IAF make functions). words, MAF IAF ﬂexible (but different) generalizations Real nvp, element individually scaled shifted function previous elements. advantage Real NVP compared MAF IAF generate data estimate densities forward pass only, MAF passes generate data IAF passes estimate densities.  Conditional MAF Given set pairs )}, conditional density estimation task estimating conditional density). autoregressive modelling extends naturally conditional density estimation. term chain rule probability conditioned side-information decomposing conditional density). refore, turn unconditional autoregressive model conditional augmenting set input variables modelling conditionals correspond order variables chosen, long masked autoregressive models, connections dropped inputs rest network. implement conditional version MAF stacking MADEs made conditional strategy. , conditional maf, vector additional input layer. special case maf, Real NVP made conditional way. section show conditional MAF significantly outperforms unconditional MAF conditional information (such data labels) available. experiments, MAF benefit conditioning considerably MADE Real nvp.  Experiments Implementation setup systematically evaluate types density estimator (made, Real NVP maf) terms density estimation performance variety datasets. code reproducing experiments (which ano]) found https://github.com/gpapamak/maf. made. versions) MADE Gaussian conditionals, denoted simply made) MADE conditionals parameterized mixture gaussians, denoted MADE mog. experiments. MADE eir MADE MoG MAF autoregressive layer. adding Gaussian components conditional stacking MADEs form MAF alternative ways increasing ﬂexibility made, interested comparing. real nvp. general-purpose implementation coupling layer, feedforward neural networks, implementing scaling function shifting function respectively. networks architecture, hyperbolic tangent hidden units, rectified linear hidden \\x0cunits found combination perform best). networks linear output. Real NVPs eir coupling layers, denoted Real NVP) Real NVP) respectively, cases base density standard gaussian. successive coupling layers alternate) copying odd-indexed variables transforming even-indexed variables) copying even-indexed variables transforming odd-indexed variables. important clarify general-purpose implementation Real NVP comparable original version], designed specifically image data. interested comparing coupling layers autoregressive layers building blocks normalizing ﬂows general-purpose density estimation tasks, design Real NVP fair comparison made. maf. versions) MAF autoregressive layers standard Gaussian base density), denoted MAF) MAF autoregressive layers standard Gaussian base density, denoted MAF) MAF autoregressive layers MADE MoG Gaussian components base density, denoted MAF MoG). MAF MoG) thought MAF) stacked top MADE MoG trained jointly. experiments, MADE MADE MoG order inputs order dataset default; alternative orders considered. maf default order autoregressive layer. layer directly models data) reverses order successive layer IAF Kingma. ]). made, MADE MoG layer MAF feedforward neural network masked weight matrices, autoregressive property holds. procedure designing masks (due Germain. ]) follows. input hidden unit assigned degree, integer ranging data dimensionality. degree input index order. outputs degrees sequentially range. unit allowed receive input units lower equal degree, enforces autoregressive property. order output connected inputs degree make conditional independences introduced, sufficient hidden layer degree. experiments cifar, sequentially assign degrees hidden layer hidden units make degrees appear. cifar high-dimensional, fewer hidden units inputs assigned degrees hidden units uniformly random Germain. ]). added batch normalization] coupling layer Real NVP autoregressive layer maf. batch normalization elementwise scaling shifting, easily invertible tractable jacobian, suitable normalizing ﬂow. found batch normalization Real NVP MAF reduces training time, increases stability training improves performance observed Dinh. ] Real nvp). section supplementary material discusses implementation batch normalization normalizing ﬂows. models trained Adam optimizer], minibatch size 100, step size MADE MADE mog Real NVP maf. small amount Table Average test log likelihood nats) unconditional density estimation. performing model dataset shown bold (multiple models highlighted difference statistically significant paired-test). error bars correspond standard deviations. power GAS Gaussian   MADE MADE MoG   Real NVP  Real NVP  MAF) MAF) MAF MoG) HEPMASS BSDS300         148  153     MINIBOONE     152  153      155      154      156  regularization added, coeﬃcient model trained early stopping improvement occurred consecutive epochs validation set. model, selected number hidden layers number hidden units based validation performance gave options models), Section supplementary material.  Unconditional density estimation Following Uria. ], perform unconditional density estimation UCI datasets (power, gas, hepmass, miniboone) dataset natural image patches (bsds300). uci datasets. datasets UCI machine learning repository]. selected datasets Uria. ], smaller, resulting expensive cross-validation procedure involving separate hyperparameter search fold. however, data preprocessing Uria. ]. sample subtracted data feature divided sample standard deviation. discrete-valued attributes eliminated, \\x0cwell attribute Pearson correlation coeﬃcient greater. procedures meant avoid trivial high densities, make comparison approaches hard interpret. section supplementary material details UCI datasets individual preprocessing image patches. dataset obtained extracting random monochrome patches BSDS300 dataset natural images]. preprocessing Uria. ]. uniform noise added dequantize pixel values, rescaled range]. pixel subtracted patch, bottom-right pixel discarded. table shows performance model dataset. gaussian fitted train data reported baseline. datasets MAF performing model, MADE MoG performing model datasets, MAF outperforms Real nvp. MINIBOONE dataset, due overlapping error bars, pairwise comparison determine model performs best, results reported Section supplementary material. maf MoG) achieves reported result BSDS300 single model 156 nats, Deep RNADE] 155. ensemble Deep RNADEs reported achieve 157 nats]. uci datasets time literature density estimation, comparison existing work made yet.  Conditional density estimation For conditional density estimation, MNIST dataset handwritten digits] cifar dataset natural images]. datasets, datapoint distinct classes. represent class label-dimensional, one-hot encoded vector model density),pwhere represents image. test time, evaluate probability test image) uniform prior labels. comparison, train model unconditional density estimator report results. table Average test log likelihood nats) conditional density estimation. performing model dataset shown bold. error bars correspond standard deviations. mnist Gaussian MADE MADE MoG cifar unconditional conditional unconditional conditional ?1366  ?1344  2367 2030 ?1380  ?1038  ?1361  ?1030  147 ?397 187 ?119 real NVP) Real NVP) ?1323  ?1370  ?1326  ?1371  2576 2568 2642 2475 MAF) MAF) MAF MoG) ?1300  ?1313  ?1100  ?591  ?605  ?1092  2936 3049 2911 5797 5872 2936 for MNIST cifar, preprocessing Dinh. ]. dequantize pixel values adding uniform noise, rescale]. transform rescaled pixel values logit space logit(?   ),  MNIST  cifar, perform density estimation space. case cifar, augment train set horizontal ﬂips train examples Dinh. ]). table shows results MNIST cifar. performance class-conditional Gaussian reported baseline conditional case. log likelihoods calculated logit space. unconditional density estimation, MADE MoG performing model mnist, MAF performing model cifar. conditional density estimation, MAF performing model datasets. cifar, MADE MADE MoG performed significantly worse Gaussian baseline. maf outperforms Real NVP cases. conditional performance MAF impressive. maf performs compared unconditional version model conditional version. facilitate comparison literature, Section supplementary material reports results bits/pixel. MAF) MAF), performing conditional models, achieve bits/pixel cifar. result close state--art bits/pixel achieved conditional pixelcnn], though, unlike pixelcnn++, version MAF incorporate prior image knowledge, pays price density estimation transformed real-valued space (pixelcnn++ directly models discrete pixel values). discussion showed improve MADE modelling density internal random numbers. alternatively, MADE improved increasing ﬂexibility conditionals. comparison MAF MADE MoG showed approach dataset specific; experiments MAF outperformed MADE MoG cases, strong evidence competitiveness. made MoG universal density approximator; suﬃciently hidden units Gaussian components, approximate continuous density arbitrarily well. open question wher MAF Gaussian base density similar property (maf MoG does). showed coupling layer Real NVP special case autoregressive layer maf. fact, MAF outperformed Real NVP experiments. real NVP achieved impressive performance image modelling incorporating knowledge image structure. results suggest replacing coupling layers autoregressive layers original version Real NVP promising direction furr improving performance. real NVP maintains advantage MAF (and autoregressive models general) samples model generated eﬃciently parallel. maf achieved impressive results conditional density estimation. models considered benefited additional information supplied labels, MAF doubled performance, coming close state--art models image modelling incorporating prior image knowledge. ability MAF benefit significantly conditional knowledge suggests automatic discovery conditional structure. finding labels clustering) promising direction improving unconditional density estimation general. density estimation types generative modelling, focus obtaining accurate densities. however, accurate densities necessarily imply good performance tasks, data generation]. alternative approaches generative modelling include variational autoencoders], capable eﬃcient inference (potentially interpretable) latent space, generative adversarial networks], capable high quality data generation. choice method informed wher application hand calls accurate densities, latent space inference high quality samples. masked Autoregressive Flow contribution goals. acknowledgments Maria Gorinova comments. george Papamakarios Pavlakou supported Centre Doctoral Training Data science, funded EPSRC (grant/l016427) University edinburgh. george Papamakarios supported Microsoft Research PhD Scholarship programme. joint density) set variables central object interest machine learning. being access manipulate) enables wide range tasks performed, inference, prediction, data completion data generation. such, problem estimating) set examples core probabilistic unsupervised learning generative modelling. recent years, neural networks density estimation successful. combining ﬂexibility learning capacity neural networks prior knowledge structure data modelled led impressive results modelling natural images] audio data]. state--art neural density estimators likelihood-free inference simulated data], variational inference], surrogates maximum entropy models]. neural density estimators differ approaches generative modelling?such variational autoencoders] generative adversarial networks readily provide exact density evaluations. such, suitable applications focus explicitly evaluating densities, rar \\x0cgenerating syntic data. for instance, density estimators learn suitable priors data large unlabelled datasets, standard Bayesian inference]. simulation-based likelihood-free inference, conditional density estimators learn models likelihood] posterior] simulated data. density estimators learn effective proposals importance sampling] sequential Monte Carlo]; proposals probabilistic programming environments speed inference]. finally, conditional density estimators ﬂexible inference networks amortized variational inference part variational autoencoders]. challenge neural density estimation construct models ﬂexible represent complex densities, tractable density functions learning algorithms. families neural density estimators ﬂexible tractable: autoregressive models] normalizing ﬂows]. autoregressive models decompose joint density product conditionals, model conditional turn. normalizing ﬂows transform base density. standard gaussian) target density invertible transformation tractable jacobian. 31st Conference Neural Information Processing Systems (nips 2017), Long beach, usa. our starting point realization pointed Kingma. ]) autoregressive models, generate data, correspond differentiable transformation external source randomness (typically obtained random number generators). this transformation tractable Jacobian design, autoregressive models invertible, precisely corresponds normalizing ﬂow. viewing autoregressive model normalizing ﬂow opens possibility increasing ﬂexibility stacking multiple models type, model provide source randomness model stack. resulting stack models normalizing ﬂow ﬂexible original model, remains tractable. paper present Masked Autoregressive Flow (maf), implementation normalizing ﬂow Masked Autoencoder Distribution Estimation (made] building block. MADE enables density evaluations sequential loop typical autoregressive models, makes MAF fast evaluate train parallel computing architectures Graphics Processing Units (gpus). show close oretical connection MAF Inverse Autoregressive Flow (iaf], designed variational inference density estimation, show correspond generalizations successful Real NVP]. experimentally evaluate MAF wide range datasets, demonstrate) MAF outperforms Real NVP general-purpose density estimation) conditional version MAF achieves close state--art performance conditional image modelling general-purpose architecture.  Background Autoregressive density estimation Using chain rule probability, joint density) decomposed product one-dimensional conditionals autoregressive density estimators] model conditional parametric density, parameters function hidden state recurrent architectures, function previous hidden state ith input variable real-valued Neural Autoregressive Density Estimator (rnade] mixtures Gaussian Laplace densities modelling conditionals, simple linear rule updating hidden state. more ﬂexible approaches updating hidden state based Long short-term Memory recurrent neural networks]. drawback autoregressive models sensitive order variables. for example, order variables matters learning density Figure assume model Gaussian conditionals. Figure shows, model order learn density, model order represent perfectly. practice hard factorially orders suitable task hand. autoregressive models trained work order chosen random developed, predictions orders combined ensemble]. our approach (section order layer, random orders possible. straightforward recurrent autoregressive models update hidden state sequentially variable, requiring sequential computations compute probability-dimensional vector, well-suited computation parallel architectures gpus. one enable parallel computation start fully-connected model inputs outputs, drop connections order ensure output connected inputs . output interpreted computing parameters ith conditional construction, resulting model satisfy autoregressive property, time calculate) eﬃciently gpu. approach Masked Autoencoder Distribution Estimation (made], drops connections multiplying weight matrices fully-connected autoencoder binary masks. mechanisms dropping connections include masked convolutions] causal convolutions].   normalizing ﬂows normalizing ﬂow] represents) invertible differentiable transformation base density). that) ). base density) chosen easily evaluated input common choice) standard gaussian). under) Target density) MADE Gaussian conditionals) MAF layers Figure) density learnt, defined x22 ) density learnt MADE order Gaussian conditionals. scatter plot shows train data transformed random numbers non-gaussian distribution model poor fit. ) Learnt density transformed train data layer MAF order invertibility assumption density) \\x0ccan calculated ) det) order Equation) tractable, transformation constructed) easy invert) determinant Jacobian easy compute. important point transformations properties, composition properties. words, transformation made deeper composing multiple instances, result valid normalizing ﬂow. approaches developing normalizing ﬂows. early Gaussianization], based successive application independent component analysis. enforcing invertibility nonsingular weight matrices proposed], approaches calculating determinant Jacobian scales cubicly data dimensionality general. planar/radial ﬂows] Inverse Autoregressive Flow (iaf] models Jacobian tractable design. however, developed primarily variational inference well-suited density estimation, eﬃciently calculate density samples externally provided datapoints. non-linear Independent Components Estimator (nice] successor Real NVP] tractable Jacobian suitable density estimation. iaf, NICE Real NVP discussed detail Section  Masked Autoregressive Flow Autoregressive models normalizing ﬂows Consider autoregressive model conditionals parameterized single gaussians. that, ith conditional (exp ) above unconstrained scalar functions compute log standard deviation ith conditional previous variables. generate data model recursion: exp ). ) above   vector random numbers model internally generate data, typically making calls random number generator called randn(). equation) alternative characterization autoregressive model transformation space random numbers space data that, express model) ). construction, easily invertible. given datapoint random numbers generate obtained recursion exp(?  ) Due autoregressive structure, Jacobian triangular design, absolute determinant easily obtained follows: det exp ) autoregressive model equivalently interpreted normalizing ﬂow, density) obtained substituting Equations) Equation). this observation pointed Kingma. ]. diagnostic assessing wher autoregressive model type fits target density transform train data random numbers Equation), assess wher independent standard normals. independent standard normals, evidence model bad fit. for instance, Figure shows scatter plot random numbers train data significantly non-gaussian model fits target density poorly. here interpret autoregressive models ﬂow, improve model fit stacking multiple instances model deeper ﬂow. given autoregressive models   model density random numbers model random numbers, finally modelling random numbers standard gaussian. this stacking adds ﬂexibility: example, Figure demonstrates ﬂow autoregressive models learn multimodal conditionals, model unimodal conditionals. stacking previously similar improve model fit deep belief nets] deep mixtures factor analyzers]. choose implement set functions masking, approach MADE]. made feedforward network takes input outputs single forward pass. autoregressive property enforced multiplying weight matrices MADE suitably constructed binary masks. words, MADE Gaussian conditionals building layer ﬂow. benefit masking enables transforming data random numbers calculating) forward pass ﬂow, eliminating sequential recursion Equation). call implementation stacking MADEs ﬂow Masked Autoregressive Flow (maf).  Relationship Inverse Autoregressive Flow Like maf, Inverse Autoregressive Flow (iaf] normalizing ﬂow MADE component layer. each layer IAF defined recursion: exp ) Similarly maf, functions computed MADE Gaussian conditionals. difference architectural: MAF directly computed previous data variables IAF directly computed previous random numbers consequence MAF IAF models computational trade-offs. maf capable calculating density) datapoint pass model, sampling requires performing sequential passes (where dimensionality). contrast, IAF generate samples calculate density pass, calculating density) externally provided datapoint requires \\x0cpasses find random numbers hence, design choice wher connect directly (obtaining maf (obtaining iaf) depends intended usage. iaf suitable recognition model stochastic variational inference], calculate density samples. contrast, MAF suitable density estimation, requires pass model IAF requires oretical equivalence MAF IAF training MAF maximum likelihood corresponds fitting implicit IAF base density stochastic variational inference. let) data density learn) base density, transformation implemented maf. density defined MAF (with added subscript disambiguation) ) det) inverse transformation describing implicit IAF base density), defines implicit density space)) det ) Training MAF maximizing total log likelihood log train data corresponds fitting) stochastically minimizing DKL)). Section supplementary material, show DKL)) DKL)). ) hence, stochastically minimizing DKL)) equivalent fitting) minimizing DKL)). since loss function variational inference) IAF base density) transformation training MAF density estimator) equivalent performing stochastic variational inference implicit iaf, posterior base density) transformation implements reparameterization trick]. this argument presented detail Section supplementary material.  Relationship Real NVP Real NVP] (nvp stands Non Volume preserving) normalizing ﬂow obtained stacking coupling layers. coupling layer invertible transformation random numbers data tractable jacobian, defined exp       ) above, denotes elementwise multiplication, exp applied element transformation copies elements, scales shifts remaining elements, amount scaling shifting function elements. when stacking coupling layers ﬂow, elements permuted layers set elements copied time. special case coupling layer NICE]. coupling layer special case \\x0cautoregressive transformation MAF Equation), autoregressive transformation IAF Equation). indeed, recover coupling layer autoregressive transformation MAF setting making functions (for IAF make functions). words, MAF IAF ﬂexible (but different) generalizations Real nvp, element individually scaled shifted function previous elements. advantage Real NVP compared MAF IAF generate data estimate densities forward pass only, MAF passes generate data IAF passes estimate densities.  Conditional MAF Given set pairs )}, conditional density estimation task estimating conditional density). autoregressive modelling extends naturally conditional density estimation. each term chain rule probability conditioned side-information decomposing conditional density). refore, turn unconditional autoregressive model conditional augmenting set input variables modelling conditionals correspond any order variables chosen, long masked autoregressive models, connections dropped inputs rest network. implement conditional version MAF stacking MADEs made conditional strategy. that, conditional maf, vector additional input layer. special case maf, Real NVP made conditional way. Section show conditional MAF significantly outperforms unconditional MAF conditional information (such data labels) available. experiments, MAF benefit conditioning considerably MADE Real nvp.  Experiments Implementation setup systematically evaluate types density estimator (made, Real NVP maf) terms density estimation performance variety datasets. code reproducing experiments (which ano]) found https://github.com/gpapamak/maf. made. versions) MADE Gaussian conditionals, denoted simply made) MADE conditionals parameterized mixture gaussians, denoted MADE mog. experiments. MADE eir MADE MoG MAF autoregressive layer. adding Gaussian components conditional stacking MADEs form MAF alternative ways increasing ﬂexibility made, interested comparing. real nvp. general-purpose implementation coupling layer, feedforward neural networks, implementing scaling function shifting function respectively. both networks architecture, hyperbolic tangent hidden units, rectified linear hidden \\x0cunits found combination perform best). both networks linear output. Real NVPs eir coupling layers, denoted Real NVP) Real NVP) respectively, cases base density standard gaussian. successive coupling layers alternate) copying odd-indexed variables transforming even-indexed variables) copying even-indexed variables transforming odd-indexed variables. important clarify general-purpose implementation Real NVP comparable original version], designed specifically image data. here interested comparing coupling layers autoregressive layers building blocks normalizing ﬂows general-purpose density estimation tasks, design Real NVP fair comparison made. maf. versions) MAF autoregressive layers standard Gaussian base density), denoted MAF) MAF autoregressive layers standard Gaussian base density, denoted MAF) MAF autoregressive layers MADE MoG Gaussian components base density, denoted MAF MoG). MAF MoG) thought MAF) stacked top MADE MoG trained jointly. experiments, MADE MADE MoG order inputs order dataset default; alternative orders considered. maf default order autoregressive layer. layer directly models data) reverses order successive layer IAF Kingma. ]). made, MADE MoG layer MAF feedforward neural network masked weight matrices, autoregressive property holds. procedure designing masks (due Germain. ]) follows. each input hidden unit assigned degree, integer ranging data dimensionality. degree input index order. outputs degrees sequentially range. unit allowed receive input units lower equal degree, enforces autoregressive property. order output connected inputs degree make conditional independences introduced, sufficient hidden layer degree. experiments cifar, sequentially assign degrees hidden layer hidden units make degrees appear. because cifar high-dimensional, fewer hidden units inputs assigned degrees hidden units uniformly random Germain. ]). added batch normalization] coupling layer Real NVP autoregressive layer maf. batch normalization elementwise scaling shifting, easily invertible tractable jacobian, suitable normalizing ﬂow. found batch normalization Real NVP MAF reduces training time, increases stability training improves performance observed Dinh. ] Real nvp). section supplementary material discusses implementation batch normalization normalizing ﬂows. all models trained Adam optimizer], minibatch size 100, step size MADE MADE mog Real NVP maf. small amount Table Average test log likelihood nats) unconditional density estimation. performing model dataset shown bold (multiple models highlighted difference statistically significant paired-test). error bars correspond standard deviations. power GAS Gaussian   MADE MADE MoG   Real NVP  Real NVP  MAF) MAF) MAF MoG) HEPMASS BSDS300         148  153     MINIBOONE     152  153      155      154      156  regularization added, coeﬃcient each model trained early stopping improvement occurred consecutive epochs validation set. for model, selected number hidden layers number hidden units based validation performance gave options models), Section supplementary material.  Unconditional density estimation Following Uria. ], perform unconditional density estimation UCI datasets (power, gas, hepmass, miniboone) dataset natural image patches (bsds300). uci datasets. datasets UCI machine learning repository]. selected datasets Uria. ], smaller, resulting expensive cross-validation procedure involving separate hyperparameter search fold. however, data preprocessing Uria. ]. sample subtracted data feature divided sample standard deviation. discrete-valued attributes eliminated, \\x0cwell attribute Pearson correlation coeﬃcient greater. procedures meant avoid trivial high densities, make comparison approaches hard interpret. section supplementary material details UCI datasets individual preprocessing image patches. this dataset obtained extracting random monochrome patches BSDS300 dataset natural images]. preprocessing Uria. ]. uniform noise added dequantize pixel values, rescaled range]. pixel subtracted patch, bottom-right pixel discarded. table shows performance model dataset. Gaussian fitted train data reported baseline. datasets MAF performing model, MADE MoG performing model datasets, MAF outperforms Real nvp. for MINIBOONE dataset, due overlapping error bars, pairwise comparison determine model performs best, results reported Section supplementary material. maf MoG) achieves reported result BSDS300 single model 156 nats, Deep RNADE] 155. ensemble Deep RNADEs reported achieve 157 nats]. UCI datasets time literature density estimation, comparison existing work made yet.  Conditional density estimation For conditional density estimation, MNIST dataset handwritten digits] cifar dataset natural images]. datasets, datapoint distinct classes. represent class label-dimensional, one-hot encoded vector model density),pwhere represents image. test time, evaluate probability test image) uniform prior labels. for comparison, train model unconditional density estimator report results. Table Average test log likelihood nats) conditional density estimation. performing model dataset shown bold. error bars correspond standard deviations. mnist Gaussian MADE MADE MoG cifar unconditional conditional unconditional conditional ?1366  ?1344  2367 2030 ?1380  ?1038  ?1361  ?1030  147 ?397 187 ?119 real NVP) Real NVP) ?1323  ?1370  ?1326  ?1371  2576 2568 2642 2475 MAF) MAF) MAF MoG) ?1300  ?1313  ?1100  ?591  ?605  ?1092  2936 3049 2911 5797 5872 2936 For MNIST cifar, preprocessing Dinh. ]. dequantize pixel values adding uniform noise, rescale]. transform rescaled pixel values logit space logit(?   ),  MNIST  cifar, perform density estimation space. case cifar, augment train set horizontal ﬂips train examples Dinh. ]). table shows results MNIST cifar. performance class-conditional Gaussian reported baseline conditional case. log likelihoods calculated logit space. for unconditional density estimation, MADE MoG performing model mnist, MAF performing model cifar. for conditional density estimation, MAF performing model datasets. cifar, MADE MADE MoG performed significantly worse Gaussian baseline. maf outperforms Real NVP cases. conditional performance MAF impressive. maf performs compared unconditional version model conditional version. facilitate comparison literature, Section supplementary material reports results bits/pixel. MAF) MAF), performing conditional models, achieve bits/pixel cifar. this result close state--art bits/pixel achieved conditional pixelcnn], though, unlike pixelcnn++, version MAF incorporate prior image knowledge, pays price density estimation transformed real-valued space (pixelcnn++ directly models discrete pixel values). Discussion showed improve MADE modelling density internal random numbers. alternatively, MADE improved increasing ﬂexibility conditionals. comparison MAF MADE MoG showed approach dataset specific; experiments MAF outperformed MADE MoG cases, strong evidence competitiveness. made MoG universal density approximator; suﬃciently hidden units Gaussian components, approximate continuous density arbitrarily well. open question wher MAF Gaussian base density similar property (maf MoG does). showed coupling layer Real NVP special case autoregressive layer maf. fact, MAF outperformed Real NVP experiments. real NVP achieved impressive performance image modelling incorporating knowledge image structure. our results suggest replacing coupling layers autoregressive layers original version Real NVP promising direction furr improving performance. real NVP maintains advantage MAF (and autoregressive models general) samples model generated eﬃciently parallel. maf achieved impressive results conditional density estimation. whereas models considered benefited additional information supplied labels, MAF doubled performance, coming close state--art models image modelling incorporating prior image knowledge. ability MAF benefit significantly conditional knowledge suggests automatic discovery conditional structure. finding labels clustering) promising direction improving unconditional density estimation general. density estimation types generative modelling, focus obtaining accurate densities. however, accurate densities necessarily imply good performance tasks, data generation]. alternative approaches generative modelling include variational autoencoders], capable eﬃcient inference (potentially interpretable) latent space, generative adversarial networks], capable high quality data generation. choice method informed wher application hand calls accurate densities, latent space inference high quality samples. masked Autoregressive Flow contribution goals. acknowledgments Maria Gorinova comments. george Papamakarios Pavlakou supported Centre Doctoral Training Data science, funded EPSRC (grant/l016427) University edinburgh. george Papamakarios supported Microsoft Research PhD Scholarship programme.',\n",
       " 'PP6860': 'generative models subarea research rapidly growing recent years, successfully applied wide range modern real-world applications., chapter]). common approach address \\x0cdensity estimation problem aims learn model distribution pmodel approximates true, unknown, data distribution pdata methods approach deal fundamental problems. first, learning behaviors performance generative models depend choice objective functions train]. widely-used objective, considered-facto standard one, follow principle maximum likelihood estimate seeks model parameters maximize likelihood training data. equivalent minimizing kullback-leibler) divergence data model distributions: DKL (pdata kpmodel observed minimization result pmodel covers multiple modes pdata produce completely unseen potentially undesirable samples]. contrast, anor approach swap arguments instead, minimize: DKL (pmodel kpdata referred reverse divergence]. observed optimization reverse divergence criteria mimics mode-seeking process pmodel concentrates single mode pdata ignoring modes, problem mode collapse. behaviors well-studied]. 31st Conference Neural Information Processing Systems (nips 2017), Long beach, usa. problem choice formulation density function pmodel]. choose define explicit density function, straightforwardly follow maximum likelihood framework estimate parameters. anor idea estimate data distribution implicit density function, analytical forms pmodel] furr discussions). notably pioneered class generative adversarial network (gan], expressive generative model capable producing sharp realistic images natural scenes. generative models maximize data likelihood lower bound, GAN takes radical approach simulates game players: generator generates data mapping samples noise space input space; discriminator acts classifier distinguish real samples dataset fake samples produced generator Both parameterized neural networks, method categorized family deep generative models generative neural models]. optimization GAN formulates minimax problem, optimal learning objective turns finding minimizes jensen-shannon divergence (jsd): DJS (pdata kpmodel behavior JSD minimization empirically proven similar reverse divergence]. this, however, leads aforementioned issue mode collapse, notorious failure GAN] generator produces similarly images, yielding low entropy distribution poor variety samples. recent attempts made solve mode collapsing problem improving training gan. idea minibatch discrimination trick] discriminator detect samples unusually similar generated samples. heuristics helps generate visually appealing samples quickly, computationally \\x0cexpensive, hidden layer discriminator. anor approach unroll optimization discriminator steps create surrogate objective update generator training]. approach train generators discover modes data]. alternatively, time, attempts employ autoencoders regularizers auxiliary losses penalize missing modes]. models avoid mode collapsing problem extent, cost computational complexity exception DFM], rendering unscalable imagenet, large-scale challenging visual dataset. addressing challenges, propose approach effectively avoid mode collapse eﬃciently scale large datasets., imagenet). approach combines reverse divergences unified objective function, exploits complementary statistical properties divergences effectively diversify estimated density capturing multi-modes. materialize idea gan framework, resulting generative adversarial architecture players: discriminator rewards high scores data sampled pdata rar generated generator distribution whilst anor discriminator conversely, favoring data rar pdata generator generates data fool discriminators. term proposed model dual discriminator generative adversarial network (d2gan). turns training D2GAN shares minimax problem gan, solved alternatively updating generator discriminators. provide oretical analysis showing that, capacity., nonparametric limit, optimal points, training criterion results minimal distance data model distribution respect reverse divergences. helps model place fair distribution probability mass modes data generating distribution, allowing recover data distribution generate diverse samples generator single shot. addition, furr introduce hyperparameters stabilize learning control effect divergence. conduct extensive experiments syntic dataset real-world large-scale datasets (mnist, cifar10, stl, imagenet) nature. evaluating generative models notoriously hard], made effort adopt number evaluation metrics literature quantitatively compare proposed model latest stateof--art baselines possible. experimental results reveal method capable improving diversity keeping good quality generated samples. importantly, proposed model scaled train large-scale ImageNet database, obtain competitive variety score generate good quality images. short, main contributions are) generative adversarial model encourages diversity samples produced generator) oretical analysis prove objective optimized minimizing reverse divergence global optimum pdata \\x0cand (iii) comprehensive evaluation effectiveness proposed method wide range quantitative criteria large-scale datasets. generative Adversarial Nets review generative adversarial network (gan) introduced] formulate game players: discriminator generator discriminator), takes point data space computes probability sampled data distribution Pdata rar generated generator time, generator maps noise vector drawn prior) data space, obtaining sample) resembles training data, sample challenge discriminator. mapping) induces generator distribution data domain probability density function). parameterized neural networks (see fig. illustration) learned solving minimax optimization: min max?pdata) [log [log )))] learning iterative procedure discriminator generator alternatively updated. fixed maximization subject results optimal discriminator pdata) ) pdata whilst optimal minimization turns minimizing) jensen-shannon) divergence data model distributions: DJS (pdata kpg]. nash equilibrium game, model distribution recovers data distribution exactly: Pdata discriminator fails differentiate real fake data. ) gan. ) d2gan. figure illustration standard GAN proposed d2gan. divergence empirically proven nature reverse divergence], GAN suffers model collapsing problem, generated data samples low level diversity]. dual Discriminator Generative Adversarial Nets tackle gan problem mode collapse, present main contribution framework seeks approximated distribution effectively cover modes multimodal data. intuition based gan, formulate three-player game consists discriminators generator Given sample data space) rewards high score drawn data distribution Pdata low score generated model distribution contrast) returns high score generated whilst giving low score sample drawn Pdata unlike gan, scores returned discriminators values rar probabilities]. generator performs similar role gan., producing data mapped noise space synsize real data fool discriminators players parameterized neural networks share parameters. term proposed model dual discriminator generative adversarial network (d2gan). fig. shows illustration d2gan. more formally, play three-player minimax optimization game: min max,  ?pdata [log?pdata)]   [log) introduced hyperparameters  serve purposes. stabilize learning model. output values discriminators positive unbounded. ) large exponentially stronger impact optimization log) log, rendering learning unstable. overcome issue, decrease effect making optimization penalize), helping stabilize learning. purpose introducing  control effect reverse divergences optimization problem. discussed part derivation optimal solution. similar GAN], proposed network trained alternatively updating refer supplementary material pseudo-code learning parameters d2gan.  oretical analysis provide formal oretical analysis proposed model, essentially shows that, capacity., nonparametric limit, optimal points, recover data distributions minimizing reverse divergences model data distributions. optimization problem respect) discriminators fixed generator. proposition fixed maximizing, yields closed-form optimal discriminators? ? ?pdata? ? ) pdata) proof. induced measure orem], expectations equal) log). objective function rewritten below,  ?pdata [log?pdata)]   [log)] [?pdata) log) ) pdata log)] Considering function inside integral, maximize function variables find? ? ). setting derivatives gain) ?pdata) ) pdata) derivatives: ?pdata)/d12 )/d22 non-positive, verifying obtained maximum solution concluding proof. next, fix? ? find optimal solution generator orem ? ? nash equilibrium point? ? ? minimax optimization problem d2gan, form component? ? ?  (log   (log  ? ) ? ? pdata proof. substituting? ? . ) objective function. ) minimax problem, gain: pdata) pdata? ?   ?pdata log log  ) )  pdata   log log pdata) pdata) (log   (log  ?dkl (pdata kpg ?dkl kpdata) DKL (pdata kpg DKL kpdata reverse divergences data model (generator) distributions, respectively. divergences nonnegative distributions equal? pdata words, generator induces distribution? identical data distribution pdata discriminators fail recognize real fake samples return score samples. concludes proof. loss generator. ) upper bound discriminators optimal. loss shows increasing promotes optimization minimizing divergence DKL (pdata kpg helping generative distribution cover multiple modes, include potentially undesirable samples; increasing encourages minimization reverse divergence DKL kpdata enabling generator capture single mode better, miss modes. empirically adjusting hyperparameters, balance effect divergences, effectively avoid mode collapsing issue.  Connection-gan Next point relations proposed D2GAN-gan model extends jensen-shannon divergence (jsd) GAN general divergences, specifically -divergences]. divergence divergence family form: ) convex, lowersemicontinuous function satisfying) function convex conjugate function fenchel conjugate] ) supu?domf )}. function convex lower-semicontinuous. true distribution generator distribution, resemble learning problem GAN minimizing -divergence based variational lower bound -divergence proposed Nguyen. ], objective function-gan derived follows: min max (?, ? ?  ? )))]  parameterized  generator gan),  function parameterized  discriminator gan) domf output activation function., discriminator decision function) specific -divergence used. functions (see tab. ]), recover minimization divergences JSD gan, (associated discriminator reverse (associated discriminator d2gan. -gan, however, considers single divergence. hand, proposed method combines \\x0creserve divergences. idea conceived pondering advantages disadvantages divergences covering multiple modes data. combining unified objective function. ) helps reversely engineer finally obtain optimization game. ) eﬃciently formulated solved principle gan. experiments section, conduct comprehensive experiments demonstrate capability improving mode coverage scalability proposed model large-scale datasets. syntic dataset visual numerical verification, datasets increasing diversity size numerical verification. made effort compare results method latest state--art gan variants replicating experimental settings original work possible. experiment, refer supplementary material model architectures additional results. common points are: discriminators? outputs softplus activations., positive version relu) Adam optimizer] learning rate.0002 first-order momentum; (iii) minibatch size samples training generator discriminators) Leaky ReLU slope) weights initialized isotropic gaussian) Symmetric-div GAN Unrolled GAN D2GAN 5000 15000 10000 Step 20000 25000) Symmetric divergence. Wasserstein estimate GAN Unrolled GAN D2GAN 5000 10000 Step 15000 20000) Wasserstein distance. 25000) Evolution data blue) generated GAN (top row), UnrolledGAN (middle row) D2GAN (bottom row) data gaussians. data sampled true mixture red. figure comparison standard gan, UnrolledGAN D2GAN syntic dataset. biases. implementation TensorFlow] published version reference1 present experiments syntic data large-scale real-world datasets.  Syntic data experiment, reuse experimental design proposed] investigate D2GAN deal multiple modes data. specifically, sample training data mixture Gaussian distributions covariance matrix.02i means arranged circle centroid radius. data low variance mixture components separated area low density. aim examine properties low probability regions low separation modes. simple architecture generator fully connected hidden layers discriminators hidden layer ReLU activations. setting identical, ensures fair comparison UnrolledGAN2]. fig. shows evolution 512 samples generated models baselines time. regular GAN generates data collapsing single mode hovering valid modes data distribution, reﬂecting mode collapse gan. time, UnrolledGAN D2GAN distribute data mixture components, demonstrating abilities successfully learn multimodal data case. steps, D2GAN captures data modes precisely UnrolledGAN, mode, UnrolledGAN generates data concentrate points mode centroid, produce fewer samples D2GAN samples fairly spread entire mode. furr quantitatively compare quality generated data. true distribution pdata case, employ measures, symmetric divergence Wasserstein distance. measures compute distance normalized histograms,000 points generated d2gan, UnrolledGAN GAN true pdata figs. demonstrate superiority approach GAN UnrolledGAN distances (lower better); notably Wasserstein metric, distance true distribution reduces zero. figures demonstrate stability D2GAN (red curves) training ﬂuctuating compared GAN (green curves) UnrolledGAN (blue curves).  real-world datasets examine performance proposed method real-world datasets increasing diversities sizes. networks convolutional layers, closely follow dcgan design]. strided convolutions discriminators fractional-strided convolutions generator pooling layers. batch normalization applied layer, https://github.com/tund/d2gan obtain code UnrolledGAN data link authors provided]. generator output layer discriminator input layers. leaky ReLU activations discriminators, ReLU generator, \\x0cput tanh rescale pixel intensities range, feeding images model. difference that, model, initializing weights) yields slightly results). refer supplementary material detailed architectures.  Evaluation protocol Evaluating quality image produced generative models notoriously challenging due variety probability criteria lack perceptually meaningful image similarity metric]. model generate plausible images, images visually similar. refore, order quantify performance covering data modes producing high quality samples-hoc metrics experiments compare baselines. adopt Inception score proposed], computed: exp [dkl))]), conditional label distribution image estimated pretrained Inception model) marginal distribution)  )). metric rewards good varied samples, easily fooled model collapses generates low quality image, fails measure wher model trapped bad mode. address problem, labeled datasets, furr recruit-called MODE score introduced]: exp [dkl ))] dkl) ))) ) empirical distribution labels estimated training data. score adequately reﬂect variety visual quality images, discussed].  Handwritten digit images start handwritten digit images mnist] consists,000 training,000 testing grayscale images digits setting], assume MNIST modes, representing connected component data manifold, digit classes. perform extensive grid search hyperparameter configurations, regularized constants . ) varied}. fair comparison, parameter ranges fully connected layers network. supplementary material details), adopt results GAN mode regularized GAN (reg-gan]. evaluation, train simple, effective-layer convolutional nets3 obtain% error MNIST testing set, employ predict label probabilities compute MODE scores generated samples. fig. (left) shows distributions MODE scores obtained models. clearly, proposed D2GAN significantly outperforms standard GAN reg-gan achieving scores maximum]. worthy note observe substantial differences average MODE scores obtained varying network size parameter searching. report result minimal network smallest number layers hidden units. study effect inspect results obtained minimal \\x0cnetwork varied fig. (right). pattern that, fixed D2GAN obtains MODE score increasing value, score significantly decrease. mnist. standard MNIST data-mode assumption fairly trivial. hence, based data, test proposed model challenging one. continue technique] construct 1000-class MNIST dataset (mnist) stacking randomly selected digits form RGB image digit image channel. resulting data assumed,000 distinct modes, combinations digits channels 000 999. experiment, powerful model convolutional layers discriminators transposed convolutions generator. measure performance number modes Network architecture similar https://github.com/fchollet/keras/blob/master/examples/mnist cnn. ???? ??  ???? ???? ???? ??? ??? ??? ??? ??? ??? ??? ??? ??? ??? ??? ??? ??? ??? ??? ??? ??? ???????? ???? ???? ??? ??? ??? ??? ??? ?????           ???  ??? ??? ???? ???? ???  ??? ???? ????? ????? ??? ??? ??? ??? ??? ??? ??? ???? ??? ??? ??? figure Distributions MODE scores (left) average MODE scores (right) varied model generated total,600 samples, reverse divergence model distribution., label distribution predicted pretrained MNIST classifier previous experiment) expected data distribution. tab. reports results D2GAN compared gan, UnrolledGAN], DCGAN reg-gan]. proposed method demonstrates superiority baselines covering modes achieving distance close zero. table Numbers modes covered reverse divergence model data distributions. model modes covered DKL (modelk data GAN] 628?140 UnrolledGAN] 817 DCGAN] 849 reg-gan] 955 D2GAN 1000 Natural scene images extend experiments investigate scalability proposed method challenging large-scale image databases natural scenes. widely-adopted datasets: cifar], stl] ImageNet]. cifar well-studied dataset,000 training images classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck. stl, subset imagenet, 100,000 unlabeled images, diverse cifar, full imagenet. rescale images times train networks resolution. imagenet large database million natural images,000 classes, challenging benchmark validate scalability deep models. follow preprocessing], subsampling resolution. code provided] compute Inception score independent partitions,000 generated samples. table Inception scores cifar. model Real data WGAN] mix+wgan] improved-gan] ALI] BEGAN] MAGAN] DCGAN] DFM] D2GAN Score???????  ????? ?????     ???? ???? ???? ???? ???? ????   ¿??? ?? figure Inception scores stl imagenet. tab. fig. show Inception scores cifar, stl ImageNet datasets obtained model baselines collected recent work literature. worthy note compare methods trained completely unsupervised manner label information. result, exist baselines cifar whilst DCGAN] denoising feature matching (dfm] stl imagenet. tensorflow implementation DCGAN network architecture model fair comparisons. experiments, D2GAN fails beat dfm, outperforms baselines large margins. lower results compared DFM suggest autoencoders matching high-level features appears effective encourage diversity. technique compatible method, integrating promising avenue future work. discriminators identical architectures, potentially share parameters schemes. explore direction creating version D2GAN hyperparameter setting. version shares parameters (output) layer. model failed discriminator fewer parameters, rendering unable capture inverse ratios density functions. shares parameters layers. version performed previous one, obtain promising Inception scores cifar10 STL10 imagenet), results worse proposed model sharing parameters. finally, show samples generated proposed model trained datasets fig.  samples fair random draws, cherry-picked. d2gan produce visually recognizable images cars, trucks, boats, horses cifar. objects harder recognize, shapes airplanes, cars, trucks animals identified stl, images backgrounds sky, underwater, mountain, forest shown imagenet. confirms diversity samples generated model. ) cifar. ) stl. ) imagenet. figure Samples generated proposed D2GAN trained natural image datasets. due space limit, refer supplementary material larger plot. conclusion summarize, introduced approach combine KullbackLeibler) reverse divergences unified objective function density estimation problem. idea exploit complementary statistical properties divergences improve quality diversity samples generated estimator. end, propose framework based generative adversarial nets (gans), formulates minimax game players: discriminators generator, termed dual discriminator GAN (d2gan). discriminators fixed, learning generator moves optimizing reverse divergences simultaneously, avoid mode collapse, notorious drawback gans. established extensive experiments demonstrate effectiveness scalability proposed approach syntic large-scale real-world datasets. compared latest state--art baselines, model scalable, trained large-scale ImageNet dataset, obtains Inception scores lower combination denoising autoencoder GAN (dfm), significantly higher ors. finally, note method orthogonal integrate techniques baselines semi-supervised learning], conditional architectures] autoencoder]. acknowledgments. work partially supported Australian Research Council (arc) Discovery Grant Project dp160109394. Generative models subarea research rapidly growing recent years, successfully applied wide range modern real-world applications., chapter]). common approach address \\x0cdensity estimation problem aims learn model distribution pmodel approximates true, unknown, data distribution pdata methods approach deal fundamental problems. first, learning behaviors performance generative models depend choice objective functions train]. widely-used objective, considered-facto standard one, follow principle maximum likelihood estimate seeks model parameters maximize likelihood training data. this equivalent minimizing kullback-leibler) divergence data model distributions: DKL (pdata kpmodel observed minimization result pmodel covers multiple modes pdata produce completely unseen potentially undesirable samples]. contrast, anor approach swap arguments instead, minimize: DKL (pmodel kpdata referred reverse divergence]. observed optimization reverse divergence criteria mimics mode-seeking process pmodel concentrates single mode pdata ignoring modes, problem mode collapse. behaviors well-studied]. 31st Conference Neural Information Processing Systems (nips 2017), Long beach, usa. problem choice formulation density function pmodel]. one choose define explicit density function, straightforwardly follow maximum likelihood framework estimate parameters. anor idea estimate data distribution implicit density function, analytical forms pmodel] furr discussions). one notably pioneered class generative adversarial network (gan], expressive generative model capable producing sharp realistic images natural scenes. different generative models maximize data likelihood lower bound, GAN takes radical approach simulates game players: generator generates data mapping samples noise space input space; discriminator acts classifier distinguish real samples dataset fake samples produced generator Both parameterized neural networks, method categorized family deep generative models generative neural models]. optimization GAN formulates minimax problem, optimal learning objective turns finding minimizes jensen-shannon divergence (jsd): DJS (pdata kpmodel behavior JSD minimization empirically proven similar reverse divergence]. this, however, leads aforementioned issue mode collapse, notorious failure GAN] generator produces similarly images, yielding low entropy distribution poor variety samples. recent attempts made solve mode collapsing problem improving training gan. one idea minibatch discrimination trick] discriminator detect samples unusually similar generated samples. although heuristics helps generate visually appealing samples quickly, computationally \\x0cexpensive, hidden layer discriminator. anor approach unroll optimization discriminator steps create surrogate objective update generator training]. approach train generators discover modes data]. alternatively, time, attempts employ autoencoders regularizers auxiliary losses penalize missing modes]. models avoid mode collapsing problem extent, cost computational complexity exception DFM], rendering unscalable imagenet, large-scale challenging visual dataset. addressing challenges, propose approach effectively avoid mode collapse eﬃciently scale large datasets., imagenet). our approach combines reverse divergences unified objective function, exploits complementary statistical properties divergences effectively diversify estimated density capturing multi-modes. materialize idea gan framework, resulting generative adversarial architecture players: discriminator rewards high scores data sampled pdata rar generated generator distribution whilst anor discriminator conversely, favoring data rar pdata generator generates data fool discriminators. term proposed model dual discriminator generative adversarial network (d2gan). turns training D2GAN shares minimax problem gan, solved alternatively updating generator discriminators. provide oretical analysis showing that, capacity., nonparametric limit, optimal points, training criterion results minimal distance data model distribution respect reverse divergences. this helps model place fair distribution probability mass modes data generating distribution, allowing recover data distribution generate diverse samples generator single shot. addition, furr introduce hyperparameters stabilize learning control effect divergence. conduct extensive experiments syntic dataset real-world large-scale datasets (mnist, cifar10, stl, imagenet) nature. since evaluating generative models notoriously hard], made effort adopt number evaluation metrics literature quantitatively compare proposed model latest stateof--art baselines possible. experimental results reveal method capable improving diversity keeping good quality generated samples. more importantly, proposed model scaled train large-scale ImageNet database, obtain competitive variety score generate good quality images. short, main contributions are) generative adversarial model encourages diversity samples produced generator) oretical analysis prove objective optimized minimizing reverse divergence global optimum pdata \\x0cand (iii) comprehensive evaluation effectiveness proposed method wide range quantitative criteria large-scale datasets. Generative Adversarial Nets review generative adversarial network (gan) introduced] formulate game players: discriminator generator discriminator), takes point data space computes probability sampled data distribution Pdata rar generated generator time, generator maps noise vector drawn prior) data space, obtaining sample) resembles training data, sample challenge discriminator. mapping) induces generator distribution data domain probability density function). both parameterized neural networks (see fig. illustration) learned solving minimax optimization: min max?pdata) [log [log )))] learning iterative procedure discriminator generator alternatively updated. given fixed maximization subject results optimal discriminator pdata) ) pdata whilst optimal minimization turns minimizing) jensen-shannon) divergence data model distributions: DJS (pdata kpg]. Nash equilibrium game, model distribution recovers data distribution exactly: Pdata discriminator fails differentiate real fake data. ) gan. ) d2gan. figure illustration standard GAN proposed d2gan. since divergence empirically proven nature reverse divergence], GAN suffers model collapsing problem, generated data samples low level diversity]. Dual Discriminator Generative Adversarial Nets tackle gan problem mode collapse, present main contribution framework seeks approximated distribution effectively cover modes multimodal data. our intuition based gan, formulate three-player game consists discriminators generator Given sample data space) rewards high score drawn data distribution Pdata low score generated model distribution contrast) returns high score generated whilst giving low score sample drawn Pdata unlike gan, scores returned discriminators values rar probabilities]. our generator performs similar role gan., producing data mapped noise space synsize real data fool discriminators all players parameterized neural networks share parameters. term proposed model dual discriminator generative adversarial network (d2gan). fig. shows illustration d2gan. More formally, play three-player minimax optimization game: min max,  ?pdata [log?pdata)]   [log) introduced hyperparameters  serve purposes. stabilize learning model. output values discriminators positive unbounded. ) large exponentially stronger impact optimization log) log, rendering learning unstable. overcome issue, decrease effect making optimization penalize), helping stabilize learning. purpose introducing  control effect reverse divergences optimization problem. this discussed part derivation optimal solution. similar GAN], proposed network trained alternatively updating refer supplementary material pseudo-code learning parameters d2gan.  oretical analysis provide formal oretical analysis proposed model, essentially shows that, capacity., nonparametric limit, optimal points, recover data distributions minimizing reverse divergences model data distributions. optimization problem respect) discriminators fixed generator. proposition given fixed maximizing, yields closed-form optimal discriminators? ? ?pdata? ? ) pdata) proof. according induced measure orem], expectations equal) log). objective function rewritten below,  ?pdata [log?pdata)]   [log)] [?pdata) log) ) pdata log)] Considering function inside integral, maximize function variables find? ? ). setting derivatives gain) ?pdata) ) pdata) derivatives: ?pdata)/d12 )/d22 non-positive, verifying obtained maximum solution concluding proof. next, fix? ? find optimal solution generator orem given? ? Nash equilibrium point? ? ? minimax optimization problem d2gan, form component? ? ?  (log   (log  ? ) ? ? pdata proof. substituting? ? . ) objective function. ) minimax problem, gain: pdata) pdata? ?   ?pdata log log  ) )  pdata   log log pdata) pdata) (log   (log  ?dkl (pdata kpg ?dkl kpdata) DKL (pdata kpg DKL kpdata reverse divergences data model (generator) distributions, respectively. divergences nonnegative distributions equal? pdata words, generator induces distribution? identical data distribution pdata discriminators fail recognize real fake samples return score samples. this concludes proof. loss generator. ) upper bound discriminators optimal. this loss shows increasing promotes optimization minimizing divergence DKL (pdata kpg helping generative distribution cover multiple modes, include potentially undesirable samples; increasing encourages minimization reverse divergence DKL kpdata enabling generator capture single mode better, miss modes. empirically adjusting hyperparameters, balance effect divergences, effectively avoid mode collapsing issue.  Connection-gan Next point relations proposed D2GAN-gan model extends jensen-shannon divergence (jsd) GAN general divergences, specifically -divergences]. divergence divergence family form: ) convex, lowersemicontinuous function satisfying) this function convex conjugate function Fenchel conjugate] ) supu?domf )}. function convex lower-semicontinuous. considering true distribution generator distribution, resemble learning problem GAN minimizing -divergence based variational lower bound -divergence proposed Nguyen. ], objective function-gan derived follows: min max (?, ? ?  ? )))]  parameterized  generator gan),  function parameterized  discriminator gan) domf output activation function., discriminator decision function) specific -divergence used. using functions (see tab. ]), recover minimization divergences JSD gan, (associated discriminator reverse (associated discriminator d2gan. -gan, however, considers single divergence. hand, proposed method combines \\x0creserve divergences. our idea conceived pondering advantages disadvantages divergences covering multiple modes data. combining unified objective function. ) helps reversely engineer finally obtain optimization game. ) eﬃciently formulated solved principle gan. Experiments section, conduct comprehensive experiments demonstrate capability improving mode coverage scalability proposed model large-scale datasets. syntic dataset visual numerical verification, datasets increasing diversity size numerical verification. made effort compare results method latest state--art gan variants replicating experimental settings original work possible. for experiment, refer supplementary material model architectures additional results. common points are: discriminators? outputs softplus activations., positive version relu) Adam optimizer] learning rate.0002 first-order momentum; (iii) minibatch size samples training generator discriminators) Leaky ReLU slope) weights initialized isotropic gaussian) Symmetric-div GAN Unrolled GAN D2GAN 5000 15000 10000 Step 20000 25000) Symmetric divergence. Wasserstein estimate GAN Unrolled GAN D2GAN 5000 10000 Step 15000 20000) Wasserstein distance. 25000) Evolution data blue) generated GAN (top row), UnrolledGAN (middle row) D2GAN (bottom row) data gaussians. data sampled true mixture red. figure comparison standard gan, UnrolledGAN D2GAN syntic dataset. biases. our implementation TensorFlow] published version reference1 present experiments syntic data large-scale real-world datasets.  Syntic data experiment, reuse experimental design proposed] investigate D2GAN deal multiple modes data. more specifically, sample training data mixture Gaussian distributions covariance matrix.02i means arranged circle centroid radius. data low variance mixture components separated area low density. aim examine properties low probability regions low separation modes. simple architecture generator fully connected hidden layers discriminators hidden layer ReLU activations. this setting identical, ensures fair comparison UnrolledGAN2]. fig. shows evolution 512 samples generated models baselines time. regular GAN generates data collapsing single mode hovering valid modes data distribution, reﬂecting mode collapse gan. time, UnrolledGAN D2GAN distribute data mixture components, demonstrating abilities successfully learn multimodal data case. steps, D2GAN captures data modes precisely UnrolledGAN, mode, UnrolledGAN generates data concentrate points mode centroid, produce fewer samples D2GAN samples fairly spread entire mode. next furr quantitatively compare quality generated data. since true distribution pdata case, employ measures, symmetric divergence Wasserstein distance. measures compute distance normalized histograms,000 points generated d2gan, UnrolledGAN GAN true pdata figs. demonstrate superiority approach GAN UnrolledGAN distances (lower better); notably Wasserstein metric, distance true distribution reduces zero. figures demonstrate stability D2GAN (red curves) training ﬂuctuating compared GAN (green curves) UnrolledGAN (blue curves).  real-world datasets examine performance proposed method real-world datasets increasing diversities sizes. for networks convolutional layers, closely follow dcgan design]. strided convolutions discriminators fractional-strided convolutions generator pooling layers. batch normalization applied layer, https://github.com/tund/d2gan obtain code UnrolledGAN data link authors provided]. generator output layer discriminator input layers. Leaky ReLU activations discriminators, ReLU generator, \\x0cput tanh rescale pixel intensities range, feeding images model. only difference that, model, initializing weights) yields slightly results). refer supplementary material detailed architectures.  Evaluation protocol Evaluating quality image produced generative models notoriously challenging due variety probability criteria lack perceptually meaningful image similarity metric]. even model generate plausible images, images visually similar. refore, order quantify performance covering data modes producing high quality samples-hoc metrics experiments compare baselines. first adopt Inception score proposed], computed: exp [dkl))]), conditional label distribution image estimated pretrained Inception model) marginal distribution)  )). this metric rewards good varied samples, easily fooled model collapses generates low quality image, fails measure wher model trapped bad mode. address problem, labeled datasets, furr recruit-called MODE score introduced]: exp [dkl ))] dkl) ))) ) empirical distribution labels estimated training data. score adequately reﬂect variety visual quality images, discussed].  Handwritten digit images start handwritten digit images mnist] consists,000 training,000 testing grayscale images digits following setting], assume MNIST modes, representing connected component data manifold, digit classes. perform extensive grid search hyperparameter configurations, regularized constants . ) varied}. for fair comparison, parameter ranges fully connected layers network. supplementary material details), adopt results GAN mode regularized GAN (reg-gan]. for evaluation, train simple, effective-layer convolutional nets3 obtain% error MNIST testing set, employ predict label probabilities compute MODE scores generated samples. fig. (left) shows distributions MODE scores obtained models. clearly, proposed D2GAN significantly outperforms standard GAN reg-gan achieving scores maximum]. worthy note observe substantial differences average MODE scores obtained varying network size parameter searching. report result minimal network smallest number layers hidden units. study effect inspect results obtained minimal \\x0cnetwork varied fig. (right). pattern that, fixed D2GAN obtains MODE score increasing value, score significantly decrease. mnist. standard MNIST data-mode assumption fairly trivial. hence, based data, test proposed model challenging one. continue technique] construct 1000-class MNIST dataset (mnist) stacking randomly selected digits form RGB image digit image channel. resulting data assumed,000 distinct modes, combinations digits channels 000 999. experiment, powerful model convolutional layers discriminators transposed convolutions generator. measure performance number modes Network architecture similar https://github.com/fchollet/keras/blob/master/examples/mnist cnn. ???? ??  ???? ???? ???? ??? ??? ??? ??? ??? ??? ??? ??? ??? ??? ??? ??? ??? ??? ??? ??? ??? ???????? ???? ???? ??? ??? ??? ??? ??? ?????           ???  ??? ??? ???? ???? ???  ??? ???? ????? ????? ??? ??? ??? ??? ??? ??? ??? ???? ??? ??? ??? figure Distributions MODE scores (left) average MODE scores (right) varied model generated total,600 samples, reverse divergence model distribution., label distribution predicted pretrained MNIST classifier previous experiment) expected data distribution. tab. reports results D2GAN compared gan, UnrolledGAN], DCGAN reg-gan]. our proposed method demonstrates superiority baselines covering modes achieving distance close zero. table Numbers modes covered reverse divergence model data distributions. model modes covered DKL (modelk data GAN] 628?140 UnrolledGAN] 817 DCGAN] 849 reg-gan] 955 D2GAN 1000 Natural scene images extend experiments investigate scalability proposed method challenging large-scale image databases natural scenes. widely-adopted datasets: cifar], stl] ImageNet]. cifar well-studied dataset,000 training images classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck. stl, subset imagenet, 100,000 unlabeled images, diverse cifar, full imagenet. rescale images times train networks resolution. imagenet large database million natural images,000 classes, challenging benchmark validate scalability deep models. follow preprocessing], subsampling resolution. code provided] compute Inception score independent partitions,000 generated samples. table Inception scores cifar. model Real data WGAN] mix+wgan] improved-gan] ALI] BEGAN] MAGAN] DCGAN] DFM] D2GAN Score???????  ????? ?????     ???? ???? ???? ???? ???? ????   ¿??? ?? figure Inception scores stl imagenet. tab. fig. show Inception scores cifar, stl ImageNet datasets obtained model baselines collected recent work literature. worthy note compare methods trained completely unsupervised manner label information. result, exist baselines cifar whilst DCGAN] denoising feature matching (dfm] stl imagenet. TensorFlow implementation DCGAN network architecture model fair comparisons. experiments, D2GAN fails beat dfm, outperforms baselines large margins. lower results compared DFM suggest autoencoders matching high-level features appears effective encourage diversity. this technique compatible method, integrating promising avenue future work. two discriminators identical architectures, potentially share parameters schemes. explore direction creating version D2GAN hyperparameter setting. version shares parameters (output) layer. this model failed discriminator fewer parameters, rendering unable capture inverse ratios density functions. shares parameters layers. this version performed previous one, obtain promising Inception scores cifar10 STL10 imagenet), results worse proposed model sharing parameters. finally, show samples generated proposed model trained datasets fig.  samples fair random draws, cherry-picked. D2GAN produce visually recognizable images cars, trucks, boats, horses cifar. objects harder recognize, shapes airplanes, cars, trucks animals identified stl, images backgrounds sky, underwater, mountain, forest shown imagenet. this confirms diversity samples generated model. ) cifar. ) stl. ) imagenet. figure Samples generated proposed D2GAN trained natural image datasets. due space limit, refer supplementary material larger plot. Conclusion summarize, introduced approach combine KullbackLeibler) reverse divergences unified objective function density estimation problem. our idea exploit complementary statistical properties divergences improve quality diversity samples generated estimator. end, propose framework based generative adversarial nets (gans), formulates minimax game players: discriminators generator, termed dual discriminator GAN (d2gan). given discriminators fixed, learning generator moves optimizing reverse divergences simultaneously, avoid mode collapse, notorious drawback gans. established extensive experiments demonstrate effectiveness scalability proposed approach syntic large-scale real-world datasets. compared latest state--art baselines, model scalable, trained large-scale ImageNet dataset, obtains Inception scores lower combination denoising autoencoder GAN (dfm), significantly higher ors. finally, note method orthogonal integrate techniques baselines semi-supervised learning], conditional architectures] autoencoder]. acknowledgments. this work partially supported Australian Research Council (arc) Discovery Grant Project dp160109394.',\n",
       " 'PP6888': 'testing datasets Conditional Independence) significant applications statistical/learning problems; ors, examples include discovering/testing edges Bayesian networks], causal inference, feature selection Markov Blankets]. triplet random variables/vectors), conditionally \\x0cindependent (denoted  ), joint distribution, factorizes). problem Conditional Independence Testing testing) defined follows: Given samples), distinguish hyposis    . paper propose data-driven model-powered test. central idea model-driven approach convert statistical testing estimation problem pipeline utilizes power supervised learning models classifiers regressors; pipelines leverage recent advances classification/regression high-dimensional settings. paper, model-powered approach (illustrated fig. ), reduces problem testing Binary classification. specifically, key steps procedure follows: Equal Contribution 31st Conference Neural Information Processing Systems (nips 2017), Long beach, usa. Original Samples Original Samples    training Set (trained classifier) y3n z3n Shuﬄe U20   original Samples x3n   nearest Neighbor Bootstrap samples close Test Set(? (test error) Figure Illustration methodology. part original samples rest samples nearest neighbor \\x0cboot-strap generate data-set U20 close distribution. samples labeled shown classifier trained training set. test error measured test set-after. test-error close, rejected, test error low rejected. ) Suppose provided samples). original samples set (refer fig. ). remaining original samples processed module, nearest-neighbor bootstrap (algorithm paper), produces simulated samples stored U20 Section show generated samples U20 fact close total variational distance (defined Section conditionally independent distribution). (note equality (.)  (.) hold; method generates samples close, hyposes). ) subsequently, original samples labeled samples simulated nearest-neighbor bootstrap U20 labeled labeled samples label U20 labeled aggregated data-set This set broken training test sets samples each. (iii) Given labeled training data-set (from step)), train powerful classifiers gradient boosted trees] deep neural networks] attempt learn classes samples. trained classifier good accuracy test set, intuitively means joint distribution (.) distinguishable (note generated samples labeled close distribution refore, reject hand, classifier accuracy close random guessing (.) fact close fail reject independence testing wher  classifiers recently]. key observation samples), coordinates randomly permuted resulting samples emulate distribution). problem converted sample test subset original samples subset permuted Binary classifiers harnessed two-sample testing; details]. however, case testing emulate samples harder permutation samples dependent (which high-dimensional). key technical contributions proving nearest-neighbor bootstrap step) achieves task. advantage modular approach harness power classifiers step (iii) above), good accuracies high-dimensions. thus, improvements field binary classification imply advancement test. moreover, added ﬂexibility choosing classifier based domain knowledge data-generation process. finally, bootstrap eﬃcient owing fast algorithms identifying nearest-neighbors].  Main Contributions) (classification based testing) reduce problem testing Binary Classification detailed steps)-(iii) fig.  sim \\x0culate samples close nearest-neighbor bootstrap (algorithm access samples joint distribution. problem testing reduces two-sample test original samples U20 effectively binary classifiers. ) (guarantees Bootstrapped samples) mentioned steps)-(iii), samples generated bootstrap U20 close testing problem reduces testing wher data-sets U20 distinguishable. oretically justify true. , denote distribution sample produced Algorithm supplied samples (.). orem prove smoothness assumptions. dimension denotes total variational distance (def. ). (iii) (generalization Bounds Classification nearindependence) samples generated nearest-neighbor bootstrap remain close. quantify property show generalization risk bounds classifier. denote class denote probability error optimal classifier function encoded classifier Let trained training set (fig. ). prove assumptions,      2dz high probability, upto log factors. , )), dimension] class Thus equivalent holds) error rate classifier close. holds loss lower. provide analysis Rademacher complexity bounds] near-independence independent interest. ) (empirical evaluation) perform extensive numerical experiments algorithm outperforms state art]. apply algorithm analyzing relations protein signaling network data ﬂow cytometry data-set]. practice observe performance respect dimension scales expected worst case oretical analysis. powerful binary classifiers perform high-dimensions.  Related Work paper address problem non-parametric testing underlying random variables continuous. literature non-parametric testing vast. review recent work field relevant paper. recent work testing kernel based]. works build study], non-parametric relations characterized covariance operators Reproducing Kernel Hilbert Spaces (rkhs]. KCIT] partial association regression functions relating RCIT] approximate version KCIT attempts improve running times number samples large. kcipt] relevant work. ], specific permutation samples simulate data expensive linear program solved order calculate permutation. hand, simple nearest-neighbor bootstrap furr provide oretical guarantees closeness samples terms total variational distance. finally two-sample test] based kernel method], binary classifiers purpose. recent work entropy estimation] nearest neighbor techniques (used density estimation); subsequently testing estimating conditional mutual information). binary classification recently two-sample testing, independence testing]. analysis generalization guarantees classification aimed recovering guarantees similar], non setting. regard (non generalization guarantees), recent work proving Rademacher complexity bounds -mixing stationary processes]. work falls category machine learning reductions, general philosophy reduce machine learning settings multi-class regression], ranking], reinforcement learning], structured prediction] binary classification. problem Setting Algorithms section describe algorithmic details testing procedure. formally define problem. describe bootstrap algorithm generating data-set mimics samples give detailed pseudo-code testing process reduces problem binary classification. finally, suggest furr improvements algorithm. problem setting: problem setting non-parametric Conditional Independence) testing samples joint distributions random variables/vectors].  samples continuous joint distribution, Rdx Rdy Rdz goal test wher   wher, factorizes, This essentially hyposis testing problem where:    . note: For notational convenience, drop subscripts context evident. instance) place). nearest-neighbor bootstrap: Algorithm procedure generate data-set consisting samples data-set samples distribution). data-set broken equally sized partitions sample find nearest neighbor terms coordinates. -coordinates sample exchanged -coordinates nearest neighbor modified sample added algorithm DataGen Given data-set samples returns data-set samples. function DATAG) Let sample-nearest Neighbor norm) data-set, Let end end function One main results samples generated Algorithm mimic samples coming distribution suppose, sample) small. case 1nn sample refore fixed smoothness assumptions, close independent sample coming ). hand) small, rare occurrence contribute adversely. testing algorithm: Now introduce testing algorithm, Algorithm binary classifiers. psuedo-code Algorithm (classifier Test -ccit). algorithm CCITv1 Given data-set samples), returns  . function ccit, Partition disjoint partitions size each, randomly. let U20 datagen) (algorithm). note —u20 Create Labeled data-set)}u2u1 2u20 Divide data-set train test set respectively. note let argming2g,‘)2dr}. Empirical Risk Minimization training classifier (finding function class).   conclude (?  , orwise, conclude  . end function Algorithm original samples nearest-neighbor bootstrapped samples U20 indistinguishable holds. however, holds, classifier trained Line easily distinguish samples labels. line denotes space functions risk minimization performed classifier. show orem variational distance distribution samples U20, small large however, samples U20 close. refore, practice finite small bias.    holds. threshold greater order(? algorithm function. section, present algorithm \\x0cthis bias corrected. Algorithm Bias correction: present improved bias-corrected version algorithm Algorithm mentioned previous section, Algorithm optimal classifier achieve loss slightly case finite true. however, classifier expected distinguish data-sets based coordinates, joint distribution remains nearest-neighbor bootstrap. key idea Algorithm train classifier coordinates, denoted train anor classier coordinates, denoted?. test loss expected roughly bias mentioned previous section. refore, (?  close however, subtract bias. thus, true(?  lower, classifier trained leveraging holds(? information encoded coordinates. algorithm CCITv2 Given data-set samples, returns wher  . function ccit, Perform Steps Algorithm let Dr0),‘)2dr similarly, De0),‘)2de training test sets-coordinates.  let argming2g,‘)2dr}. compute test loss  (?  let argming2g,‘)2dr0}. compute test loss (?   (?   conclude (?  , orwise, conclude  . end function oretical Results section, provide main oretical results. show distribution samples generated Algorithm closely resemble sample coming result holds broad class distributions, satisfy smoothness assumptions. however, samples generated Algorithm algorithm close. quantify show empirical risk minimization class classifier functions generalizes samples. before, formally state results provide definitions. definition total variational distance continuous probability distributions (.) (.) defined domain, supp2b)]— set measurable functions ]. here, [.] denotes expectation distribution prove distribution samples generated Algorithm close terms total variational distance. make assumptions joint distribution original samples. ): Smoothness assumption): assume smoothness condition), generalization boundedness max. eigenvalue Fisher Information matrix assumption Rdz zk2 generalized curvature matrix, ) log log @zi0 @zj0 zi0 zj0 require zk2 max))  analogous assumptions made Hessian density context entropy estimation]. smoothness assumptions): assume smoothness properties probability density function). smoothness assumptions Assumption subset assumptions made] (assumption Page entropy estimation. definition define)  probability mass distribution areas definition (hessian matrix) Let) denote Hessian Matrix) respect provided continuously differentiable assumption probability density function) satisfies following) continuously differentiable Hessian matrix satisfies khf cdz everywhere, cdz dependent dimension.  constant. orem , denote sample U20 produced Algorithm modifying original sample, supplied samples original joint distribution). , distribution). smoothness assumptions), large enough, have: )   (2cdz exp here,  2cdz volume unit radius ball orem characterizes variational distance distribution sample generated Algorithm conditionally independent distribution defer proof orem Appendix now, goal characterize misclassification error trained classifier Algorithm distribution samples data-set classification Algorithm —‘ marginal distribution sample label similarly—‘ denote marginal distribution label samples. note construction, , holds, holds, defined orem note marginal sample label, (equation owing nearest neighbor \\x0cbootstrap. show close refore classification risk minimization generalizes similar results classification]. first, review standard definitions results classification ory]. ideal Classification setting: ideal classification scenario testing process define standard quantities learning ory. recall set classifiers consideration.  ideal distribution ), ,  . words ideal classification scenario testing. ), loss function classifying function sample, true label algorithms loss function loss, results hold bounded loss function. ), ‘)— —. distribution classifier? ,‘? ), ‘)] expected risk function risk optimal classifier??  ?? arg ming2g). similarly set samples classifier), empirical risk set samples. define classifier minimizes empirical loss observed set samples, arg ming2g). samples generated independently?, standard results learning ory states probability log/ ?  ? ) dimension] classification model, universal constant—. guarantees near-independent samples: Our goal prove result), classification problem Algorithm however, case access samples samples U20 remain independent. close independent sense. brings main results orem orem assume joint distribution, satisfies conditions orem furr assume) bounded Lipschitz constant. classifier algorithm trained set  definition?.  have?       log/ log/?) — log(?)  probability . dimension classification function class, defined def. universal constant— bound absolute loss. ) Suppose loss— ). furr suppose class classifying functions?   here))) risk Bayes optimal classifier). loss classifier achieve classification problem]. setting have)  ) defined orem prove orem orem orem appendix. part) orem prove generalization bounds hold samples. intuitively, sample inputs coordinates away. expect resulting samples u0i u0j U20 nearly-independent. carefully capturing notion spatial near-independence, prove generalization errors orem part) orem essentially implies error trained classifier close) (under hand, error)) small. empirical Results section provide empirical results comparing proposed algorithm state art algorithms. algorithms comparison are) CCIT Algorithm paper XGBoost] classifier. experiments, data-set boot-strap samples run algorithm times. results averaged bootstrap runs1 ) KCIT Kernel test]. matlab code online. (iii) RCIT Randomized Test]. package publicly available. python package implementation found (https://github.com/rajatsen91/ccit).  Syntic Experiments perform syntic experiments regime post-nonlinear noise similar]. experiments dimension dimension scales (motivated causal settings]). generated relation)  noise term non-linear function, holds. experiments, data generated follows)  , coordinate Gaussian unit variance, cos cos here, Rdz kak kbk fixed generating single dataset. zero-mean Gaussian noise variables, independent else. set. )  , identical) cos randomly chosen constant]. fig. , plot performance algorithms dimension scales. generating point plot, 300 data-sets generated dimensions. half half algorithms run data-sets, ROC AUC (area Under Receiver Operating Characteristic curve) score calculated true labels) data-set predicted scores. observe accuracy CCIT close \\x0cdimensions upto, algorithms scale well. experiments numberpof bootstraps data-set CCIT set. set threshold Algorithm upper-bound expected variance test-statistic holds.  flow-cytometry Dataset testing algorithm verify relations protein network data ﬂowcytometry dataset], expression levels proteins experimental conditions. ground truth causal graph absolute certainty data-set, dataset widely causal structure learning literature. popular learned causal structures recovered causal discovery algorithms, verify relations assuming graphs ground truth. graph are) consensus graph] (fig. ) reconstructed graph Sachs. ] (fig. ]) (iii) reconstructed graph] (fig. ]). graph generate relations follows: node graph, identify set consisting parents, children parents children causal graph. conditioned set independent node graph (apart). create conditions types graphs. process generate relations graphs. order evaluate false positives algorithms, relations  . for, observe edge nodes, conditioning set. graph generate non relations, edge selected random conditioning set size randomly selected remaining nodes. construct negative examples graph. fig. display performance algorithms based graphs ground-truth. algorithms access observational data verifying non relations. fig. display ROC plot algorithms data-set generated graph). table display ROC AUC score algorithms graphs. algorithm outperforms ors cases, dimensionality fairly low (less cases). interesting thing note edges (pkc-raf), (pkc-mek) (pka-p38) graphs. however, testers ccit, KCIT RCIT fairly confident edges absent. edges discrepancies ground-truth graphs refore ROC AUC algorithms lower expected.  CCIT RCIT KCIT ROC AUC 100 Dimension) 150) algo. Graph) Graph) Graph (iii) CCIT RCIT KCIT.6848.6448.6528.7778.7168.7416.7156.6928.6610) Figure) plot performance ccit, KCIT RCIT post-nonlinear noise syntic data. generating point plots, 300 data-sets generated half rest algorithms run ROC AUC score plotted. ) number samples 1000, dimension varies. ) plot ROC curve algorithms based data Graph) ﬂow-cytometry dataset. roc AUC score algorithms provided), graphs ground-truth. conclusion paper present model-powered approach tests converting binary classification, empowering testing powerful supervised learning tools gradient boosted trees. provide eﬃcient nearest-neighbor bootstrap makes reduction classification possible. provide oretical guarantees bootstrapped samples, risk generalization bounds classification problem, non independent samples. conclusion model-driven data dependent approaches extremely general statistical testing estimation problems enable powerful supervised learning tools. acknowledgments This work partially supported NSF grants CNS 1320175, NSF SaTC 1704778, ARO grants w911nf-0359, w911nf-0377 DoT supported-stop Tier University Transportation center. Testing datasets Conditional Independence) significant applications statistical/learning problems; ors, examples include discovering/testing edges Bayesian networks], causal inference, feature selection Markov Blankets]. given triplet random variables/vectors), conditionally \\x0cindependent (denoted  ), joint distribution, factorizes). problem Conditional Independence Testing testing) defined follows: Given samples), distinguish hyposis    . paper propose data-driven model-powered test. central idea model-driven approach convert statistical testing estimation problem pipeline utilizes power supervised learning models classifiers regressors; pipelines leverage recent advances classification/regression high-dimensional settings. paper, model-powered approach (illustrated fig. ), reduces problem testing Binary classification. specifically, key steps procedure follows: Equal Contribution 31st Conference Neural Information Processing Systems (nips 2017), Long beach, usa. Original Samples Original Samples    Training Set (trained classifier) y3n z3n Shuﬄe U20   Original Samples x3n   nearest Neighbor Bootstrap samples close Test Set(? (test error) Figure Illustration methodology. part original samples rest samples nearest neighbor \\x0cboot-strap generate data-set U20 close distribution. samples labeled shown classifier trained training set. test error measured test set-after. test-error close, rejected, test error low rejected. ) Suppose provided samples). original samples set (refer fig. ). remaining original samples processed module, nearest-neighbor bootstrap (algorithm paper), produces simulated samples stored U20 Section show generated samples U20 fact close total variational distance (defined Section conditionally independent distribution). (note equality (.)  (.) hold; method generates samples close, hyposes). ) subsequently, original samples labeled samples simulated nearest-neighbor bootstrap U20 labeled labeled samples label U20 labeled aggregated data-set This set broken training test sets samples each. (iii) Given labeled training data-set (from step)), train powerful classifiers gradient boosted trees] deep neural networks] attempt learn classes samples. trained classifier good accuracy test set, intuitively means joint distribution (.) distinguishable (note generated samples labeled close distribution refore, reject hand, classifier accuracy close random guessing (.) fact close fail reject for independence testing wher  classifiers recently]. key observation samples), coordinates randomly permuted resulting samples emulate distribution). thus problem converted sample test subset original samples subset permuted Binary classifiers harnessed two-sample testing; details]. however, case testing emulate samples this harder permutation samples dependent (which high-dimensional). one key technical contributions proving nearest-neighbor bootstrap step) achieves task. advantage modular approach harness power classifiers step (iii) above), good accuracies high-dimensions. thus, improvements field binary classification imply advancement test. moreover, added ﬂexibility choosing classifier based domain knowledge data-generation process. finally, bootstrap eﬃcient owing fast algorithms identifying nearest-neighbors].  Main Contributions) (classification based testing) reduce problem testing Binary Classification detailed steps)-(iii) fig.  sim \\x0culate samples close nearest-neighbor bootstrap (algorithm access samples joint distribution. problem testing reduces two-sample test original samples U20 effectively binary classifiers. ) (guarantees Bootstrapped samples) mentioned steps)-(iii), samples generated bootstrap U20 close testing problem reduces testing wher data-sets U20 distinguishable. oretically justify true. let, denote distribution sample produced Algorithm supplied samples (.). orem prove smoothness assumptions. here dimension denotes total variational distance (def. ). (iii) (generalization Bounds Classification nearindependence) samples generated nearest-neighbor bootstrap remain close. quantify property show generalization risk bounds classifier. let denote class denote probability error optimal classifier function encoded classifier Let trained training set (fig. ). prove assumptions,      2dz high probability, upto log factors. here, )), dimension] class Thus equivalent holds) error rate classifier close. but holds loss lower. provide analysis Rademacher complexity bounds] near-independence independent interest. ) (empirical evaluation) perform extensive numerical experiments algorithm outperforms state art]. apply algorithm analyzing relations protein signaling network data ﬂow cytometry data-set]. practice observe performance respect dimension scales expected worst case oretical analysis. this powerful binary classifiers perform high-dimensions.  Related Work paper address problem non-parametric testing underlying random variables continuous. literature non-parametric testing vast. review recent work field relevant paper. most recent work testing kernel based]. many works build study], non-parametric relations characterized covariance operators Reproducing Kernel Hilbert Spaces (rkhs]. KCIT] partial association regression functions relating RCIT] approximate version KCIT attempts improve running times number samples large. kcipt] relevant work. ], specific permutation samples simulate data expensive linear program solved order calculate permutation. hand, simple nearest-neighbor bootstrap furr provide oretical guarantees closeness samples terms total variational distance. finally two-sample test] based kernel method], binary classifiers purpose. recent work entropy estimation] nearest neighbor techniques (used density estimation); subsequently testing estimating conditional mutual information). binary classification recently two-sample testing, independence testing]. our analysis generalization guarantees classification aimed recovering guarantees similar], non setting. regard (non generalization guarantees), recent work proving Rademacher complexity bounds -mixing stationary processes]. this work falls category machine learning reductions, general philosophy reduce machine learning settings multi-class regression], ranking], reinforcement learning], structured prediction] binary classification. Problem Setting Algorithms section describe algorithmic details testing procedure. formally define problem. describe bootstrap algorithm generating data-set mimics samples give detailed pseudo-code testing process reduces problem binary classification. finally, suggest furr improvements algorithm. problem setting: problem setting non-parametric Conditional Independence) testing samples joint distributions random variables/vectors].  samples continuous joint distribution, Rdx Rdy Rdz goal test wher   wher, factorizes, This essentially hyposis testing problem where:    . note: For notational convenience, drop subscripts context evident. for instance) place). nearest-neighbor bootstrap: Algorithm procedure generate data-set consisting samples data-set samples distribution). data-set broken equally sized partitions sample find nearest neighbor terms coordinates. -coordinates sample exchanged -coordinates nearest neighbor modified sample added algorithm DataGen Given data-set samples returns data-set samples. function DATAG) Let sample-nearest Neighbor norm) data-set, Let end end function One main results samples generated Algorithm mimic samples coming distribution suppose, sample) small. case 1nn sample refore fixed smoothness assumptions, close independent sample coming ). hand) small, rare occurrence contribute adversely. Testing algorithm: Now introduce testing algorithm, Algorithm binary classifiers. psuedo-code Algorithm (classifier Test -ccit). algorithm CCITv1 Given data-set samples), returns  . function ccit, Partition disjoint partitions size each, randomly. Let U20 datagen) (algorithm). note —u20 Create Labeled data-set)}u2u1 2u20 Divide data-set train test set respectively. note Let argming2g,‘)2dr}. this Empirical Risk Minimization training classifier (finding function class).   conclude (?  , orwise, conclude  . end function Algorithm original samples nearest-neighbor bootstrapped samples U20 indistinguishable holds. however, holds, classifier trained Line easily distinguish samples labels. Line denotes space functions risk minimization performed classifier. show orem variational distance distribution samples U20, small large however, samples U20 close. refore, practice finite small bias.    holds. threshold greater order(? algorithm function. section, present algorithm \\x0cthis bias corrected. Algorithm Bias correction: present improved bias-corrected version algorithm Algorithm mentioned previous section, Algorithm optimal classifier achieve loss slightly case finite true. however, classifier expected distinguish data-sets based coordinates, joint distribution remains nearest-neighbor bootstrap. key idea Algorithm train classifier coordinates, denoted train anor classier coordinates, denoted?. test loss expected roughly bias mentioned previous section. refore, (?  close however, subtract bias. thus, true(?  lower, classifier trained leveraging holds(? information encoded coordinates. algorithm CCITv2 Given data-set samples, returns wher  . function ccit, Perform Steps Algorithm Let Dr0),‘)2dr similarly, De0),‘)2de training test sets-coordinates.  Let argming2g,‘)2dr}. compute test loss  (?  Let argming2g,‘)2dr0}. compute test loss (?   (?   conclude (?  , orwise, conclude  . end function oretical Results section, provide main oretical results. show distribution samples generated Algorithm closely resemble sample coming this result holds broad class distributions, satisfy smoothness assumptions. however, samples generated Algorithm algorithm close. quantify show empirical risk minimization class classifier functions generalizes samples. before, formally state results provide definitions. definition total variational distance continuous probability distributions (.) (.) defined domain, supp2b)]— set measurable functions ]. here, [.] denotes expectation distribution prove distribution samples generated Algorithm close terms total variational distance. make assumptions joint distribution original samples. ): Smoothness assumption): assume smoothness condition), generalization boundedness max. eigenvalue Fisher Information matrix Assumption for Rdz zk2 generalized curvature matrix, ) log log @zi0 @zj0 zi0 zj0 require zk2 max))  analogous assumptions made Hessian density context entropy estimation]. Smoothness assumptions): assume smoothness properties probability density function). smoothness assumptions Assumption subset assumptions made] (assumption Page entropy estimation. definition for define)  this probability mass distribution areas definition (hessian matrix) Let) denote Hessian Matrix) respect provided continuously differentiable assumption probability density function) satisfies following) continuously differentiable Hessian matrix satisfies khf cdz everywhere, cdz dependent dimension.  constant. orem let, denote sample U20 produced Algorithm modifying original sample, supplied samples original joint distribution). let, distribution). under smoothness assumptions), large enough, have: )   (2cdz exp here,  2cdz volume unit radius ball orem characterizes variational distance distribution sample generated Algorithm conditionally independent distribution defer proof orem Appendix now, goal characterize misclassification error trained classifier Algorithm consider distribution samples data-set classification Algorithm let—‘ marginal distribution sample label similarly—‘ denote marginal distribution label samples. note construction, , holds, holds, defined orem note marginal sample label, (equation owing nearest neighbor \\x0cbootstrap. show close refore classification risk minimization generalizes similar results classification]. first, review standard definitions results classification ory]. ideal Classification setting: ideal classification scenario testing process define standard quantities learning ory. recall set classifiers consideration. let ideal distribution ), ,  . words ideal classification scenario testing. let), loss function classifying function sample, true label algorithms loss function loss, results hold bounded loss function. ), ‘)— —. for distribution classifier? ,‘? ), ‘)] expected risk function risk optimal classifier??  ?? arg ming2g). similarly set samples classifier), empirical risk set samples. define classifier minimizes empirical loss observed set samples, arg ming2g). samples generated independently?, standard results learning ory states probability log/ ?  ? ) dimension] classification model, universal constant—. guarantees near-independent samples: Our goal prove result), classification problem Algorithm however, case access samples samples U20 remain independent. close independent sense. this brings main results orem orem assume joint distribution, satisfies conditions orem furr assume) bounded Lipschitz constant. consider classifier Algorithm trained set let definition?. for have?       log/ log/?) — log(?)  probability here. dimension classification function class, defined def. universal constant— bound absolute loss. ) Suppose loss— ). furr suppose class classifying functions?   here))) risk Bayes optimal classifier). this loss classifier achieve classification problem]. under setting have)  ) defined orem prove orem orem orem appendix. part) orem prove generalization bounds hold samples. intuitively, sample inputs coordinates away. expect resulting samples u0i u0j U20 nearly-independent. carefully capturing notion spatial near-independence, prove generalization errors orem part) orem essentially implies error trained classifier close) (under hand, error)) small. Empirical Results section provide empirical results comparing proposed algorithm state art algorithms. algorithms comparison are) CCIT Algorithm paper XGBoost] classifier. experiments, data-set boot-strap samples run algorithm times. results averaged bootstrap runs1 ) KCIT Kernel test]. Matlab code online. (iii) RCIT Randomized Test]. package publicly available. python package implementation found (https://github.com/rajatsen91/ccit).  Syntic Experiments perform syntic experiments regime post-nonlinear noise similar]. experiments dimension dimension scales (motivated causal settings]). generated relation)  noise term non-linear function, holds. experiments, data generated follows)  , coordinate Gaussian unit variance, cos cos here, Rdz kak kbk fixed generating single dataset. zero-mean Gaussian noise variables, independent else. set. )  , identical) cos randomly chosen constant]. fig. , plot performance algorithms dimension scales. for generating point plot, 300 data-sets generated dimensions. half half algorithms run data-sets, ROC AUC (area Under Receiver Operating Characteristic curve) score calculated true labels) data-set predicted scores. observe accuracy CCIT close \\x0cdimensions upto, algorithms scale well. experiments numberpof bootstraps data-set CCIT set. set threshold Algorithm upper-bound expected variance test-statistic holds.  flow-cytometry Dataset testing algorithm verify relations protein network data ﬂowcytometry dataset], expression levels proteins experimental conditions. ground truth causal graph absolute certainty data-set, dataset widely causal structure learning literature. popular learned causal structures recovered causal discovery algorithms, verify relations assuming graphs ground truth. graph are) consensus graph] (fig. ) reconstructed graph Sachs. ] (fig. ]) (iii) reconstructed graph] (fig. ]). for graph generate relations follows: node graph, identify set consisting parents, children parents children causal graph. conditioned set independent node graph (apart). create conditions types graphs. process generate relations graphs. order evaluate false positives algorithms, relations  . for, observe edge nodes, conditioning set. for graph generate non relations, edge selected random conditioning set size randomly selected remaining nodes. construct negative examples graph. fig. display performance algorithms based graphs ground-truth. algorithms access observational data verifying non relations. fig. display ROC plot algorithms data-set generated graph). Table display ROC AUC score algorithms graphs. algorithm outperforms ors cases, dimensionality fairly low (less cases). interesting thing note edges (pkc-raf), (pkc-mek) (pka-p38) graphs. however, testers ccit, KCIT RCIT fairly confident edges absent. edges discrepancies ground-truth graphs refore ROC AUC algorithms lower expected.  CCIT RCIT KCIT ROC AUC 100 Dimension) 150) algo. Graph) Graph) Graph (iii) CCIT RCIT KCIT.6848.6448.6528.7778.7168.7416.7156.6928.6610) Figure) plot performance ccit, KCIT RCIT post-nonlinear noise syntic data. generating point plots, 300 data-sets generated half rest algorithms run ROC AUC score plotted. ) number samples 1000, dimension varies. ) plot ROC curve algorithms based data Graph) ﬂow-cytometry dataset. ROC AUC score algorithms provided), graphs ground-truth. Conclusion paper present model-powered approach tests converting binary classification, empowering testing powerful supervised learning tools gradient boosted trees. provide eﬃcient nearest-neighbor bootstrap makes reduction classification possible. provide oretical guarantees bootstrapped samples, risk generalization bounds classification problem, non independent samples. conclusion model-driven data dependent approaches extremely general statistical testing estimation problems enable powerful supervised learning tools. acknowledgments This work partially supported NSF grants CNS 1320175, NSF SaTC 1704778, ARO grants w911nf-0359, w911nf-0377 DoT supported-stop Tier University Transportation center.',\n",
       " 'PP6905': 'this paper concerned problem recovering arbitrary sparse signal set  -sparse (compressed) measurements. signal  problem, toger variants, found variety successful non-zeros applications compressed sensing, machine learning statistics. interest true signal small number linear measurements given, referred setting compressed sensing. instance exhaustively studied decade, large body elegant work devoted eﬃcient algorithms including -based convex optimization hard thresholding based greedy pursuits]. anor quintessential sparsity-constrained minimization program recently considered machine learning, set training], goal eﬃciently learn global sparse minimizer data. cases, underlying signal categorized eir classes, note object parameter logistic regression]. hence, unified analysis, paper copes arbitrary sparse signal results established quickly apply special instances above. worth mentioning characterize performance algorithm evaluate obtained estimate aspects, specifically interested quality support recovery. recall sparse recovery problems, prominent metrics: distance support recovery. oretical results phrased terms metric referred parameter estimation, previous papers emphasized. metric, popular algorithms., Lasso] hard thresholding based algorithms], guaranteed accurate approximation energy noise. support recovery anor important factor evaluate algorithm, feature 31st Conference Neural Information Processing Systems (nips 2017), Long beach, usa. selection variable selection. earliest work] offered sufficient conditions orthogonal matching pursuit basis pursuit identify support. ory developed] Lasso estimator] garrotte estimator. typically, recovering support target signal challenging parameter estimation. instance] showed restricted eigenvalue condition suﬃces Lasso produce accurate estimate order recover sign pattern, stringent mutual incoherence condition imposed]. however, recognized, support detected precisely method, solution admits optimal statistical rate]. regard, research support recovery continues central recent years]. work line studies support recovery performance hard thresholding based algorithms, enjoy superior computational eﬃciency convex programs manipulating huge volume data]. note, carried oretical understanding hard thresholding pursuit (htp], showing HTP identifies support signal iterations, neir obtained general results paper. detailed, restricted isometry property (rip) condition], iteration bound holds arbitrary sparse signal interest, results, hold eir global sparse minimizer true sparse signal. relaxed sparsity condition, obtain clear iteration complexity? log  proper condition number. contrast, hard quantify bound] (see orem rein). algorithmic perspective, general algorithm htp. fact, appeal recently proposed partial hard thresholding (pht) operator] demonstrate results, turn iteration complexity HTP orthogonal matching pursuit replacement (ompr]. reby, results paper considerably extend earlier work HTP]. worth mentioning that, analysis hinges PHT operator, support recovery results established stronger results] \\x0conly showed parameter estimation pht. finally, remark couple previous work considered signals sparse]), paper focus sparse case. extensions generic signals left interesting future directions. contribution. contribution paper summarized follows. study iteration complexity PHT algorithm, show RIP condition relaxed sparsity condition clarified), PHT recovers support arbitrary-sparse signal? log iterations. strengns oretical results] parameter estimation PHT established. generality PHT operator, results shed light support recovery performance family prevalent iterative algorithms. extreme cases pht, results immediately apply HTP ompr, imply bound. roadmap. remainder paper organized follows. describe problem setting, partial hard thresholding operator Section main results iteration complexity. section sketch proof main results list lemmas independent interest. numerical results illustrated Section Section concludes paper poses interesting future work. detailed proof oretical results deferred appendix (see supplementary file). notation. collect notation involved paper. upper-right letter subscript variants., denote absolute constants values change appearance appearance. vector norm denoted kxk. support set denoted supp) indexes non-zeros slight abuse, supp, set indices largest magnitude) elements. ties broken lexicographically. interchangeably write kxk0 —supp)— signify cardinality supp). vector restricted support set. -dimensional vector support set , }, depending context, eir —-dimensional vector extracting elements belonging-dimensional vector setting elements zero. complement set denoted   target-sparse signal support denoted quantity reserve min minimum absolute element recall  consists  pht algorithm depend carefully chosen function). write non-zeros gradient) shorthand))supp., top absolute components).   partial Hard Thresholding pursue sparse solution, hard thresholding broadly invoked popular greedy algorithms. present work, interested partial hard thresholding operator sheds light unified design analysis iterative algorithms employing operator hard thresholding operator]. formally, support set freedom parameter PHT operator produce-sparse approximation defined follows: PHTk; arg min . kxk0 supp)—  constraint simply enforces-sparse solution. gain intuition one, support set iterate iterative algorithm, constraint ensures support set differs previous positions. special case, noticed PHT operator reduces standard hard thresholding picking freedom parameter spectrum, case PHT operator yields interesting algorithm termed orthogonal matching pursuit replacement], general replaces element iteration. shown] PHT operator computed eﬃcient manner general support set freedom parameter paper, major focus case lemma] PHTk; follows: top supp PHTk; HTk ?top) HTk (?) standard hard thresholding operator sets largest absolute components vector zero. equipped PHT operator, position describe general iterative greedy algorithm, termed pht) freedom parameter).  iteration, algorithm reveals iterate support set returns solution follows   PHTk supp arg min. supp)   above, note step size) proxy function carefully chosen clarified later). typically, sparsity parameter equals sparsity target  paper, general choice leads results. signal furr clarity, comments) order. first, observed context sparsity-constrained minimization, proxy function) chosen objective function]. scenario, target signal global optimum pht) proceeds projected gradient descent. neverless, recall  realistic function) goal estimate arbitrary signal target global minimizer. remedy offer characterizing  deterministic condition analogous signalto-noise ratio condition, function) fulfilling condition suﬃces. light, find) behaves proxy guides algorithm target. remarkably, analysis encompasses situation considered]. second, made explicitly) mea?  surements training data. consider, example, recovering design matrix response (both known). natural running pht) algorithm) axk2 logistic regression model binary vector (label), collection training data (feature) logistic loss evaluated training samples. clarification, ready make assumptions). turns properties) vital analysis: restricted strong convexity restricted smoothness. conditions proposed] standard literature]. our results hold but observe size equal hence, ease exposition, This case considered]. definition differentiable function) satisfies property restricted strong  convexity (rsc) sparsity level parameter        likewise) satisfies property restricted smoothness (rss) sparsity level  parameter   holds) ?  ?    ) ?  ?        call condition number problem, essentially identical condition number Hessian matrix) restricted-sparse directions.  Deterministic Analysis proposition shows mild conditions, pht) eir terminates?  log iterations. covers support arbitrary-sparse signal Proposition pht) algorithm Suppose) -rsc eir terminates recovers rss, step size  /?+   pht)    ? log iterations provided min  support .  remarks order. first, remind reader conditions stated above, guaranteed pht) succeeds. pht) fails terminates time stamp This, example, feed bad initial point pick small step size. particular, x0min  algorithm makes progress. crux remedy issue imposing lower bound coordinates iteration, below. however, proposition asserts make pht) runs long? log iterations), recovers support arbitrary sparse signal. note neir RIP condition relaxed sparsity assumed proposition.  min -condition natural, viewed generalization wellknown signal-noise ratio (snr) condition. noisy compressed sensing problem, ) axk here, vector noise. RSC RSS imply-sparse Hence kxk kaxk  kxk  ?  (kek) min -condition times establish support recovery. see, examin fact, ple]. following, strengn prop. rip condition requires wellbounded condition number.,  ). orem pht) algorithm Suppose)  -rsc   -rss.   condition number smaller/(   . pick step size     +?    pht) recovers support  log log ?)) tmax xk0 log/?) log/?) iterations, provided constant  ,  min   ???  above, ?)(?     ). remark aspects orem. important part orem offers oretical justification pht) recovers support. achieved imposing RIP condition., bounding condition number above) proper step size. make iteration bound explicit, order examine parameter dependency. first, note tmax scales approximately linearly conforms intuition small large signal-noise ratio, easy distinguish support interest noise. freedom parameter encoded coeﬃcient quantity observe increasing scalar small fewer iterations. surprising large grants algorithm freedom current iterate. indeed, case, pht) recover support) iterations pht) min condition, find stronger) steps. however, investigate SNR condition afford large freedom parameter. interesting contrast orem], independently built state--art support recovery results htp. mentioned] made optimality target signal, restricted setting compared result. iteration bound (see orem rein), appealing insight, clear parameter dependence natural parameters problem., sparsity condition number). ] developed? xk0 iteration complexity compressed sensing. again, confined special signal carry generalization analyze family algorithms. RIP condition ubiquitous literature, researchers point realistic practical applications]. true large-scale machine learning problems, condition number grow sample size (hence upper bound constant). clever solution knowledge) suggested], showed sparsity parameter guarantees convergence projected gradient descent. idea recently employed] show rip-free condition sparse recovery, technically way. orem borrows elegant idea prove rip-free results pht).  orem pht) algorithm. suppose -rscand -rss.    condition number. furr pick  (?? min, included iterate pht)  /?+ support log log ?)) tmax log/?) log/?) iterations, provided constant  ),  min   ??? above,  ??? ??  discuss salient features orem compared prop. orem first, note pick ?+ orem, results  /?). iteration complexity essentially? log similar prop.  however, orem sparsity parameter set min}) guarantees support min -condition refined, inclusion. pose open question wher scales stringent ill-conditioned problems. anor important consequence implied orem sparsity parameter depends minimum Consider corresponds OMPR algorithm.  suﬃces. contrast, previous work] obtained oretical result), owing restricted problem setting. note original OMPR paper] latest version], rip-free condition established.  Statistical Results Until now, oretical results phrased terms deterministic conditions., rsc, min conditions satisfied prevalent statistical models RSS   linear regression logistic regression. here, give detailed statistical results sparse linear regression, refer reader] applications. sparse linear regression model hai  drawn. sub-gaussian distribution covariance   drawn. ,  presume diagonal elements properly scaled   let?   goal knowledge end, choose) axk2 recover ?min (?) ?max (?) smallest largest singulars respectively. high probability) satisfies RSC RSS properties sparsity level parameters log log ) ?min (?)   ?max (?)  respectively. high probability, holds: log    see] detailed discussion. probabilistic arguments hand, investigate suﬃcient conditions preceding deterministic results hold. prop. recall sparsity level RSC RSS. hence, pick sample size 2c1 log/?min (?)   (?)  /qc1   ?max (?)  min        ) qc1 ?min (?) right-hand side monotonically decreasing pick min concrete, covariance large enough, smaller matrix identity matrix ?min (?) ?max (?)  suppose upper bound        qc1  min -condition prop. suﬃces pick thus, order fulfill    max min For orem essentially asks well-conditioned design matrix sparsity level Note) implies ?max )/?min return requires well-conditioned covariance matrix. thus, guarantee   suﬃces choose ?max )/?min (?)  pick  log/?min (?)   ?max )/?min (?)    ?max )/?min (?) finally, orem asserts support inclusion expanding support size iterates. suppose ?+ results  min}. condition number greater pick ?22k min}. sight, weird depends condition number relies choice following, present concrete sample complexity showing condition met. focus extreme cases: For require ?22k pick 4c1 log/?min (?). way, obtain ?min (?)  2c1 )?max (?). condition number design matrix )?max )/?min (?). consequently, set parameter ?max (?)  ?min (?) note quantities depend covariance matrix. again, identity matrix, sample complexity log). likewise ?22k suﬃces. deduction above, ?max (?) ?min (?) proof Sketch sketch proof list lemmas independent interest. high-level proof technique recent work] performs RIP analysis compressed sensing. purpose, deal freedom parameter RIPfree condition. generalize arguments] show support recovery results arbitrary sparse signals. indeed, prove lemma crucial descending order analysis. assume loss generality elements magnitude. lemma pht) algorithm. assume) -rsc -rss. furr assume sequence satisfies        )    ) \\x0cfor positive  suppose iteration ),  integer   exists indices top magnitude) elements integer  determined  }          provided  +? indices top elements constant  ).  ?? xmin isolate lemma find inspiring general. lemma states proper conditions, show sequence satisfies), iterations, pht) captures correct indices iterate. note condition) states sequence contract geometric rate, condition) immediately fully corrective step., minimizing) support set). orem concludes conditions Lemma total iteration complexity support recovery proportional sparsity underlying signal. orem assume conditions Lemma pht) successfully identifies suplog??)) log log/?) log/?)  port xk0 number iterations. detailed proofs results appendix. armed remains show pht) satisfies condition) settings. proof Sketch prop.  start comparing sake, record important properties. first, due fully corrective step, support set orthogonal means subset    set    ????  note due PHT operator, element smaller critical facts toger RSS condition result         Since consists top elements show??      Using argument prop. , establish linear convergence iterates., condition). result follows. proof Sketch orem prove orem, present careful analysis problem structure. particular, supp elements largest elements, element smaller compare elements \\x0cand number components, show relationship averaged energy   terms Using equation standard relaxation, bound  follows. lemma assume) satisfies properties RSC RSS sparsity level Let   supp   support set  /?+        ???    . particular, picking /?+           note lemma applies two-stage thresholding algorithms., CoSaMP]) step expanding support set. hand,   this smallest elements   finally, note  hence) follows. upper bounded Proof Sketch orem proof idea orem inspired], give tighter general analysis. observe larger elements due pht. enables show             prove claim     end, wher larger  case, RSC condition PHT property, show       hold. partition set union  show -norm smaller addition, RSC condition          Since  implies desired bound rearranging terms. case reminiscent way. proof complete. simulation complement oretical results performing numerical experiments section. particular, interested aspects: first, number iterations required identify support-sparse signal; second, tradeoff iteration number percentage success resulted choices freedom parameter 200 100 90100 #non?zeros percentage success 100 #iterations 200 100 90100 #non?zeros percentage success #iterations compressed sensing model .01e, dimension 200 entries. normal variables. sparsity level uniformly choose assign values non-zeros. normals. configurations: support sparsity sample size independently generate 100 signals test pht) pht) succeeds trial returns iterate correct support thousands iterations. orwise mark trial failure. iteration numbers reported counted success trials. step size fixed unit, tune cross-validation performance. 100 100 150 #measurements 200 100 100 100 150 #measurements 200 Figure Iteration number success percentage sparsity sample size. panel shows iteration number grows linearly sparsity. choice suﬃces guarantee minimum iteration complexity. panel shows comparable statistical performance choices illustrates iteration number sample size panel depicts phase transition. study iteration number scales sparsity practice, fix 200 tune 100. test freedom parameter signals. results shown leftmost figure Figure ory predicted, observe) iterations, pht) precisely identifies true support. subfigure, plot percentage success sparsity. appears pht) lays top. possibly suﬃciently large sample size. next, fix vary 200. surprisingly, rightmost figure, min observe performance degrade large freedom parameter. conjecture condition established refined. figure illustrates interesting phenomenon: threshold, pht) significantly reduces iteration number increasing This explained orems paper. leave promising research direction. conclusion Future Work paper, presented principled analysis family hard thresholding algorithms. facilitate analysis, appealed recently proposed partial hard thresholding operator. shown RIP condition relaxed sparsity condition, pht) algorithm ? recovers support arbitrary sparse signal xk0 log iterations, provided generalized signal-noise ratio condition satisfied. account unified analysis, established bound HTP ompr. illustrated simulation results agree finding iteration number proportional sparsity. interesting future directions. first, interesting examine close logarithmic factor log iteration bound. second, study rip-free conditions two-stage pht?algorithms cosamp. finally, pose open question min -condition. wher improve factor acknowledgements. work supported part nsf-bigdata-1419210 nsf-iii1360971. anonymous reviewers valuable comments. This paper concerned problem recovering arbitrary sparse signal set  -sparse (compressed) measurements. signal  this problem, toger variants, found variety successful non-zeros applications compressed sensing, machine learning statistics. interest true signal small number linear measurements given, referred setting compressed sensing. such instance exhaustively studied decade, large body elegant work devoted eﬃcient algorithms including -based convex optimization hard thresholding based greedy pursuits]. anor quintessential sparsity-constrained minimization program recently considered machine learning, set training], goal eﬃciently learn global sparse minimizer data. though cases, underlying signal categorized eir classes, note object parameter logistic regression]. hence, unified analysis, paper copes arbitrary sparse signal results established quickly apply special instances above. worth mentioning characterize performance algorithm evaluate obtained estimate aspects, specifically interested quality support recovery. recall sparse recovery problems, prominent metrics: distance support recovery. oretical results phrased terms metric referred parameter estimation, previous papers emphasized. under metric, popular algorithms., Lasso] hard thresholding based algorithms], guaranteed accurate approximation energy noise. support recovery anor important factor evaluate algorithm, feature 31st Conference Neural Information Processing Systems (nips 2017), Long beach, usa. selection variable selection. earliest work] offered sufficient conditions orthogonal matching pursuit basis pursuit identify support. ory developed] Lasso estimator] garrotte estimator. typically, recovering support target signal challenging parameter estimation. for instance] showed restricted eigenvalue condition suﬃces Lasso produce accurate estimate order recover sign pattern, stringent mutual incoherence condition imposed]. however, recognized, support detected precisely method, solution admits optimal statistical rate]. regard, research support recovery continues central recent years]. our work line studies support recovery performance hard thresholding based algorithms, enjoy superior computational eﬃciency convex programs manipulating huge volume data]. note, carried oretical understanding hard thresholding pursuit (htp], showing HTP identifies support signal iterations, neir obtained general results paper. detailed, restricted isometry property (rip) condition], iteration bound holds arbitrary sparse signal interest, results, hold eir global sparse minimizer true sparse signal. using relaxed sparsity condition, obtain clear iteration complexity? log  proper condition number. contrast, hard quantify bound] (see orem rein). from algorithmic perspective, general algorithm htp. fact, appeal recently proposed partial hard thresholding (pht) operator] demonstrate results, turn iteration complexity HTP orthogonal matching pursuit replacement (ompr]. reby, results paper considerably extend earlier work HTP]. worth mentioning that, analysis hinges PHT operator, support recovery results established stronger results] \\x0conly showed parameter estimation pht. finally, remark couple previous work considered signals sparse]), paper focus sparse case. extensions generic signals left interesting future directions. contribution. contribution paper summarized follows. study iteration complexity PHT algorithm, show RIP condition relaxed sparsity condition clarified), PHT recovers support arbitrary-sparse signal? log iterations. this strengns oretical results] parameter estimation PHT established. thanks generality PHT operator, results shed light support recovery performance family prevalent iterative algorithms. extreme cases pht, results immediately apply HTP ompr, imply bound. roadmap. remainder paper organized follows. describe problem setting, partial hard thresholding operator Section main results iteration complexity. Section sketch proof main results list lemmas independent interest. numerical results illustrated Section Section concludes paper poses interesting future work. detailed proof oretical results deferred appendix (see supplementary file). notation. collect notation involved paper. upper-right letter subscript variants., denote absolute constants values change appearance appearance. for vector norm denoted kxk. support set denoted supp) indexes non-zeros with slight abuse, supp, set indices largest magnitude) elements. ties broken lexicographically. interchangeably write kxk0 —supp)— signify cardinality supp). vector restricted support set. that-dimensional vector support set , }, depending context, eir —-dimensional vector extracting elements belonging-dimensional vector setting elements zero. complement set denoted   target-sparse signal support denoted quantity reserve min minimum absolute element recall  consists  PHT algorithm depend carefully chosen function). write non-zeros gradient) shorthand))supp., top absolute components).   Partial Hard Thresholding pursue sparse solution, hard thresholding broadly invoked popular greedy algorithms. present work, interested partial hard thresholding operator sheds light unified design analysis iterative algorithms employing operator hard thresholding operator]. formally, support set freedom parameter PHT operator produce-sparse approximation defined follows: PHTk; arg min . kxk0 supp)—  constraint simply enforces-sparse solution. gain intuition one, support set iterate iterative algorithm, constraint ensures support set differs previous positions. special case, noticed PHT operator reduces standard hard thresholding picking freedom parameter spectrum, case PHT operator yields interesting algorithm termed orthogonal matching pursuit replacement], general replaces element iteration. shown] PHT operator computed eﬃcient manner general support set freedom parameter paper, major focus case Lemma] PHTk; follows: top supp PHTk; HTk ?top) HTk (?) standard hard thresholding operator sets largest absolute components vector zero. equipped PHT operator, position describe general iterative greedy algorithm, termed pht) freedom parameter).  iteration, algorithm reveals iterate support set returns solution follows   PHTk supp arg min. supp)   above, note step size) proxy function carefully chosen clarified later). typically, sparsity parameter equals sparsity target  paper, general choice leads results. for signal furr clarity, comments) order. first, observed context sparsity-constrained minimization, proxy function) chosen objective function]. scenario, target signal global optimum pht) proceeds projected gradient descent. neverless, recall  realistic function) goal estimate arbitrary signal target global minimizer. remedy offer characterizing  deterministic condition analogous signalto-noise ratio condition, function) fulfilling condition suﬃces. light, find) behaves proxy guides algorithm target. remarkably, analysis encompasses situation considered]. second, made explicitly) mea?  surements training data. consider, example, recovering design matrix response (both known). natural running pht) algorithm) axk2 one logistic regression model binary vector (label), collection training data (feature) logistic loss evaluated training samples. with clarification, ready make assumptions). turns properties) vital analysis: restricted strong convexity restricted smoothness. conditions proposed] standard literature]. Our results hold But observe size equal hence, ease exposition, This case considered]. Definition differentiable function) satisfies property restricted strong  convexity (rsc) sparsity level parameter        likewise) satisfies property restricted smoothness (rss) sparsity level  parameter   holds) ?  ?    ) ?  ?        call condition number problem, essentially identical condition number Hessian matrix) restricted-sparse directions.  Deterministic Analysis proposition shows mild conditions, pht) eir terminates?  log iterations. covers support arbitrary-sparse signal Proposition consider pht) algorithm Suppose) -rsc eir terminates recovers rss, step size  /?+ let  pht)    ? log iterations provided min  support .  remarks order. first, remind reader conditions stated above, guaranteed pht) succeeds. pht) fails terminates time stamp This, example, feed bad initial point pick small step size. particular, x0min  algorithm makes progress. crux remedy issue imposing lower bound coordinates iteration, below. however, proposition asserts make pht) runs long? log iterations), recovers support arbitrary sparse signal. note neir RIP condition relaxed sparsity assumed proposition.  min -condition natural, viewed generalization wellknown signal-noise ratio (snr) condition. this noisy compressed sensing problem, ) axk here, vector noise. now RSC RSS imply-sparse Hence kxk kaxk  kxk  ?  (kek) min -condition times establish support recovery. see, examin fact, ple]. following, strengn prop. RIP condition requires wellbounded condition number.,  ). orem consider pht) algorithm Suppose)  -rsc   -rss. let  condition number smaller/(   . pick step size     +?    pht) recovers support  log log ?)) tmax xk0 log/?) log/?) iterations, provided constant  ,  min   ???  above, ?)(?     ). remark aspects orem. important part orem offers oretical justification pht) recovers support. this achieved imposing RIP condition., bounding condition number above) proper step size. make iteration bound explicit, order examine parameter dependency. first, note tmax scales approximately linearly this conforms intuition small large signal-noise ratio, easy distinguish support interest noise. freedom parameter encoded coeﬃcient quantity observe increasing scalar small fewer iterations. this surprising large grants algorithm freedom current iterate. indeed, case, pht) recover support) iterations pht) min condition, find stronger) steps. however, investigate SNR condition afford large freedom parameter. interesting contrast orem], independently built state--art support recovery results htp. mentioned] made optimality target signal, restricted setting compared result. iteration bound (see orem rein), appealing insight, clear parameter dependence natural parameters problem., sparsity condition number). ] developed? xk0 iteration complexity compressed sensing. again, confined special signal carry generalization analyze family algorithms. though RIP condition ubiquitous literature, researchers point realistic practical applications]. this true large-scale machine learning problems, condition number grow sample size (hence upper bound constant). clever solution knowledge) suggested], showed sparsity parameter guarantees convergence projected gradient descent. idea recently employed] show rip-free condition sparse recovery, technically way. orem borrows elegant idea prove rip-free results pht).  orem consider pht) algorithm. suppose -rscand -rss. let   condition number. furr pick  (?? min, included iterate pht)  /?+ support log log ?)) tmax log/?) log/?) iterations, provided constant  ),  min   ??? above,  ??? ??  discuss salient features orem compared prop. orem first, note pick ?+ orem, results  /?). iteration complexity essentially? log similar prop.  however, orem sparsity parameter set min}) guarantees support min -condition refined, inclusion. pose open question wher scales stringent ill-conditioned problems. anor important consequence implied orem sparsity parameter depends minimum Consider corresponds OMPR algorithm.  suﬃces. contrast, previous work] obtained oretical result), owing restricted problem setting. note original OMPR paper] latest version], rip-free condition established.  Statistical Results Until now, oretical results phrased terms deterministic conditions., rsc, min conditions satisfied prevalent statistical models RSS   linear regression logistic regression. here, give detailed statistical results sparse linear regression, refer reader] applications. consider sparse linear regression model hai  drawn. sub-gaussian distribution covariance   drawn. ,  presume diagonal elements properly scaled   Let?   our goal knowledge end, choose) axk2 let recover ?min (?) ?max (?) smallest largest singulars respectively. high probability) satisfies RSC RSS properties sparsity level parameters log log ) ?min (?)   ?max (?)  respectively. high probability, holds: log    See] detailed discussion. with probabilistic arguments hand, investigate suﬃcient conditions preceding deterministic results hold. for prop. recall sparsity level RSC RSS. hence, pick sample size 2c1 log/?min (?)   (?)  /qc1   ?max (?)  min        ) qc1 ?min (?) right-hand side monotonically decreasing pick min concrete, covariance large enough, smaller matrix identity matrix ?min (?) ?max (?)  now suppose upper bound        qc1  min -condition prop. suﬃces pick thus, order fulfill    max min For orem essentially asks well-conditioned design matrix sparsity level Note) implies ?max )/?min return requires well-conditioned covariance matrix. thus, guarantee   suﬃces choose ?max )/?min (?)  pick  log/?min (?)   ?max )/?min (?)    ?max )/?min (?) finally, orem asserts support inclusion expanding support size iterates. suppose ?+ results  min}. given condition number greater pick ?22k min}. sight, weird depends condition number relies choice following, present concrete sample complexity showing condition met. focus extreme cases: For require ?22k let pick 4c1 log/?min (?). way, obtain ?min (?)  2c1 )?max (?). condition number design matrix )?max )/?min (?). consequently, set parameter ?max (?)  ?min (?) Note quantities depend covariance matrix. again, identity matrix, sample complexity log). for likewise ?22k suﬃces. following deduction above, ?max (?) ?min (?) Proof Sketch sketch proof list lemmas independent interest. high-level proof technique recent work] performs RIP analysis compressed sensing. but purpose, deal freedom parameter RIPfree condition. generalize arguments] show support recovery results arbitrary sparse signals. indeed, prove lemma crucial descending order analysis. below assume loss generality elements magnitude. Lemma consider pht) algorithm. assume) -rsc -rss. furr assume sequence satisfies        )    ) \\x0cfor positive  suppose iteration ),  integer   exists indices top magnitude) elements integer  determined  }          provided  +? indices top elements constant  ).  ?? xmin isolate lemma find inspiring general. lemma states proper conditions, show sequence satisfies), iterations, pht) captures correct indices iterate. note condition) states sequence contract geometric rate, condition) immediately fully corrective step., minimizing) support set). orem concludes conditions Lemma total iteration complexity support recovery proportional sparsity underlying signal. orem assume conditions Lemma pht) successfully identifies suplog??)) log log/?) log/?)  port xk0 number iterations. detailed proofs results appendix. armed remains show pht) satisfies condition) settings. proof Sketch prop.  start comparing for sake, record important properties. first, due fully corrective step, support set orthogonal that means subset    set    ????  note due PHT operator, element smaller critical facts toger RSS condition result         Since consists top elements show??      Using argument prop. , establish linear convergence iterates., condition). result follows. proof Sketch orem prove orem, present careful analysis problem structure. particular, supp elements since largest elements, element smaller compare elements \\x0cand though number components, show relationship averaged energy   terms Using equation standard relaxation, bound  follows. lemma assume) satisfies properties RSC RSS sparsity level Let   supp   consider support set  /?+        ???    . particular, picking /?+           note lemma applies two-stage thresholding algorithms., CoSaMP]) step expanding support set. hand,   This smallest elements   finally, note  hence) follows. upper bounded Proof Sketch orem proof idea orem inspired], give tighter general analysis. observe larger elements due pht. this enables show             prove claim     end, wher larger  case, RSC condition PHT property, show       hold. but partition set union  show -norm smaller addition, RSC condition          Since  implies desired bound rearranging terms. case reminiscent way. proof complete. Simulation complement oretical results performing numerical experiments section. particular, interested aspects: first, number iterations required identify support-sparse signal; second, tradeoff iteration number percentage success resulted choices freedom parameter 200 100 90100 #non?zeros percentage success 100 #iterations 200 100 90100 #non?zeros percentage success #iterations compressed sensing model .01e, dimension 200 entries. normal variables. given sparsity level uniformly choose assign values non-zeros. normals. configurations: support sparsity sample size given independently generate 100 signals test pht) pht) succeeds trial returns iterate correct support thousands iterations. orwise mark trial failure. iteration numbers reported counted success trials. step size fixed unit, tune cross-validation performance. 100 100 150 #measurements 200 100 100 100 150 #measurements 200 Figure Iteration number success percentage sparsity sample size. panel shows iteration number grows linearly sparsity. choice suﬃces guarantee minimum iteration complexity. panel shows comparable statistical performance choices illustrates iteration number sample size panel depicts phase transition. study iteration number scales sparsity practice, fix 200 tune 100. test freedom parameter signals. results shown leftmost figure Figure ory predicted, observe) iterations, pht) precisely identifies true support. subfigure, plot percentage success sparsity. appears pht) lays top. this possibly suﬃciently large sample size. next, fix vary 200. surprisingly, rightmost figure, min observe performance degrade large freedom parameter. conjecture condition established refined. figure illustrates interesting phenomenon: threshold, pht) significantly reduces iteration number increasing This explained orems paper. leave promising research direction. Conclusion Future Work paper, presented principled analysis family hard thresholding algorithms. facilitate analysis, appealed recently proposed partial hard thresholding operator. shown RIP condition relaxed sparsity condition, pht) algorithm ? recovers support arbitrary sparse signal xk0 log iterations, provided generalized signal-noise ratio condition satisfied. account unified analysis, established bound HTP ompr. illustrated simulation results agree finding iteration number proportional sparsity. interesting future directions. first, interesting examine close logarithmic factor log iteration bound. second, study rip-free conditions two-stage pht?algorithms cosamp. finally, pose open question min -condition. wher improve factor acknowledgements. work supported part nsf-bigdata-1419210 nsf-iii1360971. anonymous reviewers valuable comments.',\n",
       " 'PP6929': 'traditional online learning setting, sequential prediction uncertainty, learner evaluated single loss function completely iteration]. dealing multiple objectives, impossible simultaneously minimize objectives, objective chosen main function minimize, leaving ors bound pre-defined thresholds. methods dealing objective function transformed deal objective functions giving objective pre-defined weight. diﬃculty, however, lies assigning weight objective order objectives threshold. approach problematic real world applications, player required satisfy constraints. example, online portfolio selection], player maximize wealth keeping risk., variance) contained threshold. anor neyman-pearson) classification paradigm (see]) (which extends objective classical binary classification) goal learn classifier achieving low type error type error threshold. adversarial setting multiple-objective generally impossible constriants unknown-priory]. stochastic setting, Mahdavi. ] proposed framework dealing multiple objectives. case. proved exists solution minimizes main objective function keeping objectives thresholds, algorithm converge optimal solution. work, study online prediction multiple objectives challenging general case unknown underlying process stationary ergodic, allowing observations depend arbitrarily. (single-objective) sequential prediction stationary ergodic sources, considered papers application domains. example, online portfolio selection] proposed non-parametric online strategies guarantee, mild conditions, outcome. anor interesting regard work time-series prediction]. common results asymptotically optimal strategies constructed combining predictions simple 31st Conference Neural Information Processing Systems (nips 2017), Long beach, usa. experts. strategies countably infinite set experts, guarantees provided strategies asymptotic. coincidence, finite sample guarantees methods achieved additional strong assumptions source distribution]. approximate implementations non-parametric strategies (which apply finite set experts), however, turn work exceptionally and, inevitable approximation, reported] significantly outperform strategies designed work adversarial-regret setting, domains. algorithm presented paper utilizes sub-routine Weak Aggregating Algorithm (waa] handle multiple objectives. discuss case objective functions, orems extended easily fixed number functions.  problem Formulation prediction game.  compact observation space round, player required make prediction  compact convex set, based past observations, x1n  and, (x10 empty observation). making prediction observation revealed player suffers losses real-valued continuous functions convex. argument. view player prediction strategy) sequence  , player prediction forecasting functions round (for brevity, denote(x1n )). paper assume   realizations random variables   stochastic process  jointly stationary ergodic  player goal play game strategy minimizes average-loss(x1i  keeping average-loss(x1i bounded prescribed threshold formally, define following: Definition (?-bounded strategy). prediction strategy called ?-bounded (x1i  lim surely. set ?-bounded strategies denoted  result] states single objective case outcome [maxy? , )]] regular conditional probability distribution ?-algebra generated infinite past .). motivates define following: Definition (?-feasible process). stationary ergodic process  ?feasible. functions threshold exists ?    ?-feasibility holds, denote ? necessarily unique) solution minimization problem: minimize? ) subject? , ) define ?-feasible optimal  ? ? )]] note problem) convex minimization problem turn compact convex subset refore, problem equivalent finding saddle point Lagrangian function], namely, min max,  lagrangian, ? , ? , ?))  denote optimal dual ??? assume ??? unique. moreover, set constant ?max ?max ??? set , ?max define instantaneous Lagrangian function, ,   ) brief, seeking strategy  good ?-bounded strategy, terms average-loss, underlying process ?-feasible. strategy called ?-universal. optimality section, show average-loss ?-bounded prediction strategy smaller ?-feasible optimal value. result generalization well-known result] outcome single objective. stating proving optimality result, state lemmas repeatedly paper. lemma breiman generalized ergodic orem. lemmas concern continuity saddle point. probability distribution, proofs supplementary material. lemma (ergodicity]).   stationary ergodic process. positive integer denote operator shifts sequence places left.   sequence real-valued functions limn?? ) surely, function assume supn)—  ??  lim \\x0calmost surely. lemma (continuity minimax). compact real spaces.     continuous function. denote space probability measures (equipped topology weak-convergence). function  continuous ) inf)]  ??? moreover,  inf)] inf)]  ??? ???  Lemma (continuity optimal selection). compact real spaces. exist measurable selection functions? ) arg min max)] ) arg max min ??? ???    moreover, defined Equation). set? ?    ),   ),  )}, closed    importance Lemma stems fact proves continuity properties multi-valued correspondences )  ). leads knowledge limiting distribution, optimal set singleton, )  ) continuous  ready prove optimality  this done, example, imposing regularity conditions objectives (see]). orem (optimality    ?-feasible process. strategy  holds. lim inf(x1i     proof. strategy  sequence(x1i  )   (pxi Observe(x1i  (x1i x1i(x1i x1i  Since(x1i  (x1i x1i martingale difference sequence, summand converges., strong law large numbers (see]). refore, lim inf(x1i lim inf(x1i x1i   lim inf min, x1i) \\x0cwhere minimum.  (x1i )-measurable functions. process stationary,   (px0 min,  lim inf (px0 ) lim inf  ) Using levy zero-one law, PX0  weakly approaches lemma continuous. refore, apply Lemma.  ? ? ? ? ??? ? ???  ) Note also, due complementary slackness condition optimal solution., ??? ? ?    ? ? )]]  uniqueness ??? lemma  ??? approaches moreover, continuous compact set, uniformly continuous. refore, exists  , )—   refore, exists,  , ??? )—   thus, lim inf (x1i ???  lim inf(x1i   lim inf lim(x1i    lim inf (x1i  (x1i ???    Algorithm Minimax Histogram Based Aggregation (mha) input: Countable set experts  initial probability For play nature reveals Suffer loss Update cumulative loss experts Update experts? weights wny exp    ) Update experts? weights exp    Choose End For arbitrary, lim inf (x1i ???  lim inf(x1i    refore conclude lim inf (x1i ???    finish proof noticing  definition lim ??? (x1i   negative, desired result. lemma motivation find saddle point Lagrangian refore, reminder paper loss function defined Equation Minimax Histogram Based Aggregation ready present algorithm Minimax Histogram based Aggregation (mha) prove predictions good strategy. orem restate goal: find prediction strategy  ?-feasible process  holds(x1i .  lim Such strategy called ?-universal. maintaining countable set experts  constructed similar manner experts]. expert defined histogram finer grows, allowing construct empirical measure expert refore outputs pair   round pair minimax. empirical measure. show emprical measures converge weakly thus, experts? prediction converge  algorithm outputs round pair   sequence predictions  minimize average loss, sequence predictions   maximize average loss aggregation predictions   respectively. order ensure performance MHA good expert predictions, apply Weak Aggregating Algorithm] alternately. orem states selection points made experts converges optimal solution, proof orem explicit construction experts appears supplementary material. orem prove MHA applied experts defined orem generates sequence predictions ?-bounded good strategy. ?-feasible process. orem assume  ?-feasible process. construct countable set experts ?? ?? ??  lim lim lim predictions made expert round stating main orem mha, state lemma proof appears supplementary material), proof main result mha. lemma  countable set experts defined proof orem relation holds.: inf lim??    lim inf??  predictions MHA applied ready state prove optimality mha. orem (optimality mha).  predictions generated MHA applied defined proof orem ?-feasible process  mha ?-bounded ?-universal strategy. proof. show .  lim) Applying Lemma], updates guarantee expert      constants independent particular, Equation),  inf  ) refore,  lim lim inf      inf lim  inf lim  ) inequality fact lim sub-additive. Lemma)   lim inf??  ) Using similar arguments Equation) show) lim inf   summarizing   lim inf    refore, conclude. limn   lim show MHA ?-bounded strategy, special experts predictions ?max shorten notation, denote, , ?). first, Equation) applied expert that: lim  ?max lim).  ) moreover, uniformly continuous, exists  , )—   proof orem limk?? limh?? limi??  ???  refore, exist —?ik0 ???   refore,  lim     lim ???   ?ik0  lim  ) From uniform continuity learn summand bounded Equation), summand bounded thus) arbitrary, lim        thus, lim supn  ???   orem conclude limn  ???   refore, deduce lim ???   lim lim  lim inf ???   lim lim   ???   ???  results lim  lim ???    Combining Equation), lim  ???  ?max lim ???    ?max MHA ?-bounded. implies lim      now, apply Equation) expert lim inf      thus, lim    Equation), MHA ?-universal. concluding Remarks paper, introduced Minimax Histogram Aggregation (mha) algorithm multipleobjective sequential prediction. considered general setting unknown underlying process stationary ergodic., underlying process ?-feasible, extended well-known result] asymptotic lower bound prediction single objective, case multi-objectives. proved MHA ?-bounded strategy predictions converge optimal solution hindsight. proofs orems lemmas above, fact initial weights experts strictly positive implying countably infinite expert set. practice, however, maintain infinite set experts. refore, customary apply algorithms finite number experts (see]). fact proof assumed observation set priori, algorithm applied case unknown applying doubling trick. furr discussion point]. proofs, relied compactness set interesting wher universality MHA sustained unbounded processes well. interesting open question identify conditions allowing finite sample bounds predicting multiple objectives. acknowledgments This research supported Israel Science Foun dation (grant. traditional online learning setting, sequential prediction uncertainty, learner evaluated single loss function completely iteration]. when dealing multiple objectives, impossible simultaneously minimize objectives, objective chosen main function minimize, leaving ors bound pre-defined thresholds. methods dealing objective function transformed deal objective functions giving objective pre-defined weight. diﬃculty, however, lies assigning weight objective order objectives threshold. this approach problematic real world applications, player required satisfy constraints. for example, online portfolio selection], player maximize wealth keeping risk., variance) contained threshold. anor neyman-pearson) classification paradigm (see]) (which extends objective classical binary classification) goal learn classifier achieving low type error type error threshold. adversarial setting multiple-objective generally impossible constriants unknown-priory]. stochastic setting, Mahdavi. ] proposed framework dealing multiple objectives. case. proved exists solution minimizes main objective function keeping objectives thresholds, algorithm converge optimal solution. work, study online prediction multiple objectives challenging general case unknown underlying process stationary ergodic, allowing observations depend arbitrarily. (single-objective) sequential prediction stationary ergodic sources, considered papers application domains. for example, online portfolio selection] proposed non-parametric online strategies guarantee, mild conditions, outcome. anor interesting regard work time-series prediction]. common results asymptotically optimal strategies constructed combining predictions simple 31st Conference Neural Information Processing Systems (nips 2017), Long beach, usa. experts. strategies countably infinite set experts, guarantees provided strategies asymptotic. this coincidence, finite sample guarantees methods achieved additional strong assumptions source distribution]. approximate implementations non-parametric strategies (which apply finite set experts), however, turn work exceptionally and, inevitable approximation, reported] significantly outperform strategies designed work adversarial-regret setting, domains. algorithm presented paper utilizes sub-routine Weak Aggregating Algorithm (waa] handle multiple objectives. while discuss case objective functions, orems extended easily fixed number functions.  Problem Formulation prediction game. let compact observation space round, player required make prediction  compact convex set, based past observations, x1n  and, (x10 empty observation). after making prediction observation revealed player suffers losses real-valued continuous functions convex. argument. view player prediction strategy) sequence  , player prediction forecasting functions round (for brevity, denote(x1n )). throughout paper assume   realizations random variables   stochastic process  jointly stationary ergodic  player goal play game strategy minimizes average-loss(x1i  ), keeping average-loss(x1i bounded prescribed threshold formally, define following: Definition (?-bounded strategy). prediction strategy called ?-bounded (x1i  lim surely. set ?-bounded strategies denoted  result] states single objective case outcome [maxy? , )]] regular conditional probability distribution ?-algebra generated infinite past .). this motivates define following: Definition (?-feasible process). stationary ergodic process  ?feasible. functions threshold exists ?    ?-feasibility holds, denote ? necessarily unique) solution minimization problem: minimize? ) subject? , ) define ?-feasible optimal  ? ? )]] note problem) convex minimization problem turn compact convex subset refore, problem equivalent finding saddle point Lagrangian function], namely, min max,  Lagrangian, ? , ? , ?))  denote optimal dual ??? assume ??? unique. moreover, set constant ?max ?max ??? set , ?max define instantaneous Lagrangian function, ,   ) brief, seeking strategy  good ?-bounded strategy, terms average-loss, underlying process ?-feasible. such strategy called ?-universal. optimality section, show average-loss ?-bounded prediction strategy smaller ?-feasible optimal value. this result generalization well-known result] outcome single objective. before stating proving optimality result, state lemmas repeatedly paper. lemma breiman generalized ergodic orem. lemmas concern continuity saddle point. probability distribution, proofs supplementary material. lemma (ergodicity]). let  stationary ergodic process. for positive integer denote operator shifts sequence places left. let  sequence real-valued functions limn?? ) surely, function assume supn)—  ??  lim \\x0calmost surely. lemma (continuity minimax). let compact real spaces.     continuous function. denote space probability measures (equipped topology weak-convergence). function  continuous ) inf)]  ??? moreover,  inf)] inf)]  ??? ???  Lemma (continuity optimal selection). let compact real spaces. exist measurable selection functions? ) arg min max)] ) arg max min ??? ???    moreover, defined Equation). set? ?    ),   ),  )}, closed    importance Lemma stems fact proves continuity properties multi-valued correspondences )  ). this leads knowledge limiting distribution, optimal set singleton, )  ) continuous  ready prove optimality  This done, example, imposing regularity conditions objectives (see]). orem (optimality  let  ?-feasible process. strategy  holds. lim inf(x1i     proof. for strategy  sequence(x1i  )   (pxi Observe(x1i  (x1i x1i(x1i x1i  Since(x1i  (x1i x1i martingale difference sequence, summand converges., strong law large numbers (see]). refore, lim inf(x1i lim inf(x1i x1i   lim inf min, x1i) \\x0cwhere minimum.  (x1i )-measurable functions. because process stationary,   (px0 min,  lim inf (px0 ) lim inf  ) Using levy zero-one law, PX0  weakly approaches Lemma continuous. refore, apply Lemma.  ? ? ? ? ??? ? ???  ) Note also, due complementary slackness condition optimal solution., ??? ? ?    ? ? )]]  from uniqueness ??? Lemma  ??? approaches moreover, continuous compact set, uniformly continuous. refore, exists  , )—   refore, exists,  , ??? )—   thus, lim inf (x1i ???  lim inf(x1i   lim inf lim(x1i    lim inf (x1i  (x1i ???    Algorithm Minimax Histogram Based Aggregation (mha) input: Countable set experts  initial probability For play nature reveals Suffer loss Update cumulative loss experts Update experts? weights wny exp    ) Update experts? weights exp    Choose End For arbitrary, lim inf (x1i ???  lim inf(x1i    refore conclude lim inf (x1i ???    finish proof noticing  definition lim ??? (x1i   negative, desired result. lemma motivation find saddle point Lagrangian refore, reminder paper loss function defined Equation Minimax Histogram Based Aggregation ready present algorithm Minimax Histogram based Aggregation (mha) prove predictions good strategy. orem restate goal: find prediction strategy  ?-feasible process  holds(x1i .  lim Such strategy called ?-universal. maintaining countable set experts  constructed similar manner experts]. each expert defined histogram finer grows, allowing construct empirical measure expert refore outputs pair   round this pair minimax. empirical measure. show emprical measures converge weakly thus, experts? prediction converge  our algorithm outputs round pair   sequence predictions  minimize average loss, sequence predictions   maximize average loss each aggregation predictions   respectively. order ensure performance MHA good expert predictions, apply Weak Aggregating Algorithm] alternately. orem states selection points made experts converges optimal solution, proof orem explicit construction experts appears supplementary material. orem prove MHA applied experts defined orem generates sequence predictions ?-bounded good strategy. ?-feasible process. orem assume  ?-feasible process. construct countable set experts ?? ?? ??  lim lim lim predictions made expert round before stating main orem mha, state lemma proof appears supplementary material), proof main result mha. lemma let countable set experts defined proof orem relation holds.: inf lim??    lim inf??  predictions MHA applied ready state prove optimality mha. orem (optimality mha). let predictions generated MHA applied defined proof orem ?-feasible process  MHA ?-bounded ?-universal strategy. proof. show .  lim) Applying Lemma], updates guarantee expert      constants independent particular, Equation),  inf  ) refore,  lim lim inf      inf lim  inf lim  ) inequality fact lim sub-additive. using Lemma)   lim inf??  ) Using similar arguments Equation) show) lim inf   summarizing   lim inf    refore, conclude. limn   lim show MHA ?-bounded strategy, special experts predictions ?max shorten notation, denote, , ?). first, Equation) applied expert that: lim  ?max lim).  ) moreover, uniformly continuous, exists  , )—   proof orem limk?? limh?? limi??  ???  refore, exist —?ik0 ???   refore,  lim     lim ???   ?ik0  lim  ) From uniform continuity learn summand bounded Equation), summand bounded thus) arbitrary, lim        thus, lim supn  ???   orem conclude limn  ???   refore, deduce lim ???   lim lim  lim inf ???   lim lim   ???   ???  results lim  lim ???    Combining Equation), lim since ???  ?max lim ???    ?max MHA ?-bounded. this implies lim      now, apply Equation) expert lim inf      thus, lim    Equation), MHA ?-universal. Concluding Remarks paper, introduced Minimax Histogram Aggregation (mha) algorithm multipleobjective sequential prediction. considered general setting unknown underlying process stationary ergodic., underlying process ?-feasible, extended well-known result] asymptotic lower bound prediction single objective, case multi-objectives. proved MHA ?-bounded strategy predictions converge optimal solution hindsight. proofs orems lemmas above, fact initial weights experts strictly positive implying countably infinite expert set. practice, however, maintain infinite set experts. refore, customary apply algorithms finite number experts (see]). despite fact proof assumed observation set priori, algorithm applied case unknown applying doubling trick. for furr discussion point]. proofs, relied compactness set interesting wher universality MHA sustained unbounded processes well. interesting open question identify conditions allowing finite sample bounds predicting multiple objectives. Acknowledgments This research supported Israel Science Foun dation (grant.',\n",
       " 'PP7119': 'seminal work] gave recipe designing revenue maximizing auction auction settings private information players single number distribution number completely auctioneer. raises question auction designer formed prior distribution private information. recent work, starting], addresses question design optimal auctions access samples values bidders. refer reader] overview existing results literature. , give bounds sample complexity optimal auctions computational eﬃciency, recent work focused computationally eﬃcient learning bounds]. work solely focuses sample complexity computational eﬃciency related]. work, tools supervised learning, pseudodimension] variant dimension real-valued functions), compression bounds] Rademacher complexity] bound sample complexity simple auction classes. work introduces measure sample complexity, \\x0cwhich strengning Rademacher complexity analysis independent interest scope sample complexity optimal auctions. moreover, case auctions, measure greatly simplifies analysis sample complexity cases. particular, show general PAC learning settings, expected generalization error upper bounded Rademacher complexity class hyposes, rar class hyposes outcome running Expected Risk Minimization (erm) subset samples half size. number hyposes small, immediately yields small generalization error. refer growth rate set hyposes split-sample growth rate. measure complexity restricted auction design relevant general statistical learning ory. 31st Conference Neural Information Processing Systems (nips 2017), Long beach, usa. show auction classes single-item auctions player-specific reserves, single item-level auctions multiple-item item pricing auctions additive buyers, splitsample growth rate easily bounded. argument boils Empirical Risk Minimization classes set parameters auctions equal player sample. simple counting argument bounds order prior work literature pseudo-dimension]. multi-item settings improvements sample complexity bound. split-sample growth rate similar spirit notion local Rademacher complexity], Rademacher complexity subset hyposes small empirical error. particular, proof based refinement classic analysis Rademacher complexity analysis generalization error (see. ]). however, bound structural, restricting set outcomes chosen ERM process subsample half size. moreover, note counting number outputs ERM connections counting argument made] context pricing mechanisms. however, essence argument restricted transductive settings sample ?features? advance fixed reby argument straightforward similar standard notions ?effective hyposis space? -dimension arguments. measure sample complexity applicable general statistical learning ory framework applications auctions. convey high level intuition settings split-sample growth simplify sample complexity analysis, suppose output hyposis ERM uniquely defined constant number sample points. linear separators assume loss output ERM uniquely characterized choosing) points sample). means number hyposes subset size, split sample growth rate analysis immediately yields expected generalization error log), equivalently sample complexity learning hyposis class error log preliminaries sample complexity optimal auctions. case items, bidders. bidder function drawn independently distribution denote joint distribution. assume sample set   valuation vectors, let denote class dominant strategy truthful single item auctions. auctions player incentive report true auction, independent players). moreover, phi phi (?) payment function mechanism, revenue mechanism valuation vector finally) expected revenue mechanism true distribution values Given sample size compute dominant strategy truthful mechanism that ) )   refer) expected generalization error. moreover, define sample complexity auction class: Definition (sample Complexity Auction class). (additive error) sample complexity auction class class distributions accuracy target defined smallest number samples(),  ) ) interested multiplcative error sample complexity.    notion]. assumes optimal revenue distribution lower bounded constant quantity, additive error implies multiplicative error. instance, assumes player values bounded significant probability, implies lower bound revenue. assumptions instance, made work]. focus additive error work. interested proving high probability guarantees. probability   ) , ,    generalization Error split-sample Growth Rate turn general PAC learning framework, give generalization guarantees terms notion complexity hyposis space \\x0cwhich denote split-sample growth rate. arbitrary hyposis space arbitrary data space suppose set samples   drawn. distribution interested maximizing reward function  ], expectation distribution particular, denote)]. expected Reward Maximization algorithm fixed tie-breaking rule. specifically ERM defined: arg ties broken based pre-defined manner. define notion split-sample hyposis space: denote set Definition (split-sample Hyposis space). sample hyposis output ERM algorithm (with pre-defined tie-breaking rule), subset size.   ) Based split-sample hyposis space, define split-sample growth rate hyposis set size space largest size Definition (split-sample Growth rate). split-sample growth rate hyposis ERM process defined—  show generalization error upper bounded Rademacher complexity evaluated split-sample hyposis space union samples size Rademacher complexity, sample size hyposis space defined,   variable taking values}, equal probability.     independent binary random Lemma hyposis space fixed ERM process, have:  )  independent samples size proof.  optimal hyposis distribution First-write left hand side, adding subtracting expected empirical reward   ?    maximizes empirical reward?   ? independent Thus suﬃces upper bound quantity equation. )] fresh sample size have     subset size , now, set  upper bound definition split-sample hyposis \\x0cspace  quantity taking supremum   ) )  , , zt0 Now observe, rename sample zt0 sample zt0  show change distribution. moreover, change quantity invariant swaps. finally, change sign quantity, , zt0 )). denote }, Rademacher variable, quantity equal, ,     holds) vector  expectation randomly drawn, equal probability. hence   ,    splitting supremma positive negative part observing expected quantities identical, get  2es   , denotes Rademacher complexity sample hyposis observe, orem strengning fact Rademacher complexity upper bounds generalization error, simply because:  ) Thus bound Rademacher complexity lemma bound generalization error. however, reverse true. finally, show main orem, shows split-sample hyposis space small size, immediately generalization bound, furr analyze Rademacher complexity orem (main orem). hyposis space fixed ERM process, have: log(?  )  moreover, probability   )   log(? )) proof. applying massart lemma (see. ]) that:  log log(? ))   ) Combining Lemma yields part orem. finally, high probability statement observing random \\x0cvariable suph)  non-negative applying markov inequality: probability  )) log(? )  )     orem trivially extended case  [?, leading bound form: log(?  )     note unlike standard Rademacher complexity, defined), bound,  datasets equal size, imply based bounding, high probability bound mcdiarmid inequality (see. chapter] Rademacher complexity analysis), markov inequality. yields worse dependence confidence high probability bound/?, rar log/?).   depends sample terms reason quantity,  points evaluate hyposis, determining hyposis space hence, function:      , , zt0 ))?   satisfy stability property) (zi00  reason supremum hyposis space inputs. unlike case function   , , zt0 standard Rademacher complexity bound analysis, satisfies stability property. resolving wher worse dependence interesting open question. sample Complexity Auctions split-sample Growth present application measure complexity analysis sample complexity revenue optimal auctions. thoughout section assume revenue auction lies range]. results easily adapted range [?, -scaling equations, lead blow-ups sample complexity order extra   multiplicative factor. limits results bounded distributions values. however, shown], cap distribution values upper bound, case regular distributions, losing fraction revenue. apply results capped distribution. single bidder single item. case single bidder single item auction. setting, results auction ory] optimal auction belongs hyposis class {post reserve price ]}. consider, ERM rule, set case ties, favors reserve prices equal valuation wlog assume samples ordered increasing order. observe, set ERM rule subset post reserve price equal  reserve price values weakly dominated posting change subset  samples allocated increase revenue. space {post reserve price     size Thus split-sample growth ) this yields: log )  equivalently, sample complexity log/) multiple. regular bidders single item. case, results auction ory] optimal auction belongs space hyposes consisting price auctions reserve ]. ERM case ties favors reserve equals sample (assuming part tied set, outputs orwise), observe subset sample ERM subset pick reserve price equal values samples Thus )  this yields: log   )  equivalently, sample complexity log/ non. regular bidders, single item, price player specific reserves. case, results auction ory] optimal auction belongs space hyposes HSP consisting price auctions reserve , player ERM case ties favors reserve equals sample (assuming part tied set, outputs orwise), observe subset sample ERM subset pick reserve price equal values vti player sample choices player, choices reserves total.  )  yields: log ) ?hsp space dominant strategy truthful mechanisms, prophet inequalities (see]), suph?hsp) suph). thus: log )  non. irregular bidders single item. case results auction ory] optimal auction belongs space hyposes consisting virtual welfare maximizing auctions: For player pick monotone function  , allocate player highest non-negative virtual value, charging lowest bid win item. case, coarsen space auctions. particular, class-level auctions]. class, constrain functions  values discrete grid]. call class equivalent representation auctions player define vector thresholds       /. index player largest  allocate item player highest index (breaking ties lexicographically) charge minimum bid continue win. observe sample valuation vectors, weakly place thresholds values set Any threshold weakly dominated, change allocation. subset set size thresholds \\x0cplayer values player appears set thresholds player/ combinations thresholds player/ combinations thresholds players.  ) / yields: log )  moreover] that) )  picking, log get: log equivalently, sample complexity log ) ) items, bidders, additive valuations, grand bundle pricing. reserve price anonymous, reserve price output ERM subset sample size total values items buyers ) reserve price anonymous, buyer ERM pick total item values, )  sample complexity log/) items, bidders, additive valuations, item prices. reserve prices anonymous, reserve price item computed ERM subset sample size player values item.   )  reserve prices anonymous, reserve price item player player values item.  )  sample complexity log/) items, bidders, additive valuations, grand bundle pricing item pricing. erm combination values subset sample size product values classes (bundle item pricing). thus, anonymous pricing: )  non-anonymous pricing: ) ) sample complexity) log/) case single bidder, bundle pricing item pricing approximation truthful mechanism true distribution values, assuming values item drawn independently. case have log )  class truthful mechanisms. comparison]. applications analyzed], notion log/) pseudo-dimension, results lead sample complexity bounds log) simpler analysis removes extra log factor dependence. seminal work] gave recipe designing revenue maximizing auction auction settings private information players single number distribution number completely auctioneer. raises question auction designer formed prior distribution private information. recent work, starting], addresses question design optimal auctions access samples values bidders. refer reader] overview existing results literature. , give bounds sample complexity optimal auctions computational eﬃciency, recent work focused computationally eﬃcient learning bounds]. this work solely focuses sample complexity computational eﬃciency related]. work, tools supervised learning, pseudodimension] variant dimension real-valued functions), compression bounds] Rademacher complexity] bound sample complexity simple auction classes. our work introduces measure sample complexity, \\x0cwhich strengning Rademacher complexity analysis independent interest scope sample complexity optimal auctions. moreover, case auctions, measure greatly simplifies analysis sample complexity cases. particular, show general PAC learning settings, expected generalization error upper bounded Rademacher complexity class hyposes, rar class hyposes outcome running Expected Risk Minimization (erm) subset samples half size. number hyposes small, immediately yields small generalization error. refer growth rate set hyposes split-sample growth rate. this measure complexity restricted auction design relevant general statistical learning ory. 31st Conference Neural Information Processing Systems (nips 2017), Long beach, usa. show auction classes single-item auctions player-specific reserves, single item-level auctions multiple-item item pricing auctions additive buyers, splitsample growth rate easily bounded. argument boils Empirical Risk Minimization classes set parameters auctions equal player sample. simple counting argument bounds order prior work literature pseudo-dimension]. multi-item settings improvements sample complexity bound. split-sample growth rate similar spirit notion local Rademacher complexity], Rademacher complexity subset hyposes small empirical error. particular, proof based refinement classic analysis Rademacher complexity analysis generalization error (see. ]). however, bound structural, restricting set outcomes chosen ERM process subsample half size. moreover, note counting number outputs ERM connections counting argument made] context pricing mechanisms. however, essence argument restricted transductive settings sample ?features? advance fixed reby argument straightforward similar standard notions ?effective hyposis space? -dimension arguments. our measure sample complexity applicable general statistical learning ory framework applications auctions. convey high level intuition settings split-sample growth simplify sample complexity analysis, suppose output hyposis ERM uniquely defined constant number sample points. linear separators assume loss output ERM uniquely characterized choosing) points sample). means number hyposes subset size, split sample growth rate analysis immediately yields expected generalization error log), equivalently sample complexity learning hyposis class error log Preliminaries sample complexity optimal auctions. case items, bidders. each bidder function drawn independently distribution denote joint distribution. assume sample set   valuation vectors, Let denote class dominant strategy truthful single item auctions. auctions player incentive report true auction, independent players). moreover, phi phi (?) payment function mechanism, revenue mechanism valuation vector finally) expected revenue mechanism true distribution values Given sample size compute dominant strategy truthful mechanism that ) )   refer) expected generalization error. moreover, define sample complexity auction class: Definition (sample Complexity Auction class). (additive error) sample complexity auction class class distributions accuracy target defined smallest number samples(),  ) ) interested multiplcative error sample complexity.    notion]. assumes optimal revenue distribution lower bounded constant quantity, additive error implies multiplicative error. for instance, assumes player values bounded significant probability, implies lower bound revenue. such assumptions instance, made work]. focus additive error work. interested proving high probability guarantees. probability   ) , ,    Generalization Error split-sample Growth Rate turn general PAC learning framework, give generalization guarantees terms notion complexity hyposis space \\x0cwhich denote split-sample growth rate. consider arbitrary hyposis space arbitrary data space suppose set samples   drawn. distribution interested maximizing reward function  ], expectation distribution particular, denote)]. Expected Reward Maximization algorithm fixed tie-breaking rule. specifically ERM defined: arg ties broken based pre-defined manner. define notion split-sample hyposis space: denote set Definition (split-sample Hyposis space). for sample hyposis output ERM algorithm (with pre-defined tie-breaking rule), subset size.   ) Based split-sample hyposis space, define split-sample growth rate hyposis set size space largest size Definition (split-sample Growth rate). split-sample growth rate hyposis ERM process defined—  show generalization error upper bounded Rademacher complexity evaluated split-sample hyposis space union samples size Rademacher complexity, sample size hyposis space defined,   variable taking values}, equal probability.     independent binary random Lemma for hyposis space fixed ERM process, have:  )  independent samples size proof. let optimal hyposis distribution First-write left hand side, adding subtracting expected empirical reward   ?    maximizes empirical reward?   ? independent Thus suﬃces upper bound quantity equation. since)] fresh sample size have     since subset size , now, set  thus upper bound definition split-sample hyposis \\x0cspace  quantity taking supremum   ) )  , , zt0 Now observe, rename sample zt0 sample zt0  show change distribution. moreover, change quantity invariant swaps. finally, change sign quantity, , zt0 )). thus denote }, Rademacher variable, quantity equal, ,     holds) vector  expectation randomly drawn, equal probability. hence   ,    splitting supremma positive negative part observing expected quantities identical, get  2es   , denotes Rademacher complexity sample hyposis observe, orem strengning fact Rademacher complexity upper bounds generalization error, simply because:  ) Thus bound Rademacher complexity lemma bound generalization error. however, reverse true. finally, show main orem, shows split-sample hyposis space small size, immediately generalization bound, furr analyze Rademacher complexity orem (main orem). for hyposis space fixed ERM process, have: log(?  )  moreover, probability   )   log(? )) proof. applying massart lemma (see. ]) that:  log log(? ))   ) Combining Lemma yields part orem. finally, high probability statement observing random \\x0cvariable suph)  non-negative applying markov inequality: probability  )) log(? )  )     orem trivially extended case  [?, leading bound form: log(?  )     note unlike standard Rademacher complexity, defined), bound,  datasets equal size, imply based bounding, high probability bound mcdiarmid inequality (see. chapter] Rademacher complexity analysis), markov inequality. yields worse dependence confidence high probability bound/?, rar log/?).   depends sample terms reason quantity,  points evaluate hyposis, determining hyposis space hence, function:      , , zt0 ))?   satisfy stability property) (zi00  reason supremum hyposis space inputs. this unlike case function   , , zt0 standard Rademacher complexity bound analysis, satisfies stability property. resolving wher worse dependence interesting open question. Sample Complexity Auctions split-sample Growth present application measure complexity analysis sample complexity revenue optimal auctions. thoughout section assume revenue auction lies range]. results easily adapted range [?, -scaling equations, lead blow-ups sample complexity order extra   multiplicative factor. this limits results bounded distributions values. however, shown], cap distribution values upper bound, case regular distributions, losing fraction revenue. apply results capped distribution. single bidder single item. consider case single bidder single item auction. setting, results auction ory] optimal auction belongs hyposis class {post reserve price ]}. consider, ERM rule, set case ties, favors reserve prices equal valuation Wlog assume samples ordered increasing order. observe, set ERM rule subset post reserve price equal  any reserve price values weakly dominated posting change subset  which samples allocated increase revenue. thus space {post reserve price     size Thus split-sample growth ) This yields: log )  equivalently, sample complexity log/) Multiple. regular bidders single item. case, results auction ory] optimal auction belongs space hyposes consisting price auctions reserve ]. again ERM case ties favors reserve equals sample (assuming part tied set, outputs orwise), observe subset sample ERM subset pick reserve price equal values samples Thus )  This yields: log   )  equivalently, sample complexity log/ non. regular bidders, single item, price player specific reserves. case, results auction ory] optimal auction belongs space hyposes HSP consisting price auctions reserve , player again ERM case ties favors reserve equals sample (assuming part tied set, outputs orwise), observe subset sample ERM subset pick reserve price equal values vti player sample choices player, choices reserves total. thus )  this yields: log ) ?hsp space dominant strategy truthful mechanisms, prophet inequalities (see]), suph?hsp) suph). thus: log )  non. irregular bidders single item. case results auction ory] optimal auction belongs space hyposes consisting virtual welfare maximizing auctions: For player pick monotone function  , allocate player highest non-negative virtual value, charging lowest bid win item. case, coarsen space auctions. particular, class-level auctions]. class, constrain functions  values discrete grid]. call class equivalent representation auctions player define vector thresholds       /. index player largest  allocate item player highest index (breaking ties lexicographically) charge minimum bid continue win. observe sample valuation vectors, weakly place thresholds values set Any threshold weakly dominated, change allocation. thus subset set size thresholds \\x0cplayer values player appears set thresholds player/ combinations thresholds player/ combinations thresholds players. thus ) / this yields: log )  moreover] that) )  picking, log get: log equivalently, sample complexity log ) ) items, bidders, additive valuations, grand bundle pricing. reserve price anonymous, reserve price output ERM subset sample size total values items buyers ) reserve price anonymous, buyer ERM pick total item values, )  thus sample complexity log/) items, bidders, additive valuations, item prices. reserve prices anonymous, reserve price item computed ERM subset sample size player values item.   )  reserve prices anonymous, reserve price item player player values item.  )  thus sample complexity log/) items, bidders, additive valuations, grand bundle pricing item pricing. erm combination values subset sample size product values classes (bundle item pricing). thus, anonymous pricing: )  non-anonymous pricing: ) ) thus sample complexity) log/) case single bidder, bundle pricing item pricing approximation truthful mechanism true distribution values, assuming values item drawn independently. thus case have log )  class truthful mechanisms. comparison]. applications analyzed], notion log/) pseudo-dimension, results lead sample complexity bounds log) thus simpler analysis removes extra log factor dependence.',\n",
       " 'PP7170': 'cognitive neuroscience enjoying rapid increase extensive public brainimaging datasets. opens door large-scale statistical models. finding unified perspective data calls scalable automated solutions challenge: aggregate heterogeneous information brain function universal cognitive system relates mental operations/cognitive processes/psychological tasks brain networks? cast challenge machine-learning approach predict conditions statistical brain maps studies. this, leverage multi-task learning \\x0cand multi-scale dimension reduction learn low-dimensional representations brain images carry cognitive information robustly psychological stimuli. multi-dataset classification model achieves prediction performance large reference datasets, compared models cognitive-aware low-dimension representations; brings substantial performance boost analysis small datasets, introspected identify universal template cognitive concepts. due advent functional brain-imaging technologies, cognitive neuroscience accumulating quantitative maps neural activity responses specific tasks stimuli. rapidly increasing number neuroimaging studies publicly shared., human connectome project, HCP]), opening door applying large-scale statistical approaches]. yet, remains major challenge formally extract structured knowledge heterogeneous neuroscience repositories. stressed], aggregating knowledge cognitive neuroscience experiments intrinsically diﬃcult due diverse nature hyposes conclusions investigators. cognitive neuroscience experiments aim isolating brain effects underlying specific psychological processes: yield statistical maps brain activity measure neural responses carefully designed stimulus. unfortunately, neir regional brain responses experimental stimuli considered atomic: experimental stimulus recruits spatially distributed set brain regions], brain region observed react diverse stimuli. taking advantage resulting data richness build formal models describing psychological processes requires describe cognitive  inria, cea, universit? paris-saclay, 91191 Gif sur yvette, France univ. Grenoble alpes, inria, cnrs, Grenoble inp, ljk, 38000 grenoble, France 31st Conference Neural Information Processing Systems (nips 2017), Long beach, usa. conclusion common basis brain response experimental study design. uncovering atomic basis functions capture neural building blocks underlying cognitive processes refore primary goal neuroscience], propose data-driven approach. statistical approaches proposed tackle problem knowledge aggregation functional imaging. set approaches relies coordinate-based metaanalysis define robust neural correlates cognitive processes: extracted descriptions experiments based categories defined text mining] expert]? correlated brain coordinates related experiments. quantitative meta-analysis techniques provide summaries existing literature, hindered label noise experiment descriptions, weak information brain activation maps reduced coordinates]. second, recent set approaches models directly brain maps studies, eir focusing studies similar cognitive processes], tackling entire scope cognition]. decoding. predicting cognitive process brain activity, studies touching cognitive questions key goal cognitive neuroimaging principled answer reverse inference]. however, major roadblock scaling approach necessity label cognitive tasks studies rich consistent way., building ontology]. follow automated approach cast dataset accumulation multi-task learning problem: model trained decode simultaneously datasets, shared architecture. machine-learning techniques learn universal representations inputs give good performance multiple supervised problems]. successful, development deep neural network [see], sharing representations transferring knowledge dataset prediction model anor., computer-vision] audioprocessing]). popular approach simultaneously learn represent inputs datasets low-dimensional space predict outputs low-dimensional representatives. deep model architectures functional MRI thwarted signal-noise ratio recordings relative size datasets] compared computer vision text corpora. yet, show multi-dataset representation learning fertile ground identifying cognitive systems predictive power mental operations. contribution. introduce model architecture dedicated multi-dataset classification, performs successive linear dimension reductions input statistical brain images predicts psychological conditions learned low-dimensional representation images, linked cognitive processes. contrast previous ontology-based approaches, imposing structure cognitive experiments needed model: representation brain images learned raw set experimental conditions dataset. knowledge, work propose knowledge aggregation transfer learning functional MRI studies modest level supervision. demonstrate performance model openly accessible rich reference datasets brain-imaging domain. aspects architecture bring substantial increase out-sample accuracy compared models forgo learning cognitive-aware low-dimensional representation brain maps. model remains simple interpretable: collapsed collection classification maps, space low-dimensional representatives explored uncover set meaningful latent components. model: multi-dataset classification brain statistical images Our general goal extract integrate biological knowledge brain-imaging studies statistical learning framework. outline analyzing large repositories fmri experiments cast classification problem. here, success capturing brain-behavior relationships measured out-sample prediction accuracy. proposed model (figure solves range classification problems identical statistical estimation imposes shared latent structure single-dataset classification parameters. shared model parameters viewed chain dimension reductions. reduction layer leverages knowledge brain spatial regularities; learned resting-state data designed cap \\x0cture neural activity patterns coarseness levels. reduction layer projects data directions generally relevant cognitive-state prediction. combination reductions yields low-dimensional representatives affected noise subject variance Figure Model architecture: three-layer multi-dataset classification. layer (orange) learned data acquired cognitive experiments captures spatially coherent signal multiple scales, layer (blue) embeds representations space common datasets, conditions predicted (pink) multinomial models. high-dimensional samples: classification expected out-sample prediction performance.  Problem setting: predicting conditions brain activity multiple studies introduce notations terminology, formalize general prediction problem applicable task fmri dataset. single fmri study, subject performs experiments scanner. experiment, subjects presented set sensory stimuli., conditions) aim recruiting target set cognitive processes. fit first-level general linear model record obtain-score maps quantify importance condition explaining voxel. formally, statistical maps? ] study form sequence number voxels brain. observation labelled condition, effect captures single study typically features experiments repeated) statistical map condition subject, present conditions. studies, observed brain maps modeled generated unknown joint distribution brain activity cognitive conditions? ] variability trials subjects acts confounding noise. context, learn decoding model predicts condition brain activity measured subjects studies. inspired recent work], frame condition prediction problem estimation multinomial classification model. models estimate probability vector labeled condition This vector modeled function takes softmax form.  coordinate defined ) Fitting model weights minimizing cross-entropy true labels? ] respect), imposing parameter regularization. model, input image \\x0cclassified conditions presented study. restrict classification set conditions experiment empirical results study reproduced setting. challenge model parameter estimation. major inconvenience vanilla multinomial model lies ratio limited number samples provided typical fmri dataset overwhelming number model weights estimated. fitting model amounts estimating discriminative brain map. millions parameters conditions hcp), brain-imaging studies yield hundred observations refore thousands samples. makes hard approximate population parameters successful generalization, variance subjects high compared variance conditions. obstacle tackled major ways brainimaging: impose sparsity-priori structure model weights. alternatively, reduce dimension input data performing spatial clustering univariate feature selection anova. however, note that, hand, regularization strategies frequently incur costly computational budgets obtain interpretable weights] introduce artificial bias. hand, existing techniques developed fmri analysis dimension reduction lead distorted signal accuracy losses]. importantly, previous statistical approaches tuned identifying conditions task fmri data. refore propose dimension reduction estimated data tuned capture common hidden aspects shared statistical maps studies aggregate classification models share parameters.  Learning shared representation studies decoding fmri studies. ? ] union statistical maps datasets. write set studies, set conditions study total number conditions subset] index samples study For study estimate parameters classification problem defined above. adapting multi-task learning framework], constrain weights share common latent structure: namely, fix latent dimension enforce datasets Wd0) matrix shared datasets, (wd0 dataset-specific classification matrices dimensional input space. intuitively, ?consensus? projection matrix, project sample dataset lower dimensional representation¿ easy label correctly. latent dimension chosen larger case, regularization ensure factorization) useful., multi-dataset classification problem reduce separate multinomial regressions dataset. regularize model, apply Dropout] projected data representation. namely, successive training iterations, set random fraction reduced data features prevents-adaptation matrices (wd0 \\x0censures direction classifying dataset. formally, Dropout amounts sample binary diagonal matrices training, Bernouilli distributed coeﬃcients; datasets Wd0 estimated task classifying dropout-corrupted reduced data (mwe practice, matrices (wd0 learned jointly minimizing expected risk, objective sum single-study cross-entropies, averaged Dropout noise: min  log MWd0 Imposing common structure classification matrices natural classes distinguished share common neural organization brain maps correlated spatial structure, psychological conditions diffent datasets trigger shared cognitive primitives underlying human cognition]. design, aim learning matrix captures common aspects benefits generalization performance classifiers. estimated data, brain maps study enriched maps studies, conditions classified shared studies. doing, modeling approach transfer learning classification tasks. unfortunately, estimators provided solving) limited generalization performance remain small , 000) compared number parameters. address issue performing initial dimension reduction captures spatial structure brain maps.  Initial dimension reduction localized rest-fmri activity patterns projection expressed ignores signal structure statistical brain maps. acknowledging structure commonly acquired brain measurements reduce dimensionality data signal loss, possibly additional benefit denoising effect. recent studies] brain-imaging domain suggest fmri data acquired experiment-free studies dimension reduction. reason, introduce reduction dimension estimated statistical maps, resting-state data. formally, enforce We0 300),  We0  intuitively, multiplication matrix summarize spatial distribution brain maps, multiplying We0 estimated solving), find low-dimensional representations capture cognitive features. we0 reasonable size  15000): solving) estimate parameters generalization performance. defining matrix purpose paragaphs. resting-state decomposition. initial dimension reduction determines relative contribution statistical brain maps commonly interpreted neuroscience investigators functional networks. discover macroscopical brain networks performing sparse matrix factorization massive resting-state dataset provided HCP900 release]: decomposition technique] eﬃciently., order hours) number sparse spatial maps \\x0cthat decompose resting state signal good reconstruction performance. , finds sparse positive matrix loadings resting-state brain images Xrs approximated. set slightly overlapping networks voxel belongs networks. maximally preserve Euclidian distance performing reduction, perform orthogonal projection, amounts setting replacing), obtain reduced expected risk minimization problem, input dimension number dictionary components: min  log¿ We0 MWd0  Multiscale projection. selecting ?best? number brain networks ill-posed problem]: size functional networks prove relevant condition classification unknown investigator. address issue, propose reduce high-resolution data multi-scale fashion: initially extract sparse spatial dictionaries? , 512 components respectively. project statistical maps dictionaries, concatenate loadings, process analogous projecting overcomplete dictionary computer vision]. amounts define matrix concatenation¿ ? +512) ) With definition, reduced data¿ carry information network activations scales. such, makes classification maps learned model regular single-scale dictionary, yields interpretable classification maps. however, brings small improvement term predictive accuracy, compared simple dictionary size 512. furr discuss multi-scale decomposition Appendix.  Training stochastic gradient descent illustrated Figure model interpreted three-layer neural network linear activations read-out heads, specific dataset. model trained stochastic gradient descent, previously employed alternated training scheme]: cycle datasets select, iteration, mini-batch samples size datasets. perform gradient step weights Wd0 We0 updated, ors left unchanged. optimizer sees number samples dataset, expected stochastic gradient gradient), empirical risk decreases expectation find critical point) asymptotically. adam solver] ﬂavor stochastic gradient descent, faster convergence. computational cost. training model projected data¿ takes minutes conventional single CPU machine Intel Xeon.21ghz. initial step computing \\x0cdictionaries HCP900 resting-state (4tb data) records takes hours], transforming data studies projection takes hour. adding dataset subjects model performing joint training takes minutes. cost fitting first-level GLM dataset subject). experiments characterize behavior performance model large, publicly brain-imaging datasets. first, validate relevance elements model, perform ablation study. proves multi-scale spatial dimension reduction multi-dataset classification improves substancially classification performance, suggests proposed model captures interesting latent structure brain images. furr illustrate effect transfer learning, systematically varying number subjects single dataset: show multi-dataset learning helps mitigating decrease accuracy due smaller train size result analysing cognitive experiments small cohorts. finally, illustrate interpretability model show latent ?cognitivespace? explored uncover template brain maps related conditions datasets.  Datasets tools datasets. experimental study features publicly-available task fmri study. restingstate records HCP900 release] compute sparse dictionaries dimension reduction materialized succinctly describe conditions dataset refer reader original publications furr details.      hcp: gambling, working memory, motor, language, social relational tasks. 800 subjects. archi]: localizer protocol, motor, social relational task. subjects. brainomics]: localizer protocol. subjects. camcan]: audio-video task, frequency variation. 606 subjects. la5c consortium]: task-switching, balloon analog risk taking, stop-signal spatial working memory capacity tasks high-level tasks. 200 subjects. datasets target datasets, measure out-ofsample prediction performance. larger HCP dataset serves knowledge transfering dataset, boost performance considered multi-dataset model. register task time-series reference MNI space fitting general linear model (glm) computing maps (standardized-scoring) base condition manual design contrast involved. details pipeline-map extraction provided Appendix. tools. pytorch define train proposed models, nilearn] handle brain datasets, scikitlearn] design experimental pipelines. sparse brain decompositions computed HCP900 resting-state data. code reproducing experiments http://github.com/arthurmensch/cogspaces. model involves noncritical hyperparameters: batches size \\x0c256, set latent dimension 100 Dropout rate latent cognitive space perform slightly. multi-scale dictionary, 512 components, yields quantitative qualitative results finally, test accuracy measured half subjects dataset, removed training sets beforehand. benchmarks repeated times random split folds estimate variance performance.  Dimension reduction transfer improves test accuracy For benchmark studies, proposed model brings% extra test accuracy compared simple multinomial classification. furr quantify aspects model improve performance, perform ablation study: measure prediction accuracy models, simplest complete model Section experiments study effect initial dimension reduction regularization3 experiments measure performance proposed factored model, effect multi-dataset classification. http://pytorch.org/ Note 512-components dictionary yields comparable predictive accuracy. quantitatively, multi-scale approach beneficial dictionary components, 128) appendix quantitative validation multi-scale approach. for models, Dropout regularization parameter estimated nested cross-validation. % Full input dim. reduction Test accuracy% Transfer HCP Transfer datasets \\x0cdim. red. dropout Factored model dropout Archi% Brainomics CamCan LA5C Figure Ablation results. dimension reduction model relevant contribution. dropout regularization effective applied cognitive latent space. learning latent space transfer knowledge datasets. test accuracy transfer Transfer HCP Transfer datasets% Archi Train subjects% Brainomics% Camcan% Train size 100 200 302 Figure Learning curves single-dataset multi-dataset setting. estimating latent cognitive space multiple datasets studying small cohorts.       baseline -penalized multinomial classification, predict directly. multinomial classification projection dictionary. predicting experience Dropout noise projected data Factored model single-study case: solving) target study only. factored model two-study case: target study alongside \\x0chcp. factored model multi-study case: target study alongside studies. results summarized Figure average, dimension reduction introduced We0 beneficial generalization performance. datasets prediction brings furr increase performance, providing evidence transfer learning datasets. detail, comparison experiments confirms projecting brain images functional networks interest good strategy capture cognitive information]. note addition improving statistical properties estimators, projection reduces drastically computational complexity training full model. experiment measure impact regularization method learning furr latent projection. Dropout input space performs consistently regularization%); explained view], interpret input-dropout regularization natural model parametrization. experiment shows Dropout regularization powerful learning dimension reduction. solving problem). single study learning, observe significant improvement%) performance datasets. learning latent space projection toger dropout-based data augmentation space regularization strategy simple input-dropout regularization. finally, comparison experiments exhibits expected transfer effect. target studies, learning projection matrix We0 datasets leads accuracy gain%, consistent folds. datasets used, higher accuracy gain note gain increases smaller train size. jointly classifying images datasets brings extra information cognitive model, find representative brain maps target study. particular, conjecture large number subjects HCP helps modeling inter-subject noises. hand, observe negative transfer effect la5c, tasks dataset share cognitive aspects tasks datasets. encourages richer dataset repositories furr improvement. face=-10mm Audio calculation=46mm Latent cognitive Latent cognitive multi-scale spatial projection space (single) (multi-study) Latent cognitive Latent cognitive multi-scale spatial projection space (single) (multi-study) Figure Classification maps model specific higher level functions: focus FFA faces, left intraparietal suci calculations. figure latent space model explored unveil template brain statistical maps, corresponds bags conditions related \\x0cacross color-coded datasets.  Transfer learning effective small datasets furr demonstrate benefits multi-dataset model, vary size target datasets (archi, Brainomics camcan) compare performance single-study model model aggregates archi, brainomics, CamCan HCP studies. figure shows effect transfer learning increases reduce training size target dataset. suggests learned data embedding We0 capture universal cognitive information, learned data sources. consequence, aggregating larger study mitigate small number training samples target dataset. subjects, gain accuracy due transfer% archi% brainomics% camcan. multistudy learning proves classify conditions studies ten subjects, common neuroimaging.  Introspecting classification maps prediction time, multi-dataset model collapsed multinomial model dataset. dataset classified matrix We0 Wd0 similar linear models classically decoding, model weights condition represented brain map. figure shows maps digit computation face viewing, Archi dataset. models ablation study compared. hard assess intrinsic quality maps, introduction projection layer multistudy problem formulation (here, appending HCP dataset) yields maps weight high-level functional regions specific task: face viewing, FFA stands compared primary visual cortices; calculations, weights intraparietal sulci left lateralized, reported symbolic number processing].  Exploring latent space Within model, classification performed-dimensional space datasets, learned training. furr show space captures cognitive information, extract template brain images general cognitive concepts. fitting model archi, brainomics, CamCan HCP studies, extract representative vectors-means clustering projected data centroids clusters. centroid brain image lies span , backward model obtain representative delineated spatial regions. forward, compute classification probability vectors¿ Wd0¿ study toger, probability vectors give indication cognitive functions captures. figure represents template images, probability vectors, shown word clouds. obtain interpretable pairs brain image/cognitive concepts. pairs capture datasets clusters experiment conditions similar brain representations. discussion compare model previously proposed formulation brain image classification. show model differs convex multi-task learning, stress importance dropout. task fmri classification. model related previous semi-supervised classification model] performs multinomial classification conditions low-dimensional space: dimension reduction propose equivalent projection approach differs aspects. first, replace initial semi-supervised dimension reduction unsupervised analysis resting-state, tractable approach shown conservative cognitive signals. second, introduce additional cognitive-aware projection We0 learned multiple studies. substancially improves out-sample prediction performance, small datasets, uncover cognitive-aware latent space, shown experiments. convex multi-task learning. due Dropout regularization fact allowed larger formulation differs classical approach] multi-task problem, estimate we0 [w10 wd0  solving convex empirical risk minimization problem trace-norm penalization, encourages low-rank. tested formulation, perform explicit factorization formulation Dropout regularization. trace-norm regularized regression furr drawback slower train, typically operates full gradients. fista]. contrast, non-convex explicit factorization model easily amenable large-scale stochastic optimization focus. importance dropout. dropout regularization crucial model. dropout, single-study case solving factored problem) yields solution worse term empirical risk solving simple multinomial problem¿ finds global minimizer). yet, Figure shows model enriched latent space (red) performance test accuracy simple model (orange), Dropout noise applied latent-space representation input data. dropout promising regularizing fmri models.   conclusion proposed characterized cognitive neuroimaging modeling scheme blends latent factor discovery transfer learning. applied cognitive studies jointly requiring explicit correspondences cognitive tasks. model helps identifying fundamental building blocks underlying diversity cognitive processes human mind realize. produces basis cognitive processes generalization power validated quantitatively, extracts representations brain activity grounds transfer knowledge existing fmri repositories newly acquired task data. captured cognitive representations improve provide model growing number studies cognitive conditions. acknowledgments This project received funding European union Horizon 2020 Framework Programme Research Innovation grant agreement 720270 (human Brain Project sga1). julien Mairal supported ERC grant SOLARIS 714381) grant ANR (macaron project anr-ce23-0003). olivier Grisel helpful insights. Cognitive neuroscience enjoying rapid increase extensive public brainimaging datasets. opens door large-scale statistical models. finding unified perspective data calls scalable automated solutions challenge: aggregate heterogeneous information brain function universal cognitive system relates mental operations/cognitive processes/psychological tasks brain networks? cast challenge machine-learning approach predict conditions statistical brain maps studies. for this, leverage multi-task learning \\x0cand multi-scale dimension reduction learn low-dimensional representations brain images carry cognitive information robustly psychological stimuli. our multi-dataset classification model achieves prediction performance large reference datasets, compared models cognitive-aware low-dimension representations; brings substantial performance boost analysis small datasets, introspected identify universal template cognitive concepts. due advent functional brain-imaging technologies, cognitive neuroscience accumulating quantitative maps neural activity responses specific tasks stimuli. rapidly increasing number neuroimaging studies publicly shared., human connectome project, HCP]), opening door applying large-scale statistical approaches]. yet, remains major challenge formally extract structured knowledge heterogeneous neuroscience repositories. stressed], aggregating knowledge cognitive neuroscience experiments intrinsically diﬃcult due diverse nature hyposes conclusions investigators. cognitive neuroscience experiments aim isolating brain effects underlying specific psychological processes: yield statistical maps brain activity measure neural responses carefully designed stimulus. unfortunately, neir regional brain responses experimental stimuli considered atomic: experimental stimulus recruits spatially distributed set brain regions], brain region observed react diverse stimuli. taking advantage resulting data richness build formal models describing psychological processes requires describe cognitive  inria, cea, universit? paris-saclay, 91191 Gif sur yvette, France univ. Grenoble alpes, inria, cnrs, Grenoble inp, ljk, 38000 grenoble, France 31st Conference Neural Information Processing Systems (nips 2017), Long beach, usa. conclusion common basis brain response experimental study design. uncovering atomic basis functions capture neural building blocks underlying cognitive processes refore primary goal neuroscience], propose data-driven approach. several statistical approaches proposed tackle problem knowledge aggregation functional imaging. set approaches relies coordinate-based metaanalysis define robust neural correlates cognitive processes: extracted descriptions experiments based categories defined text mining] expert]? correlated brain coordinates related experiments. although quantitative meta-analysis techniques provide summaries existing literature, hindered label noise experiment descriptions, weak information brain activation maps reduced coordinates]. second, recent set approaches models directly brain maps studies, eir focusing studies similar cognitive processes], tackling entire scope cognition]. decoding. predicting cognitive process brain activity, studies touching cognitive questions key goal cognitive neuroimaging principled answer reverse inference]. however, major roadblock scaling approach necessity label cognitive tasks studies rich consistent way., building ontology]. follow automated approach cast dataset accumulation multi-task learning problem: model trained decode simultaneously datasets, shared architecture. machine-learning techniques learn universal representations inputs give good performance multiple supervised problems]. successful, development deep neural network [see], sharing representations transferring knowledge dataset prediction model anor., computer-vision] audioprocessing]). popular approach simultaneously learn represent inputs datasets low-dimensional space predict outputs low-dimensional representatives. using deep model architectures functional MRI thwarted signal-noise ratio recordings relative size datasets] compared computer vision text corpora. yet, show multi-dataset representation learning fertile ground identifying cognitive systems predictive power mental operations. contribution. introduce model architecture dedicated multi-dataset classification, performs successive linear dimension reductions input statistical brain images predicts psychological conditions learned low-dimensional representation images, linked cognitive processes. contrast previous ontology-based approaches, imposing structure cognitive experiments needed model: representation brain images learned raw set experimental conditions dataset. knowledge, work propose knowledge aggregation transfer learning functional MRI studies modest level supervision. demonstrate performance model openly accessible rich reference datasets brain-imaging domain. aspects architecture bring substantial increase out-sample accuracy compared models forgo learning cognitive-aware low-dimensional representation brain maps. our model remains simple interpretable: collapsed collection classification maps, space low-dimensional representatives explored uncover set meaningful latent components. model: multi-dataset classification brain statistical images Our general goal extract integrate biological knowledge brain-imaging studies statistical learning framework. outline analyzing large repositories fmri experiments cast classification problem. here, success capturing brain-behavior relationships measured out-sample prediction accuracy. proposed model (figure solves range classification problems identical statistical estimation imposes shared latent structure single-dataset classification parameters. shared model parameters viewed chain dimension reductions. reduction layer leverages knowledge brain spatial regularities; learned resting-state data designed cap \\x0cture neural activity patterns coarseness levels. reduction layer projects data directions generally relevant cognitive-state prediction. combination reductions yields low-dimensional representatives affected noise subject variance Figure Model architecture: three-layer multi-dataset classification. layer (orange) learned data acquired cognitive experiments captures spatially coherent signal multiple scales, layer (blue) embeds representations space common datasets, conditions predicted (pink) multinomial models. high-dimensional samples: classification expected out-sample prediction performance.  Problem setting: predicting conditions brain activity multiple studies introduce notations terminology, formalize general prediction problem applicable task fmri dataset. single fmri study, subject performs experiments scanner. during experiment, subjects presented set sensory stimuli., conditions) aim recruiting target set cognitive processes. fit first-level general linear model record obtain-score maps quantify importance condition explaining voxel. formally, statistical maps? ] study form sequence number voxels brain. each observation labelled condition, effect captures single study typically features experiments repeated) statistical map condition subject, present conditions. across studies, observed brain maps modeled generated unknown joint distribution brain activity cognitive conditions? ] variability trials subjects acts confounding noise. context, learn decoding model predicts condition brain activity measured subjects studies. inspired recent work], frame condition prediction problem estimation multinomial classification model. our models estimate probability vector labeled condition This vector modeled function takes softmax form. for coordinate defined ) Fitting model weights minimizing cross-entropy true labels? ] respect), imposing parameter regularization. model, input image \\x0cclassified conditions presented study. restrict classification set conditions experiment empirical results study reproduced setting. challenge model parameter estimation. major inconvenience vanilla multinomial model lies ratio limited number samples provided typical fmri dataset overwhelming number model weights estimated. fitting model amounts estimating discriminative brain map. millions parameters conditions hcp), brain-imaging studies yield hundred observations refore thousands samples. this makes hard approximate population parameters successful generalization, variance subjects high compared variance conditions. obstacle tackled major ways brainimaging: impose sparsity-priori structure model weights. alternatively, reduce dimension input data performing spatial clustering univariate feature selection anova. however, note that, hand, regularization strategies frequently incur costly computational budgets obtain interpretable weights] introduce artificial bias. hand, existing techniques developed fmri analysis dimension reduction lead distorted signal accuracy losses]. most importantly, previous statistical approaches tuned identifying conditions task fmri data. refore propose dimension reduction estimated data tuned capture common hidden aspects shared statistical maps studies aggregate classification models share parameters.  Learning shared representation studies decoding fmri studies. ? ] union statistical maps datasets. write set studies, set conditions study total number conditions subset] index samples study For study estimate parameters classification problem defined above. adapting multi-task learning framework], constrain weights share common latent structure: namely, fix latent dimension enforce datasets Wd0) matrix shared datasets, (wd0 dataset-specific classification matrices dimensional input space. intuitively, ?consensus? projection matrix, project sample dataset lower dimensional representation¿ easy label correctly. latent dimension chosen larger case, regularization ensure factorization) useful., multi-dataset classification problem reduce separate multinomial regressions dataset. regularize model, apply Dropout] projected data representation. namely, successive training iterations, set random fraction reduced data features this prevents-adaptation matrices (wd0 \\x0censures direction classifying dataset. formally, Dropout amounts sample binary diagonal matrices training, Bernouilli distributed coeﬃcients; datasets Wd0 estimated task classifying dropout-corrupted reduced data (mwe practice, matrices (wd0 learned jointly minimizing expected risk, objective sum single-study cross-entropies, averaged Dropout noise: min  log MWd0 Imposing common structure classification matrices natural classes distinguished share common neural organization brain maps correlated spatial structure, psychological conditions diffent datasets trigger shared cognitive primitives underlying human cognition]. with design, aim learning matrix captures common aspects benefits generalization performance classifiers. estimated data, brain maps study enriched maps studies, conditions classified shared studies. doing, modeling approach transfer learning classification tasks. unfortunately, estimators provided solving) limited generalization performance remain small , 000) compared number parameters. address issue performing initial dimension reduction captures spatial structure brain maps.  Initial dimension reduction localized rest-fmri activity patterns projection expressed ignores signal structure statistical brain maps. acknowledging structure commonly acquired brain measurements reduce dimensionality data signal loss, possibly additional benefit denoising effect. several recent studies] brain-imaging domain suggest fmri data acquired experiment-free studies dimension reduction. for reason, introduce reduction dimension estimated statistical maps, resting-state data. formally, enforce We0 300),  We0  intuitively, multiplication matrix summarize spatial distribution brain maps, multiplying We0 estimated solving), find low-dimensional representations capture cognitive features. we0 reasonable size  15000): solving) estimate parameters generalization performance. defining matrix purpose paragaphs. resting-state decomposition. initial dimension reduction determines relative contribution statistical brain maps commonly interpreted neuroscience investigators functional networks. discover macroscopical brain networks performing sparse matrix factorization massive resting-state dataset provided HCP900 release]: decomposition technique] eﬃciently., order hours) number sparse spatial maps \\x0cthat decompose resting state signal good reconstruction performance. that, finds sparse positive matrix loadings resting-state brain images Xrs approximated. set slightly overlapping networks voxel belongs networks. maximally preserve Euclidian distance performing reduction, perform orthogonal projection, amounts setting replacing), obtain reduced expected risk minimization problem, input dimension number dictionary components: min  log¿ We0 MWd0  Multiscale projection. selecting ?best? number brain networks ill-posed problem]: size functional networks prove relevant condition classification unknown investigator. address issue, propose reduce high-resolution data multi-scale fashion: initially extract sparse spatial dictionaries? , 512 components respectively. project statistical maps dictionaries, concatenate loadings, process analogous projecting overcomplete dictionary computer vision]. this amounts define matrix concatenation¿ ? +512) ) With definition, reduced data¿ carry information network activations scales. such, makes classification maps learned model regular single-scale dictionary, yields interpretable classification maps. however, brings small improvement term predictive accuracy, compared simple dictionary size 512. furr discuss multi-scale decomposition Appendix.  Training stochastic gradient descent illustrated Figure model interpreted three-layer neural network linear activations read-out heads, specific dataset. model trained stochastic gradient descent, previously employed alternated training scheme]: cycle datasets select, iteration, mini-batch samples size datasets. perform gradient step weights Wd0 We0 updated, ors left unchanged. optimizer sees number samples dataset, expected stochastic gradient gradient), empirical risk decreases expectation find critical point) asymptotically. Adam solver] ﬂavor stochastic gradient descent, faster convergence. computational cost. training model projected data¿ takes minutes conventional single CPU machine Intel Xeon.21ghz. initial step computing \\x0cdictionaries HCP900 resting-state (4tb data) records takes hours], transforming data studies projection takes hour. adding dataset subjects model performing joint training takes minutes. this cost fitting first-level GLM dataset subject). Experiments characterize behavior performance model large, publicly brain-imaging datasets. first, validate relevance elements model, perform ablation study. proves multi-scale spatial dimension reduction multi-dataset classification improves substancially classification performance, suggests proposed model captures interesting latent structure brain images. furr illustrate effect transfer learning, systematically varying number subjects single dataset: show multi-dataset learning helps mitigating decrease accuracy due smaller train size result analysing cognitive experiments small cohorts. finally, illustrate interpretability model show latent ?cognitivespace? explored uncover template brain maps related conditions datasets.  Datasets tools datasets. our experimental study features publicly-available task fmri study. restingstate records HCP900 release] compute sparse dictionaries dimension reduction materialized succinctly describe conditions dataset refer reader original publications furr details.      hcp: gambling, working memory, motor, language, social relational tasks. 800 subjects. archi]: localizer protocol, motor, social relational task. subjects. brainomics]: localizer protocol. subjects. camcan]: audio-video task, frequency variation. 606 subjects. la5c consortium]: task-switching, balloon analog risk taking, stop-signal spatial working memory capacity tasks high-level tasks. 200 subjects. datasets target datasets, measure out-ofsample prediction performance. larger HCP dataset serves knowledge transfering dataset, boost performance considered multi-dataset model. register task time-series reference MNI space fitting general linear model (glm) computing maps (standardized-scoring) base condition manual design contrast involved. more details pipeline-map extraction provided Appendix. tools. pytorch define train proposed models, nilearn] handle brain datasets, scikitlearn] design experimental pipelines. sparse brain decompositions computed HCP900 resting-state data. code reproducing experiments http://github.com/arthurmensch/cogspaces. our model involves noncritical hyperparameters: batches size \\x0c256, set latent dimension 100 Dropout rate latent cognitive space perform slightly. multi-scale dictionary, 512 components, yields quantitative qualitative results finally, test accuracy measured half subjects dataset, removed training sets beforehand. benchmarks repeated times random split folds estimate variance performance.  Dimension reduction transfer improves test accuracy For benchmark studies, proposed model brings% extra test accuracy compared simple multinomial classification. furr quantify aspects model improve performance, perform ablation study: measure prediction accuracy models, simplest complete model Section experiments study effect initial dimension reduction regularization3 experiments measure performance proposed factored model, effect multi-dataset classification. http://pytorch.org/ Note 512-components dictionary yields comparable predictive accuracy. quantitatively, multi-scale approach beneficial dictionary components, 128) Appendix quantitative validation multi-scale approach. For models, Dropout regularization parameter estimated nested cross-validation. % Full input dim. reduction Test accuracy% Transfer HCP Transfer datasets \\x0cdim. red. dropout Factored model dropout Archi% Brainomics CamCan LA5C Figure Ablation results. each dimension reduction model relevant contribution. dropout regularization effective applied cognitive latent space. learning latent space transfer knowledge datasets. test accuracy transfer Transfer HCP Transfer datasets% Archi Train subjects% Brainomics% Camcan% Train size 100 200 302 Figure Learning curves single-dataset multi-dataset setting. estimating latent cognitive space multiple datasets studying small cohorts.       baseline -penalized multinomial classification, predict directly. multinomial classification projection dictionary. predicting same experience Dropout noise projected data Factored model single-study case: solving) target study only. factored model two-study case: target study alongside \\x0chcp. factored model multi-study case: target study alongside studies. results summarized Figure average, dimension reduction introduced We0 beneficial generalization performance. using datasets prediction brings furr increase performance, providing evidence transfer learning datasets. detail, comparison experiments confirms projecting brain images functional networks interest good strategy capture cognitive information]. note addition improving statistical properties estimators, projection reduces drastically computational complexity training full model. experiment measure impact regularization method learning furr latent projection. using Dropout input space performs consistently regularization%); explained view], interpret input-dropout regularization natural model parametrization. experiment shows Dropout regularization powerful learning dimension reduction. solving problem). even single study learning, observe significant improvement%) performance datasets. learning latent space projection toger dropout-based data augmentation space regularization strategy simple input-dropout regularization. finally, comparison experiments exhibits expected transfer effect. target studies, learning projection matrix We0 datasets leads accuracy gain%, consistent folds. datasets used, higher accuracy gain note gain increases smaller train size. jointly classifying images datasets brings extra information cognitive model, find representative brain maps target study. particular, conjecture large number subjects HCP helps modeling inter-subject noises. hand, observe negative transfer effect la5c, tasks dataset share cognitive aspects tasks datasets. this encourages richer dataset repositories furr improvement. Face=-10mm Audio calculation=46mm Latent cognitive Latent cognitive multi-scale spatial projection space (single) (multi-study) Latent cognitive Latent cognitive multi-scale spatial projection space (single) (multi-study) Figure Classification maps model specific higher level functions: focus FFA faces, left intraparietal suci calculations. figure latent space model explored unveil template brain statistical maps, corresponds bags conditions related \\x0cacross color-coded datasets.  Transfer learning effective small datasets furr demonstrate benefits multi-dataset model, vary size target datasets (archi, Brainomics camcan) compare performance single-study model model aggregates archi, brainomics, CamCan HCP studies. figure shows effect transfer learning increases reduce training size target dataset. this suggests learned data embedding We0 capture universal cognitive information, learned data sources. consequence, aggregating larger study mitigate small number training samples target dataset. with subjects, gain accuracy due transfer% archi% brainomics% camcan. multistudy learning proves classify conditions studies ten subjects, common neuroimaging.  Introspecting classification maps prediction time, multi-dataset model collapsed multinomial model dataset. each dataset classified matrix We0 Wd0 similar linear models classically decoding, model weights condition represented brain map. figure shows maps digit computation face viewing, Archi dataset. models ablation study compared. although hard assess intrinsic quality maps, introduction projection layer multistudy problem formulation (here, appending HCP dataset) yields maps weight high-level functional regions specific task: face viewing, FFA stands compared primary visual cortices; calculations, weights intraparietal sulci left lateralized, reported symbolic number processing].  Exploring latent space Within model, classification performed-dimensional space datasets, learned training. furr show space captures cognitive information, extract template brain images general cognitive concepts. fitting model archi, brainomics, CamCan HCP studies, extract representative vectors-means clustering projected data centroids clusters. each centroid brain image lies span , backward model obtain representative delineated spatial regions. going forward, compute classification probability vectors¿ Wd0¿ study toger, probability vectors give indication cognitive functions captures. figure represents template images, probability vectors, shown word clouds. obtain interpretable pairs brain image/cognitive concepts. pairs capture datasets clusters experiment conditions similar brain representations. discussion compare model previously proposed formulation brain image classification. show model differs convex multi-task learning, stress importance dropout. task fmri classification. our model related previous semi-supervised classification model] performs multinomial classification conditions low-dimensional space: dimension reduction propose equivalent projection our approach differs aspects. first, replace initial semi-supervised dimension reduction unsupervised analysis resting-state, tractable approach shown conservative cognitive signals. second, introduce additional cognitive-aware projection We0 learned multiple studies. substancially improves out-sample prediction performance, small datasets, uncover cognitive-aware latent space, shown experiments. convex multi-task learning. due Dropout regularization fact allowed larger formulation differs classical approach] multi-task problem, estimate We0 [w10 Wd0  solving convex empirical risk minimization problem trace-norm penalization, encourages low-rank. tested formulation, perform explicit factorization formulation Dropout regularization. trace-norm regularized regression furr drawback slower train, typically operates full gradients. FISTA]. contrast, non-convex explicit factorization model easily amenable large-scale stochastic optimization focus. importance dropout. Dropout regularization crucial model. without dropout, single-study case solving factored problem) yields solution worse term empirical risk solving simple multinomial problem¿ finds global minimizer). yet, Figure shows model enriched latent space (red) performance test accuracy simple model (orange), Dropout noise applied latent-space representation input data. dropout promising regularizing fmri models.   Conclusion proposed characterized cognitive neuroimaging modeling scheme blends latent factor discovery transfer learning. applied cognitive studies jointly requiring explicit correspondences cognitive tasks. model helps identifying fundamental building blocks underlying diversity cognitive processes human mind realize. produces basis cognitive processes generalization power validated quantitatively, extracts representations brain activity grounds transfer knowledge existing fmri repositories newly acquired task data. captured cognitive representations improve provide model growing number studies cognitive conditions. Acknowledgments This project received funding European union Horizon 2020 Framework Programme Research Innovation grant agreement 720270 (human Brain Project sga1). julien Mairal supported ERC grant SOLARIS 714381) grant ANR (macaron project anr-ce23-0003). Olivier Grisel helpful insights.',\n",
       " 'PP7178': 'interested problem manipulating natural images controlling attributes interest. example, photograph face person gender, age, expression, generate realistic version person older happier, image hypotical twin opposite gender. task related problem unsupervised domain transfer recently received lot interest], case study conditional generative models applications automatic image edition. key challenge trans \\x0cformations ill-defined training unsupervised: training set images annotated attributes interest, transformation: cases ?gender swapping? above, pairs images representing person male female. cases, collecting examples requires costly annotation process, taking pictures person glasses. approach relies encoder-decoder architecture where, input image attributes encoder maps latent representation decoder trained reconstruct). inference time, test image encoded latent space, user chooses attribute values fed decoder. binary attribute values train time, attribute considered continuous variable inference control perceived final image. call architecture Fader networks, analogy sliders audio mixing console, user choose attribute incorporate. facebook Research Sorbonne universit, UPMC Univ Paris, UMR 7606, LIP6 lscp, ens, ehess, cnrs, PSL Research university, INRIA 31st Conference Neural Information Processing Systems (nips 2017), Long beach, usa. figure Interpolation attributes (zoom resolution). line shows reconstructions face attribute values, attribute controlled continuous variable. make person older younger, man manly imagine female version. left images originals. fundamental feature approach constrain latent space invariant attributes interest. concretely, means distribution images latent representations identical attribute values. invariance obtained procedure similar domain-adversarial training (see]). process, classifier learns predict attributes latent representation training encoder-decoder trained based objectives time. objective reconstruction error decoder., latent representation information reconstruction input. objective consists fooling attribute classifier., latent representation prevent predicting correct attribute values. model, achieving invariance means filter out, hide, properties image related attributes interest. single latent representation corresponds images share common structure attribute values. reconstruction objective forces decoder attribute values choose, latent representation, intended image. motivation learn disentangled latent space explicit control attributes interest, supervision intended result modifying attribute values. similar motivation, approaches tested tasks], related image-image translation problems], specific applications creation parametrized avatars addition reconstruction loss, vast majority works]. rely adversarial training pixel space, compares training images generated intentional change attributes genuine images target attribute values. approach adversarial training latent space output, adversarial training aims learning invariance attributes. assumption underlying work high fidelity input image conﬂicting invariance criterion, criterion forces hallucinated image match images training set. consequence principle, approach results simpler training pipelines based adversarial training pixel space, readily amenable controlling multiple attributes, adding output variables discriminator latent space. shown Figure test images CelebA dataset], model make subtle portraits end suﬃcient alter perceived attributes preserving natural aspect image identity person. experiments show model outperforms previous methods based adversarial training decoders? output] terms reconstruction loss generation quality measured human subjects. disentanglement approach competitor widespread adversarial losses decoder output tasks. remainder paper, discuss details related work Section present training procedure Section describing network architecture implementation Section experimental results shown Section related work substantial literature attribute-based and conditional image generation split terms required supervision, levels. extreme fully supervised approaches developed model transformations, examples form (input, transformation, result transformation). case, model learn desired transformation. setting previously explored learn aﬃne transformations], rotations], lighting variations] video game animations]. methods developed works rely supervised setting, applied setup. extreme supervision spectrum lie fully unsupervised methods aim learning deep neural networks disentangle factors variations data, specification attributes. methods InfoGAN], predictability minimization framework proposed]. neural photo editor] disentangles factors variations natural images image edition. ] introduced beta-vae, modification variational autoencoder (vae) framework learn latent factorized representations completely unsupervised manner. setting considerably harder consider, general, diﬃcult methods automatically discover high-level concepts gender age. work lies related information]. previous settings. methods developed unsupervised domain transfer] applied case: domains images ?drawings? ?photograph?, map image domain supervision; case, domain correspond attribute value. mappings trained adversarial training pixel space mentioned introduction, separate encoders and decoders domain, scale multiple attributes. line work specifically problem modifying attributes, Invertible conditional GAN] trains GAN conditioned attribute values, step learns map input images latent space gan, invertible gans. baseline experiments. antipov. ] pre-trained face recognition system conditional GAN learn latent space, focuses age attribute. attribute-image approach] variational auto-encoder disentangles foreground background generate images attribute values only. conditional generation performed inferring latent state correct attributes changing attributes. additionally, work related work learning invariant latent spaces adversarial training domain adaptation], fair classification] robust inference]. training criterion enforcing invariance similar works, difference end-goal works filter nuisance variables sensitive information. case, learn generative models, invariance means force decoder attribute information reconstruction. finally, application automatically modifying faces attributes, feature interpolation approach] presents means generate alterations images based attributes pre-trained network imagenet. approach interesting application perspective, inference costly relies pre-trained models, naturally incorporate factors attributes foreseen pre-training. fader Networks Let image domain set attributes images case people faces typical attributes glasses glasses, man/woman, young/old. simplicity, case attributes binary, approach extended categorical attributes. setting number attributes. training set )}, pairs (image, attribute  ). end goal learn model generate, attribute vector version input image attribute values correspond encoder-decoder architecture Our model, Figure based encoder-decoder architecture domain-adversarial training latent space. encoder?enc convolutional neural network parameters ?enc maps input image -dimensional latent representation?enc). decoder?dec deconvolutional network parameters ?dec produces version input image latent representation?enc) attribute vector context clear, simply denote?dec?enc precise architectures neural networks Section auto-encoding loss architecture classical squared error (mse) measures quality reconstruction training input true attribute vector lae (?enc ?dec?enc), dec exact choice reconstruction loss fundamental approach, adversarial losses PatchGAN] addition MSE stage obtain textures sharper images]. absolute squared error ensure reconstruction matches original image. ideally, modifying), generate images perceived attributes, similar aspect. however, additional constraints, decoder learns ignore attributes, modifying test time effect. learning attribute-invariant latent representations avoid behavior, approach learn latent representations invariant respect attributes. invariance, versions object attribute values, instance images person glasses, latent representations same. invariance satisfied, decoder attribute reconstruct original image. training set versions image, constraint trivially added loss. propose incorporate constraint adversarial training latent space. idea inspired work predictability minimization] adversarial training domain adaptation] objective learn invariant latent representation adversarial formulation learning objective. end, additional neural network called discriminator trained identify true attributes training pair). invariance obtained learning encoder discriminator unable identify attributes. gans], corresponds two-player game discriminator aims maximizing ability identify attributes, aims preventing good discriminator. exact structure discriminator Section discriminator objective discriminator outputs probabilities attribute vector?dis)), ?dis discriminator parameters. subscript refer attribute, log?dis)) log?dis)). objective discriminator predict attributes input image latent representation, loss depends current state encoder written: Ldis (?dis —?enc log?dis?enc Adversarial objective objective encoder compute latent representation optimizes objectives. first, decoder reconstruct) time discriminator predict). mistake made discriminator predicts attribute Given discriminator parameters, complete loss encoder-decoder architecture(?enc ?dec —?dis ?enc),  log?dis ?enc) dec controls trade-off quality reconstruction invariance latent representations. large values restrain amount information contained), result blurry images, low values limit decoder dependency latent code result poor effects altering attributes. figure Main architecture. (image, attribute) pair, input. encoder maps latent representation discriminator trained predict encoder trained make impossible discriminator predict only. decoder reconstruct). test time, discriminator discarded model generate versions fed attribute values. learning algorithm overall, current state encoder, optimal discriminator parameters satisfy ?dis (?enc argmin?dis Ldis (?dis —?enc ignore problems related multiple (and local) minima, objective function   ?enc ?dec argmin(?enc ?dec —?dis (?enc ?enc ,?dec practice, unreasonable solve ?dis (?enc update ?enc practice adversarial training deep networks, stochastic gradient updates parameters, consid? ering current ?dis approximation ?dis (?enc training), denote Ldis ?dis ?enc auto-encoder loss restricted, ?enc ?dec ?dis) discriminator loss. update time current parameters ?dis ?enc) ?dec training) ?dis  ?dis Ldis ?dis ?enc) [?enc ?dec [?enc ?dec  ?enc ,?dec ?enc ?dec ?dis) ?dis details training models section. implementation adapt architecture network]. convolutionbatchnorm-relu layer filters. convolutions kernel size stride padding layer encoder divides size input leaky-relus slope encoder, simple ReLUs decoder. encoder consists layers: C16 c32 c64 c128 c256 c512 c512 Input images size 256 256. result, latent representation image consists 512 feature maps size  experiments, layers gave similar results, layers significantly decreased performance, feature maps latent state. provide decoder image attributes, append latent code layer input decoder, latent code image concatenation one-hot vectors representing Model Real Image IcGAN IcGAN Swap FadNet FadNet Swap Mouth Naturalness Smile Glasses Mouth Accuracy Smile Glasses Table Perceptual evaluation naturalness swap accuracy model. naturalness score percentage images labeled ?real? human evaluators question image real photograph fake generated graphics engine??. accuracy score classification accuracy human evaluators values attribute. values attributes (binary attributes represented]). append latent code additional constant input channels convolutions decoder. denoting number attributes, (hence code size), decoder symmetric encoder, transposed convolutions-sampling: c512 c512 c256 c128 c64 c32 c16 discriminator C512 layer fullyconnected neural network layers size 512 repsectively. dropout found beneficial add dropout discriminator. hyposized dropout helped discriminator rely wider set features order infer current attributes, improving stabilizing accuracy, giving feedback encoder. set dropout rate experiments. ], add dropout layers decoder, experiments, turned significantly decrease performance. discriminator cost scheduling Similarly], variable weight discriminator loss coeﬃcient initially set model trained normal auto-encoder. linearly increased.0001 500, 000 iterations slowly encourage model produce invariant representations. scheduling turned critical experiments. , observed encoder affected loss coming discriminator, low values Model selection Model selection performed automatically criteria. first, reconstruction error original images measured mse. second, model properly swap attributes image. criterion, train classifier predict image attributes. end epoch, swap attributes image validation set measure classifier performs decoded images. metrics filter potentially good models. final model selected based human evaluation images train set reconstructed swapped attributes. Experiments Experiments celeba dataset Experimental setup present experiments celeba dataset], 200, 000 images celebrity shape 178 218 annotated attributes. standard training, validation test split. pictures presented paper evaluation test set. pre-processing, cropped images 178 178, resized 256 256, resolution figures paper. image values normalized]. models trained Adam], learning rate.002, batch size. performed data augmentation ﬂipping horizontally images probability iteration. model baseline, IcGAN] model provided authors trained dataset. https://github.com/guim3/icgan Figure Swapping attributes faces. zoom resolution. qualitative evaluation Figure shows examples images generated swapping attributes: generated images high visual quality handle attribute changes, adding realistic glasses faces. generated images confirm latent representation learned Fader Networks invariant attribute values, captures information needed generate version face, attribute value. indeed, shape generated glasses, glasses shapes colors integrated original face depending face: model adding ?generic? glasses faces, generates plausible glasses depending input. quantitative evaluation protocol performed quantitative evaluation Fader Networks Mechanical turk, IcGAN baseline. chose attributes Mouth (open/close), Smile (with/without) Glasses (with/without) attributes common IcGAN model. evaluated aspects generated images: naturalness, measures quality generated images, accuracy, measures swapping attribute reﬂected generation. measures assess generate natural images, swap effective. compare: EAL MAGE original images transformation, FAD GAN reconstruct original images attribute alteration, FAD WAP \\x0cand GAN WAP generate images swapped attribute., With Glasses glasses. submitted Mechanical turk, images cropped resized processing icgan. result, output images displayed resolution, preventing Workers basing judgment sharpness presented images exclusively. technically, assess identity person preserved swapping attributes. problem gan-based methods, reconstruction quality model good (rmse test.0009, compared.028 icgan), observe issue. refore, evaluate aspect. naturalness, 500 images test set 250 images attribute shown Mechanical Turk workers, 100 models presented above. image, asked wher image natural generated. description Workers understand task showed examples real images, examples fake images FAD FAD WAP GAN GAN WAP accuracy model attribute evaluated classification task, resulting total experiments. example, fadnet/glasses experiment consisted Workers wher people glasses added FAD WAP effectively possess glasses, vice-versa. evaluate perceptible swaps human eye. experiment, 100 images shown images class, order test set). quantitative evaluations, experiment performed workers, resulting 000 samples experiment naturalness, 000 samples classification experiment swapped attributes. results tasks shown Table figure (zoom resolution.) examples multi-attribute swap (gender Opened eyes Eye glasses) performed model. left images originals. quantitative results naturalness experiments% real images classified ?real? workers, indicating high level requirement generate natural images. model obtained high naturalness accuracies reconstructing images swapping attributes%, compared IcGAN reconstructions accuracy exceed%, wher reconstructed swapped images. swap, FAD WAP consistently outperforms GAN WAP large margin. however, naturalness accuracy varies lot based swapped attribute% opening mouth% smile. classification experiments show reconstructions FAD GAN high classification scores, par real images Mouth smile. fad WAP obtains accuracy% mouth% glasses% smile, indicating model swap attributes high eﬃciency. hand, accuracies% attributes, GAN WAP generate convincing swaps. multi-attributes swapping present qualitative results ability model swap multiple attributes Figure \\x0cjointly modifying gender, open eyes glasses attributes. diﬃcult setting, model generate convincing images multiple swaps.  Experiments Flowers dataset performed additional experiments oxford-102 dataset, 000 images ﬂowers classified 102 categories]. dataset labels ﬂower categories, built list color attributes ﬂower captions provided]. ﬂower provided captions. color, gave ﬂower color attribute, color appears captions. naive, approach create accurate labels. resized images . figure represents reconstructed ﬂowers values ?pink? attribute. observe color ﬂower desired direction, keeping background cleanly unchanged. figure Examples reconstructed ﬂowers values pink attribute. row images originals. increasing attribute turn ﬂower colors pink, decreasing images originally pink ﬂowers make turn yellow orange. conclusion presented approach generate variations images changing attribute values. approach based enforcing invariance latent space. attributes. key advantage method compared recent models] generates realistic images high resolution needing apply GAN decoder output. result, easily extended domains speech, text, backpropagation decoder challenging non-differentiable text generation process instance. however, methods commonly vision assess visual quality generated images, patchgan, totally applied top model. acknowledgments authors Yedid Hoshen initial discussions core ideas paper, Christian Pursch Alexander Miller setting experiments Mechanical Turk evaluations. authors grateful David lopez-paz Mouhamadou Moustapha Cisse feedback support project. interested problem manipulating natural images controlling attributes interest. for example, photograph face person gender, age, expression, generate realistic version person older happier, image hypotical twin opposite gender. this task related problem unsupervised domain transfer recently received lot interest], case study conditional generative models applications automatic image edition. key challenge trans \\x0cformations ill-defined training unsupervised: training set images annotated attributes interest, transformation: cases ?gender swapping? above, pairs images representing person male female. cases, collecting examples requires costly annotation process, taking pictures person glasses. our approach relies encoder-decoder architecture where, input image attributes encoder maps latent representation decoder trained reconstruct). inference time, test image encoded latent space, user chooses attribute values fed decoder. even binary attribute values train time, attribute considered continuous variable inference control perceived final image. call architecture Fader networks, analogy sliders audio mixing console, user choose attribute incorporate. Facebook Research Sorbonne universit, UPMC Univ Paris, UMR 7606, LIP6 lscp, ens, ehess, cnrs, PSL Research university, INRIA 31st Conference Neural Information Processing Systems (nips 2017), Long beach, usa. figure Interpolation attributes (zoom resolution). each line shows reconstructions face attribute values, attribute controlled continuous variable. make person older younger, man manly imagine female version. left images originals. fundamental feature approach constrain latent space invariant attributes interest. concretely, means distribution images latent representations identical attribute values. this invariance obtained procedure similar domain-adversarial training (see]). process, classifier learns predict attributes latent representation training encoder-decoder trained based objectives time. objective reconstruction error decoder., latent representation information reconstruction input. objective consists fooling attribute classifier., latent representation prevent predicting correct attribute values. model, achieving invariance means filter out, hide, properties image related attributes interest. single latent representation corresponds images share common structure attribute values. reconstruction objective forces decoder attribute values choose, latent representation, intended image. our motivation learn disentangled latent space explicit control attributes interest, supervision intended result modifying attribute values. with similar motivation, approaches tested tasks], related image-image translation problems], specific applications creation parametrized avatars addition reconstruction loss, vast majority works]. rely adversarial training pixel space, compares training images generated intentional change attributes genuine images target attribute values. our approach adversarial training latent space output, adversarial training aims learning invariance attributes. assumption underlying work high fidelity input image conﬂicting invariance criterion, criterion forces hallucinated image match images training set. consequence principle, approach results simpler training pipelines based adversarial training pixel space, readily amenable controlling multiple attributes, adding output variables discriminator latent space. shown Figure test images CelebA dataset], model make subtle portraits end suﬃcient alter perceived attributes preserving natural aspect image identity person. our experiments show model outperforms previous methods based adversarial training decoders? output] terms reconstruction loss generation quality measured human subjects. disentanglement approach competitor widespread adversarial losses decoder output tasks. remainder paper, discuss details related work Section present training procedure Section describing network architecture implementation Section experimental results shown Section Related work substantial literature attribute-based and conditional image generation split terms required supervision, levels. extreme fully supervised approaches developed model transformations, examples form (input, transformation, result transformation). case, model learn desired transformation. this setting previously explored learn aﬃne transformations], rotations], lighting variations] video game animations]. methods developed works rely supervised setting, applied setup. extreme supervision spectrum lie fully unsupervised methods aim learning deep neural networks disentangle factors variations data, specification attributes. example methods InfoGAN], predictability minimization framework proposed]. neural photo editor] disentangles factors variations natural images image edition. ] introduced beta-vae, modification variational autoencoder (vae) framework learn latent factorized representations completely unsupervised manner. this setting considerably harder consider, general, diﬃcult methods automatically discover high-level concepts gender age. our work lies related information]. previous settings. methods developed unsupervised domain transfer] applied case: domains images ?drawings? ?photograph?, map image domain supervision; case, domain correspond attribute value. mappings trained adversarial training pixel space mentioned introduction, separate encoders and decoders domain, scale multiple attributes. line work specifically problem modifying attributes, Invertible conditional GAN] trains GAN conditioned attribute values, step learns map input images latent space gan, invertible gans. baseline experiments. antipov. ] pre-trained face recognition system conditional GAN learn latent space, focuses age attribute. attribute-image approach] variational auto-encoder disentangles foreground background generate images attribute values only. conditional generation performed inferring latent state correct attributes changing attributes. additionally, work related work learning invariant latent spaces adversarial training domain adaptation], fair classification] robust inference]. training criterion enforcing invariance similar works, difference end-goal works filter nuisance variables sensitive information. case, learn generative models, invariance means force decoder attribute information reconstruction. finally, application automatically modifying faces attributes, feature interpolation approach] presents means generate alterations images based attributes pre-trained network imagenet. while approach interesting application perspective, inference costly relies pre-trained models, naturally incorporate factors attributes foreseen pre-training. Fader Networks Let image domain set attributes images case people faces typical attributes glasses glasses, man/woman, young/old. for simplicity, case attributes binary, approach extended categorical attributes. setting number attributes. training set )}, pairs (image, attribute  ). end goal learn model generate, attribute vector version input image attribute values correspond encoder-decoder architecture Our model, Figure based encoder-decoder architecture domain-adversarial training latent space. encoder?enc convolutional neural network parameters ?enc maps input image -dimensional latent representation?enc). decoder?dec deconvolutional network parameters ?dec produces version input image latent representation?enc) attribute vector when context clear, simply denote?dec?enc precise architectures neural networks Section auto-encoding loss architecture classical squared error (mse) measures quality reconstruction training input true attribute vector lae (?enc ?dec?enc), dec exact choice reconstruction loss fundamental approach, adversarial losses PatchGAN] addition MSE stage obtain textures sharper images]. using absolute squared error ensure reconstruction matches original image. ideally, modifying), generate images perceived attributes, similar aspect. however, additional constraints, decoder learns ignore attributes, modifying test time effect. learning attribute-invariant latent representations avoid behavior, approach learn latent representations invariant respect attributes. invariance, versions object attribute values, instance images person glasses, latent representations same. when invariance satisfied, decoder attribute reconstruct original image. since training set versions image, constraint trivially added loss. propose incorporate constraint adversarial training latent space. this idea inspired work predictability minimization] adversarial training domain adaptation] objective learn invariant latent representation adversarial formulation learning objective. end, additional neural network called discriminator trained identify true attributes training pair). invariance obtained learning encoder discriminator unable identify attributes. GANs], corresponds two-player game discriminator aims maximizing ability identify attributes, aims preventing good discriminator. exact structure discriminator Section discriminator objective discriminator outputs probabilities attribute vector?dis)), ?dis discriminator parameters. using subscript refer attribute, log?dis)) log?dis)). since objective discriminator predict attributes input image latent representation, loss depends current state encoder written: Ldis (?dis —?enc log?dis?enc Adversarial objective objective encoder compute latent representation optimizes objectives. first, decoder reconstruct) time discriminator predict). mistake made discriminator predicts attribute Given discriminator parameters, complete loss encoder-decoder architecture(?enc ?dec —?dis ?enc),  log?dis ?enc) dec controls trade-off quality reconstruction invariance latent representations. large values restrain amount information contained), result blurry images, low values limit decoder dependency latent code result poor effects altering attributes. Figure Main architecture. (image, attribute) pair, input. encoder maps latent representation discriminator trained predict encoder trained make impossible discriminator predict only. decoder reconstruct). test time, discriminator discarded model generate versions fed attribute values. learning algorithm overall, current state encoder, optimal discriminator parameters satisfy ?dis (?enc argmin?dis Ldis (?dis —?enc ignore problems related multiple (and local) minima, objective function   ?enc ?dec argmin(?enc ?dec —?dis (?enc ?enc ,?dec practice, unreasonable solve ?dis (?enc update ?enc following practice adversarial training deep networks, stochastic gradient updates parameters, consid? ering current ?dis approximation ?dis (?enc given training), denote Ldis ?dis ?enc auto-encoder loss restricted, ?enc ?dec ?dis) discriminator loss. update time current parameters ?dis ?enc) ?dec training) ?dis  ?dis Ldis ?dis ?enc) [?enc ?dec [?enc ?dec  ?enc ,?dec ?enc ?dec ?dis) ?dis details training models section. Implementation adapt architecture network]. let convolutionbatchnorm-relu layer filters. convolutions kernel size stride padding layer encoder divides size input leaky-relus slope encoder, simple ReLUs decoder. encoder consists layers: C16 c32 c64 c128 c256 c512 c512 Input images size 256 256. result, latent representation image consists 512 feature maps size  experiments, layers gave similar results, layers significantly decreased performance, feature maps latent state. provide decoder image attributes, append latent code layer input decoder, latent code image concatenation one-hot vectors representing Model Real Image IcGAN IcGAN Swap FadNet FadNet Swap Mouth Naturalness Smile Glasses Mouth Accuracy Smile Glasses Table Perceptual evaluation naturalness swap accuracy model. naturalness score percentage images labeled ?real? human evaluators question image real photograph fake generated graphics engine??. accuracy score classification accuracy human evaluators values attribute. values attributes (binary attributes represented]). append latent code additional constant input channels convolutions decoder. denoting number attributes, (hence code size), decoder symmetric encoder, transposed convolutions-sampling: c512 c512 c256 c128 c64 c32 c16 discriminator C512 layer fullyconnected neural network layers size 512 repsectively. dropout found beneficial add dropout discriminator. hyposized dropout helped discriminator rely wider set features order infer current attributes, improving stabilizing accuracy, giving feedback encoder. set dropout rate experiments. following], add dropout layers decoder, experiments, turned significantly decrease performance. discriminator cost scheduling Similarly], variable weight discriminator loss coeﬃcient initially set model trained normal auto-encoder. linearly increased.0001 500, 000 iterations slowly encourage model produce invariant representations. this scheduling turned critical experiments. without, observed encoder affected loss coming discriminator, low values Model selection Model selection performed automatically criteria. first, reconstruction error original images measured mse. second, model properly swap attributes image. for criterion, train classifier predict image attributes. end epoch, swap attributes image validation set measure classifier performs decoded images. metrics filter potentially good models. final model selected based human evaluation images train set reconstructed swapped attributes. Experiments Experiments celeba dataset Experimental setup present experiments celeba dataset], 200, 000 images celebrity shape 178 218 annotated attributes. standard training, validation test split. all pictures presented paper evaluation test set. for pre-processing, cropped images 178 178, resized 256 256, resolution figures paper. image values normalized]. all models trained Adam], learning rate.002, batch size. performed data augmentation ﬂipping horizontally images probability iteration. model baseline, IcGAN] model provided authors trained dataset. https://github.com/guim3/icgan Figure Swapping attributes faces. zoom resolution. qualitative evaluation Figure shows examples images generated swapping attributes: generated images high visual quality handle attribute changes, adding realistic glasses faces. generated images confirm latent representation learned Fader Networks invariant attribute values, captures information needed generate version face, attribute value. indeed, shape generated glasses, glasses shapes colors integrated original face depending face: model adding ?generic? glasses faces, generates plausible glasses depending input. quantitative evaluation protocol performed quantitative evaluation Fader Networks Mechanical turk, IcGAN baseline. chose attributes Mouth (open/close), Smile (with/without) Glasses (with/without) attributes common IcGAN model. evaluated aspects generated images: naturalness, measures quality generated images, accuracy, measures swapping attribute reﬂected generation. both measures assess generate natural images, swap effective. compare: EAL MAGE original images transformation, FAD GAN reconstruct original images attribute alteration, FAD WAP \\x0cand GAN WAP generate images swapped attribute., With Glasses without glasses. before submitted Mechanical turk, images cropped resized processing icgan. result, output images displayed resolution, preventing Workers basing judgment sharpness presented images exclusively. technically, assess identity person preserved swapping attributes. this problem gan-based methods, reconstruction quality model good (rmse test.0009, compared.028 icgan), observe issue. refore, evaluate aspect. for naturalness, 500 images test set 250 images attribute shown Mechanical Turk workers, 100 models presented above. for image, asked wher image natural generated. description Workers understand task showed examples real images, examples fake images FAD FAD WAP GAN GAN WAP accuracy model attribute evaluated classification task, resulting total experiments. for example, fadnet/glasses experiment consisted Workers wher people glasses added FAD WAP effectively possess glasses, vice-versa. this evaluate perceptible swaps human eye. experiment, 100 images shown images class, order test set). quantitative evaluations, experiment performed workers, resulting 000 samples experiment naturalness, 000 samples classification experiment swapped attributes. results tasks shown Table Figure (zoom resolution.) examples multi-attribute swap (gender Opened eyes Eye glasses) performed model. left images originals. quantitative results naturalness experiments% real images classified ?real? workers, indicating high level requirement generate natural images. our model obtained high naturalness accuracies reconstructing images swapping attributes%, compared IcGAN reconstructions accuracy exceed%, wher reconstructed swapped images. for swap, FAD WAP consistently outperforms GAN WAP large margin. however, naturalness accuracy varies lot based swapped attribute% opening mouth% smile. classification experiments show reconstructions FAD GAN high classification scores, par real images Mouth smile. fad WAP obtains accuracy% mouth% glasses% smile, indicating model swap attributes high eﬃciency. hand, accuracies% attributes, GAN WAP generate convincing swaps. multi-attributes swapping present qualitative results ability model swap multiple attributes Figure \\x0cjointly modifying gender, open eyes glasses attributes. even diﬃcult setting, model generate convincing images multiple swaps.  Experiments Flowers dataset performed additional experiments oxford-102 dataset, 000 images ﬂowers classified 102 categories]. since dataset labels ﬂower categories, built list color attributes ﬂower captions provided]. each ﬂower provided captions. for color, gave ﬂower color attribute, color appears captions. although naive, approach create accurate labels. resized images . figure represents reconstructed ﬂowers values ?pink? attribute. observe color ﬂower desired direction, keeping background cleanly unchanged. figure Examples reconstructed ﬂowers values pink attribute. first row images originals. increasing attribute turn ﬂower colors pink, decreasing images originally pink ﬂowers make turn yellow orange. Conclusion presented approach generate variations images changing attribute values. approach based enforcing invariance latent space. attributes. key advantage method compared recent models] generates realistic images high resolution needing apply GAN decoder output. result, easily extended domains speech, text, backpropagation decoder challenging non-differentiable text generation process instance. however, methods commonly vision assess visual quality generated images, patchgan, totally applied top model. acknowledgments authors Yedid Hoshen initial discussions core ideas paper, Christian Pursch Alexander Miller setting experiments Mechanical Turk evaluations. authors grateful David lopez-paz Mouhamadou Moustapha Cisse feedback support project.',\n",
       " 'PP7240': 'generative adversarial networks (gans] achieved outstanding results generating realistic images, producing text]. gans learn complex generative models maximum likelihood variational approximations infeasible. likelihood, discriminator network serves objective generative model, generator. gan learning game generator, constructs syntic data random variables, discriminator, separates syntic data \\x0cfrom real world data. generator goal construct data discriminator real world data. thus, discriminator minimize syntic-real discrimination error generator maximize error. training GANs game solution Nash equilibrium, gradient descent fail converge]. local Nash equilibria found, gradient descent local optimization method. exists local neighborhood point parameter space neir generator discriminator unilaterally decrease respective losses, call point local Nash equilibrium. characterize convergence properties training general GANs open challenge]. special GAN variants, convergence proved assumptions], 31st Conference Neural Information Processing Systems (nips 2017), Long beach, usa.  5000 10000 15000 Iteration Iteration Convergence deterministic algorithm step sizes. fig.  500 orig orig TTUR Flow) Flow) Flow) 400 Convergence noisy feedback biased case).         , Flow) Flow) Constant step size Diminishing step size Flow 300) FID Flow *—— Flow) 200 100 5000 Iteration 10000 15000 100 150 mini-batch 200 250 Iteration Figure left: Original. TTUR GAN celeba. right: Figure 2007in] fig. training ?zoomed? convergence behavior Zhang iterates Figure shows distance parameter optimum time-scale update node network ﬂow problem. upper bounds errors (?, small, iterates TOCHASTIC TABILITY IME CALE oscillate repeatedly return neighborhood ofa optimal solution. supplement Section). convergence neighborhood LGORITHM NDER OISY EEDBACK diminishing step sizes, convergence however, upper bounds errors large, sections, iterates typically diverge. previous applied dual decom Convergence noisy feedback unbiased case). obability optimal points made possible. position method Problem) devised primal-dual algorithm, single time-scale algorithm. noted ability Stochastic algorithm: Biased case: Section Section Recent convergence hat gradient estimation biased, local stabilityerror] (see Supplement). proofs decomposition GANs hold formethods. particular, primal decomposition method hope obtain convergence optimal expectations training samples number examples infinity], machinery problem coupled variables instead, shown provided biased mini-batch learning leads stochastic gradient]. variables fixed, rest problem asymptotically uniformly bounded, iterates return decouple subproblems. naturally yields ontraction region? infinitely often. example, Recently GANs analyzed stochastic approximation algorithms multiple time-scale algorithms. ], alsohowever, great interest) ) uniformly bounded examine stability approximation multiple time-scale min/max formulation loss function. stochastic beenalgorithms positive value. alsofor assume)  concave presence noisy feedback, compare single). ) ), applied actor-critic learning, Prasad. ] showed time-scale update rule time-scale algorithms, terms complexity robustness. plot iterates (using ensures relative distance training reaches stationaryto local Nash equilibrium sense critic learns faster concrete twothan time-scale alpoints) fig. furr ?zoomed? actor. convergence proved ordinary differential equation (ode), stable limit gorithms based primal decomposition, points observed fig. upperfollowing NUM problem: local Nash prove GANs ) arecoincide small, stationary iterates return equilibria. follow approach. trained maximize converge local Nash equilibrium time-scale update rule (ttur., borhood optimal solution. however errors large, recurrent behavior separate learning discriminator generator This leads results subject rates. ) ) occur, iterates diverge. cllocal hminimum experiments. main premise discriminator converges), rates oretical analysis. furr observe generator fixed. generator smaller upper-bound, smaller slowly enough, discriminator converges, linkensuring capacities functions specific ction region?  becomes, thatperturbations iteratesare sinceindicating generator small. convergence, performance MAC parameters (forpatterns instance, transmission ?closer? optimal points. improve discriminator learnpnew ybeare transferred toprobabilities generator. contrast, generator overly fast, drives discrimi \\x0cnator steadily regions capturing gared information. recent GAN implementations, discriminator learned faster generator. objective slowed generator prevent overtraining current discriminator]. wasserstein GAN algorithm update steps discriminator generator]. compare TTUR standard GAN training. fig. shows left panel stochastic gradient CelebA original GAN training (orig), leads oscillations, ttur. panel node network ﬂow problem Zhang. ] shown. distance actual parameter optimum time-scale update rule shown iterates. upper bounds errors small, iterates return neighborhood optimal solution, large errors iterates diverge (see Supplement Section). contributions paper are) time-scale update rule gans) proof GANs trained TTUR converge stationary local Nash equilibrium, (iii) description Adam heavy ball friction resulting order differential equation) convergence GANs trained TTUR Adam stationary local Nash equilibrium?chet Inception distance? (fid) evaluate gans, consistent Inception score. time-scale Update Rule GANs discriminator(. parameter vector generator(.  parameter vector learning based stochastic gradient (?, discriminator loss function stochastic gradient(?, generator loss function loss functions original introduced Goodfellow. ], improved versions], recently proposed losses GANs Wasserstein GAN]. gradients stochastic, mini-batches real world samples) syntic samples) randomly chosen. true gradients(?, (?)  (?, define ) (?)  stochastic random variables thus, gradients approximations true gradients. consequently, analyze convergence GANs timescale stochastic approximations algorithms. time-scale update rule (ttur), learning rates) discriminator generator update, respectively(?)  ) For details convergence proof assumptions Supplement Section. prove convergence GANs learned ttur, make assumptions actual assumption ended text comments explanations) gradients lipschitz. consequently, networks Lipschitz smooth activation functions ELUs ] fulfill assumption ReLU networks.  \\x0c(?) ) stochastic gradient errors martingale difference sequences (?) . increasing ?-field Fnh  Mil), (?) (?) ) kmn kmn positive deterministic constants original Assumption) Borkar 1997 Lemma] (see]). assumption fulfilled robbins-monro setting, mini-batches randomly sampled gradients bounded.  ) local asymptotically stable attractor) For ODE) ?(?) domain attraction  lipschitz. ode )  ), ?(? )) local asymptotically stable attractor domain attraction discriminator converge minimum fixed generator parameters generator, turn, converge minimum fixed discriminator minimum. borkar 1997 required unique global asymptotically stable equilibria]. assumption global attractors relaxed local attractors Assumption) orem Karmakar Bhatnagar]. details Assumption) Supplement Section. here, GAN objectives serve Lyapunov functions. assumptions locally stable ODEs ensured additional weight decay term loss function increases eigenvalues hessian. refore, problems region-wise constant discriminator order derivatives avoided. furr discussion Supplement Section). ) supn supn kwn Typically ensured objective weight decay term. orem proved seminal paper Borkar 1997]. orem (borkar). assumptions satisfied, updates. ) converge  ?(?  . solution  ?(?  stationary local Nash equilibrium],  ?(?    local asymptotically stable attractors ?(?   ?(?   alternative approach proof convergence Poisson equation ensuring solution fast update rule found Supplement Section. approach assumes linear update function fast update rule which, however, linear approximation nonlinear gradient]. rate convergence Supplement Section, Section focuses linear Section nonlinear updates. equal time-scales proven updates revisit environment solution infinitely often, which, however, large]. details analysis equal time-scales Supplement Section. main idea proof Borkar, perturbed ODEs Hirsch 1989] (see Appendix Section bhatnagar, prasad, Prashanth 2013]). proof relies fact eventually time point perturbation slow update rule small (given fast update rule converge. experiments ttur, aim finding learning rates slow update small fast converge. typically, slow update generator fast update discriminator. adjust learning rates generator affect discriminator learning undesired perturb much. however, larger learning rate generator discriminator ensure discriminator low perturbations. learning rates translated directly perturbation perturbation discriminator generator perturbation generator discriminator. Adam Follows HBF ODE Ensures TTUR Convergence experiments, aim Adam stochastic approximation avoid mode collapsing. gans suffer ?mode collapsing? large masses probability mapped modes cover small regions. regions represent meaningful samples, variety real world data lost prototype samples generated. methods proposed avoid mode collapsing]. obviate mode collapsing Adam stochastic approximation]. adam Heavy Ball Friction (hbf) (see below), averages past gradients. averaging corresponds velocity makes generator resistant pushed small regions. adam HBF method typically overshoots small local minima correspond mode collapse find ﬂat minima generalize]. fig. depicts dynamics hbf, ball settles ﬂat minimum. next, analyze Figure Heavy Ball friction, wher GANs trained TTUR converge ball mass overshoots local minimum settles ﬂat minimum   adam. details Supplement Section recapitulate Adam update rule step learning rate exponential averaging factors moment gradient            ) )  ) operations meant componentwise: product square root division line. learning rate introduce damping coeﬃcient??   ]. adam parameters averaging gradient parametrized positive averaging squared gradient. parameters considered defining memory adam. characterize pnfollowing, define exponential memory) polynomial memory) positive constant orem \\x0cdescribes Adam differential equation, turn apply idea, perturbed ODEs ttur. consequently, learning GANs TTUR Adam converges. orem adam ), ) full gradient lower bounded, continuously differentiable objective stationary moments gradient, Adam differential equation Heavy Ball Friction (hbf): Adam converges gradients-lipschitz. proof. gadat. derived discrete stochastic version polyak Heavy Ball method], Heavy Ball Friction (hbf   ) update rules moment update rules Adam]. hbf formulated differential equation. ]. gadat. showed update rules. ) converge loss functions quadratic grow stated convergence proofed-lipschitz]. convergence proved continuously differentiable quasiconvex (orem Goudou Munier]). convergence proved-lipschitz bounded (orem Attouch. ]). adam normalizes average moments gradient componentwise divided square root components assume moments stationary case normalization considered additional noise normalization factor randomly deviates mean. hbf interpretation normalization corresponds introducing gravitation. obtain      )   For stationary moment   ). use componentwise?linear approximation adam moment normalization   operations meant componentwise.    ) set )),   stationary moment) random variable martingale difference sequence bounded moment.  ) refore subsumed update rules. ). factor componentwise incorporated gradient corresponds rescaling parameters changing minimum.   )) According Attouch. ] energy, Lyapunov function—? )—  ) )— Adam expressed differential equation Lyapunov function, idea, perturbed ODEs, carries adam. refore convergence Adam TTUR proved timescale stochastic approximation analysis Borkar] stationary moments gradient. supplement furr discuss convergence time-scale stochastic approximation algorithms additive noise, linear update functions depending Markov chains, nonlinear update functions, updates depending controlled Markov processes. furmore, supplement presents work rate convergence linear nonlinear update rules similar techniques local stability analysis Nagarajan Kolter]. finally, elaborate equal time-scale updates, investigated saddle point problems actor-critic learning. experiments Performance measure. presenting experiments, introduce quality measure models learned gans. objective generative learning model produces data matches observed data. refore, distance probability observing real world data (.) probability generating model data(.) serve performance measure generative models. however, defining performance measures generative models diﬃcult]. measure likelihood, estimated annealed importance sampling]. however, likelihood heavily depends noise assumptions real data dominated single samples]. approaches density estimates drawbacks]. well-performing approach measure performance GANs ?inception score? correlates human judgment]. generated samples fed inception model trained imagenet. images meaningful objects supposed low label (output) entropy, belong object classes. hand, entropy images high, variance images large. drawback Inception Score statistics real world samples compared statistics syntic samples. next, improve RInception score. equality(.) (.) holds non-measurable set(.   basis (.) spanning function space(.) (.) live. equalities expectations describe distributions moments cumulants) polynomials data generalize polynomials replacing coding layer inception model 400 350 350 300 300 300 250 250 250 200 \\x0cfid 400 350 FID FID 400 200 200 150 150 150 100 100 100 disturbance level disturbance level 250 disturbance level 300 600 200 200 FID FID \\x0cdisturbance level FID 400 100 250 500 150 300 150 200 100 100 disturbance level disturbance level Figure FID evaluated upper left: Gaussian noise, upper middle: Gaussian blur, upper right: implanted black rectangles, lower left: swirled images, lower middle: salt pepper noise, lower right: CelebA dataset contaminated ImageNet images. disturbance level rises increases highest level. fid captures disturbance level monotonically increasing. order obtain vision-relevant features. practical reasons polynomials, moments: covariance. gaussian maximum entropy distribution covariance, refore assume coding units follow multidimensional gaussian. difference Gaussians (syntic real-world images) measured?chet distance] wasserstein distance]. call?chet distance(., gaussian, obtained(.) gaussian obtained (.) ?chet Inception distance? (fid k22 ccw show FID consistent increasing disturbances human judgment. fig. evaluates FID Gaussian noise, Gaussian blur, implanted black rectangles, swirled images, salt pepper noise, CelebA dataset contaminated ImageNet images. fid captures disturbance level well. experiments FID evaluate performance gans. details comparison FID Inception Score Supplement Section show FID \\x0cmore consistent noise level Inception score. model Selection evaluation. compare time-scale update rule (ttur) GANs original GAN training wher TTUR improves convergence speed performance gans. selected Adam stochastic optimization reduce risk mode collapsing. advantage Adam confirmed MNIST experiments, Adam considerably reduced cases observed mode collapsing. TTUR ensures discriminator converges learning, practicable learning rates found experiment. face trade-off learning rates small. generator) ensure convergence time large fast learning. experiments, learning rates optimized large ensuring stable training decreasing FID jensen-shannon-divergence (jsd). furr fixed time point stopping training update step FID jensen-shannon-divergence models longer decreasing. models, observed FID diverges starts increase time point. behaviour shown fig.  performance generative models evaluated?chet Inception Distance (fid) introduced above. One Billion Word experiment, normalized JSD served performance measure. computing fid, propagated images training dataset pretrained inception model computation Inception Score], however, pooling layer coding layer. coding layer, calculated covariance matrix thus, approximate central moment function Inception coding layer real world distribution. approximate moments model distribution, generate,000 images, propagate inception model, compute covariance matrix For computational eﬃciency, evaluate FID,000 DCGAN mini-batch updates,000 wgan outer iterations image experiments, 100 outer iterations wgan language model. time-scale updates wgan outer iteration image model consists discriminator mini-batches ten discriminator mini-batches language model, follow original implementation. TTUR however, discriminator updated iteration. repeat training single time-scale (orig) TTUR learning rate times image datasets ten times language benchmark. additionally FID training progress show minimum maximum FID runs evaluation time-step. details, implementations furr results Supplement Section simple Toy data. demonstrate difference single time-scale update rule TTUR simple toy min/max problem saddle point found. objective, )(100 fig. (left) saddle point, fulfills assumption. norm measures distance parameter vector, saddle point. update, gradient descent gradient ascent additive Gaussian noise order simulate stochastic update. updates converge saddle point, objective, 100 norm fig. (right), rows show time-scale update rules. large learning rate row diverges large ﬂuctuations. smaller learning rate row converges slower TTUR row slow-updates. ttur slow-updates fourth row converges slower. 1000 760 521 282 objective 200 150 110 100 125 2000 4000 100 125 100 norm 2000 4000 Figure left: Plot objective saddle point). right: Training progress equal learning rates (first row.001 (second row)) TTUR learning rate.0001 (third row) larger learning rate.0001 (fourth row). columns show function values (left), norms (middle, (right). ttur (third row) converges faster equal time-scale updates directly moves saddle point shown norm)-plot. dcgan Image data. test TTUR deep convolutional GAN (dcgan] celeba, cifar, SVHN LSUN Bedrooms dataset. fig. shows FID learning original learning method \\x0c(orig) ttur. original training method faster beginning, TTUR eventually achieves performance. dcgan trained TTUR reaches constantly lower FID original method CelebA LSUN Bedrooms time-scale runs diverge. DCGAN learning rate generator larger discriminator, which, however, contradict TTUR ory (see Supplement Section). table report FID TTUR time-scale training optimized number updates learning rates. ttur constantly outperforms standard training stable. wgan Image data. wgangp image model] test TTUR cifar LSUN Bedrooms datasets. contrast original code discriminator trained times generator update, TTUR updates discriminator once, refore align training progress wall-clock time. learning rate original training optimized large leads stable learning. TTUR higher learning rate discriminator TTUR stabilizes learning. fig. shows FID learning original learning method ttur. table shows FID TTUR time-scale training 200 100 150 mini-batch 200 250 orig orig orig TTUR FID 400 200 orig orig orig TTUR mini-batch 100 120 orig orig orig TTUR 400 FID FID 400 120 100 FID orig orig orig TTUR 200 100 125 150 175 mini-batch 100 150 200 250 300 350 400 mini-batch \\x0cfigure Mean FID (solid line) surrounded shaded area bounded maximum minimum runs DCGAN celeba, cifar, svhn, LSUN bedrooms. ttur learning rates discriminator generator: ?ttur?. top left: celeba. top right: cifar, starting mini-batch update 10k visualisation. bottom left: svhn. bottom right: LSUN bedrooms. training TTUR (red) stable, lower variance, leads fid. orig orig orig TTUR FID 100 400 orig orig orig TTUR 300 200 FID 150 100 200 400 600 minutes 800 1000 500 1000 minutes 1500 2000 Figure Mean FID (solid line) surrounded shaded area bounded maximum minimum runs wgan celeba, cifar, svhn, LSUN bedrooms. ttur learning rates discriminator generator: ?ttur?. left: cifar, starting minute. right: LSUN bedrooms. training TTUR (red) lower variance leads fid. optimized number iterations learning rates. TTUR reaches lower FIDs time-scale training. wgan Language data. finally One Billion Word Benchmark] serves evaluate TTUR wgan. character-level generative language model convolutional neural network (cnn) maps latent vector sequence one-hot character vectors dimension maximum softmax output. discriminator CNN applied sequences one-hot vectors characters. FID criterium works images, measured performance jensen-shannon-divergence (jsd) model real world distribution previously]. contrast original code critic trained ten times generator update, TTUR updates discriminator once, refore align training progress wall-clock time. learning rate original training \\x0coptimized large leads stable learning. TTUR higher learning rate discriminator TTUR stabilizes learning. report-gram word evaluation normalized JSD ten runs original training TTUR training fig.  table report JSD optimal time-step TTUR outperforms standard training measures. improvement TTUR-gram statistics original training shows TTUR enables learn generate subtle pseudo-words resembles real words.  orig TTUR JSD JSD orig TTUR 200 400 600 800 minutes 1000 1200 200 400 600 800 minutes 1000 1200 Figure Performance wgan models trained original (orig) TTUR method One Billion Word benchmark. performance measured normalized jensenshannon-divergence based-gram (left-gram (right) statistics averaged (solid line) surrounded shaded area bounded maximum minimum runs, aligned wall-clock time starting minute 150. ttur learning (red) outperforms original time-scale learning. table performance DCGAN wgan trained original time-scale update rule TTUR celeba, cifar, svhn, LSUN Bedrooms One Billion Word benchmark. training compare performance respect FID JSD optimized number updates. ttur exhibits consistently FID jsd. dcgan Image dataset method CelebA TTUR cifar TTUR SVHN TTUR LSUN TTUR wgan Image dataset method cifar TTUR LSUN TTUR wgan Language ngram method-gram TTUR-gram TTUR updates 225k 75k 165k 340k \\x0cfid method orig orig orig orig updates 70k 100k 185k 70k FID time) 700 1900 FID method orig orig time) 800 2010 FID time) 1150 1120 JSD method orig orig time) 1040 1070 JSD Conclusion For learning gans, introduced time-scale update rule (ttur), proved converge stationary local Nash equilibrium. Adam stochastic optimization heavy ball friction (hbf) dynamics, shows Adam converges Adam find ﬂat minima avoiding small local minima. order differential equation describes learning dynamics Adam HBF system. differential equation, convergence GANs trained TTUR stationary local Nash equilibrium extended adam. finally, evaluate gans, introduced?chet Inception distance? (fid) captures similarity generated images real Inception score. experiments compared GANs trained TTUR conventional GAN training time-scale update rule celeba, cifar, svhn, LSUN bedrooms, One Billion Word benchmark. ttur outperforms conventional GAN training consistently experiments. acknowledgment This work supported NVIDIA corporation, Bayer Research Agreement/2017, Zalando Research Agreement/2016, audi.jku Deep Learning center, Audi Electronic Venture gmbh, IWT research grant IWT150865 (exaptation), H2020 project grant 671555 (excape) FWF grant 28660-n31. Generative adversarial networks (gans] achieved outstanding results generating realistic images, producing text]. gans learn complex generative models maximum likelihood variational approximations infeasible. instead likelihood, discriminator network serves objective generative model, generator. gan learning game generator, constructs syntic data random variables, discriminator, separates syntic data \\x0cfrom real world data. generator goal construct data discriminator real world data. thus, discriminator minimize syntic-real discrimination error generator maximize error. since training GANs game solution Nash equilibrium, gradient descent fail converge]. only local Nash equilibria found, gradient descent local optimization method. exists local neighborhood point parameter space neir generator discriminator unilaterally decrease respective losses, call point local Nash equilibrium. characterize convergence properties training general GANs open challenge]. for special GAN variants, convergence proved assumptions], 31st Conference Neural Information Processing Systems (nips 2017), Long beach, usa.  5000 10000 15000 Iteration Iteration Convergence deterministic algorithm step sizes. fig.  500 orig orig TTUR Flow) Flow) Flow) 400 Convergence noisy feedback biased case).         , Flow) Flow) Constant step size Diminishing step size Flow 300) FID Flow *—— Flow) 200 100 5000 Iteration 10000 15000 100 150 mini-batch 200 250 Iteration Figure left: Original. TTUR GAN celeba. right: Figure 2007in] fig. training ?zoomed? convergence behavior Zhang iterates Figure shows distance parameter optimum time-scale update node network ﬂow problem. when upper bounds errors (?, small, iterates TOCHASTIC TABILITY IME CALE oscillate repeatedly return neighborhood ofa optimal solution. supplement Section). convergence neighborhood LGORITHM NDER OISY EEDBACK diminishing step sizes, convergence however, upper bounds errors large, sections, iterates typically diverge. previous applied dual decom Convergence noisy feedback unbiased case). obability optimal points made possible. position method Problem) devised primal-dual algorithm, single time-scale algorithm. noted ability Stochastic algorithm: Biased case: Section Section Recent convergence hat gradient estimation biased, local stabilityerror] (see Supplement). proofs decomposition GANs hold formethods. particular, primal decomposition method hope obtain convergence optimal expectations training samples number examples infinity], machinery problem coupled variables instead, shown provided biased mini-batch learning leads stochastic gradient]. variables fixed, rest problem asymptotically uniformly bounded, iterates return decouple subproblems. this naturally yields ontraction region? infinitely often. example, Recently GANs analyzed stochastic approximation algorithms multiple time-scale algorithms. ], alsohowever, great interest) ) uniformly bounded examine stability approximation multiple time-scale min/max formulation loss function. stochastic beenalgorithms positive value. alsofor assume)  concave presence noisy feedback, compare single). ) ), applied actor-critic learning, Prasad. ] showed time-scale update rule time-scale algorithms, terms complexity robustness. plot iterates (using ensures relative distance training reaches stationaryto local Nash equilibrium sense critic learns faster concrete twothan time-scale alpoints) fig. furr ?zoomed? actor. convergence proved ordinary differential equation (ode), stable limit gorithms based primal decomposition, points observed fig. upperfollowing NUM problem: local Nash prove GANs ) arecoincide small, stationary iterates return equilibria. follow approach. trained maximize converge local Nash equilibrium time-scale update rule (ttur., borhood optimal solution. however errors large, recurrent behavior separate learning discriminator generator This leads results subject rates. ) ) occur, iterates diverge. this cllocal hminimum experiments. main premise discriminator converges), rates oretical analysis. furr observe generator fixed. generator smaller upper-bound, smaller slowly enough, discriminator converges, linkensuring capacities functions specific ction region?  becomes, thatperturbations iteratesare sinceindicating generator small. besides convergence, performance MAC parameters (forpatterns instance, transmission ?closer? optimal points. improve discriminator learnpnew ybeare transferred toprobabilities generator. contrast, generator overly fast, drives discrimi \\x0cnator steadily regions capturing gared information. recent GAN implementations, discriminator learned faster generator. objective slowed generator prevent overtraining current discriminator]. Wasserstein GAN algorithm update steps discriminator generator]. compare TTUR standard GAN training. fig. shows left panel stochastic gradient CelebA original GAN training (orig), leads oscillations, ttur. panel node network ﬂow problem Zhang. ] shown. distance actual parameter optimum time-scale update rule shown iterates. when upper bounds errors small, iterates return neighborhood optimal solution, large errors iterates diverge (see Supplement Section). our contributions paper are) time-scale update rule gans) proof GANs trained TTUR converge stationary local Nash equilibrium, (iii) description Adam heavy ball friction resulting order differential equation) convergence GANs trained TTUR Adam stationary local Nash equilibrium?chet Inception distance? (fid) evaluate gans, consistent Inception score. two time-scale Update Rule GANs discriminator(. parameter vector generator(.  parameter vector learning based stochastic gradient (?, discriminator loss function stochastic gradient(?, generator loss function loss functions original introduced Goodfellow. ], improved versions], recently proposed losses GANs Wasserstein GAN]. gradients stochastic, mini-batches real world samples) syntic samples) randomly chosen. true gradients(?, (?)  (?, define ) (?)  stochastic random variables thus, gradients approximations true gradients. consequently, analyze convergence GANs timescale stochastic approximations algorithms. for time-scale update rule (ttur), learning rates) discriminator generator update, respectively(?)  ) For details convergence proof assumptions Supplement Section. prove convergence GANs learned ttur, make assumptions actual assumption ended text comments explanations) gradients lipschitz. consequently, networks Lipschitz smooth activation functions ELUs ] fulfill assumption ReLU networks.  \\x0c(?) ) stochastic gradient errors martingale difference sequences (?) . increasing ?-field Fnh  Mil), (?) (?) ) kmn kmn positive deterministic constants original Assumption) Borkar 1997 Lemma] (see]). assumption fulfilled robbins-monro setting, mini-batches randomly sampled gradients bounded.  ) local asymptotically stable attractor) For ODE) ?(?) domain attraction  lipschitz. ODE )  ), ?(? )) local asymptotically stable attractor domain attraction discriminator converge minimum fixed generator parameters generator, turn, converge minimum fixed discriminator minimum. borkar 1997 required unique global asymptotically stable equilibria]. assumption global attractors relaxed local attractors Assumption) orem Karmakar Bhatnagar]. see details Assumption) Supplement Section. here, GAN objectives serve Lyapunov functions. assumptions locally stable ODEs ensured additional weight decay term loss function increases eigenvalues hessian. refore, problems region-wise constant discriminator order derivatives avoided. for furr discussion Supplement Section). ) supn supn kwn Typically ensured objective weight decay term. orem proved seminal paper Borkar 1997]. orem (borkar). assumptions satisfied, updates. ) converge  ?(?  . solution  ?(?  stationary local Nash equilibrium],  ?(?    local asymptotically stable attractors ?(?   ?(?   alternative approach proof convergence Poisson equation ensuring solution fast update rule found Supplement Section. this approach assumes linear update function fast update rule which, however, linear approximation nonlinear gradient]. for rate convergence Supplement Section, Section focuses linear Section nonlinear updates. for equal time-scales proven updates revisit environment solution infinitely often, which, however, large]. for details analysis equal time-scales Supplement Section. main idea proof Borkar, perturbed ODEs Hirsch 1989] (see Appendix Section bhatnagar, prasad, Prashanth 2013]). proof relies fact eventually time point perturbation slow update rule small (given fast update rule converge. for experiments ttur, aim finding learning rates slow update small fast converge. typically, slow update generator fast update discriminator. adjust learning rates generator affect discriminator learning undesired perturb much. however, larger learning rate generator discriminator ensure discriminator low perturbations. learning rates translated directly perturbation perturbation discriminator generator perturbation generator discriminator. Adam Follows HBF ODE Ensures TTUR Convergence experiments, aim Adam stochastic approximation avoid mode collapsing. gans suffer ?mode collapsing? large masses probability mapped modes cover small regions. while regions represent meaningful samples, variety real world data lost prototype samples generated. different methods proposed avoid mode collapsing]. obviate mode collapsing Adam stochastic approximation]. adam Heavy Ball Friction (hbf) (see below), averages past gradients. this averaging corresponds velocity makes generator resistant pushed small regions. adam HBF method typically overshoots small local minima correspond mode collapse find ﬂat minima generalize]. fig. depicts dynamics hbf, ball settles ﬂat minimum. next, analyze Figure Heavy Ball friction, wher GANs trained TTUR converge ball mass overshoots local minimum settles ﬂat minimum   adam. for details Supplement Section recapitulate Adam update rule step learning rate exponential averaging factors moment gradient            ) )  ) operations meant componentwise: product square root division line. instead learning rate introduce damping coeﬃcient??   ]. adam parameters averaging gradient parametrized positive averaging squared gradient. parameters considered defining memory adam. characterize pnfollowing, define exponential memory) polynomial memory) positive constant orem \\x0cdescribes Adam differential equation, turn apply idea, perturbed ODEs ttur. consequently, learning GANs TTUR Adam converges. orem Adam ), ) full gradient lower bounded, continuously differentiable objective stationary moments gradient, Adam differential equation Heavy Ball Friction (hbf): Adam converges gradients-lipschitz. proof. gadat. derived discrete stochastic version polyak Heavy Ball method], Heavy Ball Friction (hbf   ) update rules moment update rules Adam]. HBF formulated differential equation. ]. gadat. showed update rules. ) converge loss functions quadratic grow stated convergence proofed-lipschitz]. convergence proved continuously differentiable quasiconvex (orem Goudou Munier]). convergence proved-lipschitz bounded (orem Attouch. ]). adam normalizes average moments gradient componentwise divided square root components assume moments stationary case normalization considered additional noise normalization factor randomly deviates mean. HBF interpretation normalization corresponds introducing gravitation. obtain      )   For stationary moment   ). use componentwise?linear approximation adam moment normalization   operations meant componentwise.    ) set )),   for stationary moment) random variable martingale difference sequence bounded moment.  ) refore subsumed update rules. ). factor componentwise incorporated gradient corresponds rescaling parameters changing minimum.   )) According Attouch. ] energy, Lyapunov function—? )—  ) )— since Adam expressed differential equation Lyapunov function, idea, perturbed ODEs, carries adam. refore convergence Adam TTUR proved timescale stochastic approximation analysis Borkar] stationary moments gradient. supplement furr discuss convergence time-scale stochastic approximation algorithms additive noise, linear update functions depending Markov chains, nonlinear update functions, updates depending controlled Markov processes. furmore, supplement presents work rate convergence linear nonlinear update rules similar techniques local stability analysis Nagarajan Kolter]. finally, elaborate equal time-scale updates, investigated saddle point problems actor-critic learning. Experiments Performance measure. before presenting experiments, introduce quality measure models learned gans. objective generative learning model produces data matches observed data. refore, distance probability observing real world data (.) probability generating model data(.) serve performance measure generative models. however, defining performance measures generative models diﬃcult]. measure likelihood, estimated annealed importance sampling]. however, likelihood heavily depends noise assumptions real data dominated single samples]. approaches density estimates drawbacks]. well-performing approach measure performance GANs ?inception score? correlates human judgment]. generated samples fed inception model trained imagenet. images meaningful objects supposed low label (output) entropy, belong object classes. hand, entropy images high, variance images large. drawback Inception Score statistics real world samples compared statistics syntic samples. next, improve RInception score. equality(.) (.) holds non-measurable set(.   basis (.) spanning function space(.) (.) live. equalities expectations describe distributions moments cumulants) polynomials data generalize polynomials replacing coding layer inception model 400 350 350 300 300 300 250 250 250 200 \\x0cfid 400 350 FID FID 400 200 200 150 150 150 100 100 100 disturbance level disturbance level 250 disturbance level 300 600 200 200 FID FID \\x0cdisturbance level FID 400 100 250 500 150 300 150 200 100 100 disturbance level disturbance level Figure FID evaluated upper left: Gaussian noise, upper middle: Gaussian blur, upper right: implanted black rectangles, lower left: swirled images, lower middle: salt pepper noise, lower right: CelebA dataset contaminated ImageNet images. disturbance level rises increases highest level. FID captures disturbance level monotonically increasing. order obtain vision-relevant features. for practical reasons polynomials, moments: covariance. Gaussian maximum entropy distribution covariance, refore assume coding units follow multidimensional gaussian. difference Gaussians (syntic real-world images) measured?chet distance] wasserstein distance]. call?chet distance(., Gaussian, obtained(.) Gaussian obtained (.) ?chet Inception distance? (fid k22 CCw next show FID consistent increasing disturbances human judgment. fig. evaluates FID Gaussian noise, Gaussian blur, implanted black rectangles, swirled images, salt pepper noise, CelebA dataset contaminated ImageNet images. FID captures disturbance level well. experiments FID evaluate performance gans. for details comparison FID Inception Score Supplement Section show FID \\x0cmore consistent noise level Inception score. model Selection evaluation. compare time-scale update rule (ttur) GANs original GAN training wher TTUR improves convergence speed performance gans. selected Adam stochastic optimization reduce risk mode collapsing. advantage Adam confirmed MNIST experiments, Adam considerably reduced cases observed mode collapsing. although TTUR ensures discriminator converges learning, practicable learning rates found experiment. face trade-off learning rates small. generator) ensure convergence time large fast learning. for experiments, learning rates optimized large ensuring stable training decreasing FID jensen-shannon-divergence (jsd). furr fixed time point stopping training update step FID jensen-shannon-divergence models longer decreasing. for models, observed FID diverges starts increase time point. behaviour shown fig.  performance generative models evaluated?chet Inception Distance (fid) introduced above. for One Billion Word experiment, normalized JSD served performance measure. for computing fid, propagated images training dataset pretrained inception model computation Inception Score], however, pooling layer coding layer. for coding layer, calculated covariance matrix thus, approximate central moment function Inception coding layer real world distribution. approximate moments model distribution, generate,000 images, propagate inception model, compute covariance matrix For computational eﬃciency, evaluate FID,000 DCGAN mini-batch updates,000 wgan outer iterations image experiments, 100 outer iterations wgan language model. for time-scale updates wgan outer iteration image model consists discriminator mini-batches ten discriminator mini-batches language model, follow original implementation. for TTUR however, discriminator updated iteration. repeat training single time-scale (orig) TTUR learning rate times image datasets ten times language benchmark. additionally FID training progress show minimum maximum FID runs evaluation time-step. for details, implementations furr results Supplement Section simple Toy data. demonstrate difference single time-scale update rule TTUR simple toy min/max problem saddle point found. objective, )(100 fig. (left) saddle point, fulfills assumption. norm measures distance parameter vector, saddle point. update, gradient descent gradient ascent additive Gaussian noise order simulate stochastic update. updates converge saddle point, objective, 100 norm fig. (right), rows show time-scale update rules. large learning rate row diverges large ﬂuctuations. smaller learning rate row converges slower TTUR row slow-updates. ttur slow-updates fourth row converges slower. 1000 760 521 282 objective 200 150 110 100 125 2000 4000 100 125 100 norm 2000 4000 Figure left: Plot objective saddle point). right: Training progress equal learning rates (first row.001 (second row)) TTUR learning rate.0001 (third row) larger learning rate.0001 (fourth row). columns show function values (left), norms (middle, (right). ttur (third row) converges faster equal time-scale updates directly moves saddle point shown norm)-plot. dcgan Image data. test TTUR deep convolutional GAN (dcgan] celeba, cifar, SVHN LSUN Bedrooms dataset. fig. shows FID learning original learning method \\x0c(orig) ttur. original training method faster beginning, TTUR eventually achieves performance. dcgan trained TTUR reaches constantly lower FID original method CelebA LSUN Bedrooms time-scale runs diverge. for DCGAN learning rate generator larger discriminator, which, however, contradict TTUR ory (see Supplement Section). Table report FID TTUR time-scale training optimized number updates learning rates. ttur constantly outperforms standard training stable. wgan Image data. WGANGP image model] test TTUR cifar LSUN Bedrooms datasets. contrast original code discriminator trained times generator update, TTUR updates discriminator once, refore align training progress wall-clock time. learning rate original training optimized large leads stable learning. TTUR higher learning rate discriminator TTUR stabilizes learning. fig. shows FID learning original learning method ttur. table shows FID TTUR time-scale training 200 100 150 mini-batch 200 250 orig orig orig TTUR FID 400 200 orig orig orig TTUR mini-batch 100 120 orig orig orig TTUR 400 FID FID 400 120 100 FID orig orig orig TTUR 200 100 125 150 175 mini-batch 100 150 200 250 300 350 400 mini-batch \\x0cfigure Mean FID (solid line) surrounded shaded area bounded maximum minimum runs DCGAN celeba, cifar, svhn, LSUN bedrooms. TTUR learning rates discriminator generator: ?ttur?. top left: celeba. top right: cifar, starting mini-batch update 10k visualisation. bottom left: svhn. bottom right: LSUN bedrooms. training TTUR (red) stable, lower variance, leads fid. orig orig orig TTUR FID 100 400 orig orig orig TTUR 300 200 FID 150 100 200 400 600 minutes 800 1000 500 1000 minutes 1500 2000 Figure Mean FID (solid line) surrounded shaded area bounded maximum minimum runs wgan celeba, cifar, svhn, LSUN bedrooms. TTUR learning rates discriminator generator: ?ttur?. left: cifar, starting minute. right: LSUN bedrooms. training TTUR (red) lower variance leads fid. optimized number iterations learning rates. again TTUR reaches lower FIDs time-scale training. wgan Language data. finally One Billion Word Benchmark] serves evaluate TTUR wgan. character-level generative language model convolutional neural network (cnn) maps latent vector sequence one-hot character vectors dimension maximum softmax output. discriminator CNN applied sequences one-hot vectors characters. since FID criterium works images, measured performance jensen-shannon-divergence (jsd) model real world distribution previously]. contrast original code critic trained ten times generator update, TTUR updates discriminator once, refore align training progress wall-clock time. learning rate original training \\x0coptimized large leads stable learning. TTUR higher learning rate discriminator TTUR stabilizes learning. report-gram word evaluation normalized JSD ten runs original training TTUR training fig.  Table report JSD optimal time-step TTUR outperforms standard training measures. improvement TTUR-gram statistics original training shows TTUR enables learn generate subtle pseudo-words resembles real words.  orig TTUR JSD JSD orig TTUR 200 400 600 800 minutes 1000 1200 200 400 600 800 minutes 1000 1200 Figure Performance wgan models trained original (orig) TTUR method One Billion Word benchmark. performance measured normalized jensenshannon-divergence based-gram (left-gram (right) statistics averaged (solid line) surrounded shaded area bounded maximum minimum runs, aligned wall-clock time starting minute 150. ttur learning (red) outperforms original time-scale learning. table performance DCGAN wgan trained original time-scale update rule TTUR celeba, cifar, svhn, LSUN Bedrooms One Billion Word benchmark. during training compare performance respect FID JSD optimized number updates. ttur exhibits consistently FID jsd. dcgan Image dataset method CelebA TTUR cifar TTUR SVHN TTUR LSUN TTUR wgan Image dataset method cifar TTUR LSUN TTUR wgan Language ngram method-gram TTUR-gram TTUR updates 225k 75k 165k 340k \\x0cfid method orig orig orig orig updates 70k 100k 185k 70k FID time) 700 1900 FID method orig orig time) 800 2010 FID time) 1150 1120 JSD method orig orig time) 1040 1070 JSD Conclusion For learning gans, introduced time-scale update rule (ttur), proved converge stationary local Nash equilibrium. Adam stochastic optimization heavy ball friction (hbf) dynamics, shows Adam converges Adam find ﬂat minima avoiding small local minima. order differential equation describes learning dynamics Adam HBF system. via differential equation, convergence GANs trained TTUR stationary local Nash equilibrium extended adam. finally, evaluate gans, introduced?chet Inception distance? (fid) captures similarity generated images real Inception score. experiments compared GANs trained TTUR conventional GAN training time-scale update rule celeba, cifar, svhn, LSUN bedrooms, One Billion Word benchmark. ttur outperforms conventional GAN training consistently experiments. acknowledgment This work supported NVIDIA corporation, Bayer Research Agreement/2017, Zalando Research Agreement/2016, audi.jku Deep Learning center, Audi Electronic Venture gmbh, IWT research grant IWT150865 (exaptation), H2020 project grant 671555 (excape) FWF grant 28660-n31.',\n",
       " 'PP7245': 'extension Gaussian processes (gps]) multiple outputs referred multi-output Gaussian processes (mogps). mogps model temporal spatial relationships infinitelymany random variables, scalar gps, account statistical dependence sources data channels). crucial number real-world applications fault detection, data imputation denoising. input points covariance function-channel MOGP, symmetric positivedefinite matrix scalar covariance functions. design matrixvalued kernel challenging deal trade) choosing broad class  cross-covariances auto-covariances, time) ensuring positive definiteness symmetric matrix \\x0ccontaining covariance functions pair inputs particular, unlike widely families auto-covariance functions]), cross-covariances bound positive definite refore designed freely; construction functions interpretable functional form main focus article. classical approach define crosscovariances MOGP linearly combine independent latents gps, case Linear Model Coregionalization (lmc]) Convolution Model (conv]). cases, resulting kernel function covariance functions latent GPs parameters linear operator considered; results symmetric centred cross-covariances. approaches simple, lack interpretability dependencies learnt force auto-covariances similar behaviour channels. lmc method inspired cross-spectral Mixture (csm) kernel], 31st Conference Neural Information Processing Systems (nips 2017), Long beach, usa. spectral Mixture) kernel] LMC model phase differences channels manually introducing shift cosine exponential factors kernel. exhibiting improved performance wrt previous approaches, addition shift parameter CSM poses question: Can spectral design multiouput covariance functions ﬂexible? approach extend spectral mixture concept multiple outputs: Recall stationary scalar-valued gps] designs power spectral density (psd) process mixture square exponential functions supported bochner orem], present Spectral Mixture kernel inverse Fourier transform-constructed psd. lines, main contribution propose expressive family complex-valued square-exponential cross-spectral densities, build cram orem], multivariate extension bochner, construct multi-output Spectral Mixture kernel (mosm). proposed multivariate covariance function accounts properties cross-spectral Mixture kernel] delay component channels variable parameters auto-covariances channels. additionally, proposed MOSM clear interpretation parameters spectral terms. experimental contribution includes illustrative trivariate syntic signal validation aforementioned literature real-world datasets. background Definition gaussian process) input set real-valued stochastic process finite subset inputs random variables jointly gaussian. loss generality choose ] defines distribution functions) uniquely determined function)), typically assumed) covariance function (also kernel, cov )),  equip reader background follow proposal: review spectral-based \\x0capproach design scalar-valued covariance kernels present definition multi-output.  Spectral Mixture kernel bypass explicit construction positive-definite functions design stationary covariance kernels, design power spectral density (psd] transform covariance function inverse Fourier transform. motivated fact strict positivity requirement PSD easier achieve positive definiteness requirement covariance kernel. oretical support construction orem: orem (bochner orem) integrable1 function covariance function weakly-stationary mean-square-continuous stochastic process admits representation(? ??  ? (?) non-negative bounded function denotes imaginary unit. proof]. orem explicit relationship spectral density covariance function stochastic process sense] proposed model spectral density weighted mixture square-exponential functions, weights centres diagonal covariance matrices(?) ?   exp       ) Relying orem kernel spectral density(?) . ) spectral mixture kernel defined follows. function) integrable definition spectral Mixture) kernel positive-definite stationary kernel(? ) exp   cos(?¿ ) diag     due universal function approximation property mixtures Gaussians (considered frequency domain) relationship orem kernel approximate continuous stationary kernels arbitrary precision spectral components]. concept points direction sidestepping kernel selection problem GPs extended cater multivariate GPs Section  \\x0cmulti-output Gaussian Processes multivariate extension GPs constructed ensemble scalar-valued stochastic processes finite collection values processes jointly gaussian. formalise definition follows. definition -channel multi-output Gaussian process),   )), -tuple stochastic processes    (finite) subset inputs random variables jointly Gaussian choice indices) ,   }. recall construction scalar-valued GPs requires choosing scalar-valued function scalar-valued covariance function. conversely-channel MOGP defined -valued function, ith element denotes function ith channel, valued covariance function element denotes covariance ith channels. symmetry positive-definiteness conditions MOGP kernel defined follows. definition two-input matrix-valued function,   defined element-wise kij, multivariate kernel (covariance function) symmetric, ) Positive definite.,   that cpi  cpi cqj kij  ) furrmore, multivariate kernel, stationary equivalently kij, kij , }, case, denote   design MOGP covariance kernel involves jointly choosing functions model covariance channel (diagonal elements functions model crosscovariance channels input locations (off-diagonal elements). choosing covariance functions challenging expressive include, instance, delays, phase shifts, negative correlations enforce specific spectral content time maintaining positive definiteness reader referred] comprehensive review MOGP models. designing multi-output Gaussian Processes Fourier Domain extend spectral-mixture approach] multi-output Gaussian processes relying multivariate version orem proved cram referred cram orem, orem (cram orem) family {kij  integrable functions covariance functions weakly-stationary multivariate stochastic process) admit representation kij ??  sij? , ,   ) Sij integrable complex-valued function Sij spectral density covariance function kij ) fulfil positive definiteness condition Sij (?)          denotes complex conjugate note. ) states covariance function kij inverse Fourier transform spectral density Sij refore, functions Fourier pairs. accordingly, refer set arguments covariance function  time space Domain depending application considered, set arguments spectral densities  fourier spectral domain. furrmore, direct consequence orem element fourier domain, matrix defined(?) [sij  hermitian sij (?) (?) , orem guidelines construct covariance functions MOGP designing spectral densities instead., design performed Fourier rar space domain. simplicity design Fourier domain stems positive-definiteness condition spectral densities. ), easier achieve covariance functions. ). understood analogy univariate model: single-output case positive-definiteness condition kernel requires positivity spectral density, multioutput case positive-definiteness condition multivariate kernel requires matrix(?   positive definite constraints function Sij  sij (?).  multi-output Spectral Mixture kernel propose family Hermitian positive-definite complex-valued functions {sij  fulfilling requirements orem. ), cross-spectral densities mogp. family functions designed aim providing physical parametric interpretation closed-form covariance functions applying inverse Fourier transform. recall complex-valued positive-definite matrices decomposed form(?)   (? meaning entry(?) expressed Sij (?) (?)   (?) column(? (?) denotes Hermitian (transpose conjugate) operator. note factor decomposition fulfils. ) choice(?)   (?) (? ——         ) refer rank decomposition, choosing rank(?) (?)  ease notation \\x0cchoose2 columns(?) complex-valued functions(?) modeled rank-one matrix Sij (?)   (?). Fourier transforms multiplications square exponential) functions, model (?) complex-valued function ensure closed-form expression covariance kernel, (?) exp       exp ¿    )  diag     choice functions spectral densities {sij Sij (?) wij exp              ) extension arbitrary presented end section. meaning cross-spectral density channels modeled complex-valued function parameters: covariance mean magnitude: wij exp    delay  phase -constructed magnitudes wij ensure positive definiteness and, particular, autospectral densities Sii realvalued functions (since standard (scalarvalued) spectral mixture approach]. power spectral density. ) corresponds complex-valued kernel refore complex-valued] order restrict generative model real-valued gps, proposed power spectral density symmetric respect ], make Sij (?) symmetric simply reassigning Sij (?)  (sij (?) sij (?? )), equivalent choosing (?) vector mirrored complex functions. resulting (symmetric respect cross-spectral density ith channels Sij (?) real-valued kernel kij {sij (?)}(? fourier pairs wij (??   (??  )+?  )+?   Sij (?) kij  exp    cos ) magnitude parameter wij?)  absorbs constant resulting inverse Fourier transform. confirm autocovariances real-valued square-exponential cosine factors scalar approach  conversely, proposed model cross-covariance channels) negativelyand positively-correlated signals ) delayed channels delay parameter (iii) out-phase channels covariance symmetric respect delay fig. shows cross-spectral densities kernel choice delay phase parameters. cross Covariances cross-spectral Densities Figure Power spectral density kernels generated proposed model. ) parameters. bottom: cross-spectral densities, real part blue imaginary part green. top: cross-covariance functions blue reference envelope dashed line. left right: delay phase; delay non-zero phase; non-zero delay phase; non-zero delay non-zero phase. kernel. ) resulted low rank choice PSD matrix Sij refore, increasing rank proposed model Sij equivalent kernel components. arbitrarily choosing components yields expression proposed multivariate kernel: Definition multi-output Spectral Mixture kernel (mosm) form) kij  exp    cos  wij?)  superindex ) denotes parameter component spectral mixture. multivariate covariance function spectral-mixture positive-definite kernels autocovariances, cross-covariances spectral mixture functions parameters output pairs) non-positive-definite) non-symmetric, (iii) delayed respect anor. refore, MOSM kernel multi-output generalisation spectral mixture approach] positive definiteness guaranteed factor decomposition Sij shown. ).   training model computing predictive posterior Fitting model observed data rationale standard, maximising log-probability data. recall observations multioutput case consist) location ) channel identifier , }, (iii) observed refore, denote observations set-tuples observations jointly gaussian, concatenate observations vectors        ,       express negative loglikelihood (nll) log, log log —kxi) hyperparameters denoted Kxi covariance matrix observed samples element [kxi covariance process (location: channel: process (location: channel: recall that, proposed MOSM model, covariance [kxi. , kir ?i2r ,noise ?i2r ,noise diagonal term cater uncorrelated observation noise. nll minimised respect) ,noise, original parameters chosen construct(?) section, noise hyperparameters. hyperparameters optimised, computing predictive posterior proposed MOSM standard procedure joint covariances. ).  Related work Generalising scalar spectral mixture kernel MOGPs achieved LMC framework pointed] (denoted-lmc). formulation considers real-valued cross spectral densities, authors propose multivariate covariance function including complex component cross spectral densities cater phase differences channels, call Cross Spectral Mixture kernel (denoted csm). multivariate covariance function proposed MOSM model  , , consequence-lmc case proposed MOSM model, parameters restricted channels refore phase shifts delays allowed?unlike MOSM fig.  additionally, cram orem similar fashion] real-valued-student cross-spectral densities yielding cross-covariances eir positive-definite negative-definite.   experiments show sets experiments. first, validated ability proposed MOSM model identification autoand cross-covariances syntic data. second, compared MOSM spectral-mixture linear model coregionalization (smlmc]), Gaussian convolution model (conv]), crossspectral mixture model (csm]) estimation missing real-world data distributed settings: climate signals metal concentrations. models implemented Tensorﬂow] GPﬂow] order make automatic differentiation compute gradients nll. performance models experiments measured absolute error MAE ) denotes true MOGP estimate.  Syntic example: Learning derivatives delayed signals All models implemented recover autoand cross-covariances three-output components) reference signal sampled) , KSM spectral mixture covariance kernel KSM mean) derivative), (iii) delayed version ) ?). motivation illustrative covariances cross covariances aforementioned processes explicitly (see, sec. ]) refore compare estimates true model. derivative computed numerically (first order finite differences) training samples generated follows: chose 500 samples reference function interval], 400 samples derivative signal interval], 400 samples delayed signal interval]. samples randomly uniformly chosen intervals mentioned Gaussian noised added yield realistic observations. experiment consisted reconstruction reference signal interval], imputation derivative delayed signals interval]. fig. shows ground truth MOSM estimates syntic signals covariances (normalised), Table reports MAE models ten realisations experiment. notice proposed model successfully learnt cross-covariances cov )) cov),  )), autocovariances prior information delayed derivative relationship channels. furrmore, MOSM model successfully extrapolated derivate signal delayed signal simultaneously, due fact cross-covariances needed setting linear combinations univariate kernels, models based latent processes fail syntic example. syntic example: MOSM Reference Signal k11  Derivative Signal cov: Reference cov: Reference Derivative k22 k21  cov: Delayed   k31  Input k33  cov: Reference Delayed Delayed Signal cov: Derivative Figure MOSM learning covariance functions syntic reference signal, derivative delayed version. left: syntic signals, middle: \\x0cautocovariances, right: cross-covariances. dashed line ground truth, solid colour lines MOSM estimates, shaded area% confidence interval. training points shown green. table Reconstruction syntic signal, derivative delayed version: Mean absolute error models one-standard-deviation error bars ten realisations.  Model Reference Derivative Delayed CONV-lmc CSM MOSM.211 .085.166 .009.148 .010.127 .011.759 .075.747 .101.262 .032.223 .015.524 .097.398 .042.368 .089.146 .017 Climate data real-world dataset contained measurements3 sensor network climate stations south england: cambermet, chimet, Sotonmet bramblemet. considered normalised air temperature signal march, 2017 march, 2017-minute intervals (5692 samples), randomly chose 1000 samples training. ], simulated sensor failure removing half measurements sensor leaving remaining sensors operating correctly; reproduced setup sensors producing experiments. models considered latent signals/spectral components. models considered, fig. shows estimates missing data Cambermetfailure case. table shows absolute error models failure cases missing data region. observe models capture behaviour signal missing range, considered climate signals similar anor. shows MOSM collapse models share parameters pairs outputs required. temperature? ] MOSM CONV?lmc CSM time?[days]? figure Imputation Cambermet sensor measurements remaining sensors. red points denote observations, dashed black line true signal, solid colour lines predictive means. left right: mosm, conv-lmc csm. table Imputation climate sensor measurements remaining sensors. absolute error \\x0cfour experiments one-standard-deviation error bars ten realisations. model Cambermet Chimet Sotonmet Bramblemet CONV-lmc CSM MOSM.098 .008.084 .004.094 .003.097 .006.192 .015.176 .003.129 .004.137 .007.211 .038.273 .001.195 .011.162 .011.163 .009.134 .002.130 .004.129 .003 results show significant difference proposed order test statistical model latent processes based models. significance, kolmogorov-smirnov test. significance level , concluding Sotonmet sensor assure MOSM model yields results. conversely, cambermet, Chimet Bramblemet sensors, MOSM CSM provided similar results, confirm difference statistically significant. however, high correlation signals data obtained www.cambermet. sites rein. similarity MOSM model CSM model, close perfor mance models dataset expected.  Heavy metal concentration Jura dataset] contains, addition geological data, concentration heavy metals region km2 Swiss jura, divided training set (259 locations) validation set (100 locations). ], motivation aid prediction variable expensive measure abundant measurements correlated variables expensive acquire. specifically, estimated Cadmium Copper validation locations measurements related variables training test locations: Nickel Zinc cadmium; lead, Nickel Zinc copper. mae?see.  shown Table results CONV model obtained] models considered latent signals/spectral components, independent Gaussian process (denoted igp). observe proposed MOSM model outperforms models Cadmium data, statistical significant significance level . conversely, guarantee statistically-significant difference CSM model MOSM Copper case. cases, testing statistical significance CONV model results obtained]. hand, higher variability non-gaussianity Copper data reason simplest MOGP model-lmc) achieves results. table Mean absolute error estimation Cadmium Copper concentrations onestandard-deviation error bars ten repetitions experiment. model Cadmium Copper IGP CONV-lmc CSM MOSM .005.443 .006         Discussion proposed multioutput spectral mixture (mosm) kernel model rich relationships multiple outputs Gaussian processes regression models. achieved constructing positive-definite matrix complex-valued spectral densities, transforming inverse Fourier transform cram orem. resulting kernel clear interpretation spectral viewpoint, parameters identified frequency, magnitude, phase delay pair channels. furrmore, key feature unique proposed kernel ability joint model delays phase differences, due complex-valued model cross-spectral density considered validated experimentally syntic example?see fig.  mosm kernel compared existing MOGP models realworld datasets, proposed model performed competitively terms absolute error. furr research point sparse implementation proposed MOGP build] design inducing variables exploit spectral content processes]. acknowledgements crist?bal Silva (universidad chile) recommendations GPU implementation, Rasmus Bonnevie GPﬂow team assistance experimental MOGP module gpﬂow, anonymous reviewers. work financially supported Conicyt basal-cmm. extension Gaussian processes (gps]) multiple outputs referred multi-output Gaussian processes (mogps). mogps model temporal spatial relationships infinitelymany random variables, scalar gps, account statistical dependence sources data channels). this crucial number real-world applications fault detection, data imputation denoising. for input points covariance function-channel MOGP, symmetric positivedefinite matrix scalar covariance functions. design matrixvalued kernel challenging deal trade) choosing broad class  cross-covariances auto-covariances, time) ensuring positive definiteness symmetric matrix \\x0ccontaining covariance functions pair inputs particular, unlike widely families auto-covariance functions]), cross-covariances bound positive definite refore designed freely; construction functions interpretable functional form main focus article. classical approach define crosscovariances MOGP linearly combine independent latents gps, case Linear Model Coregionalization (lmc]) Convolution Model (conv]). cases, resulting kernel function covariance functions latent GPs parameters linear operator considered; results symmetric centred cross-covariances. while approaches simple, lack interpretability dependencies learnt force auto-covariances similar behaviour channels. LMC method inspired cross-spectral Mixture (csm) kernel], 31st Conference Neural Information Processing Systems (nips 2017), Long beach, usa. spectral Mixture) kernel] LMC model phase differences channels manually introducing shift cosine exponential factors kernel. despite exhibiting improved performance wrt previous approaches, addition shift parameter CSM poses question: Can spectral design multiouput covariance functions ﬂexible? approach extend spectral mixture concept multiple outputs: Recall stationary scalar-valued gps] designs power spectral density (psd) process mixture square exponential functions supported bochner orem], present Spectral Mixture kernel inverse Fourier transform-constructed psd. along lines, main contribution propose expressive family complex-valued square-exponential cross-spectral densities, build cram orem], multivariate extension bochner, construct multi-output Spectral Mixture kernel (mosm). proposed multivariate covariance function accounts properties cross-spectral Mixture kernel] delay component channels variable parameters auto-covariances channels. additionally, proposed MOSM clear interpretation parameters spectral terms. our experimental contribution includes illustrative trivariate syntic signal validation aforementioned literature real-world datasets. Background Definition Gaussian process) input set real-valued stochastic process finite subset inputs random variables jointly gaussian. without loss generality choose ] defines distribution functions) uniquely determined function)), typically assumed) covariance function (also kernel, cov )),  equip reader background follow proposal: review spectral-based \\x0capproach design scalar-valued covariance kernels present definition multi-output.  Spectral Mixture kernel bypass explicit construction positive-definite functions design stationary covariance kernels, design power spectral density (psd] transform covariance function inverse Fourier transform. this motivated fact strict positivity requirement PSD easier achieve positive definiteness requirement covariance kernel. oretical support construction orem: orem (bochner orem) integrable1 function covariance function weakly-stationary mean-square-continuous stochastic process admits representation(? ??  ? (?) non-negative bounded function denotes imaginary unit. for proof]. orem explicit relationship spectral density covariance function stochastic process sense] proposed model spectral density weighted mixture square-exponential functions, weights centres diagonal covariance matrices(?) ?   exp       ) Relying orem kernel spectral density(?) . ) spectral mixture kernel defined follows. function) integrable Definition Spectral Mixture) kernel positive-definite stationary kernel(? ) exp   cos(?¿ ) diag     due universal function approximation property mixtures Gaussians (considered frequency domain) relationship orem kernel approximate continuous stationary kernels arbitrary precision spectral components]. this concept points direction sidestepping kernel selection problem GPs extended cater multivariate GPs Section  \\x0cmulti-output Gaussian Processes multivariate extension GPs constructed ensemble scalar-valued stochastic processes finite collection values processes jointly gaussian. formalise definition follows. definition -channel multi-output Gaussian process),   )), -tuple stochastic processes    (finite) subset inputs random variables jointly Gaussian choice indices) ,   }. recall construction scalar-valued GPs requires choosing scalar-valued function scalar-valued covariance function. conversely-channel MOGP defined -valued function, ith element denotes function ith channel, valued covariance function element denotes covariance ith channels. symmetry positive-definiteness conditions MOGP kernel defined follows. definition two-input matrix-valued function,   defined element-wise kij, multivariate kernel (covariance function) symmetric, ) Positive definite.,   that cpi  cpi cqj kij  ) furrmore, multivariate kernel, stationary equivalently kij, kij , }, case, denote   design MOGP covariance kernel involves jointly choosing functions model covariance channel (diagonal elements functions model crosscovariance channels input locations (off-diagonal elements). choosing covariance functions challenging expressive include, instance, delays, phase shifts, negative correlations enforce specific spectral content time maintaining positive definiteness reader referred] comprehensive review MOGP models. Designing multi-output Gaussian Processes Fourier Domain extend spectral-mixture approach] multi-output Gaussian processes relying multivariate version orem proved cram referred cram orem, orem (cram orem) family {kij  integrable functions covariance functions weakly-stationary multivariate stochastic process) admit representation kij ??  sij? , ,   ) Sij integrable complex-valued function Sij spectral density covariance function kij ) fulfil positive definiteness condition Sij (?)          denotes complex conjugate Note. ) states covariance function kij inverse Fourier transform spectral density Sij refore, functions Fourier pairs. accordingly, refer set arguments covariance function  time space Domain depending application considered, set arguments spectral densities  Fourier spectral domain. furrmore, direct consequence orem element Fourier domain, matrix defined(?) [sij  hermitian Sij (?) (?) , orem guidelines construct covariance functions MOGP designing spectral densities instead., design performed Fourier rar space domain. simplicity design Fourier domain stems positive-definiteness condition spectral densities. ), easier achieve covariance functions. ). this understood analogy univariate model: single-output case positive-definiteness condition kernel requires positivity spectral density, multioutput case positive-definiteness condition multivariate kernel requires matrix(?   positive definite constraints function Sij  sij (?).  multi-output Spectral Mixture kernel propose family Hermitian positive-definite complex-valued functions {sij  fulfilling requirements orem. ), cross-spectral densities mogp. this family functions designed aim providing physical parametric interpretation closed-form covariance functions applying inverse Fourier transform. recall complex-valued positive-definite matrices decomposed form(?)   (? meaning entry(?) expressed Sij (?) (?)   (?) column(? (?) denotes Hermitian (transpose conjugate) operator. note factor decomposition fulfils. ) choice(?)   (?) (? ——         ) refer rank decomposition, choosing rank(?) (?)  for ease notation \\x0cchoose2 columns(?) complex-valued functions(?) modeled rank-one matrix Sij (?)   (?). since Fourier transforms multiplications square exponential) functions, model (?) complex-valued function ensure closed-form expression covariance kernel, (?) exp       exp ¿    )  diag     with choice functions spectral densities {sij Sij (?) wij exp              ) extension arbitrary presented end section. meaning cross-spectral density channels modeled complex-valued function parameters: covariance mean magnitude: wij exp    delay  phase -constructed magnitudes wij ensure positive definiteness and, particular, autospectral densities Sii realvalued functions (since standard (scalarvalued) spectral mixture approach]. power spectral density. ) corresponds complex-valued kernel refore complex-valued] order restrict generative model real-valued gps, proposed power spectral density symmetric respect ], make Sij (?) symmetric simply reassigning Sij (?)  (sij (?) Sij (?? )), equivalent choosing (?) vector mirrored complex functions. resulting (symmetric respect cross-spectral density ith channels Sij (?) real-valued kernel kij {sij (?)}(? Fourier pairs wij (??   (??  )+?  )+?   Sij (?) kij  exp    cos ) magnitude parameter wij?)  absorbs constant resulting inverse Fourier transform. confirm autocovariances real-valued square-exponential cosine factors scalar approach  conversely, proposed model cross-covariance channels) negativelyand positively-correlated signals ) delayed channels delay parameter (iii) out-phase channels covariance symmetric respect delay fig. shows cross-spectral densities kernel choice delay phase parameters. cross Covariances cross-spectral Densities Figure Power spectral density kernels generated proposed model. ) parameters. bottom: cross-spectral densities, real part blue imaginary part green. top: cross-covariance functions blue reference envelope dashed line. from left right: delay phase; delay non-zero phase; non-zero delay phase; non-zero delay non-zero phase. kernel. ) resulted low rank choice PSD matrix Sij refore, increasing rank proposed model Sij equivalent kernel components. arbitrarily choosing components yields expression proposed multivariate kernel: Definition multi-output Spectral Mixture kernel (mosm) form) kij  exp    cos  wij?)  superindex ) denotes parameter component spectral mixture. this multivariate covariance function spectral-mixture positive-definite kernels autocovariances, cross-covariances spectral mixture functions parameters output pairs) non-positive-definite) non-symmetric, (iii) delayed respect anor. refore, MOSM kernel multi-output generalisation spectral mixture approach] positive definiteness guaranteed factor decomposition Sij shown. ).   training model computing predictive posterior Fitting model observed data rationale standard, maximising log-probability data. recall observations multioutput case consist) location ) channel identifier , }, (iii) observed refore, denote observations set-tuples observations jointly gaussian, concatenate observations vectors        ,       express negative loglikelihood (nll) log, log log —kxi) hyperparameters denoted Kxi covariance matrix observed samples element [kxi covariance process (location: channel: process (location: channel: recall that, proposed MOSM model, covariance [kxi. , kir ?i2r ,noise ?i2r ,noise diagonal term cater uncorrelated observation noise. NLL minimised respect) ,noise, original parameters chosen construct(?) Section, noise hyperparameters. once hyperparameters optimised, computing predictive posterior proposed MOSM standard procedure joint covariances. ).  Related work Generalising scalar spectral mixture kernel MOGPs achieved LMC framework pointed] (denoted-lmc). formulation considers real-valued cross spectral densities, authors propose multivariate covariance function including complex component cross spectral densities cater phase differences channels, call Cross Spectral Mixture kernel (denoted csm). this multivariate covariance function proposed MOSM model  , , consequence-lmc case proposed MOSM model, parameters restricted channels refore phase shifts delays allowed?unlike MOSM fig.  additionally, cram orem similar fashion] real-valued-student cross-spectral densities yielding cross-covariances eir positive-definite negative-definite.   Experiments show sets experiments. first, validated ability proposed MOSM model identification autoand cross-covariances syntic data. second, compared MOSM spectral-mixture linear model coregionalization (smlmc]), Gaussian convolution model (conv]), crossspectral mixture model (csm]) estimation missing real-world data distributed settings: climate signals metal concentrations. all models implemented Tensorﬂow] GPﬂow] order make automatic differentiation compute gradients nll. performance models experiments measured absolute error MAE ) denotes true MOGP estimate.  Syntic example: Learning derivatives delayed signals All models implemented recover autoand cross-covariances three-output components) reference signal sampled) , KSM spectral mixture covariance kernel KSM mean) derivative), (iii) delayed version ) ?). motivation illustrative covariances cross covariances aforementioned processes explicitly (see, sec. ]) refore compare estimates true model. derivative computed numerically (first order finite differences) training samples generated follows: chose 500 samples reference function interval], 400 samples derivative signal interval], 400 samples delayed signal interval]. all samples randomly uniformly chosen intervals mentioned Gaussian noised added yield realistic observations. experiment consisted reconstruction reference signal interval], imputation derivative delayed signals interval]. fig. shows ground truth MOSM estimates syntic signals covariances (normalised), Table reports MAE models ten realisations experiment. notice proposed model successfully learnt cross-covariances cov )) cov),  )), autocovariances prior information delayed derivative relationship channels. furrmore, MOSM model successfully extrapolated derivate signal delayed signal simultaneously, due fact cross-covariances needed setting linear combinations univariate kernels, models based latent processes fail syntic example. syntic example: MOSM Reference Signal k11  Derivative Signal cov: Reference cov: Reference Derivative k22 k21  cov: Delayed   k31  Input k33  cov: Reference Delayed Delayed Signal cov: Derivative Figure MOSM learning covariance functions syntic reference signal, derivative delayed version. left: syntic signals, middle: \\x0cautocovariances, right: cross-covariances. dashed line ground truth, solid colour lines MOSM estimates, shaded area% confidence interval. training points shown green. Table Reconstruction syntic signal, derivative delayed version: Mean absolute error models one-standard-deviation error bars ten realisations.  Model Reference Derivative Delayed CONV-lmc CSM MOSM.211 .085.166 .009.148 .010.127 .011.759 .075.747 .101.262 .032.223 .015.524 .097.398 .042.368 .089.146 .017 Climate data real-world dataset contained measurements3 sensor network climate stations south england: cambermet, chimet, Sotonmet bramblemet. considered normalised air temperature signal march, 2017 march, 2017-minute intervals (5692 samples), randomly chose 1000 samples training. following], simulated sensor failure removing half measurements sensor leaving remaining sensors operating correctly; reproduced setup sensors producing experiments. all models considered latent signals/spectral components. for models considered, fig. shows estimates missing data Cambermetfailure case. table shows absolute error models failure cases missing data region. observe models capture behaviour signal missing range, considered climate signals similar anor. this shows MOSM collapse models share parameters pairs outputs required. temperature? ] MOSM CONV?lmc CSM time?[days]? figure Imputation Cambermet sensor measurements remaining sensors. red points denote observations, dashed black line true signal, solid colour lines predictive means. from left right: mosm, conv-lmc csm. table Imputation climate sensor measurements remaining sensors. mean absolute error \\x0cfour experiments one-standard-deviation error bars ten realisations. model Cambermet Chimet Sotonmet Bramblemet CONV-lmc CSM MOSM.098 .008.084 .004.094 .003.097 .006.192 .015.176 .003.129 .004.137 .007.211 .038.273 .001.195 .011.162 .011.163 .009.134 .002.130 .004.129 .003 results show significant difference proposed order test statistical model latent processes based models. significance, kolmogorov-smirnov test. significance level , concluding Sotonmet sensor assure MOSM model yields results. conversely, cambermet, Chimet Bramblemet sensors, MOSM CSM provided similar results, confirm difference statistically significant. however, high correlation signals data obtained www.cambermet. sites rein. similarity MOSM model CSM model, close perfor mance models dataset expected.  Heavy metal concentration Jura dataset] contains, addition geological data, concentration heavy metals region km2 Swiss jura, divided training set (259 locations) validation set (100 locations). ], motivation aid prediction variable expensive measure abundant measurements correlated variables expensive acquire. specifically, estimated Cadmium Copper validation locations measurements related variables training test locations: Nickel Zinc cadmium; lead, Nickel Zinc copper. mae?see.  shown Table results CONV model obtained] models considered latent signals/spectral components, independent Gaussian process (denoted igp). observe proposed MOSM model outperforms models Cadmium data, statistical significant significance level . conversely, guarantee statistically-significant difference CSM model MOSM Copper case. cases, testing statistical significance CONV model results obtained]. hand, higher variability non-gaussianity Copper data reason simplest MOGP model-lmc) achieves results. table Mean absolute error estimation Cadmium Copper concentrations onestandard-deviation error bars ten repetitions experiment. Model Cadmium Copper IGP CONV-lmc CSM MOSM .005.443 .006         Discussion proposed multioutput spectral mixture (mosm) kernel model rich relationships multiple outputs Gaussian processes regression models. this achieved constructing positive-definite matrix complex-valued spectral densities, transforming inverse Fourier transform cram orem. resulting kernel clear interpretation spectral viewpoint, parameters identified frequency, magnitude, phase delay pair channels. furrmore, key feature unique proposed kernel ability joint model delays phase differences, due complex-valued model cross-spectral density considered validated experimentally syntic example?see fig.  MOSM kernel compared existing MOGP models realworld datasets, proposed model performed competitively terms absolute error. furr research point sparse implementation proposed MOGP build] design inducing variables exploit spectral content processes]. acknowledgements crist?bal Silva (universidad chile) recommendations GPU implementation, Rasmus Bonnevie GPﬂow team assistance experimental MOGP module gpﬂow, anonymous reviewers. this work financially supported Conicyt basal-cmm.',\n",
       " 'PP7248': 'due ability capture long-term dependencies, autoregressive models recurrent neural networks (rnn) generative models choice dealing sequential data. leveraging weight sharing timesteps, model variable length sequences fixed parameter space. rnn dynamics involve hidden state updated timestep summarize information previously sequence. hidden state current timestep, network predicts desired output, cases corresponds input sequence. due deterministic evolution hidden state, RNNs capture entropy observed sequences shaping conditional output distributions step, simple parametric form. unimodal mixtures unimodal. insuﬃcient highly structured natural sequences, correlation output variables step. simultaneities (boulanger-lewandowski., 2012), complex dependencies variables timesteps. long-term dependencies. reasons, recent efforts recur highly multi-modal output distribution augmenting RNN stochastic latent variables trained amortised variational inference, variational auto-encoding framework (vae) (kingma welling, 2014; Fraccaro., 2016; Kingma welling, 2014). vae framework eﬃcient approximate inference parametrizing approximate posterior generative model neural networks trainable end-end backpropagation. 31st Conference Neural Information Processing Systems (nips 2017), Long beach, usa. anor motivation including stochastic latent variables autoregressive models infer, observed variables sequence. pixels sound-waves), higher-level abstractions. objects speakers). disentangling factors variations appealing increase high-level control generation, ease semi-supervised transfer learning, enhance interpretability trained model (kingma., 2014., 2017). stochastic recurrent models proposed literature vary stochastic variables perform output prediction parametrize posterior approximation variational inference. paper, propose stochastic recurrent generative model incorporates single framework successful techniques earlier models. associate latent variable timestep generation process. similar Fraccaro. (2016), (deterministic) RNN runs backwards sequence form approximate posterior, allowing capture future sequence. however, akin Chung. (2015); Bayer Osendorfer (2014), latent variables condition recurrent dynamics future steps, injecting highlevel decisions upcoming elements output sequence. architectural choices motivated interpreting latent variables encoding ?plan? future sequence. latent plan injected recurrent dynamics order shape distribution future hidden states. show mixing stochastic forward pass, conditional prior backward recognition network helps building effective stochastic recurrent models. recent surge generative models suggests extracting meaningful latent representations diﬃcult powerful autoregressive decoder. captures entropy data distribution (bowman., 2015; Kingma., 2016; Chen., 2017; Gulrajani., 2017). show auxiliary, task-agnostic loss, ease training latent variables which, turn, \\x0chelps achieving higher performance tasks hand. latent variables model forced information predicting state backward encoder. predicting future information sequence. work contributions: unify successful architectural choices generative stochastic model sequences: backward posterior, conditional prior latent variables condition hidden dynamics network. model achieves state--art speech modeling.  propose simple improving model performance providing latent variables auxiliary, task-agnostic objective. explored tasks, auxiliary cost yielded performance strategies annealing. finally, show auxiliary signal helps model learn interpretable representations language modeling task. background operate well-known VAE framework (kingma, 2014; Burda., 2015; Rezende mohamed, 2015), neural network based approach training generative latent variable models. observation random variable, taking values assume generation involves latent variable taking values means joint density ), parametrized set observed datapoints   goal maximum likelihood estimation (mle) estimate parameters maximize marginal log-likelihood(? ): arg max? (? log   Optimizing marginal log-likelihood intractable, due integration latent variables. common approach maximize variational lower bound marginal loglikelihood. evidence lower bound (elbo) obtained introducing approximate posterior ) yielding: , log ) log log) dkl ; ) ) ) denotes kullback-leibler divergence. elbo appealing bound tight approximate posterior matches true posterior. reduces) STORN) VRNN) SRNN) Our model Figure Computation graph generative models sequences latent variables: STORN (bayer osendorfer, 2014), VRNN (chung., 2015), SRNN (fraccaro., 2016) model. picture, task generative model consists predicting observation sequence, previous ones. diamonds represent deterministic states, latent variables sequence input step Dashed lines represent computation part inference model. double lines auxiliary predictions implied proposed auxiliary cost. differently VRNN srnn, STORN model latent variable participates prediction step marginal loglikelihood. elbo rewritten minimum description length loss function (honkela valpola, 2004; log ) dkl ) ) ) term measures degree dependence. dkl ) ) independent usually, parameters generative model ), prior ) inference model ) computed neural networks. case, ELBO maximized gradient ascent Monte Carlo approximation expectation. simple parametric forms . multivariate diagonal Gaussian, generally, reparamatrizable distributions (kingma welling, 2014), backpropagate sampling process  ) applying reparametrization trick, simulates sampling ) sampling fixed distribution (), applying deterministic transformation , makes approach appealing comparison approximate inference approaches. order generative model overall, efforts put augmenting capacity approximate posteriors (rezende mohamed, 2015; Kingma., 2016; Louizos welling, 2017), prior distribution (chen., 2017; Serban., 2017a) decoder (gulrajani., 2017; Oord., 2016). powerful decoders ), model complex distributions idea explored \\x0cwhile applying VAEs sequences   decoding distribution ) modeled autoregressive model, )  (bayer osendorfer, 2014; Chung., 2015; Fraccaro., 2016). models, typically decomposes sequence latent variables   yielding )  operate setting and, section, present choices parametrizing generative model, prior inference model. proposed Approach Figure report dependencies inference generative parts model, compared existing models. broad perspective, backward recurrent network approximate posterior (akin SRNN (fraccaro., 2016)), condition recurrent state forward auto-regressive model stochastic variables conditional prior (akin VRNN (chung., 2015), STORN (bayer osendorfer, 2014)). order make latent variables, auxiliary costs (double arrows) force latent variables encode information future. following, describe components.  Generative Model Decoder Given sequence observations   desired set labels predictions   assume exists set stochastic latent variables    following, loss generality, suppose set predictions corresponds shifted version input sequence. model predict observation previous ones, common setting language speech modeling (fraccaro., 2016; Chung., 2015). generative model couples observations latent variables autoregressive model. exploiting LSTM architecture (hochreiter schmidhuber, 1997), runs sequence:   ) parameters conditional probability distribution observation  computed multi-layered feed-forward network conditions case continuous-valued observations) output log parameters Gaussian distribution, categorical proportions case one-hot predictions. note that) simple unimodal distribution, marginal distribution  highly multimodal, due integration sequence latent variables note) condition. directly computation output conditional probabilities. observed performance avoiding latent variables directly producing output. prior parameters prior distribution  latent variable obtained non-linear transformation previous hidden state forward network. common choice VAE framework Gaussian latent variables. refore) produces parameters diagonal multivariate Gaussian distribution)  log ) This type conditional prior proven previous work (chung., 2015).   inference Model inference model responsible approximating true posterior) order provide tractable latent variables lower-bound log-likelihood. posterior approximation LSTM processing sequence backwards:   ) Each state information future sequence shape approximate posterior latent forward LSTM condition future predictions, latent variable directly inform recurrent dynamics future states, acting ?plan? future sequence. information channeled posterior distribution feed-forward neural network) taking input previous forward state backward state)  log ) injecting stochasticity hidden state forward recurrent model, true posterior distribution variable depends variables dependence order formulate eﬃcient posterior approximation, drop dependence cost introducing intrinsic bias posterior approximation. exclude true posterior space functions modelled function approximator. contrast SRNN (fraccaro., 2016), posterior distribution factorizes tractable manner cost including latent variables forward autoregressive dynamics. latent variables don condition hidden state, shaping multi-modal distribution current prediction.  Auxiliary Cost domains, text images, empirically observed diﬃcult make latent variables coupled strong autoregressive decoder (bowman., 2015; Gulrajani., 2017; Chen., 2017). diﬃculty learning meaningful latent variables, cases interest, related fact abstractions underlying observed data encoded smaller number bits observed variables. example, multiple ways picturing ?cat? . poses, colors lightning) varying abstract properties concept ?cat?. cases, maximum-likelihood training objective \\x0cmay sensitive abstractions encoded, causing latent variables ?shut off. local correlations pixel level strong bias learning process finding parameter solutions latent variables unused. cases, posterior approximation provide weak noisy signal, due variance induced stochastic gradient approximation. result, decoder learn ignore rely solely autoregressive properties causing independent. term. vanishes. recent solutions problem generally propose reduce capacity autoregressive decoder (bowman., 2015; bachman, 2016; Chen., 2017; Semeniuta., 2017). constraints decoder capacity inherently bias learning finding parameter solutions dependent. shortcomings approach that, general, hard achieve desired solutions architecture search. instead, investigate wher expressiveness autoregressive decoder force latent variables encode information adding auxiliary training signal latent variables alone. practice, results show auxiliary cost, albeit simple, helps achieving performance objective interest. specifically, training additional conditional generative model backward states   forward states )  ? ) [log ) log ) log )]. additional model trained amortized variational inference. however, share prior ) approximate posterior , ?primary? model deterministic function. approximate posterior conditioned). practice, solely learn additional parameters decoding model )  auxiliary reconstruction model trains relevant information future sequence contained hidden state backward network)  log) means auxiliary reconstruction cost, approximate posterior prior primary model trained additional signal escaping local minima due short term reconstructions appearing lower bound, similarly recently noted Karl. (2016).  Learning training objective regularized version lower-bound data log-likelihood based variational free-energy, regularization imposed auxiliary cost; log  log  ) ?dkl   learn parameters model backpropagation time (rumelhart., 1988) \\x0capproximate expectation sample posterior ) reparametrization. optimizing. disconnect gradients auxiliary prediction affecting backward network. don gradients log  train parameters approximate posterior: intuitively, backward network agnostic auxiliary task assigned latent variables. performed empirically. approximate posterior trained gradient ﬂowing elbo, backward states receiving weak training signal early training, hamper usefulness auxiliary generative cost. backward states concentrated vector. refore, additionally train backward network predict output variables reverse (see Figure; log  log  log  ) ?dkl    Connection previous models Our model similar previous stochastic recurrent models: similarly STORN (bayer osendorfer, 2014) VRNN (chung., 2015) latent variables provided input autoregressive decoder. differently storn, conditional prior parametrization proposed Chung. (2015). however, generation process VRNN differs approach. vrnn, directly used produce output found model performed relieved latent variables producing output. vrnn ?myopic? posterior latent variables informed future sequence. srnn (fraccaro., 2016) addresses issue running posterior backward sequence providing future context current prediction. however, autoregressive decoder informed future sequence latent variables. efforts made order bias learning process parameter solutions latent variables (bowman., 2015; Karl., 2016; Kingma., 2016; Chen., 2017; Zhao., 2017). bowman. (2015) tackle problem language modeling setting dropping words input random order weaken autoregressive decoder annealing divergence term training. achieve similar latent interpolations auxiliary cost. similarly, Chen. (2017) propose restrict receptive field pixel-level decoder image generation tasks. kingma. (2016) propose reserve free bits divergence. parallel work, idea task-agnostic loss latent variables considered (zhao., 2017). authors force latent variables predict bag-words representation dialog utterance. instead, work sequential setting, latent variable timestep sequence. experiments section, evaluate proposed model diverse modeling tasks \\x0c(speech, images text). show model achieve state-art results speech modeling datasets: Blizzard (king karaiskos, 2013) TIMIT raw audio datasets (also Chung. (2015)). approach competitive results sequential generation MNIST (salakhutdinov murray, 2008). text, show auxiliary cost helps latent variables capture information latent structure language. sequence length, sentiment). experiments, ADAM optimizer (kingma, 2014).  Speech Modeling Sequential MNIST Blizzard TIMIT test model speech modeling datasets. blizzard consists 300 hours english, spoken single female speaker. timit widely speech recognition consists 6300 English sentences read 630 speakers. train model directly raw sequences represented sequence 200 real-valued amplitudes normalized global standard deviation training set. adopt train, validation test split Chung. (2015). blizzard, report average log-likelihood half-second sequences (fraccaro., 2016), TIMIT report average log-likelihood sequences test set. setting, models fully factorized multivariate Gaussian distribution output distribution timestep. order model comparable state--art, number parameters comparable SRNN (fraccaro., 2016). forward/backward networks LSTMs 2048 recurrent units Blizzard 1024 recurrent units timit. dimensionality Gaussian latent variables 256. prior) inference) auxiliary networks) single hidden layer, 1024 units Blizzard 512 units timit, leaky rectified nonlinearities leakiness clipped (fraccaro., 2016). blizzard, learning rate.0003 batch size 128, TIMIT Model Blizzard TIMIT rnn-gauss rnn-gmm vrnn-gauss vrnn-gauss vrnn-gmm SRNN (smooth+resq 3539 7413 8933 9223 9392 11991 -1900 26643 28340 28805 28982 60550 Ours Ours kla 14435 14226 68132 68903 Ours aux Ours kla, aux 15430 15024 Models DBN 2hl (germain., 2015) NADE (uria., 2016) EoNADE5 2hl (raiko., 2014) DLGM (salimans., 2014) DARN 1hl (gregor., 2015) DRAW (gregor., 2015) PixelVAE (gulrajani., 2016-forcing-layer) (goyal., 2016) pixelrnn-layer) (oord., 2016) pixelrnn-layer) (oord., 2016) MatNets (bachman, 2016) 69530 70469 ours layer) Ours aux layer) MNIST     .02h.58h.20h.50h   Table left, report average log-likelihood sequence test sets Blizzard TIMIT datasets. ?kla? ?aux? denote annealing proposed auxiliary costs. right, report test set negative log-likelihood sequential mnist, denotes lower performance model respect baselines. mnist, observed annealing hurts performance. .001 respectively. previous work reliably anneal term ELBO temperature weight training annealing) (fraccaro., 2016; Chung., 2015). report results obtained model training annealing without. annealing used, temperature linearly annealed update increments.00005 (fraccaro., 2016). show results Table (left), results obtained models comparable size srnn. similar (fraccaro., 2016; Chung., 2015), report conservative evidence lower bound log-likelihood. blizzard, annealing strategy (ours kla) effective training iterations, eventually converges slightly lower log-likelihood model trained annealing (ours). explored annealing strategies didn observe improvements performance. models trained proposed auxiliary cost outperform models trained annealing strategy datasets. timit, appears slightly synergistic effect annealing auxiliary cost. explicitly reported table, similar performance gains observed training sets. sequential MNIST task consists pixel-pixel generation binarized MNIST digits. standard binarized MNIST dataset Larochelle Murray (2011). forward backward networks LSTMs layer 1024 hidden units. learning rate.001 batch size. report results Table (right). setting, observed annealing hurt performance model. architecturally ﬂat, model competitive respect strong baselines. draw (gregor., 2015), outperformed deeper version autoregressive models latent variables. pixelvae (gated) (gulrajani., 2016), deep autoregressive models PixelRNN (oord., 2016) MatNets (bachman, 2016).  Language modeling well-known result language modeling tasks generative model fit observed data storing information latent variables. divergence term ELBO (bowman., 2015; Zhao., 2017; Serban., 2017b). test proposed stochastic recurrent model trained auxiliary cost medium-sized IMDB text corpus 350k movie reviews (diao., 2014). setting. (2017), sentences \\x0cwords fixed vocabulary size 16k words. split dataset train/valid/test sets ratios respectively%. special delimiter tokens added beginning end sentence learned 3000 3500 2500 3000 2000 1500 kla aux kla, aux 1000 500 Updates TIMIT 4000 (nats) (nats) Blizzard 2500 2000 1500 kla aux kla, aux 1000 500 1e4 Updates 1e4 Figure Evolution divergence term (measured nats) ELBO auxiliary cost training Blizzard (left) TIMIT (right). plot curves models performed hyperparameter annealing auxiliary cost weights) selection validation set. auxiliary cost puts pressure latent variables resulting higher divergence. models trained auxiliary cost (ours aux) exhibit stable evolution divergence. models trained auxiliary cost achieve performance annealing (ours kla) similar, performance blizzard, compared annealing auxiliary cost (ours kla, aux). model Ours Ours aux Ours aux .0025.005 Valid \\x0ctest ELBO IWAE ELBO IWAE Table IMDB language modeling results models trained maximizing standard evidence lower-bound. report word perplexity evaluated ELBO IWAE bound divergence approximate posterior prior distribution, values auxiliary cost hyperparameters gap perplexity ELBO IWAE (evaluated samples) increases greater divergence values. generate end sentence token. single layered LSTM 500 hidden recurrent units, fix dimensionality word embeddings 300 dimensional latent variables. (?) networks single-layered 500 hidden units leaky relu activations. learning rate.001 batch size. results shown Table expected, hard obtain perplexity baseline model latent variables language models. found IWAE (importance Weighted autoencoder) (burda., 2015) bound gave great improvements perplexity. observation highlights fact that, text domain, ELBO severely underestimating likelihood model: approximate posterior loosely match true posterior IWAE bound correct mismatch tightening posterior approximation. iwae bound interpreted standard VAE lower bound implicit posterior distribution (bachman precup, 2015). basis observation, attempted training models IWAE bound, observed noticeable improvement validation perplexity. analyze wher latent variables capture characteristics language interpolating latent space (bowman., 2015). sentence, infer latent variables step running approximate posterior concatenate order form contiguous latent encoding input sentence. perform linear interpolation latent space latent encodings sentences. step interpolation, latent encoding run decoder network generate sentence. show results Table movie terrible watch Argmax Sampling movie work movie work powerful story movie life movie work powerful story powerful piece film dark part film dark movie great ending  dark movie great message dark great dark movie great film classic give felt movie film acting good acting good acting great acting good give kids acting pretty good story great thing film funny movie great performances acting good story interesting movie great watch ) violence  Argmax Sampling darkness      watchable characters likable characters likable characters fun characters house characters house darkness  screenplay rating ****  film violence give movie chance children actors excellent lot things describe title movie funny lot fun movie table Results linear interpolation latent space. left column reports greedy argmax decoding obtained selecting, step decoding, word maximum probability model distribution, column reports random samples model. interpolation parameter. general, latent variables capture length sentences. conclusion paper, proposed recurrent stochastic generative model builds recent architectures latent variables condition recurrent dynamics network. augmented inference network recurrent network runs backward input sequence added auxiliary cost forces latent variables reconstruct state backward network, explicitly encoding summary future observations. model achieves state--art results standard speech benchmarks TIMIT blizzard. proposed auxiliary cost, albeit simple, appears promote latent variables effectively compared similar strategies annealing. future work, interesting multitask learning setting. sentiment analysis., 2017). also, interesting incorporate proposed approach powerful autogressive models. pixelrnn/pixelcnn (oord., 2016). acknowledgments authors Phil bachman, Alex Lamb Adam Trischler discussions. nserc, cifar, google, samsung, IBM Canada Research Chairs funding, Compute Canada NVIDIA computing resources. authors express debt gratitude contributed ano years longer maintained), making great tool. Due ability capture long-term dependencies, autoregressive models recurrent neural networks (rnn) generative models choice dealing sequential data. leveraging weight sharing timesteps, model variable length sequences fixed parameter space. rnn dynamics involve hidden state updated timestep summarize information previously sequence. given hidden state current timestep, network predicts desired output, cases corresponds input sequence. due deterministic evolution hidden state, RNNs capture entropy observed sequences shaping conditional output distributions step, simple parametric form. unimodal mixtures unimodal. this insuﬃcient highly structured natural sequences, correlation output variables step. simultaneities (boulanger-lewandowski., 2012), complex dependencies variables timesteps. long-term dependencies. for reasons, recent efforts recur highly multi-modal output distribution augmenting RNN stochastic latent variables trained amortised variational inference, variational auto-encoding framework (vae) (kingma welling, 2014; Fraccaro., 2016; Kingma welling, 2014). VAE framework eﬃcient approximate inference parametrizing approximate posterior generative model neural networks trainable end-end backpropagation. 31st Conference Neural Information Processing Systems (nips 2017), Long beach, usa. anor motivation including stochastic latent variables autoregressive models infer, observed variables sequence. pixels sound-waves), higher-level abstractions. objects speakers). disentangling factors variations appealing increase high-level control generation, ease semi-supervised transfer learning, enhance interpretability trained model (kingma., 2014., 2017). stochastic recurrent models proposed literature vary stochastic variables perform output prediction parametrize posterior approximation variational inference. paper, propose stochastic recurrent generative model incorporates single framework successful techniques earlier models. associate latent variable timestep generation process. similar Fraccaro. (2016), (deterministic) RNN runs backwards sequence form approximate posterior, allowing capture future sequence. however, akin Chung. (2015); Bayer Osendorfer (2014), latent variables condition recurrent dynamics future steps, injecting highlevel decisions upcoming elements output sequence. our architectural choices motivated interpreting latent variables encoding ?plan? future sequence. latent plan injected recurrent dynamics order shape distribution future hidden states. show mixing stochastic forward pass, conditional prior backward recognition network helps building effective stochastic recurrent models. recent surge generative models suggests extracting meaningful latent representations diﬃcult powerful autoregressive decoder. captures entropy data distribution (bowman., 2015; Kingma., 2016; Chen., 2017; Gulrajani., 2017). show auxiliary, task-agnostic loss, ease training latent variables which, turn, \\x0chelps achieving higher performance tasks hand. latent variables model forced information predicting state backward encoder. predicting future information sequence. our work contributions: unify successful architectural choices generative stochastic model sequences: backward posterior, conditional prior latent variables condition hidden dynamics network. our model achieves state--art speech modeling.  propose simple improving model performance providing latent variables auxiliary, task-agnostic objective. explored tasks, auxiliary cost yielded performance strategies annealing. finally, show auxiliary signal helps model learn interpretable representations language modeling task. Background operate well-known VAE framework (kingma, 2014; Burda., 2015; Rezende mohamed, 2015), neural network based approach training generative latent variable models. let observation random variable, taking values assume generation involves latent variable taking values means joint density ), parametrized given set observed datapoints   goal maximum likelihood estimation (mle) estimate parameters maximize marginal log-likelihood(? ): arg max? (? log   Optimizing marginal log-likelihood intractable, due integration latent variables. common approach maximize variational lower bound marginal loglikelihood. evidence lower bound (elbo) obtained introducing approximate posterior ) yielding: , log ) log log) dkl ; ) ) ) denotes kullback-leibler divergence. ELBO appealing bound tight approximate posterior matches true posterior. reduces) STORN) VRNN) SRNN) Our model Figure Computation graph generative models sequences latent variables: STORN (bayer osendorfer, 2014), VRNN (chung., 2015), SRNN (fraccaro., 2016) model. picture, task generative model consists predicting observation sequence, previous ones. diamonds represent deterministic states, latent variables sequence input step Dashed lines represent computation part inference model. double lines auxiliary predictions implied proposed auxiliary cost. differently VRNN srnn, STORN model latent variable participates prediction step marginal loglikelihood. ELBO rewritten minimum description length loss function (honkela valpola, 2004; log ) dkl ) ) ) term measures degree dependence. DKL ) ) independent usually, parameters generative model ), prior ) inference model ) computed neural networks. case, ELBO maximized gradient ascent Monte Carlo approximation expectation. for simple parametric forms . multivariate diagonal Gaussian, generally, reparamatrizable distributions (kingma welling, 2014), backpropagate sampling process  ) applying reparametrization trick, simulates sampling ) sampling fixed distribution (), applying deterministic transformation , this makes approach appealing comparison approximate inference approaches. order generative model overall, efforts put augmenting capacity approximate posteriors (rezende mohamed, 2015; Kingma., 2016; Louizos welling, 2017), prior distribution (chen., 2017; Serban., 2017a) decoder (gulrajani., 2017; Oord., 2016). powerful decoders ), model complex distributions this idea explored \\x0cwhile applying VAEs sequences   decoding distribution ) modeled autoregressive model, )  (bayer osendorfer, 2014; Chung., 2015; Fraccaro., 2016). models, typically decomposes sequence latent variables   yielding )  operate setting and, section, present choices parametrizing generative model, prior inference model. Proposed Approach Figure report dependencies inference generative parts model, compared existing models. from broad perspective, backward recurrent network approximate posterior (akin SRNN (fraccaro., 2016)), condition recurrent state forward auto-regressive model stochastic variables conditional prior (akin VRNN (chung., 2015), STORN (bayer osendorfer, 2014)). order make latent variables, auxiliary costs (double arrows) force latent variables encode information future. following, describe components.  Generative Model Decoder Given sequence observations   desired set labels predictions   assume exists set stochastic latent variables    following, loss generality, suppose set predictions corresponds shifted version input sequence. model predict observation previous ones, common setting language speech modeling (fraccaro., 2016; Chung., 2015). generative model couples observations latent variables autoregressive model. exploiting LSTM architecture (hochreiter schmidhuber, 1997), runs sequence:   ) parameters conditional probability distribution observation  computed multi-layered feed-forward network conditions case continuous-valued observations) output log parameters Gaussian distribution, categorical proportions case one-hot predictions. note that) simple unimodal distribution, marginal distribution  highly multimodal, due integration sequence latent variables note) condition. directly computation output conditional probabilities. observed performance avoiding latent variables directly producing output. prior parameters prior distribution  latent variable obtained non-linear transformation previous hidden state forward network. common choice VAE framework Gaussian latent variables. refore) produces parameters diagonal multivariate Gaussian distribution)  log ) This type conditional prior proven previous work (chung., 2015).   inference Model inference model responsible approximating true posterior) order provide tractable latent variables lower-bound log-likelihood. our posterior approximation LSTM processing sequence backwards:   ) Each state information future sequence shape approximate posterior latent forward LSTM condition future predictions, latent variable directly inform recurrent dynamics future states, acting ?plan? future sequence. this information channeled posterior distribution feed-forward neural network) taking input previous forward state backward state)  log ) injecting stochasticity hidden state forward recurrent model, true posterior distribution variable depends variables dependence order formulate eﬃcient posterior approximation, drop dependence this cost introducing intrinsic bias posterior approximation. exclude true posterior space functions modelled function approximator. this contrast SRNN (fraccaro., 2016), posterior distribution factorizes tractable manner cost including latent variables forward autoregressive dynamics. latent variables don condition hidden state, shaping multi-modal distribution current prediction.  Auxiliary Cost domains, text images, empirically observed diﬃcult make latent variables coupled strong autoregressive decoder (bowman., 2015; Gulrajani., 2017; Chen., 2017). diﬃculty learning meaningful latent variables, cases interest, related fact abstractions underlying observed data encoded smaller number bits observed variables. for example, multiple ways picturing ?cat? . poses, colors lightning) varying abstract properties concept ?cat?. cases, maximum-likelihood training objective \\x0cmay sensitive abstractions encoded, causing latent variables ?shut off. local correlations pixel level strong bias learning process finding parameter solutions latent variables unused. cases, posterior approximation provide weak noisy signal, due variance induced stochastic gradient approximation. result, decoder learn ignore rely solely autoregressive properties causing independent. term. vanishes. recent solutions problem generally propose reduce capacity autoregressive decoder (bowman., 2015; bachman, 2016; Chen., 2017; Semeniuta., 2017). constraints decoder capacity inherently bias learning finding parameter solutions dependent. one shortcomings approach that, general, hard achieve desired solutions architecture search. instead, investigate wher expressiveness autoregressive decoder force latent variables encode information adding auxiliary training signal latent variables alone. practice, results show auxiliary cost, albeit simple, helps achieving performance objective interest. specifically, training additional conditional generative model backward states   forward states )  ? ) [log ) log ) log )]. this additional model trained amortized variational inference. however, share prior ) approximate posterior , ?primary? model deterministic function. approximate posterior conditioned). practice, solely learn additional parameters decoding model )  auxiliary reconstruction model trains relevant information future sequence contained hidden state backward network)  log) means auxiliary reconstruction cost, approximate posterior prior primary model trained additional signal escaping local minima due short term reconstructions appearing lower bound, similarly recently noted Karl. (2016).  Learning training objective regularized version lower-bound data log-likelihood based variational free-energy, regularization imposed auxiliary cost; log  log  ) ?dkl   learn parameters model backpropagation time (rumelhart., 1988) \\x0capproximate expectation sample posterior ) reparametrization. when optimizing. disconnect gradients auxiliary prediction affecting backward network. don gradients log  train parameters approximate posterior: intuitively, backward network agnostic auxiliary task assigned latent variables. performed empirically. approximate posterior trained gradient ﬂowing elbo, backward states receiving weak training signal early training, hamper usefulness auxiliary generative cost. backward states concentrated vector. refore, additionally train backward network predict output variables reverse (see Figure; log  log  log  ) ?dkl    Connection previous models Our model similar previous stochastic recurrent models: similarly STORN (bayer osendorfer, 2014) VRNN (chung., 2015) latent variables provided input autoregressive decoder. differently storn, conditional prior parametrization proposed Chung. (2015). however, generation process VRNN differs approach. vrnn, directly used produce output found model performed relieved latent variables producing output. vrnn ?myopic? posterior latent variables informed future sequence. srnn (fraccaro., 2016) addresses issue running posterior backward sequence providing future context current prediction. however, autoregressive decoder informed future sequence latent variables. several efforts made order bias learning process parameter solutions latent variables (bowman., 2015; Karl., 2016; Kingma., 2016; Chen., 2017; Zhao., 2017). bowman. (2015) tackle problem language modeling setting dropping words input random order weaken autoregressive decoder annealing divergence term training. achieve similar latent interpolations auxiliary cost. similarly, Chen. (2017) propose restrict receptive field pixel-level decoder image generation tasks. kingma. (2016) propose reserve free bits divergence. parallel work, idea task-agnostic loss latent variables considered (zhao., 2017). authors force latent variables predict bag-words representation dialog utterance. instead, work sequential setting, latent variable timestep sequence. Experiments section, evaluate proposed model diverse modeling tasks \\x0c(speech, images text). show model achieve state-art results speech modeling datasets: Blizzard (king karaiskos, 2013) TIMIT raw audio datasets (also Chung. (2015)). our approach competitive results sequential generation MNIST (salakhutdinov murray, 2008). for text, show auxiliary cost helps latent variables capture information latent structure language. sequence length, sentiment). experiments, ADAM optimizer (kingma, 2014).  Speech Modeling Sequential MNIST Blizzard TIMIT test model speech modeling datasets. blizzard consists 300 hours english, spoken single female speaker. timit widely speech recognition consists 6300 English sentences read 630 speakers. train model directly raw sequences represented sequence 200 real-valued amplitudes normalized global standard deviation training set. adopt train, validation test split Chung. (2015). for blizzard, report average log-likelihood half-second sequences (fraccaro., 2016), TIMIT report average log-likelihood sequences test set. setting, models fully factorized multivariate Gaussian distribution output distribution timestep. order model comparable state--art, number parameters comparable SRNN (fraccaro., 2016). our forward/backward networks LSTMs 2048 recurrent units Blizzard 1024 recurrent units timit. dimensionality Gaussian latent variables 256. prior) inference) auxiliary networks) single hidden layer, 1024 units Blizzard 512 units timit, leaky rectified nonlinearities leakiness clipped (fraccaro., 2016). for blizzard, learning rate.0003 batch size 128, TIMIT Model Blizzard TIMIT rnn-gauss rnn-gmm vrnn-gauss vrnn-gauss vrnn-gmm SRNN (smooth+resq 3539 7413 8933 9223 9392 11991 -1900 26643 28340 28805 28982 60550 Ours Ours kla 14435 14226 68132 68903 Ours aux Ours kla, aux 15430 15024 Models DBN 2hl (germain., 2015) NADE (uria., 2016) EoNADE5 2hl (raiko., 2014) DLGM (salimans., 2014) DARN 1hl (gregor., 2015) DRAW (gregor., 2015) PixelVAE (gulrajani., 2016-forcing-layer) (goyal., 2016) pixelrnn-layer) (oord., 2016) pixelrnn-layer) (oord., 2016) MatNets (bachman, 2016) 69530 70469 ours layer) Ours aux layer) MNIST     .02h.58h.20h.50h   Table left, report average log-likelihood sequence test sets Blizzard TIMIT datasets. ?kla? ?aux? denote annealing proposed auxiliary costs. right, report test set negative log-likelihood sequential mnist, denotes lower performance model respect baselines. for mnist, observed annealing hurts performance. .001 respectively. previous work reliably anneal term ELBO temperature weight training annealing) (fraccaro., 2016; Chung., 2015). report results obtained model training annealing without. when annealing used, temperature linearly annealed update increments.00005 (fraccaro., 2016). show results Table (left), results obtained models comparable size srnn. similar (fraccaro., 2016; Chung., 2015), report conservative evidence lower bound log-likelihood. blizzard, annealing strategy (ours kla) effective training iterations, eventually converges slightly lower log-likelihood model trained annealing (ours). explored annealing strategies didn observe improvements performance. models trained proposed auxiliary cost outperform models trained annealing strategy datasets. timit, appears slightly synergistic effect annealing auxiliary cost. even explicitly reported table, similar performance gains observed training sets. sequential MNIST task consists pixel-pixel generation binarized MNIST digits. standard binarized MNIST dataset Larochelle Murray (2011). both forward backward networks LSTMs layer 1024 hidden units. learning rate.001 batch size. report results Table (right). setting, observed annealing hurt performance model. although architecturally ﬂat, model competitive respect strong baselines. draw (gregor., 2015), outperformed deeper version autoregressive models latent variables. pixelvae (gated) (gulrajani., 2016), deep autoregressive models PixelRNN (oord., 2016) MatNets (bachman, 2016).  Language modeling well-known result language modeling tasks generative model fit observed data storing information latent variables. divergence term ELBO (bowman., 2015; Zhao., 2017; Serban., 2017b). test proposed stochastic recurrent model trained auxiliary cost medium-sized IMDB text corpus 350k movie reviews (diao., 2014). following setting. (2017), sentences \\x0cwords fixed vocabulary size 16k words. split dataset train/valid/test sets ratios respectively%. special delimiter tokens added beginning end sentence learned 3000 3500 2500 3000 2000 1500 kla aux kla, aux 1000 500 Updates TIMIT 4000 (nats) (nats) Blizzard 2500 2000 1500 kla aux kla, aux 1000 500 1e4 Updates 1e4 Figure Evolution divergence term (measured nats) ELBO auxiliary cost training Blizzard (left) TIMIT (right). plot curves models performed hyperparameter annealing auxiliary cost weights) selection validation set. auxiliary cost puts pressure latent variables resulting higher divergence. models trained auxiliary cost (ours aux) exhibit stable evolution divergence. models trained auxiliary cost achieve performance annealing (ours kla) similar, performance blizzard, compared annealing auxiliary cost (ours kla, aux). model Ours Ours aux Ours aux .0025.005 Valid \\x0ctest ELBO IWAE ELBO IWAE Table IMDB language modeling results models trained maximizing standard evidence lower-bound. report word perplexity evaluated ELBO IWAE bound divergence approximate posterior prior distribution, values auxiliary cost hyperparameters gap perplexity ELBO IWAE (evaluated samples) increases greater divergence values. generate end sentence token. single layered LSTM 500 hidden recurrent units, fix dimensionality word embeddings 300 dimensional latent variables. all (?) networks single-layered 500 hidden units leaky relu activations. learning rate.001 batch size. results shown Table expected, hard obtain perplexity baseline model latent variables language models. found IWAE (importance Weighted autoencoder) (burda., 2015) bound gave great improvements perplexity. this observation highlights fact that, text domain, ELBO severely underestimating likelihood model: approximate posterior loosely match true posterior IWAE bound correct mismatch tightening posterior approximation. IWAE bound interpreted standard VAE lower bound implicit posterior distribution (bachman precup, 2015). basis observation, attempted training models IWAE bound, observed noticeable improvement validation perplexity. analyze wher latent variables capture characteristics language interpolating latent space (bowman., 2015). given sentence, infer latent variables step running approximate posterior concatenate order form contiguous latent encoding input sentence. perform linear interpolation latent space latent encodings sentences. step interpolation, latent encoding run decoder network generate sentence. show results Table movie terrible watch Argmax Sampling movie work movie work powerful story movie life movie work powerful story powerful piece film  dark part film dark movie great ending  dark movie great message dark great dark movie great film classic give felt movie film acting good acting good acting great acting good give kids acting pretty good story great thing film funny movie great performances acting good story interesting movie great watch ) violence  Argmax Sampling darkness      watchable characters likable characters likable characters fun characters house characters house darkness  screenplay rating ****  film violence give movie chance children actors excellent lot things describe title movie funny lot fun movie table Results linear interpolation latent space. left column reports greedy argmax decoding obtained selecting, step decoding, word maximum probability model distribution, column reports random samples model. interpolation parameter. general, latent variables capture length sentences. Conclusion paper, proposed recurrent stochastic generative model builds recent architectures latent variables condition recurrent dynamics network. augmented inference network recurrent network runs backward input sequence added auxiliary cost forces latent variables reconstruct state backward network, explicitly encoding summary future observations. model achieves state--art results standard speech benchmarks TIMIT blizzard. proposed auxiliary cost, albeit simple, appears promote latent variables effectively compared similar strategies annealing. future work, interesting multitask learning setting. sentiment analysis., 2017). also, interesting incorporate proposed approach powerful autogressive models. pixelrnn/pixelcnn (oord., 2016). acknowledgments authors Phil bachman, Alex Lamb Adam Trischler discussions. nserc, cifar, google, samsung, IBM Canada Research Chairs funding, Compute Canada NVIDIA computing resources. authors express debt gratitude contributed ano years longer maintained), making great tool.',\n",
       " 'PP7263': 'transformations, rotating varying thickness handwritten digit) capture important invariances data, dimensionality reduction], improving generative models data augmentation], removing nuisance variables discriminative tasks]. however, current methods learning transformations limitations. first, rely explicit transformation pairs?for example, pairs image patches undergoing rotation]. second, improvements transformation learning focused problems transformation classes, orthogonal rotational groups], algorithms general transformations require solving diﬃcult, nonconvex objective]. tackle challenges, propose semiparametric approach unsupervised transformation learning. specifically, data points find linear transforma. vector nearest neighbor tions lies span   idea nearest neighbors unsupervised learning explored manifold learning], unlike approaches recent work representation learning    ], seek model full data distribution. thus, parameters, transformations learn naturally extrapolate training distribution applied types points., types images). contribution express transformation matrices sum rank-one matrices based samples data. objective convex, avoiding local minima (which show problem practice), scales real-world problems image patches considered past work, derive disentangled transformations trace norm penalty. empirically, show method fast effective recovering disentangled transformations, improving past baseline methods based gradient descent expectation maximization]. handwritten digits (mnist) celebrity faces (celeba) datasets, method finds interpretable disentangled transformations?for handwritten digits, thickness lines size loops digits celebrity faces, degree smile. 31st Conference Neural Information Processing Systems (nips 2017), Long beach, usa. problem statement Given data point ., image) strength scalar transformation smooth function   example, rotated image. collection transformations, entangled transformations, defined vector strengths , problem estimating collection transformations ? random observations follows: distribution points transformation strength vectors components independent iid iid     observe transformations   unobserved. goal estimate functions?      Learning transformations based matrix Lie groups paper, subset generic transformations defined matrix Lie groups. natural map form family invertible transformations parameterize exponential map. begin giving simple (rotation points dimensions) establish idea exponential map linear approximation. linear approximations transformation learning. matrix Lie group set invertible matrices closed multiplication inversion. rotation dimensions, set rotationsis parameterized angle cos(?)  sin(?) rotation representation  set rotation matrices form sin(?) cos(?) lie group, ?? rotations closed composition. linear approximation. context, important property matrix Lie groups transformations identity, local linear approximations (tangent spaces, Lie algebra), local linearizations map back Lie group exponential map]. simple example, rotation satisfies  exp) (here exp matrix exponential). infinitesimal structure Lie groups means relationships hold generally exponential map: matrix Lie group  exists      exp!. case one-dimensional Lie group, more: satisfying exp)    matrix log exponential map derivative transformation   small) analogous locally linear neighborhoods manifold learning]. exponential map states transformations close identity, linear approximation accurate. matrix generate collection-dimensional letting set {exp manifolds follows: manifold nearby points exp exp, local linearity exponential map shows exp  )axs   )axs ) Single transformation learning. approximation) suggests learning algorithm finding transformation points one-dimensional manifold points sampled pair point nearest neighbor attempt learn transformation matrix satisfying axi small nearest neighbor pairs. nearest neighbor distances kxi    ], linear approximation) eventually holds. one-dimensional manifold transformation, solve problem minimize   Axi   ) nearest neighbors, pairs directly supervision, objective form first-order matrix Lie group learning]. sampling extrapolation. learning problem) semiparametric: goal learn transformation matrix density points nonparametric nuisance variable. focusing modeling differences nearby, pairs, avoid density results advantages: first, parametric nature model means transformations defined support training data; second, modeling full density learn transformation data highly non-smooth distributions arbitrary cluster structure. convex learning transformations problem) makes sense one-dimensional manifolds superposition transformations, extend ideas (using expo \\x0cnential map linear approximation) full matrix Lie group learning problem. derive natural objective function problem provide oretical results.  Problem setup real-world data multiple degrees freedom, learn one-dimensional transformations, giving multiple Lie group learning problem: Definition. data    nearest neighbor nonconvex transformation learning problem objective minimize) tik    This problem nonconvex, prior authors commented difficulty optimizing similar objectives]. avoid diﬃculty, construct convex relaxation. define matrix  row unrolling transformation approximately takes . ) written min rank kmat   mat  matricization operator. note rank number transformations. relax rank constraint trace norm penalty min kmat   kzk?   however, matrix  large handle real-world problems. refore, propose approximating objective function modeling transformation matrices weighted sums observed transformation pairs. idea sampled pairs similar kernel method: show true transformation matrices written linear combination rank-one matrices ¿ intuition, assume single point   unobserved. approximate rank-one approximation ¿ kxj Axi shows captures behavior single point sampling suﬃciently examples appropriately weighting example, construct accurate approximation points. section supplemental material introduces kernelized version extends idea general manifolds. subsample (wlog, points). samples, write transformation weighted sum rank-one matrices ¿ weights  optimize weights:  ) min    ?    Next show high probability, weighted sum samples close operator norm true transformation matrix (lemma orem).  Learning transformation subsampling begin giving intuition sampling based objective one-transformation case. correctness rank-one reconstruction obvious special case number samples define canonical basis vector. case unknown easily reconstruct weighted thus wep combination rank-one samples ¿ general case, observe effects non-orthogonal set vectors     similar argument changing basis make canonical basis vector reconstructing basis. change basis matrix case map  . lemma makes intuition precise shows samples, exists weights    ¿ product matrix above. justifies objective. ), whiten ensure exists weights minimizes objective reconstructing  lemma.    drawn. density full-rank covariance, neighboring points   defined unknown    exists weights  recover unknown    ¿ /(rti . proof. identity implies       Summing sides weights multiplying yields   ¿            construction   refore,         spans invertible symmetric giving orem statement.  Learning multiple transformations case multiple transformations, definition recovering single transformation matrix ambiguous transformations matrices  locally generate family transformations. refer transformations   strengths  disentangled scalar  criterion implies activation strengths uncorrelated observed data. show section definition disentangling captures intuition, closed form estimate, closely connected optimization problem. show analogous result one-transformation case (lemma) shows samples find weights  reconstruct disentangled transformation matrices  ¿ implies minimization leads estimates  contrast Lemma, multiple transformation recovery guarantee probabilistic inexact. summand ¿ effects transformations, weighting scheme isolates effects single transformation instead, utilize randomness estimate approximately canceling contributions transformations. orem.      isotropic random variables ], define     draws symmetric random variable   tik kxi probability one. given   neighbors   defined tik  exists   ],   supk    exp (2c12 C22 supk  proof. give proof sketch defer details supplement (section). claim ?tik satisfies orem statement. one-dimensional case, expand outer product terms transformation   tik0  before, control terms Zkk0 tik0 zk0 close tik identity Our choice tik0 random sign, resulting Rademacher concentration bounds zero, Bernstein bounds show Zkk   Disentangling transformations Given estimated transformations      strengths  invertible matrix  find equivalent family transformations Wik Wkj tij unidentifiability, choice    equivalent  disentangled, meaning observed transformation pairs strengths pairs transformations uncorrelated condition captures intuition disentangled transformations independent strength distributions. example, set images generated changing lighting conditions sharpness, expect sharpness image uncorrelated lighting condition. formally, define set that:  uncorrelated observed data, pair transformations generate decorrelated outputs. contrast mutual information based approaches finding disentangled representations, approach seeks control moments, enforces decorrelation latent space (tik observed space). orem.    tik define  SVD row tik vec transformation mat¿ strengths Uik fulfils properties:   tik tik (correct behavior), ?¿ (uncorrelated latent space), ] random variable (uncorrelated observed space). proof. property rank construction, rank SVD preserves tik exactly. property svd, property  implying linearity trace(mat )mat interestingly, SVD appears convex subsampling algorithm. part proximal step trace norm optimization. rank sparsity induced trace norm naturally favors small number disentangled transformations. experiments evaluate effectiveness sampling-based convex relaxation learning transformations ways. section, check wher recover set rotation translation transformations applied downsampled celebrity face image dataset. next, section perform qualitative evaluation learning transformations raw celebrity faces (celeba) MNIST digits, recent evaluations disentangling adversarial networks].  Recovering transformations validate convex relaxation sampling procedure recovering syntic data generated transformations, compare existing approaches learning linear transformations. experiment consists recovering syntic transformations applied image subsets downsampled version ) celeba. resolution dataset size restrictions due runtime restrictions baseline methods. compare versions matrix Lie group learning algorithm baselines. method, implement compare convex relaxation sampling. convex relaxation sampling gradient descent. method ensures achieve desired number transformations trace norm regularization guarantee fixed rank constraint. full convex relaxation. covered here, slow run smallest experiments. baselines, compare gradient descent restarts nonconvex objective. algorithm Miao Rao] run iterations augmented SVD based disentangling method (orem). methods represent classes existing approaches estimating general linear transformations pairwise data]. optimization methods gradient descent minibatch proximal gradient descent Adagrad], proximal step trace norm penalties subsampling thousand points randomized svd. learned transformations disentangled SVD method orwise noted (orem). figures show results recovering single horizontal translation transformation error measured operator norm. convex relaxation gradient descent (convex+gradient) achieves low error sampled image subsets. gradient descent, convex relaxation achieve low error, trace norm penalty produce rankone results. gradient descent hand stuck local minima stepsize tuning restarts wide variance error runs. methods outperform substantially time. next, test disentangling multiple-transformation recovery random rotations, horizontal translations, vertical translations (figure). experiment, apply types transformations downsampled CelebA images, evaluate outputs measuring minimum-cost matching operator norm error learned transformation matrices ground truth. minimizing metric requires recovering true trans \\x0cformations label permutation. find results consistent onetransform recovery case, convex relaxation gradient descent outperforms baselines. additionally find SVD based disentangling critical recovering multiple transformations. find removing SVD nonconvex gradient descent baseline leads substantially worse results (figure). ) Operator norm error recovering single translation transform) Sampled convex relaxations faster baselines) Multiple transformations recovered SVD based disentangling Figure Sampled convex relaxation gradient descent achieves lower error recovering single transformation (panel), runs faster baselines (panel recovers multiple disentangled transformations accurately (panel).  Qualitative outputs test convex relaxation sampling MNIST celebrity faces. show subset learned transformations include full set supplemental Jupyter notebook. ) Thickness) Blur) Loop size) Angle Figure Matrix transformations learned MNIST (top rows) extrapolating Kannada handwriting (bottom row). center column original digit, ﬂanking columns generated applying transformation matrix. mnist digits trained five-dimensional linear transformation model,000 subset data, minutes. components extracted approach represent coherent stylistic features identified earlier work neural networks] thickness, rotation transformations loop size blur. examples images generated learned transformations shown figure center column original image images generated repeatedly applying transformation matrices). found transformations extrapolate handwritten symbols, Kannada handwriting] (last row, figure). finally, visualize learned transformations summing estimated transformation strength transformation minimum spanning tree observed data (see supplement section details). visualization demonstrates learned representation data captures style digit, thickness loop size ignores digit identity. highly desirable trait algorithm, means extract continuous factors variations digit thickness explicitly removing cluster structure data (figure). ) PCA) InfoGAN \\x0cfigure Embedding MNIST digits based transformations: thickness loop size. learned transformations captures extracts continuous, stylistic features apply multiple clusters cluster information. figure Baselines applied MNIST data entangle digit identity style. contrast method, baseline methods inadvertently capture digit identity part learned transformation. example, component PCA simply adds image (figure), component InfoGAN higher fidelity exchange training instability, results mixing digit identity multiple transformations (figure). finally, apply method celebrity faces dataset find extract high-level transformations linear models. trained model 1000-dimensional PCA projection CelebA constructed original 116412 dimensions, found global scene transformation sharpness contrast (figure) high level-transformations adding smile (figure). ) Contrast Sharpness) Smiling Skin tone Figure Learned transformations celebrity faces capture simple (sharpness) high-level (smiling) transformations. panel, center column original image, columns left generated repeatedly applying learnt transformation. related Work Discussion Learning transformation matrices, Lie group learning, long history closest work Miao Rao] Rao Ruderman]. earlier methods Taylor approximation learn set small ) transformation matrices pairs image patches undergoing small transformation. contrast, work require supervision form transformation pairs scalable convex objective function. improvements Rao Ruderman] focusing removing Taylor approximation order learn transformations distant examples: Cohen Welling, learned commutative-rotation Lie groups strong assumption uniform density rotations. sohl-dickstein. ] learn commutative transformations generated normal matrices eigendecompositions supervision form successive image patches video. work differs seek learn multiple, general transformation matrices large, high-dimensional datasets. difference, algorithm focuses scalability avoiding local minima expense utilizing accurate first-order Taylor approximation. approximation reasonable, fit model nearest neighbor pairs definition close. empirically, find approximations result scalable algorithm unsupervised recovery transformations. learning transform \\x0cbetween neighbors nonlinear manifold explored doll. ] Bengio Monperrus]. works model manifold predicting linear neighborhoods points nonlinear functions (radial basis functions doll. ] one-layer neural net Bengio Monperrus]). contrast methods, begin goal learning manifolds, focus class linear transformations, treat general manifold problem special kernelization. benefits: first, avoid high model complexity general manifold learning. second, extrapolation training data occurs explicitly linear parametric form model., digits kannada). finally, linearity leads definition disentangling based correlations SVD based method recovering disentangled representations. summary, presented unsupervised approach learning disentangled representations linear Lie groups. demonstrated image data, linear model surprisingly effective learning semantically meaningful transformations. results suggest semi-parametric transformation models promising identifying semantically meaningful low-dimensional continuous structures high-dimensional real-world data. acknowledgements. arun Chaganty helpful discussions comments. work supported nsf-career award 1553086, DARPA (grant n66001-4055), DAIS ITA program (w911nf-0001). reproducibility. code, data, experiments found Codalab Worksheets (http://bit/2aj5tti). Transformations, rotating varying thickness handwritten digit) capture important invariances data, dimensionality reduction], improving generative models data augmentation], removing nuisance variables discriminative tasks]. however, current methods learning transformations limitations. first, rely explicit transformation pairs?for example, pairs image patches undergoing rotation]. second, improvements transformation learning focused problems transformation classes, orthogonal rotational groups], algorithms general transformations require solving diﬃcult, nonconvex objective]. tackle challenges, propose semiparametric approach unsupervised transformation learning. specifically, data points find linear transforma. vector nearest neighbor tions lies span   idea nearest neighbors unsupervised learning explored manifold learning], unlike approaches recent work representation learning    ], seek model full data distribution. thus, parameters, transformations learn naturally extrapolate training distribution applied types points., types images). our contribution express transformation matrices sum rank-one matrices based samples data. this objective convex, avoiding local minima (which show problem practice), scales real-world problems image patches considered past work, derive disentangled transformations trace norm penalty. empirically, show method fast effective recovering disentangled transformations, improving past baseline methods based gradient descent expectation maximization]. handwritten digits (mnist) celebrity faces (celeba) datasets, method finds interpretable disentangled transformations?for handwritten digits, thickness lines size loops digits celebrity faces, degree smile. 31st Conference Neural Information Processing Systems (nips 2017), Long beach, usa. Problem statement Given data point ., image) strength scalar transformation smooth function   for example, rotated image. for collection transformations, entangled transformations, defined vector strengths , problem estimating collection transformations ? random observations follows: distribution points transformation strength vectors components independent iid iid     observe transformations   unobserved. our goal estimate functions?      Learning transformations based matrix Lie groups paper, subset generic transformations defined matrix Lie groups. natural map form family invertible transformations parameterize exponential map. begin giving simple (rotation points dimensions) establish idea exponential map linear approximation. linear approximations transformation learning. matrix Lie group set invertible matrices closed multiplication inversion. rotation dimensions, set rotationsis parameterized angle cos(?)  sin(?) rotation representation  set rotation matrices form sin(?) cos(?) lie group, ?? rotations closed composition. linear approximation. context, important property matrix Lie groups transformations identity, local linear approximations (tangent spaces, Lie algebra), local linearizations map back Lie group exponential map]. simple example, rotation satisfies  exp) (here exp matrix exponential). infinitesimal structure Lie groups means relationships hold generally exponential map: matrix Lie group  exists      exp!. case one-dimensional Lie group, more: satisfying exp)    matrix log exponential map derivative transformation   small) analogous locally linear neighborhoods manifold learning]. exponential map states transformations close identity, linear approximation accurate. for matrix generate collection-dimensional letting set {exp manifolds follows: manifold given nearby points exp exp, local linearity exponential map shows exp  )axs   )axs ) Single transformation learning. approximation) suggests learning algorithm finding transformation points one-dimensional manifold points sampled pair point nearest neighbor attempt learn transformation matrix satisfying Axi small nearest neighbor pairs. nearest neighbor distances kxi    ], linear approximation) eventually holds. for one-dimensional manifold transformation, solve problem minimize   Axi   ) nearest neighbors, pairs directly supervision, objective form first-order matrix Lie group learning]. sampling extrapolation. learning problem) semiparametric: goal learn transformation matrix density points nonparametric nuisance variable. focusing modeling differences nearby, pairs, avoid density results advantages: first, parametric nature model means transformations defined support training data; second, modeling full density learn transformation data highly non-smooth distributions arbitrary cluster structure. Convex learning transformations problem) makes sense one-dimensional manifolds superposition transformations, extend ideas (using expo \\x0cnential map linear approximation) full matrix Lie group learning problem. derive natural objective function problem provide oretical results.  Problem setup real-world data multiple degrees freedom, learn one-dimensional transformations, giving multiple Lie group learning problem: Definition. given data    nearest neighbor nonconvex transformation learning problem objective minimize) tik    This problem nonconvex, prior authors commented difficulty optimizing similar objectives]. avoid diﬃculty, construct convex relaxation. define matrix  row unrolling transformation approximately takes . ) written min rank kmat   mat  matricization operator. note rank number transformations. relax rank constraint trace norm penalty min kmat   kzk?   however, matrix  large handle real-world problems. refore, propose approximating objective function modeling transformation matrices weighted sums observed transformation pairs. this idea sampled pairs similar kernel method: show true transformation matrices written linear combination rank-one matrices ¿ intuition, assume single point   unobserved. approximate rank-one approximation ¿ kxj Axi this shows captures behavior single point sampling suﬃciently examples appropriately weighting example, construct accurate approximation points. Section supplemental material introduces kernelized version extends idea general manifolds. let subsample (wlog, points). given samples, write transformation weighted sum rank-one matrices ¿ weights  optimize weights:  ) min    ?    Next show high probability, weighted sum samples close operator norm true transformation matrix (lemma orem).  Learning transformation subsampling begin giving intuition sampling based objective one-transformation case. correctness rank-one reconstruction obvious special case number samples define canonical basis vector. case unknown easily reconstruct weighted Thus wep combination rank-one samples ¿ general case, observe effects non-orthogonal set vectors     similar argument changing basis make canonical basis vector reconstructing basis. change basis matrix case map  . our lemma makes intuition precise shows samples, exists weights    ¿ product matrix above. this justifies objective. ), whiten ensure exists weights minimizes objective reconstructing  lemma. given   drawn. density full-rank covariance, neighboring points   defined unknown    exists weights  recover unknown    ¿ /(rti . proof. identity implies       Summing sides weights multiplying yields   ¿            construction   refore,         when spans invertible symmetric giving orem statement.  Learning multiple transformations case multiple transformations, definition recovering single transformation matrix ambiguous transformations matrices  locally generate family transformations. refer transformations   strengths  disentangled scalar  this criterion implies activation strengths uncorrelated observed data. show section definition disentangling captures intuition, closed form estimate, closely connected optimization problem. show analogous result one-transformation case (lemma) shows samples find weights  reconstruct disentangled transformation matrices  ¿ this implies minimization leads estimates  contrast Lemma, multiple transformation recovery guarantee probabilistic inexact. this summand ¿ effects transformations, weighting scheme isolates effects single transformation instead, utilize randomness estimate approximately canceling contributions transformations. orem. let     isotropic random variables ], define     draws symmetric random variable   tik kxi probability one. Given   neighbors   defined tik  exists   ],   supk    exp (2c12 C22 supk  proof. give proof sketch defer details supplement (section). claim ?tik satisfies orem statement. following one-dimensional case, expand outer product terms transformation   tik0  before, control terms Zkk0 tik0 Zk0 close tik identity Our choice tik0 random sign, resulting Rademacher concentration bounds zero, Bernstein bounds show Zkk   Disentangling transformations Given estimated transformations      strengths  invertible matrix  find equivalent family transformations Wik Wkj tij despite unidentifiability, choice    equivalent  disentangled, meaning observed transformation pairs strengths pairs transformations uncorrelated this condition captures intuition disentangled transformations independent strength distributions. for example, set images generated changing lighting conditions sharpness, expect sharpness image uncorrelated lighting condition. formally, define set that:  uncorrelated observed data, pair transformations generate decorrelated outputs. contrast mutual information based approaches finding disentangled representations, approach seeks control moments, enforces decorrelation latent space (tik observed space). orem. given   tik define  SVD row tik vec transformation mat¿ strengths Uik fulfils properties:   tik tik (correct behavior), ?¿ (uncorrelated latent space), ] random variable (uncorrelated observed space). proof. property rank construction, rank SVD preserves tik exactly. property svd, property  implying linearity trace(mat )mat interestingly, SVD appears convex subsampling algorithm. part proximal step trace norm optimization. thus rank sparsity induced trace norm naturally favors small number disentangled transformations. Experiments evaluate effectiveness sampling-based convex relaxation learning transformations ways. section, check wher recover set rotation translation transformations applied downsampled celebrity face image dataset. next, section perform qualitative evaluation learning transformations raw celebrity faces (celeba) MNIST digits, recent evaluations disentangling adversarial networks].  Recovering transformations validate convex relaxation sampling procedure recovering syntic data generated transformations, compare existing approaches learning linear transformations. our experiment consists recovering syntic transformations applied image subsets downsampled version ) celeba. resolution dataset size restrictions due runtime restrictions baseline methods. compare versions matrix Lie group learning algorithm baselines. for method, implement compare convex relaxation sampling. convex relaxation sampling gradient descent. this method ensures achieve desired number transformations trace norm regularization guarantee fixed rank constraint. full convex relaxation. covered here, slow run smallest experiments. baselines, compare gradient descent restarts nonconvex objective. algorithm Miao Rao] run iterations augmented SVD based disentangling method (orem). methods represent classes existing approaches estimating general linear transformations pairwise data]. optimization methods gradient descent minibatch proximal gradient descent Adagrad], proximal step trace norm penalties subsampling thousand points randomized svd. all learned transformations disentangled SVD method orwise noted (orem). figures show results recovering single horizontal translation transformation error measured operator norm. Convex relaxation gradient descent (convex+gradient) achieves low error sampled image subsets. without gradient descent, convex relaxation achieve low error, trace norm penalty produce rankone results. gradient descent hand stuck local minima stepsize tuning restarts wide variance error runs. all methods outperform substantially time. next, test disentangling multiple-transformation recovery random rotations, horizontal translations, vertical translations (figure). experiment, apply types transformations downsampled CelebA images, evaluate outputs measuring minimum-cost matching operator norm error learned transformation matrices ground truth. minimizing metric requires recovering true trans \\x0cformations label permutation. find results consistent onetransform recovery case, convex relaxation gradient descent outperforms baselines. additionally find SVD based disentangling critical recovering multiple transformations. find removing SVD nonconvex gradient descent baseline leads substantially worse results (figure). ) Operator norm error recovering single translation transform) Sampled convex relaxations faster baselines) Multiple transformations recovered SVD based disentangling Figure Sampled convex relaxation gradient descent achieves lower error recovering single transformation (panel), runs faster baselines (panel recovers multiple disentangled transformations accurately (panel).  Qualitative outputs test convex relaxation sampling MNIST celebrity faces. show subset learned transformations include full set supplemental Jupyter notebook. ) Thickness) Blur) Loop size) Angle Figure Matrix transformations learned MNIST (top rows) extrapolating Kannada handwriting (bottom row). center column original digit, ﬂanking columns generated applying transformation matrix. MNIST digits trained five-dimensional linear transformation model,000 subset data, minutes. components extracted approach represent coherent stylistic features identified earlier work neural networks] thickness, rotation transformations loop size blur. examples images generated learned transformations shown figure center column original image images generated repeatedly applying transformation matrices). found transformations extrapolate handwritten symbols, Kannada handwriting] (last row, figure). finally, visualize learned transformations summing estimated transformation strength transformation minimum spanning tree observed data (see supplement section details). this visualization demonstrates learned representation data captures style digit, thickness loop size ignores digit identity. this highly desirable trait algorithm, means extract continuous factors variations digit thickness explicitly removing cluster structure data (figure). ) PCA) InfoGAN \\x0cfigure Embedding MNIST digits based transformations: thickness loop size. learned transformations captures extracts continuous, stylistic features apply multiple clusters cluster information. figure Baselines applied MNIST data entangle digit identity style. contrast method, baseline methods inadvertently capture digit identity part learned transformation. for example, component PCA simply adds image (figure), component InfoGAN higher fidelity exchange training instability, results mixing digit identity multiple transformations (figure). finally, apply method celebrity faces dataset find extract high-level transformations linear models. trained model 1000-dimensional PCA projection CelebA constructed original 116412 dimensions, found global scene transformation sharpness contrast (figure) high level-transformations adding smile (figure). ) Contrast Sharpness) Smiling Skin tone Figure Learned transformations celebrity faces capture simple (sharpness) high-level (smiling) transformations. for panel, center column original image, columns left generated repeatedly applying learnt transformation. Related Work Discussion Learning transformation matrices, Lie group learning, long history closest work Miao Rao] Rao Ruderman]. earlier methods Taylor approximation learn set small ) transformation matrices pairs image patches undergoing small transformation. contrast, work require supervision form transformation pairs scalable convex objective function. improvements Rao Ruderman] focusing removing Taylor approximation order learn transformations distant examples: Cohen Welling, learned commutative-rotation Lie groups strong assumption uniform density rotations. sohl-dickstein. ] learn commutative transformations generated normal matrices eigendecompositions supervision form successive image patches video. our work differs seek learn multiple, general transformation matrices large, high-dimensional datasets. because difference, algorithm focuses scalability avoiding local minima expense utilizing accurate first-order Taylor approximation. this approximation reasonable, fit model nearest neighbor pairs definition close. empirically, find approximations result scalable algorithm unsupervised recovery transformations. learning transform \\x0cbetween neighbors nonlinear manifold explored doll. ] Bengio Monperrus]. both works model manifold predicting linear neighborhoods points nonlinear functions (radial basis functions doll. ] one-layer neural net Bengio Monperrus]). contrast methods, begin goal learning manifolds, focus class linear transformations, treat general manifold problem special kernelization. this benefits: first, avoid high model complexity general manifold learning. second, extrapolation training data occurs explicitly linear parametric form model., digits kannada). finally, linearity leads definition disentangling based correlations SVD based method recovering disentangled representations. summary, presented unsupervised approach learning disentangled representations linear Lie groups. demonstrated image data, linear model surprisingly effective learning semantically meaningful transformations. our results suggest semi-parametric transformation models promising identifying semantically meaningful low-dimensional continuous structures high-dimensional real-world data. acknowledgements. Arun Chaganty helpful discussions comments. this work supported nsf-career award 1553086, DARPA (grant n66001-4055), DAIS ITA program (w911nf-0001). reproducibility. code, data, experiments found Codalab Worksheets (http://bit/2aj5tti).'}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(body):\n",
    "    tokenized_body = tokenizer.tokenize(body_dict[body]) #tokenizing the string\n",
    "    return (body, tokenized_body) # return a tuple of file name and a list of tokens\n",
    "\n",
    "#calling the tokenize method in a loop for all the elements in the dictionary\n",
    "tokenized_body = dict(tokenize(body) for body in body_dict.keys()) \n",
    "#data['tokenized'] = data['body'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = list(chain.from_iterable(tokenized_body.values()))  #maybe this should be done finally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer = \"word\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 12168)\n"
     ]
    }
   ],
   "source": [
    "data_features = vectorizer.fit_transform([' '.join(value) for value in tokenized_body.values()])\n",
    "print (data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(chain.from_iterable(tokenized_body.values()))\n",
    "vocab = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer = \"word\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('results', 100), ('this', 99), ('for', 99), ('set', 98), ('however', 96)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_per_doc = list(chain.from_iterable([set(value) for value in tokenized_body.values()]))\n",
    "wpd = FreqDist(words_per_doc)\n",
    "wpd.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_remove = []\n",
    "for word, count in wpd.items():\n",
    "    if (count/len(list(tokenized_body.keys())) < 0.03) or (count/len(list(tokenized_body.keys())) > 0.95):\n",
    "        word_to_remove.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = [token for token in all_tokens if token not in word_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in list(tokenized_body.keys()):\n",
    "    tokenized_body[file] = [token for token in tokenized_body[file] if token not in word_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the top 200 bigrams\n",
    "finder=BigramCollocationFinder.from_words(all_tokens)\n",
    "bigrams=finder.nbest(BigramAssocMeasures.likelihood_ratio, 200)\n",
    "bigrams_list=[x for x in bigrams if not any(c.isdigit() for c in x)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminating numbers from bigrams\n",
    "bigrams_list=[x for x in bigrams if not any(c.isdigit() for c in x)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preserving these bigrams and putting it back in the dictionary, along with the unigrams\n",
    "mwetokenizer = MWETokenizer(bigrams_list,separator='__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colloc_body is a dictionary that contains both the bigrams as well as the unigrams\n",
    "colloc_body =  dict((body, mwetokenizer.tokenize(data)) for body,data in tokenized_body.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the porterstemmer method\n",
    "ps = PorterStemmer()\n",
    "#An empty string to store the content of a particular body\n",
    "strcontent=''\n",
    "#An empty dictionary to append the stemmed data back \n",
    "stemmed_dict=dict()\n",
    "\n",
    "#Looping to stem each value in the dictionary\n",
    "for key,body in colloc_body.items():  \n",
    "    for word in body:\n",
    "        if (word[0].islower()) and ('__' not in word):\n",
    "        #Temporarily storing the data in an empty string\n",
    "            strcontent=strcontent+ ' ' + ps.stem(word)\n",
    "    \n",
    "    #Assigning the string to the respective key\n",
    "    stemmed_dict[key]=strcontent\n",
    "    #Again emptying the string to store the next resume\n",
    "    strcontent=''\n",
    "\n",
    "#Loop to again word tokenize each body in the dictionary and assigning it back to its body number \n",
    "for key,body in stemmed_dict.items():\n",
    "    stemmed_dict[key]=word_tokenize(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,body in colloc_body.items():  \n",
    "    for word in body:\n",
    "        if (word[0].isupper) and ('__' in word):\n",
    "            stemmed_dict[key].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_tokens = []\n",
    "for file in list(stemmed_dict.keys()):\n",
    "    for word in stemmed_dict[file]:\n",
    "        uni_tokens.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(uni_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2579"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vocab import Vocab, UnkVocab\n",
    "v = Vocab()\n",
    "vocab_index = v.word2index(vocab, train=True)\n",
    "vocab_serial = dict(zip(vocab,vocab_index))\n",
    "\n",
    "\n",
    "file = open('Group102_vocab.txt', \"w\", encoding = 'utf')\n",
    "\n",
    "for k, v in vocab_serial.items():\n",
    "    file.write(str(k) + ':'+ str(v) + '\\n')\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grants afosr/darpa fa9550-12-10411 fa9550-13-1-0036. Stanford'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selective_lower('from grants AFOSR/DARPA FA9550-12-10411 and FA9550-13-1-0036. We thank the Stanford ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_set = re.findall(r'(?!^)\\b([A-Z]\\w+)','from grants AFOSR/DARPA FA9550-12-10411 and FA9550-13-1-0036. We thank the Stanford ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
