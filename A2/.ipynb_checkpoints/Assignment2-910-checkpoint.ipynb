{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessment Requirements \n",
    "Each group is required to complete the following two tasks: \n",
    "1. Generate a sparse representation for Paper Bodies (i.e. paper text without Title, Authors, Abstract and References). The sparse representation consists of two files: a. Vocabulary index file b. Sparse count vectors file \n",
    " \n",
    "2. Generate  a CSV file (stats.csv) containing three columns: a. Top 10 most frequent terms appearing in all Titles b. Top 10 most frequent Authors c. Top 10 most frequent terms appearing in all Abstracts \n",
    " \n",
    "Note: In case of ties in any of the above fields, settle the tie based on alphabetical ascending order. (example:  if the author named John appeared as many times as Mark, then John shall be selected over Mark) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vocab\n",
      "  Downloading https://files.pythonhosted.org/packages/c2/75/6f312f7159f7353ce679b4e27296a77593e24cd266d6cd513ab37401450a/vocab-0.0.4.tar.gz\n",
      "Building wheels for collected packages: vocab\n",
      "  Building wheel for vocab (setup.py): started\n",
      "  Building wheel for vocab (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Bot1\\AppData\\Local\\pip\\Cache\\wheels\\e3\\d8\\e7\\c6aa517ea6ac4c3ed8155741f0b3da0a23380585367d3cea84\n",
      "Successfully built vocab\n",
      "Installing collected packages: vocab\n",
      "Successfully installed vocab-0.0.4\n"
     ]
    }
   ],
   "source": [
    "!pip install vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Caddy'sLenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "#import pdfminer\n",
    "import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# segmentation first, then find capital words, then loop through and lower each word, then tokenize.\n",
    "import io\n",
    "from io import StringIO\n",
    "#from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "#from pdfminer.converter import TextConverter\n",
    "#from pdfminer.layout import LAParams\n",
    "#from pdfminer.pdfpage import PDFPage\n",
    "import os\n",
    "import sys, getopt\n",
    "\n",
    "import nltk.data\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "from nltk.collocations import BigramCollocationFinder \n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from itertools import chain\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "\n",
    "from nltk.probability import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "from nltk.collocations import BigramCollocationFinder \n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from itertools import chain\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "nltk.download('punkt')\n",
    "\n",
    "# segmentation first, then find capital words, then loop through and lower each word, then tokenize.\n",
    "import io\n",
    "from io import StringIO\n",
    "#from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "#from pdfminer.converter import TextConverter\n",
    "#from pdfminer.layout import LAParams\n",
    "#from pdfminer.pdfpage import PDFPage\n",
    "import os\n",
    "import sys, getopt\n",
    "\n",
    "import nltk.data\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install Tabula-py #uncomment to install Tabula-py\n",
    "#!pip install pdfminer.six #uncomment to install Tabula-py\n",
    "#!pip install pdfminer3k #uncomment to install Tabula-py\n",
    "#!pip install tqdm\n",
    "#pdfminer3k is a Python 3 port of pdfminer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "JavaNotFoundError",
     "evalue": "`java` command is not found from this Python process.Please ensure Java is installed and PATH is set for `java`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mD:\\Anacoda\\lib\\site-packages\\tabula\\wrapper.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(java_options, options, path, encoding)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacoda\\lib\\subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[1;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    394\u001b[0m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[1;32m--> 395\u001b[1;33m                **kwargs).stdout\n\u001b[0m\u001b[0;32m    396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacoda\\lib\\subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 472\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    473\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacoda\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    774\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 775\u001b[1;33m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[0;32m    776\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacoda\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1177\u001b[0m                                          \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcwd\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m                                          startupinfo)\n\u001b[0m\u001b[0;32m   1179\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJavaNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-d00c9f23c404>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# you can find find the pdf file with complete code in below\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# read_pdf will save the pdf table into Pandas Dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtabula\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Group102.pdf\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'all'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m# in order to print first 5 lines of Table\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacoda\\lib\\site-packages\\tabula\\wrapper.py\u001b[0m in \u001b[0;36mread_pdf\u001b[1;34m(input_path, output_format, encoding, java_options, pandas_options, multiple_tables, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtemporary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacoda\\lib\\site-packages\\tabula\\wrapper.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(java_options, options, path, encoding)\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mJavaNotFoundError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mJAVA_NOT_FOUND_ERROR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Error: {}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJavaNotFoundError\u001b[0m: `java` command is not found from this Python process.Please ensure Java is installed and PATH is set for `java`"
     ]
    }
   ],
   "source": [
    "import tabula\n",
    "# readinf the PDF file that contain Table Data\n",
    "# you can find find the pdf file with complete code in below\n",
    "# read_pdf will save the pdf table into Pandas Dataframe\n",
    "df = tabula.read_pdf(\"Group102.pdf\", pages='all')\n",
    "# in order to print first 5 lines of Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-d8f1835e2bc1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'filename'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'filename'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df = df[df['filename'] != 'filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data') # make a dataset, to store all the pdf files downloaded\n",
    "    for each in tqdm(df.iterrows()):\n",
    "        response = requests.get(each[1][1])\n",
    "        with open('data/'+ str(each[1][0]),'wb') as f:\n",
    "            f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts pdf, returns its text content as a string\n",
    "def convert(fname, pages=None):\n",
    "    if not pages:\n",
    "        pagenums = set()\n",
    "    else:\n",
    "        pagenums = set(pages)\n",
    "\n",
    "    output = io.StringIO()\n",
    "    manager = PDFResourceManager()\n",
    "    converter = TextConverter(manager, output, laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(manager, converter)\n",
    "\n",
    "    infile = open(fname, 'rb')\n",
    "    for page in PDFPage.get_pages(infile, pagenums):\n",
    "        interpreter.process_page(page)\n",
    "    infile.close()\n",
    "    converter.close()\n",
    "    text = output.getvalue()\n",
    "    output.close\n",
    "    return text \n",
    "\n",
    "#converts all pdfs in directory pdfDir, saves all resulting txt files to txtdir\n",
    "def convertMultiple(pdfDir, txtDir):\n",
    "    if pdfDir == \"\": pdfDir = os.getcwd() + \"\\\\\" #if no pdfDir passed in \n",
    "    for pdf in tqdm(os.listdir(pdfDir)): #iterate through pdfs in pdf directory\n",
    "        fileExtension = pdf.split(\".\")[-1]\n",
    "        if fileExtension == \"pdf\":\n",
    "            pdfFilename = pdfDir + pdf \n",
    "            text = convert(pdfFilename) #get string of text content of pdf\n",
    "            textFilename = txtDir + pdf + \".txt\"\n",
    "            textFile = open(textFilename, \"w\") #make text file\n",
    "            textFile.write(text) #write text to text file\n",
    "\n",
    "# set paths accordingly:\n",
    "pdfDir = \"C:/your_path_here/\"\n",
    "txtDir = \"C://your_path_here/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An empty list to store all the given stopwords\n",
    "stopwords=[]\n",
    "\n",
    "#Opening the given stopwords file and storing the words in the stopwords list\n",
    "with open('stopwords_en.txt') as f:\n",
    "    stopwords = f.read().splitlines()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_lower(sentence): #E\n",
    "    aux_sentence = ''\n",
    "     #A.\n",
    "    sentence = sentence.replace('the','')\n",
    "    sentence = sentence.replace('The','')\n",
    "    sentence = sentence.replace('ﬁ','fi')\n",
    "    sentence = sentence.replace('ﬀ','ff')\n",
    "    sentence = sentence.replace('- ','')\n",
    "    sentence = sentence.replace('-\\n','')\n",
    "    sentence = sentence.replace('\\n',' ')\n",
    "    cap_set = re.findall(r'(?!^)\\b([A-Z]\\w+)',sentence)\n",
    "    for word in sentence.split(\" \"):\n",
    "        if (len(word) > 2) and (word not in stopwords):\n",
    "            if (word not in cap_set):\n",
    "                aux_sentence += word.lower() + str(' ')\n",
    "            elif (len(word) > 2) and (word not in stopwords):\n",
    "                aux_sentence += word + str(' ')\n",
    "                \n",
    "    aux_sentence = re.sub('[^A-Za-z]+', ' ', aux_sentence.strip())\n",
    "    \n",
    "    return  aux_sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "body_dict={}\n",
    "\n",
    "def get_data(directory):\n",
    "    shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')    \n",
    "    body_dict={}   \n",
    "    for filename in tqdm(os.listdir(directory)):\n",
    "            filepdf = filename.replace('.pdf','')\n",
    "            raw_body = convert(str(os.path.join(directory, filename)))\n",
    "                     \n",
    "            sentence_list = sent_detector.tokenize(raw_body.strip())\n",
    "\n",
    "\n",
    "            body = []\n",
    "            start = 0\n",
    "            stop = 0\n",
    "            for i in range(len(sentence_list)):\n",
    "                if 'Paper Body' in sentence_list[i]:\n",
    "                    start = i\n",
    "                    sentence_list[i] = sentence_list[i].replace('Paper Body','')\n",
    "                if '2 Reference' in sentence_list[i]:\n",
    "                    stop = i\n",
    "                sentence_list[i] = sentence_list[i].replace('w indows','windows')\n",
    "                sentence_list[i] = sentence_list[i].replace('W indows','Windows')\n",
    "            # this is to find the start and stop of Paper body\n",
    "            for i in range(start, stop):\n",
    "                body.append(sentence_list[i])\n",
    "            for i in range(len(body)):\n",
    "                body[i] = body[i].replace('\\n',' ') #replace all the new line\n",
    "                body[i] = selective_lower(body[i]) #E:.\n",
    "                \n",
    "            for i in range(start, stop):\n",
    "                body.append(sentence_list[i])\n",
    "            for i in range(len(body)):\n",
    "                body[i] = body[i].replace('\\n',' ') #replace all the new line\n",
    "                body[i] = selective_lower(body[i]).strip() #E:.\n",
    "                body[i] = shortword.sub('',body[i]) #make sure shortword removed\n",
    "            body_dict[filepdf] = \" \".join(body)\n",
    "    return body_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                          | 0/200 [00:00<?, ?it/s]\n",
      "  0%|▍                                                                                 | 1/200 [00:00<03:06,  1.07it/s]\n",
      "  1%|▊                                                                                 | 2/200 [00:01<03:09,  1.05it/s]\n",
      "  2%|█▏                                                                                | 3/200 [00:02<02:55,  1.12it/s]\n",
      "  2%|█▋                                                                                | 4/200 [00:05<04:28,  1.37s/it]\n",
      "  2%|██                                                                                | 5/200 [00:06<04:18,  1.32s/it]\n",
      "  3%|██▍                                                                               | 6/200 [00:07<03:58,  1.23s/it]\n",
      "  4%|██▊                                                                               | 7/200 [00:08<03:42,  1.15s/it]\n",
      "  4%|███▎                                                                              | 8/200 [00:09<03:26,  1.08s/it]\n",
      "  4%|███▋                                                                              | 9/200 [00:10<03:19,  1.04s/it]\n",
      "  5%|████                                                                             | 10/200 [00:11<03:15,  1.03s/it]\n",
      "  6%|████▍                                                                            | 11/200 [00:12<03:23,  1.08s/it]\n",
      "  6%|████▊                                                                            | 12/200 [00:13<03:13,  1.03s/it]\n",
      "  6%|█████▎                                                                           | 13/200 [00:14<03:11,  1.03s/it]\n",
      "  7%|█████▋                                                                           | 14/200 [00:15<03:15,  1.05s/it]\n",
      "  8%|██████                                                                           | 15/200 [00:16<03:02,  1.02it/s]\n",
      "  8%|██████▍                                                                          | 16/200 [00:17<03:06,  1.01s/it]\n",
      "  8%|██████▉                                                                          | 17/200 [00:18<02:59,  1.02it/s]\n",
      "  9%|███████▎                                                                         | 18/200 [00:19<02:50,  1.07it/s]\n",
      " 10%|███████▋                                                                         | 19/200 [00:20<02:57,  1.02it/s]\n",
      " 10%|████████                                                                         | 20/200 [00:21<03:36,  1.20s/it]\n",
      " 10%|████████▌                                                                        | 21/200 [00:23<03:36,  1.21s/it]\n",
      " 11%|████████▉                                                                        | 22/200 [00:24<03:21,  1.13s/it]\n",
      " 12%|█████████▎                                                                       | 23/200 [00:25<03:18,  1.12s/it]\n",
      " 12%|█████████▋                                                                       | 24/200 [00:26<03:07,  1.07s/it]\n",
      " 12%|██████████▏                                                                      | 25/200 [00:27<02:59,  1.02s/it]\n",
      " 13%|██████████▌                                                                      | 26/200 [00:28<02:57,  1.02s/it]\n",
      " 14%|██████████▉                                                                      | 27/200 [00:29<02:54,  1.01s/it]\n",
      " 14%|███████████▎                                                                     | 28/200 [00:30<03:01,  1.06s/it]\n",
      " 14%|███████████▋                                                                     | 29/200 [00:31<02:59,  1.05s/it]\n",
      " 15%|████████████▏                                                                    | 30/200 [00:32<02:59,  1.06s/it]\n",
      " 16%|████████████▌                                                                    | 31/200 [00:33<03:04,  1.09s/it]\n",
      " 16%|████████████▉                                                                    | 32/200 [00:35<03:46,  1.35s/it]\n",
      " 16%|█████████████▎                                                                   | 33/200 [00:36<03:27,  1.24s/it]\n",
      " 17%|█████████████▊                                                                   | 34/200 [00:37<03:21,  1.21s/it]\n",
      " 18%|██████████████▏                                                                  | 35/200 [00:38<03:12,  1.16s/it]\n",
      " 18%|██████████████▌                                                                  | 36/200 [00:39<02:53,  1.06s/it]\n",
      " 18%|██████████████▉                                                                  | 37/200 [00:40<02:49,  1.04s/it]\n",
      " 19%|███████████████▍                                                                 | 38/200 [00:41<02:45,  1.02s/it]\n",
      " 20%|███████████████▊                                                                 | 39/200 [00:42<02:53,  1.08s/it]\n",
      " 20%|████████████████▏                                                                | 40/200 [00:43<02:50,  1.06s/it]\n",
      " 20%|████████████████▌                                                                | 41/200 [00:44<02:47,  1.05s/it]\n",
      " 21%|█████████████████                                                                | 42/200 [00:45<02:50,  1.08s/it]\n",
      " 22%|█████████████████▍                                                               | 43/200 [00:46<02:52,  1.10s/it]\n",
      " 22%|█████████████████▊                                                               | 44/200 [00:47<02:41,  1.04s/it]\n",
      " 22%|██████████████████▏                                                              | 45/200 [00:48<02:43,  1.05s/it]\n",
      " 23%|██████████████████▋                                                              | 46/200 [00:50<03:26,  1.34s/it]"
     ]
    }
   ],
   "source": [
    "pathpdf = 'data/'\n",
    "tokenizer = RegexpTokenizer(\"[A-Za-z]\\w+(?:[-'?]\\w+)?\")\n",
    "body_dict = get_data(pathpdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame.from_dict(body_dict,orient='index')\n",
    "data = data.reset_index()\n",
    "data.columns = ['filename','body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(body):\n",
    "    tokenized_body = tokenizer.tokenize(body_dict[body]) #tokenizing the string\n",
    "    return (body, tokenized_body) # return a tuple of file name and a list of tokens\n",
    "\n",
    "#calling the tokenize method in a loop for all the elements in the dictionary\n",
    "tokenized_body = dict(tokenize(body) for body in body_dict.keys()) \n",
    "#data['tokenized'] = data['body'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = list(chain.from_iterable(tokenized_body.values()))  #maybe this should be done finally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer = \"word\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 17019)\n"
     ]
    }
   ],
   "source": [
    "data_features = vectorizer.fit_transform([' '.join(value) for value in tokenized_body.values()])\n",
    "print (data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(chain.from_iterable(tokenized_body.values()))\n",
    "vocab = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer = \"word\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('for', 199),\n",
       " ('this', 198),\n",
       " ('set', 198),\n",
       " ('results', 195),\n",
       " ('number', 193),\n",
       " ('however', 191),\n",
       " ('based', 189),\n",
       " ('show', 186),\n",
       " ('function', 185),\n",
       " ('work', 185),\n",
       " ('case', 183),\n",
       " ('Figure', 181),\n",
       " ('problem', 181),\n",
       " ('note', 179),\n",
       " ('data', 179),\n",
       " ('shown', 178),\n",
       " ('large', 177),\n",
       " ('shows', 177),\n",
       " ('form', 176),\n",
       " ('paper', 175),\n",
       " ('section', 173),\n",
       " ('small', 173),\n",
       " ('time', 171),\n",
       " ('approach', 171),\n",
       " ('model', 170)]"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_per_doc = list(chain.from_iterable([set(value) for value in tokenized_body.values()]))\n",
    "wpd = FreqDist(words_per_doc)\n",
    "wpd.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_remove = []\n",
    "for word, count in wpd.items():\n",
    "    if (count/200 < 0.03) or (count/200 > 0.95):\n",
    "        word_to_remove.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16564"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = [token for token in all_tokens if token not in word_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4297"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(all_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in list(tokenized_body.keys()):\n",
    "    tokenized_body[file] = [token for token in tokenized_body[file] if token not in word_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the top 200 bigrams\n",
    "finder=BigramCollocationFinder.from_words(all_tokens)\n",
    "bigrams=finder.nbest(BigramAssocMeasures.likelihood_ratio, 200)\n",
    "bigrams_list=[x for x in bigrams if not any(c.isdigit() for c in x)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('log', 'log'),\n",
       " ('ground', 'truth'),\n",
       " ('gradient', 'descent'),\n",
       " ('state', 'art'),\n",
       " ('lower', 'bound'),\n",
       " ('coe', 'cients'),\n",
       " ('Monte', 'Carlo'),\n",
       " ('Markov', 'chain'),\n",
       " ('supplementary', 'material'),\n",
       " ('machine', 'learning'),\n",
       " ('coe', 'cient'),\n",
       " ('upper', 'bound'),\n",
       " ('neural', 'networks'),\n",
       " ('high', 'dimensional'),\n",
       " ('low', 'rank'),\n",
       " ('trace', 'norm'),\n",
       " ('Processing', 'Systems'),\n",
       " ('real', 'world'),\n",
       " ('Conference', 'Neural'),\n",
       " ('Systems', 'nips'),\n",
       " ('Information', 'Processing'),\n",
       " ('cross', 'validation'),\n",
       " ('latent', 'variables'),\n",
       " ('optimization', 'problem'),\n",
       " ('reinforcement', 'learning'),\n",
       " ('nearest', 'neighbor'),\n",
       " ('Neural', 'Information'),\n",
       " ('objective', 'function'),\n",
       " ('figure', 'shows'),\n",
       " ('computer', 'vision'),\n",
       " ('running', 'time'),\n",
       " ('log', 'likelihood'),\n",
       " ('step', 'size'),\n",
       " ('random', 'variables'),\n",
       " ('variational', 'inference'),\n",
       " ('closed', 'form'),\n",
       " ('special', 'case'),\n",
       " ('beach', 'usa'),\n",
       " ('neural', 'network'),\n",
       " ('sample', 'size'),\n",
       " ('Long', 'beach'),\n",
       " ('active', 'learning'),\n",
       " ('convergence', 'rate'),\n",
       " ('data', 'sets'),\n",
       " ('mini', 'batch'),\n",
       " ('training', 'examples'),\n",
       " ('standard', 'deviation'),\n",
       " ('covariance', 'matrix'),\n",
       " ('exponential', 'family'),\n",
       " ('arg', 'min'),\n",
       " ('ROC', 'curve'),\n",
       " ('nips', 'Long'),\n",
       " ('random', 'variable'),\n",
       " ('large', 'scale'),\n",
       " ('nuclear', 'norm'),\n",
       " ('gold', 'standard'),\n",
       " ('meta', 'inference'),\n",
       " ('user', 'item'),\n",
       " ('latent', 'variable'),\n",
       " ('empirical', 'risk'),\n",
       " ('common', 'neighbors'),\n",
       " ('max', 'norm'),\n",
       " ('future', 'work'),\n",
       " ('metropolis', 'hastings'),\n",
       " ('lower', 'bounds'),\n",
       " ('monotone', 'submodular'),\n",
       " ('high', 'probability'),\n",
       " ('optimal', 'solution'),\n",
       " ('linear', 'combination'),\n",
       " ('squared', 'error'),\n",
       " ('learning', 'rate'),\n",
       " ('polynomial', 'code'),\n",
       " ('loss', 'function'),\n",
       " ('fig', 'shows'),\n",
       " ('non', 'zero'),\n",
       " ('closely', 'related'),\n",
       " ('sample', 'complexity'),\n",
       " ('low', 'dimensional'),\n",
       " ('max', 'max'),\n",
       " ('label', 'complexity'),\n",
       " ('stochastic', 'gradient'),\n",
       " ('mutual', 'information'),\n",
       " ('hand', 'side'),\n",
       " ('shown', 'Figure'),\n",
       " ('training', 'data'),\n",
       " ('positive', 'definite'),\n",
       " ('dimensionality', 'reduction'),\n",
       " ('supervised', 'learning'),\n",
       " ('min', 'max'),\n",
       " ('time', 'varying'),\n",
       " ('high', 'level'),\n",
       " ('mean', 'field'),\n",
       " ('Gibbs', 'sampling'),\n",
       " ('online', 'learning'),\n",
       " ('Gaussian', 'process'),\n",
       " ('learning', 'algorithms'),\n",
       " ('graphical', 'model'),\n",
       " ('exploration', 'exploitation'),\n",
       " ('spectral', 'clustering'),\n",
       " ('worst', 'case'),\n",
       " ('recovery', 'threshold'),\n",
       " ('feature', 'space'),\n",
       " ('trade', 'off'),\n",
       " ('data', 'points'),\n",
       " ('mirror', 'descent'),\n",
       " ('exp', 'exp'),\n",
       " ('maximum', 'likelihood'),\n",
       " ('feature', 'selection'),\n",
       " ('deep', 'learning'),\n",
       " ('hinge', 'loss'),\n",
       " ('multi', 'scale'),\n",
       " ('metric', 'learning'),\n",
       " ('loss', 'functions'),\n",
       " ('semi', 'supervised'),\n",
       " ('posterior', 'distribution'),\n",
       " ('fold', 'cross'),\n",
       " ('choice', 'probabilities'),\n",
       " ('end', 'end'),\n",
       " ('kullback', 'leibler'),\n",
       " ('deep', 'neural'),\n",
       " ('hyposis', 'class'),\n",
       " ('upper', 'bounds'),\n",
       " ('loss', 'generality'),\n",
       " ('time', 'series'),\n",
       " ('matrix', 'completion'),\n",
       " ('arg', 'max'),\n",
       " ('vice', 'versa'),\n",
       " ('real', 'valued'),\n",
       " ('barcelona', 'spain'),\n",
       " ('feature', 'vector'),\n",
       " ('shown', 'fig'),\n",
       " ('computational', 'complexity'),\n",
       " ('Gaussian', 'noise'),\n",
       " ('proof', 'orem'),\n",
       " ('fully', 'connected'),\n",
       " ('confidence', 'intervals'),\n",
       " ('newton', 'method'),\n",
       " ('sequentially', 'robust'),\n",
       " ('spike', 'sorting'),\n",
       " ('unlabeled', 'data'),\n",
       " ('regularization', 'parameter'),\n",
       " ('basis', 'functions'),\n",
       " ('wij', 'wij'),\n",
       " ('constraint', 'violation'),\n",
       " ('work', 'supported'),\n",
       " ('learning', 'algorithm'),\n",
       " ('Markov', 'chains'),\n",
       " ('generative', 'model'),\n",
       " ('higher', 'order'),\n",
       " ('transition', 'matrix'),\n",
       " ('convex', 'optimization'),\n",
       " ('graphical', 'models'),\n",
       " ('strongly', 'convex'),\n",
       " ('bandit', 'feedback'),\n",
       " ('ROC', 'curves'),\n",
       " ('probability', 'distribution'),\n",
       " ('long', 'term'),\n",
       " ('local', 'minima'),\n",
       " ('link', 'function'),\n",
       " ('http', 'www'),\n",
       " ('switching', 'costs'),\n",
       " ('state', 'space'),\n",
       " ('nity', 'graph'),\n",
       " ('triangle', 'inequality'),\n",
       " ('principal', 'components'),\n",
       " ('well', 'known'),\n",
       " ('positive', 'semidefinite'),\n",
       " ('logistic', 'regression'),\n",
       " ('Figure', 'left'),\n",
       " ('importance', 'sampling'),\n",
       " ('previous', 'section'),\n",
       " ('hidden', 'states'),\n",
       " ('plug', 'estimator'),\n",
       " ('recent', 'work'),\n",
       " ('weighted', 'trace'),\n",
       " ('wide', 'range'),\n",
       " ('potential', 'correlation'),\n",
       " ('weak', 'learners'),\n",
       " ('auto', 'encoder'),\n",
       " ('main', 'result'),\n",
       " ('weight', 'vector'),\n",
       " ('early', 'stopping'),\n",
       " ('paper', 'organized'),\n",
       " ('exponential', 'families'),\n",
       " ('gradient', 'ascent'),\n",
       " ('nips', 'barcelona'),\n",
       " ('disagreement', 'region'),\n",
       " ('feature', 'extraction'),\n",
       " ('hyper', 'parameters'),\n",
       " ('semi', 'definite'),\n",
       " ('optimal', 'policy'),\n",
       " ('time', 'steps'),\n",
       " ('upper', 'bounded'),\n",
       " ('log', 'exp'),\n",
       " ('random', 'walk'),\n",
       " ('Poisson', 'process'),\n",
       " ('Related', 'Work'),\n",
       " ('correlation', 'coe'),\n",
       " ('multi', 'armed'),\n",
       " ('acknowledgments', 'This')]"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminating numbers from bigrams\n",
    "bigrams_list=[x for x in bigrams if not any(c.isdigit() for c in x)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preserving these bigrams and putting it back in the dictionary, along with the unigrams\n",
    "mwetokenizer = MWETokenizer(bigrams_list,separator='__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colloc_body is a dictionary that contains both the bigrams as well as the unigrams\n",
    "colloc_body =  dict((body, mwetokenizer.tokenize(data)) for body,data in tokenized_body.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the porterstemmer method\n",
    "ps = PorterStemmer()\n",
    "#An empty string to store the content of a particular body\n",
    "strcontent=''\n",
    "#An empty dictionary to append the stemmed data back \n",
    "stemmed_dict=dict()\n",
    "\n",
    "#Looping to stem each value in the dictionary\n",
    "for key,body in colloc_body.items():  \n",
    "    for word in body:\n",
    "        if (word[0].islower()) and ('__' not in word):\n",
    "        #Temporarily storing the data in an empty string\n",
    "            strcontent=strcontent+ ' ' + ps.stem(word)\n",
    "    \n",
    "    #Assigning the string to the respective key\n",
    "    stemmed_dict[key]=strcontent\n",
    "    #Again emptying the string to store the next resume\n",
    "    strcontent=''\n",
    "\n",
    "#Loop to again word tokenize each body in the dictionary and assigning it back to its body number \n",
    "for key,body in stemmed_dict.items():\n",
    "    stemmed_dict[key]=word_tokenize(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,body in colloc_body.items():  \n",
    "    for word in body:\n",
    "        if (word[0].isupper) and ('__' in word):\n",
    "            stemmed_dict[key].append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_tokens = []\n",
    "for file in list(stemmed_dict.keys()):\n",
    "    for word in stemmed_dict[file]:\n",
    "        uni_tokens.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2327"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(uni_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(uni_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'index2word'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-474-b817c4f22ac6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mword2index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtoken_index\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muni_tokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mword2index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hi'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m30308\u001b[0m  \u001b[1;31m# True\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'index2word'"
     ]
    }
   ],
   "source": [
    "word2index = {token: token_index for token_index, token in enumerate(uni_tokens.index2word)} \n",
    "word2index['hi'] == 30308  # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Conference__Neural',\n",
       " 'Figure__left',\n",
       " 'Gaussian__noise',\n",
       " 'Gaussian__process',\n",
       " 'Gibbs__sampling',\n",
       " 'Information__Processing',\n",
       " 'Long__beach',\n",
       " 'Markov__chain',\n",
       " 'Markov__chains',\n",
       " 'Monte__Carlo',\n",
       " 'Neural__Information',\n",
       " 'Poisson__process',\n",
       " 'Processing__Systems',\n",
       " 'ROC__curve',\n",
       " 'ROC__curves',\n",
       " 'Related__Work',\n",
       " 'Systems__nips',\n",
       " 'abil',\n",
       " 'about',\n",
       " 'abov',\n",
       " 'absenc',\n",
       " 'absolut',\n",
       " 'abstract',\n",
       " 'abus',\n",
       " 'acceler',\n",
       " 'accept',\n",
       " 'access',\n",
       " 'accommod',\n",
       " 'accomplish',\n",
       " 'accord',\n",
       " 'accordingli',\n",
       " 'account',\n",
       " 'accumul',\n",
       " 'accur',\n",
       " 'accuraci',\n",
       " 'achiev',\n",
       " 'acknowledg',\n",
       " 'acknowledgments__This',\n",
       " 'acquir',\n",
       " 'acquisit',\n",
       " 'across',\n",
       " 'act',\n",
       " 'action',\n",
       " 'activ',\n",
       " 'active__learning',\n",
       " 'actual',\n",
       " 'ad',\n",
       " 'adapt',\n",
       " 'add',\n",
       " 'addit',\n",
       " 'address',\n",
       " 'adequ',\n",
       " 'adjac',\n",
       " 'adjust',\n",
       " 'admit',\n",
       " 'adopt',\n",
       " 'advanc',\n",
       " 'advantag',\n",
       " 'adversari',\n",
       " 'affect',\n",
       " 'aforement',\n",
       " 'after',\n",
       " 'again',\n",
       " 'age',\n",
       " 'agent',\n",
       " 'aggreg',\n",
       " 'aggress',\n",
       " 'agnost',\n",
       " 'agre',\n",
       " 'agreement',\n",
       " 'aij',\n",
       " 'aim',\n",
       " 'akin',\n",
       " 'albeit',\n",
       " 'alg',\n",
       " 'algebra',\n",
       " 'algo',\n",
       " 'algorithm',\n",
       " 'align',\n",
       " 'all',\n",
       " 'allevi',\n",
       " 'alloc',\n",
       " 'allow',\n",
       " 'almost',\n",
       " 'alon',\n",
       " 'also',\n",
       " 'alter',\n",
       " 'altern',\n",
       " 'although',\n",
       " 'amazon',\n",
       " 'ambient',\n",
       " 'ambigu',\n",
       " 'amen',\n",
       " 'among',\n",
       " 'amort',\n",
       " 'amount',\n",
       " 'amplitud',\n",
       " 'analog',\n",
       " 'analogu',\n",
       " 'analys',\n",
       " 'analysi',\n",
       " 'analyt',\n",
       " 'analyz',\n",
       " 'and',\n",
       " 'angl',\n",
       " 'angular',\n",
       " 'ani',\n",
       " 'anim',\n",
       " 'annot',\n",
       " 'anonym',\n",
       " 'anor',\n",
       " 'answer',\n",
       " 'anymor',\n",
       " 'apart',\n",
       " 'appar',\n",
       " 'appeal',\n",
       " 'appear',\n",
       " 'appendix',\n",
       " 'appli',\n",
       " 'applic',\n",
       " 'approach',\n",
       " 'appropri',\n",
       " 'approxim',\n",
       " 'arbitrari',\n",
       " 'arbitrarili',\n",
       " 'architectur',\n",
       " 'are',\n",
       " 'area',\n",
       " 'arg',\n",
       " 'arg__max',\n",
       " 'arg__min',\n",
       " 'argmax',\n",
       " 'argmin',\n",
       " 'argu',\n",
       " 'arguabl',\n",
       " 'argument',\n",
       " 'aris',\n",
       " 'arithmet',\n",
       " 'arm',\n",
       " 'arrang',\n",
       " 'array',\n",
       " 'arriv',\n",
       " 'arrow',\n",
       " 'art',\n",
       " 'articl',\n",
       " 'artifici',\n",
       " 'ascent',\n",
       " 'ask',\n",
       " 'aspect',\n",
       " 'assess',\n",
       " 'assign',\n",
       " 'associ',\n",
       " 'assum',\n",
       " 'assumpt',\n",
       " 'asymmetr',\n",
       " 'asymptot',\n",
       " 'atom',\n",
       " 'attain',\n",
       " 'attempt',\n",
       " 'attent',\n",
       " 'attract',\n",
       " 'attribut',\n",
       " 'auc',\n",
       " 'audio',\n",
       " 'augment',\n",
       " 'author',\n",
       " 'auto',\n",
       " 'auto__encoder',\n",
       " 'autoencod',\n",
       " 'autom',\n",
       " 'automat',\n",
       " 'auxiliari',\n",
       " 'avail',\n",
       " 'averag',\n",
       " 'avg',\n",
       " 'avoid',\n",
       " 'awar',\n",
       " 'award',\n",
       " 'axe',\n",
       " 'axi',\n",
       " 'back',\n",
       " 'background',\n",
       " 'backpropag',\n",
       " 'backward',\n",
       " 'bad',\n",
       " 'bag',\n",
       " 'balanc',\n",
       " 'ball',\n",
       " 'band',\n",
       " 'bandit',\n",
       " 'bandit__feedback',\n",
       " 'bandwidth',\n",
       " 'bank',\n",
       " 'bar',\n",
       " 'barcelona__spain',\n",
       " 'base',\n",
       " 'baselin',\n",
       " 'basi',\n",
       " 'basic',\n",
       " 'basis__functions',\n",
       " 'batch',\n",
       " 'bay',\n",
       " 'bayesian',\n",
       " 'beach',\n",
       " 'becaus',\n",
       " 'befor',\n",
       " 'begin',\n",
       " 'behav',\n",
       " 'behavior',\n",
       " 'behaviour',\n",
       " 'belief',\n",
       " 'belong',\n",
       " 'below',\n",
       " 'benchmark',\n",
       " 'benefici',\n",
       " 'benefit',\n",
       " 'bernoulli',\n",
       " 'besid',\n",
       " 'best',\n",
       " 'beta',\n",
       " 'better',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'bfg',\n",
       " 'bia',\n",
       " 'bias',\n",
       " 'big',\n",
       " 'bigger',\n",
       " 'bilinear',\n",
       " 'bin',\n",
       " 'binar',\n",
       " 'binari',\n",
       " 'binomi',\n",
       " 'biolog',\n",
       " 'bipartit',\n",
       " 'bit',\n",
       " 'black',\n",
       " 'blind',\n",
       " 'block',\n",
       " 'blue',\n",
       " 'bodi',\n",
       " 'bold',\n",
       " 'boldfac',\n",
       " 'book',\n",
       " 'boost',\n",
       " 'bootstrap',\n",
       " 'both',\n",
       " 'bottleneck',\n",
       " 'bottom',\n",
       " 'bound',\n",
       " 'boundari',\n",
       " 'box',\n",
       " 'brain',\n",
       " 'branch',\n",
       " 'break',\n",
       " 'breviti',\n",
       " 'bridg',\n",
       " 'brie',\n",
       " 'bring',\n",
       " 'broad',\n",
       " 'broader',\n",
       " 'broadli',\n",
       " 'build',\n",
       " 'built',\n",
       " 'burden',\n",
       " 'burn',\n",
       " 'but',\n",
       " 'caci',\n",
       " 'calcul',\n",
       " 'calculu',\n",
       " 'calibr',\n",
       " 'call',\n",
       " 'camera',\n",
       " 'can',\n",
       " 'candid',\n",
       " 'canon',\n",
       " 'capabl',\n",
       " 'capac',\n",
       " 'captur',\n",
       " 'car',\n",
       " 'cardin',\n",
       " 'care',\n",
       " 'carlo',\n",
       " 'carri',\n",
       " 'case',\n",
       " 'cast',\n",
       " 'cat',\n",
       " 'categor',\n",
       " 'categori',\n",
       " 'caus',\n",
       " 'causal',\n",
       " 'ccf',\n",
       " 'cdf',\n",
       " 'ce',\n",
       " 'celebr',\n",
       " 'cell',\n",
       " 'center',\n",
       " 'central',\n",
       " 'chain',\n",
       " 'challeng',\n",
       " 'chanc',\n",
       " 'chang',\n",
       " 'channel',\n",
       " 'charact',\n",
       " 'character',\n",
       " 'characterist',\n",
       " 'check',\n",
       " 'child',\n",
       " 'children',\n",
       " 'choic',\n",
       " 'choice__probabilities',\n",
       " 'choos',\n",
       " 'chose',\n",
       " 'chosen',\n",
       " 'cial',\n",
       " 'cienci',\n",
       " 'cient',\n",
       " 'cientli',\n",
       " 'cifar',\n",
       " 'cij',\n",
       " 'circl',\n",
       " 'circumv',\n",
       " 'citi',\n",
       " 'claim',\n",
       " 'clarifi',\n",
       " 'clariti',\n",
       " 'class',\n",
       " 'classic',\n",
       " 'classif',\n",
       " 'classifi',\n",
       " 'clean',\n",
       " 'clear',\n",
       " 'clearli',\n",
       " 'click',\n",
       " 'clip',\n",
       " 'close',\n",
       " 'closed__form',\n",
       " 'closely__related',\n",
       " 'closer',\n",
       " 'closest',\n",
       " 'cluster',\n",
       " 'cnn',\n",
       " 'co',\n",
       " 'coars',\n",
       " 'code',\n",
       " 'codebook',\n",
       " 'coe__cient',\n",
       " 'coe__cients',\n",
       " 'coher',\n",
       " 'coin',\n",
       " 'coincid',\n",
       " 'collabor',\n",
       " 'collect',\n",
       " 'color',\n",
       " 'column',\n",
       " 'com',\n",
       " 'combin',\n",
       " 'combinatori',\n",
       " 'come',\n",
       " 'comment',\n",
       " 'common',\n",
       " 'common__neighbors',\n",
       " 'commonli',\n",
       " 'commun',\n",
       " 'compact',\n",
       " 'compactli',\n",
       " 'compar',\n",
       " 'comparison',\n",
       " 'compens',\n",
       " 'compet',\n",
       " 'competit',\n",
       " 'complement',\n",
       " 'complementari',\n",
       " 'complet',\n",
       " 'complex',\n",
       " 'complic',\n",
       " 'compon',\n",
       " 'compos',\n",
       " 'composit',\n",
       " 'comprehens',\n",
       " 'compress',\n",
       " 'compris',\n",
       " 'compromis',\n",
       " 'comput',\n",
       " 'computational__complexity',\n",
       " 'computer__vision',\n",
       " 'con',\n",
       " 'concaten',\n",
       " 'concav',\n",
       " 'concentr',\n",
       " 'concept',\n",
       " 'conceptu',\n",
       " 'concern',\n",
       " 'conclud',\n",
       " 'conclus',\n",
       " 'concret',\n",
       " 'condit',\n",
       " 'conduct',\n",
       " 'confid',\n",
       " 'confidence__intervals',\n",
       " 'configur',\n",
       " 'confirm',\n",
       " 'conjectur',\n",
       " 'conjug',\n",
       " 'conjunct',\n",
       " 'connect',\n",
       " 'consecut',\n",
       " 'consequ',\n",
       " 'conserv',\n",
       " 'consid',\n",
       " 'consider',\n",
       " 'consist',\n",
       " 'constant',\n",
       " 'constantli',\n",
       " 'constitut',\n",
       " 'constrain',\n",
       " 'constraint',\n",
       " 'constraint__violation',\n",
       " 'construct',\n",
       " 'contain',\n",
       " 'content',\n",
       " 'context',\n",
       " 'contextu',\n",
       " 'contigu',\n",
       " 'continu',\n",
       " 'contract',\n",
       " 'contradict',\n",
       " 'contrari',\n",
       " 'contrast',\n",
       " 'contribut',\n",
       " 'control',\n",
       " 'conv',\n",
       " 'conveni',\n",
       " 'convent',\n",
       " 'converg',\n",
       " 'convergence__rate',\n",
       " 'convers',\n",
       " 'convert',\n",
       " 'convex',\n",
       " 'convex__optimization',\n",
       " 'convolut',\n",
       " 'coordin',\n",
       " 'copi',\n",
       " 'copyright',\n",
       " 'core',\n",
       " 'corner',\n",
       " 'corollari',\n",
       " 'corpu',\n",
       " 'correct',\n",
       " 'correctli',\n",
       " 'correl',\n",
       " 'correlation__coe',\n",
       " 'correspond',\n",
       " 'correspondingli',\n",
       " 'corrupt',\n",
       " 'cortex',\n",
       " 'cortic',\n",
       " 'cosin',\n",
       " 'cost',\n",
       " 'costli',\n",
       " 'could',\n",
       " 'count',\n",
       " 'counter',\n",
       " 'counterpart',\n",
       " 'coupl',\n",
       " 'cours',\n",
       " 'cov',\n",
       " 'covari',\n",
       " 'covariance__matrix',\n",
       " 'cover',\n",
       " 'coverag',\n",
       " 'creat',\n",
       " 'criteria',\n",
       " 'criterion',\n",
       " 'critic',\n",
       " 'cross',\n",
       " 'cross__validation',\n",
       " 'crossvalid',\n",
       " 'crucial',\n",
       " 'cube',\n",
       " 'cubic',\n",
       " 'cue',\n",
       " 'cult',\n",
       " 'culti',\n",
       " 'cumul',\n",
       " 'current',\n",
       " 'curs',\n",
       " 'curv',\n",
       " 'curvatur',\n",
       " 'cut',\n",
       " 'cycl',\n",
       " 'dark',\n",
       " 'darpa',\n",
       " 'dash',\n",
       " 'data',\n",
       " 'data__points',\n",
       " 'data__sets',\n",
       " 'databas',\n",
       " 'datapoint',\n",
       " 'dataset',\n",
       " 'date',\n",
       " 'day',\n",
       " 'deal',\n",
       " 'decad',\n",
       " 'decay',\n",
       " 'decid',\n",
       " 'decis',\n",
       " 'decod',\n",
       " 'decompos',\n",
       " 'decomposit',\n",
       " 'decoupl',\n",
       " 'decreas',\n",
       " 'dedic',\n",
       " 'deep',\n",
       " 'deep__learning',\n",
       " 'deep__neural',\n",
       " 'deeper',\n",
       " 'def',\n",
       " 'default',\n",
       " 'defer',\n",
       " 'defin',\n",
       " 'definit',\n",
       " 'degener',\n",
       " 'degrad',\n",
       " 'degre',\n",
       " 'delay',\n",
       " 'delta',\n",
       " 'demand',\n",
       " 'demonstr',\n",
       " 'denois',\n",
       " 'denomin',\n",
       " 'denot',\n",
       " 'dens',\n",
       " 'densiti',\n",
       " 'depend',\n",
       " 'depict',\n",
       " 'deploy',\n",
       " 'depth',\n",
       " 'deriv',\n",
       " 'descent',\n",
       " 'describ',\n",
       " 'descript',\n",
       " 'descriptor',\n",
       " 'design',\n",
       " 'desir',\n",
       " 'despit',\n",
       " 'det',\n",
       " 'detail',\n",
       " 'detect',\n",
       " 'detector',\n",
       " 'determin',\n",
       " 'determinist',\n",
       " 'develop',\n",
       " 'deviat',\n",
       " 'devic',\n",
       " 'devis',\n",
       " 'devot',\n",
       " 'di',\n",
       " 'diag',\n",
       " 'diagnosi',\n",
       " 'diagon',\n",
       " 'diamet',\n",
       " 'dictionari',\n",
       " 'differ',\n",
       " 'differenti',\n",
       " 'diffus',\n",
       " 'digit',\n",
       " 'dim',\n",
       " 'dimens',\n",
       " 'dimension',\n",
       " 'dimensionality__reduction',\n",
       " 'diminish',\n",
       " 'dir',\n",
       " 'direct',\n",
       " 'directli',\n",
       " 'disadvantag',\n",
       " 'disagr',\n",
       " 'disagreement__region',\n",
       " 'discard',\n",
       " 'disconnect',\n",
       " 'discount',\n",
       " 'discov',\n",
       " 'discoveri',\n",
       " 'discret',\n",
       " 'discrimin',\n",
       " 'discuss',\n",
       " 'disjoint',\n",
       " 'display',\n",
       " 'dissimilar',\n",
       " 'dist',\n",
       " 'distanc',\n",
       " 'distant',\n",
       " 'distinct',\n",
       " 'distinguish',\n",
       " 'distort',\n",
       " 'distribut',\n",
       " 'diverg',\n",
       " 'divers',\n",
       " 'divid',\n",
       " 'dm',\n",
       " 'do',\n",
       " 'document',\n",
       " 'doe',\n",
       " 'doesn',\n",
       " 'dom',\n",
       " 'domain',\n",
       " 'domin',\n",
       " 'don',\n",
       " 'dot',\n",
       " 'doubl',\n",
       " 'down',\n",
       " 'download',\n",
       " 'downstream',\n",
       " 'dramat',\n",
       " 'drastic',\n",
       " 'draw',\n",
       " 'drawback',\n",
       " 'drawn',\n",
       " 'drift',\n",
       " 'drive',\n",
       " 'driven',\n",
       " 'drop',\n",
       " 'dropout',\n",
       " 'dual',\n",
       " 'dualiti',\n",
       " 'due',\n",
       " 'durat',\n",
       " 'dure',\n",
       " 'dynam',\n",
       " 'each',\n",
       " 'earli',\n",
       " 'earlier',\n",
       " 'earliest',\n",
       " 'early__stopping',\n",
       " 'eas',\n",
       " 'easi',\n",
       " 'easier',\n",
       " 'easili',\n",
       " 'ect',\n",
       " 'edg',\n",
       " 'edu',\n",
       " 'effect',\n",
       " 'effort',\n",
       " 'eigen',\n",
       " 'eigendecomposit',\n",
       " 'eigenvalu',\n",
       " 'eigenvector',\n",
       " 'eir',\n",
       " 'elbo',\n",
       " 'eleg',\n",
       " 'element',\n",
       " 'elementari',\n",
       " 'elimin',\n",
       " 'emb',\n",
       " 'embed',\n",
       " 'emerg',\n",
       " 'emiss',\n",
       " 'emphas',\n",
       " 'empir',\n",
       " 'empirical__risk',\n",
       " 'employ',\n",
       " 'empti',\n",
       " 'enabl',\n",
       " 'encod',\n",
       " 'encount',\n",
       " 'encourag',\n",
       " 'end',\n",
       " 'end__end',\n",
       " 'energi',\n",
       " 'enforc',\n",
       " 'engin',\n",
       " 'enhanc',\n",
       " 'enjoy',\n",
       " 'enough',\n",
       " 'ensembl',\n",
       " 'ensur',\n",
       " 'entail',\n",
       " 'entir',\n",
       " 'entiti',\n",
       " 'entri',\n",
       " 'entropi',\n",
       " 'envelop',\n",
       " 'environ',\n",
       " 'episod',\n",
       " 'epoch',\n",
       " 'eq',\n",
       " 'equal',\n",
       " 'equat',\n",
       " 'equilibrium',\n",
       " 'equip',\n",
       " 'equival',\n",
       " 'ergod',\n",
       " 'erm',\n",
       " 'err',\n",
       " 'error',\n",
       " 'escap',\n",
       " 'essenti',\n",
       " 'establish',\n",
       " 'estim',\n",
       " 'etc',\n",
       " 'euclidean',\n",
       " 'evalu',\n",
       " 'even',\n",
       " 'event',\n",
       " 'eventu',\n",
       " 'everi',\n",
       " 'evid',\n",
       " 'evolut',\n",
       " 'evolv',\n",
       " 'exact',\n",
       " 'exactli',\n",
       " 'examin',\n",
       " 'exampl',\n",
       " 'exce',\n",
       " 'exceed',\n",
       " 'excel',\n",
       " 'except',\n",
       " 'excess',\n",
       " 'exchang',\n",
       " 'exclud',\n",
       " 'exclus',\n",
       " 'execut',\n",
       " 'exhaust',\n",
       " 'exhibit',\n",
       " 'exibl',\n",
       " 'exist',\n",
       " 'exp',\n",
       " 'exp__exp',\n",
       " 'expand',\n",
       " 'expans',\n",
       " 'expect',\n",
       " 'expens',\n",
       " 'experi',\n",
       " 'experiment',\n",
       " 'expert',\n",
       " 'explain',\n",
       " 'explan',\n",
       " 'explicit',\n",
       " 'explicitli',\n",
       " 'exploit',\n",
       " 'explor',\n",
       " 'exploration__exploitation',\n",
       " 'exploratori',\n",
       " 'expon',\n",
       " 'exponenti',\n",
       " 'exponential__families',\n",
       " 'exponential__family',\n",
       " 'exposit',\n",
       " 'express',\n",
       " 'extend',\n",
       " 'extens',\n",
       " 'extent',\n",
       " 'extern',\n",
       " 'extra',\n",
       " 'extract',\n",
       " 'extrem',\n",
       " 'face',\n",
       " 'facilit',\n",
       " 'fact',\n",
       " 'factor',\n",
       " 'fail',\n",
       " 'failur',\n",
       " 'fair',\n",
       " 'fairli',\n",
       " 'fall',\n",
       " 'fals',\n",
       " 'famili',\n",
       " 'familiar',\n",
       " 'famou',\n",
       " 'fan',\n",
       " 'far',\n",
       " 'fashion',\n",
       " 'fast',\n",
       " 'faster',\n",
       " 'fastest',\n",
       " 'favor',\n",
       " 'feasibl',\n",
       " 'featur',\n",
       " 'feature__extraction',\n",
       " 'feature__selection',\n",
       " 'feature__space',\n",
       " 'feature__vector',\n",
       " 'fed',\n",
       " 'feed',\n",
       " 'feedback',\n",
       " 'feedforward',\n",
       " 'fellowship',\n",
       " 'fewer',\n",
       " 'fidel',\n",
       " 'field',\n",
       " 'fig',\n",
       " 'fig__shows',\n",
       " 'figur',\n",
       " 'figure__shows',\n",
       " 'filter',\n",
       " 'final',\n",
       " 'financi',\n",
       " 'find',\n",
       " 'fine',\n",
       " 'finer',\n",
       " 'finit',\n",
       " 'fire',\n",
       " 'first',\n",
       " 'firstli',\n",
       " 'fisher',\n",
       " 'fit',\n",
       " 'five',\n",
       " 'fix',\n",
       " 'focu',\n",
       " 'focus',\n",
       " 'fold',\n",
       " 'fold__cross',\n",
       " 'follow',\n",
       " 'forc',\n",
       " 'form',\n",
       " 'formal',\n",
       " 'format',\n",
       " 'formul',\n",
       " 'formula',\n",
       " 'fortun',\n",
       " 'forward',\n",
       " 'found',\n",
       " 'foundat',\n",
       " 'four',\n",
       " 'fourth',\n",
       " 'fraction',\n",
       " 'frame',\n",
       " 'framework',\n",
       " 'free',\n",
       " 'freedom',\n",
       " 'frequenc',\n",
       " 'frequent',\n",
       " 'fresh',\n",
       " 'frobeniu',\n",
       " 'from',\n",
       " 'front',\n",
       " 'full',\n",
       " 'fulli',\n",
       " 'fully__connected',\n",
       " 'function',\n",
       " 'fund',\n",
       " 'fundament',\n",
       " 'furr',\n",
       " 'furrmor',\n",
       " 'futur',\n",
       " 'future__work',\n",
       " 'gain',\n",
       " 'game',\n",
       " 'gamma',\n",
       " 'gap',\n",
       " 'gare',\n",
       " 'gaussian',\n",
       " 'gave',\n",
       " 'gene',\n",
       " 'gener',\n",
       " 'generalis',\n",
       " 'generative__model',\n",
       " 'genet',\n",
       " 'geometr',\n",
       " 'geometri',\n",
       " 'get',\n",
       " 'ghz',\n",
       " 'gibb',\n",
       " 'github',\n",
       " 'give',\n",
       " 'given',\n",
       " 'global',\n",
       " 'gmm',\n",
       " 'goal',\n",
       " 'gold',\n",
       " 'gold__standard',\n",
       " 'good',\n",
       " 'googl',\n",
       " 'govern',\n",
       " 'gradient',\n",
       " 'gradient__ascent',\n",
       " 'gradient__descent',\n",
       " 'gradual',\n",
       " 'grain',\n",
       " 'grant',\n",
       " 'graph',\n",
       " 'graphic',\n",
       " 'graphical__model',\n",
       " 'graphical__models',\n",
       " 'grate',\n",
       " 'gray',\n",
       " 'great',\n",
       " 'greater',\n",
       " 'greatli',\n",
       " 'greedi',\n",
       " 'greedili',\n",
       " 'green',\n",
       " 'grid',\n",
       " 'ground',\n",
       " 'ground__truth',\n",
       " 'group',\n",
       " 'grow',\n",
       " 'growth',\n",
       " 'guarante',\n",
       " 'guess',\n",
       " 'guid',\n",
       " 'guidanc',\n",
       " 'half',\n",
       " 'hand',\n",
       " 'hand__side',\n",
       " 'handl',\n",
       " 'handwritten',\n",
       " 'happen',\n",
       " 'hard',\n",
       " 'harder',\n",
       " 'hardwar',\n",
       " 'harmon',\n",
       " 'hast',\n",
       " 'have',\n",
       " 'head',\n",
       " 'heart',\n",
       " 'heat',\n",
       " 'heavi',\n",
       " 'heavili',\n",
       " 'height',\n",
       " 'held',\n",
       " 'help',\n",
       " 'henc',\n",
       " 'henceforth',\n",
       " 'here',\n",
       " 'heterogen',\n",
       " 'heurist',\n",
       " 'hidden',\n",
       " 'hidden__states',\n",
       " 'hierarch',\n",
       " 'hierarchi',\n",
       " 'high',\n",
       " 'high__dimensional',\n",
       " 'high__level',\n",
       " 'high__probability',\n",
       " 'highdimension',\n",
       " 'higher',\n",
       " 'higher__order',\n",
       " 'highest',\n",
       " 'highli',\n",
       " 'highlight',\n",
       " 'hindsight',\n",
       " 'hing',\n",
       " 'hinge__loss',\n",
       " 'histogram',\n",
       " 'histor',\n",
       " 'histori',\n",
       " 'hmm',\n",
       " 'hoc',\n",
       " 'hoeffd',\n",
       " 'hold',\n",
       " 'homogen',\n",
       " 'hope',\n",
       " 'horizon',\n",
       " 'horizont',\n",
       " 'hour',\n",
       " 'how',\n",
       " 'html',\n",
       " 'http',\n",
       " 'http__www',\n",
       " 'huge',\n",
       " 'hull',\n",
       " 'human',\n",
       " 'hundr',\n",
       " 'hxi',\n",
       " 'hybrid',\n",
       " 'hyper',\n",
       " 'hyper__parameters',\n",
       " 'hyperparamet',\n",
       " 'hyperplan',\n",
       " 'hypos',\n",
       " 'hyposi',\n",
       " 'hyposis__class',\n",
       " 'idea',\n",
       " 'ideal',\n",
       " 'ident',\n",
       " 'identif',\n",
       " 'identifi',\n",
       " 'iff',\n",
       " 'ignor',\n",
       " 'ii',\n",
       " 'iid',\n",
       " 'iii',\n",
       " 'ill',\n",
       " 'illustr',\n",
       " 'imag',\n",
       " 'imagin',\n",
       " 'immedi',\n",
       " 'impact',\n",
       " 'implement',\n",
       " 'impli',\n",
       " 'implic',\n",
       " 'implicit',\n",
       " 'implicitli',\n",
       " 'import',\n",
       " 'importance__sampling',\n",
       " 'importantli',\n",
       " 'impos',\n",
       " 'imposs',\n",
       " 'impract',\n",
       " ...]"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vocab import Vocab, UnkVocab\n",
    "v = Vocab()\n",
    "vocab_index = v.word2index(vocab, train=True)\n",
    "vocab_serial = dict(zip(vocab,vocab_index))\n",
    "\n",
    "\n",
    "file = open('Group102_vocab.txt', \"w\")\n",
    "\n",
    "for k, v in vocab_serial.items():\n",
    "    file.write(str(k) + ':'+ str(v) + '\\n')\n",
    "\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
