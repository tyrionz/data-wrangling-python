{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Assessment 2\n",
    "#### Student Name: Haoheng Zhu\n",
    "#### Student ID: 30376467\n",
    "\n",
    "Date: 09/11/2019\n",
    "\n",
    "Version: 1.4\n",
    "\n",
    "Environment: Python 3.6.5 and Anaconda 4.3.0 (64-bit)\n",
    "\n",
    "Libraries used:\n",
    "* pandas 0.19.2 (for data frame, included in Anaconda Python 3.6) \n",
    "* re 2.2.1 (for regular expression, included in Anaconda Python 3.6) \n",
    "* nltk 3.2.2 (Natural Language Toolkit, included in Anaconda Python 3.6)\n",
    "* nltk.collocations (for finding bigrams, included in Anaconda Python 3.6)\n",
    "* nltk.tokenize (for tokenization, included in Anaconda Python 3.6)\n",
    "* nltk.corpus (for stop words, not included in Anaconda, `nltk.download('stopwords')` provided)\n",
    "* pdfminer.six (for extracting info from PDF, included in Anaconda Python 3.6)\n",
    "* requests (allows to send http requests, included in Anaconda Python 3.6)\n",
    "* sklearn (for data mining and analysis, included in Anaconda Python 3.6)\n",
    "* os (operating syster interface, included in Anaconda Python 3.6)\n",
    "* tqdm (an extensible progress bar, included in Anaconda Python 3.6)\n",
    "* vocab (for nltk processing, included in Anaconda Python 3.6)\n",
    "* pandas (for data structures and data analysis, included in Anaconda Python 3.6)\n",
    "\n",
    "## 1. Introduction\n",
    "This assignment exams the skills to parse pdf files with various nltk tools. The objective PDF files can be obtained by downloading the urls in `Group102.pdf`. Tasks are the following:\n",
    "\n",
    "1. Generate a sparse representation for Paper Bodies and save them to \n",
    "  1.  Vocabulary index file\n",
    "  2.  Sparse count vector file\n",
    "2. Generate a `CSV` file named (stats.csv) containing three columns:\n",
    " 1. Top 10 most frequent terms appearing in all __*Titles*__\n",
    " 2. Top 10 most frequent __*Authors*__\n",
    " 3. Top 10 most frequent terms appearing in all __*Abstracts*__\n",
    " \n",
    "More details for each task will be given in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Import libraries \n",
    " * __*Main*__ libraries are:\n",
    "   * pdfminer\n",
    "   * requests\n",
    "   * __*nltk*__\n",
    "   * __*sklearn*__\n",
    "   * vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/home/angu0069/FIT5196/jupyter/lib/python3.6/site-packages/tqdm/autonotebook/__init__.py:18: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /srv/home/angu0069/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import pdfminer\n",
    "import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm.autonotebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import io\n",
    "from io import StringIO\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import sys, getopt\n",
    "\n",
    "import nltk.data\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "from nltk.collocations import BigramCollocationFinder \n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from itertools import chain\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "\n",
    "from nltk.probability import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "nltk.download('punkt')\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "from vocab import Vocab, UnkVocab\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert PDF to TXT \n",
    "\n",
    "* 3.1  Define a pdf to txt function to convert PDF to TEXT\n",
    "\n",
    "__Functions to use__:\n",
    "   * PDFResourceManager()\n",
    "   * io.StringIO()\n",
    "   * TextConverter()\n",
    "   * PDFPageInterpreter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdftable_2txt(fname): \n",
    "    \n",
    "    #########################################\n",
    "    # This function use pdfminer to extract\n",
    "    # texts from pdf file, and return output\n",
    "    # as raw data\n",
    "    #########################################\n",
    "    \n",
    "    \n",
    "    output = io.StringIO()\n",
    "    # io.StringIO is a class. It handles Unicode. It reflects the preferred Python 3 library structure\n",
    "    \n",
    "    manager = PDFResourceManager()\n",
    "    # generate a PDF resource manager which is an object that stores shared resources\n",
    "    \n",
    "    txt_converter = TextConverter(manager, output, laparams=LAParams())\n",
    "    # convert PDF to txt\n",
    "    \n",
    "    interpreter = PDFPageInterpreter(manager, txt_converter)\n",
    "    # generate a PDF interpreter object.\n",
    "    \n",
    "    file_name = open(fname, 'rb') # open in read and binary mode\n",
    "    \n",
    "    for item in PDFPage.get_pages(file_name):\n",
    "        interpreter.process_page(item)\n",
    "        # process each page at a time\n",
    "    file_name.close()\n",
    "    txt_converter.close()\n",
    "    txt = output.getvalue()\n",
    "    output.close\n",
    "    return txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Use pdf_txt to convert Group102.pdf\n",
    "\n",
    "* store the data with __*dataframe*__ so that they can be processed by each row\n",
    "  * open 'Group102.pdf' file\n",
    "  * create dataframe with pandas\n",
    "  * rename column names with 'filename' and 'url'\n",
    "  * drop the original column name '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 0: read the data table pdf to download file. \n",
    "\n",
    "data = pdftable_2txt(\"Group102.pdf\")\n",
    "# generate a txt file to store the strings extracted from Group102.pdf (the url)\n",
    "file = open('Group102.txt', \"w\")\n",
    "file.write(data)\n",
    "file.close()\n",
    "filepath = 'Group102.txt'\n",
    "\n",
    "\n",
    "with open(filepath) as f: lineList = f. readlines()\n",
    "# retrieve urls from each line\n",
    "\n",
    "lineList = [line for line in lineList if line[0] == 'P']\n",
    "    # each url line starts with PP\\w+.pdf then followed by a space then the url (except the top few lines)\n",
    "\n",
    "df = pd.DataFrame(lineList)\n",
    "    # generate a dataframe to store the urls for faster process\n",
    "\n",
    "df[0] = df[0].apply(lambda x: x.split(\" \"))\n",
    "    # apply split() to each line in the series to separte filenames from urls\n",
    "\n",
    "df['filename'] = df[0].apply(lambda x: x[0])\n",
    "    # create a column 'filename' and store the filename\n",
    "df['url'] = df[0].apply(lambda x: x[1].strip())\n",
    "    # create a column 'url' and store the urls\n",
    "    \n",
    "df=df.drop(0,axis = 1)\n",
    "    # drop the original column\n",
    "os.remove('Group102.txt')\n",
    "    # remove the txt file because it's no longer needed. All data are now in df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Download the required pdf\n",
    "\n",
    "Main function to use:\n",
    " * check if the directory exists, using control structure with __os.path.exists()__\n",
    " * __makedirs__ --> create directory\n",
    " * __requests.get()__ --> retrieve data from specified resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data'): \n",
    "    os.makedirs('data') # make a directory for dataset, to store all the pdf files downloaded\n",
    "    for each in tqdm(df.iterrows(), total = len(df['filename'])): \n",
    "        response = requests.get(each[1][1])\n",
    "        # each[1] is the pandas series that stores filename and urls\n",
    "        # each[1][1] is the urls\n",
    "        # get is a request method that download data from specified resource\n",
    "        # store the information retrieved in a Response object, named response\n",
    "        with open('data/'+ str(each[1][0]),'wb') as f:\n",
    "            # generate the pdf files according to their filenames\n",
    "            f.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Preparation\n",
    "\n",
    "Load stopword from given **stopwords_en.txt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An empty list to store all the given stopwords\n",
    "stopwords=[]\n",
    "\n",
    "#Opening the given stopwords file and storing the words in the stopwords list\n",
    "with open('stopwords_en.txt') as f:\n",
    "    stopwords = f.read().splitlines()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdfminer sometimes cannot convert latin symbols like ﬀ (ff) and fi, or ffl, so this function is to translate\n",
    "#these into normal ascii symbols for more accurate data. ﬀ, ﬁ\n",
    "LATIN2ASCII = {\n",
    "  0xfb00: 'ff',\n",
    "  0xfb01: 'fi',\n",
    "  0xfb02: 'fl',\n",
    "  0xfb03: 'ffi',\n",
    "  0xfb04: 'ffl',\n",
    "  0x00df: 'ss',\n",
    "  0xfb05: 'ft',\n",
    "  0xfb06: 'st',\n",
    "}\n",
    "\n",
    "def latin2ascii(s):\n",
    "    return ''.join(LATIN2ASCII.get(ord(c),c) for c in s )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Convert all pdf to txt files\n",
    " * use _**pdf2txt.py**_ command to convert\n",
    " * makedirs() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_text(file):\n",
    "    \n",
    "    #############################################\n",
    "    # pdf_to_text will make directory text_data\n",
    "    # to store all converted files if the text_\n",
    "    # data directory does not exist. \n",
    "    #############################################\n",
    "    \n",
    "    if not os.path.exists('text_data'): #if the text_data dir not exist\n",
    "        os.makedirs('text_data') #make one\n",
    "        \n",
    "    file_txt = file[:-3]+'txt'\n",
    "        #create name for text file\n",
    "    \n",
    "    !pdf2txt.py -o text_data/$file_txt data/$file\n",
    "        #convert to txt files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Guideline (Orders)\n",
    "1. Tokenize \n",
    " * segmentation before tokenize\n",
    "2. Normalize \n",
    "3. Bigram\n",
    "4. Stopwords Removal\n",
    "5. Remove Rare Tokens \n",
    "6. Remove Tokens Less Than 3 Length\n",
    "7. Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1 - (E & A) : Tokenize - Normalize\n",
    "\n",
    "Tokens must be normalized to lowercase except the capital tokens appearing in the middle of a sentence/line. (use sentence segmentation to achieve this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_lower(sentence): \n",
    "    \n",
    "    ########################################################\n",
    "    # This blocks of code is to deal with the problem of \n",
    "    # pdfminer, some software producing pdf files may not \n",
    "    # properly display these symbols like ff, fi,fl, so \n",
    "    ########################################################\n",
    "    \n",
    "    sentence = re.sub(r'[\\357\\254\\200]+', 'ff', sentence)\n",
    "    sentence = re.sub(r'[\\357\\254\\201]+', 'fi', sentence)\n",
    "    sentence = re.sub(r'[\\357\\254\\202]+', 'fl', sentence)\n",
    "    sentence = re.sub('fffi ', 'fi', sentence)\n",
    "    sentence = re.sub('ff ', 'ff', sentence)\n",
    "    ########################################################\n",
    "    \n",
    "    \n",
    "    aux_sentence = '' #inititate sentence\n",
    "    cap_set = re.findall(r'(?!^)\\b([A-Z]\\w+)',sentence) \n",
    "        #store the capital words that are not in a cap_set\n",
    "        #that are not in the begninning of the sentence\n",
    "        \n",
    "    # If the words is not in the cap_set, then word is lowered\n",
    "    # first and then added to aux_sentence.\n",
    "    for word in sentence.split(\" \"):\n",
    "        if (word not in cap_set):\n",
    "            aux_sentence += word.lower() + str(' ')\n",
    "        else:\n",
    "            aux_sentence += word + str(' ')\n",
    "            \n",
    "    return  aux_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step E - Normalize\n",
    "\n",
    "### 4.2 Define get_data function to extract **body** , __author__ , __title__ , __abstract__\n",
    "\n",
    "store the data in the format of dictionary. With **file_name** being keys\n",
    " * segment the sentence using Punkt's sent_detector\n",
    " * loop through sentence to find marking words\n",
    "   * __*title*__ is before 'Authored by'\n",
    "   * __*authors*__ are between 'Authored by:' and 'Abstract'\n",
    "   * __*abstract*__ is between '1 Abstract' and 'Paper Body'\n",
    "   * __*body*__ is after '2 Paper Body'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(directory): \n",
    "    \n",
    "    body_dict={}   \n",
    "    author_dict = {}\n",
    "    title_dict = {}\n",
    "    abstract_dict = {}\n",
    "    for filename in tqdm(os.listdir(directory)):\n",
    "        # listdir returns a list containing the names of the entry in the directory given the path\n",
    "            filepdf = filename.replace('.txt','')\n",
    "            \n",
    "            with open(str(os.path.join(directory, filename))) as f: raw_body = latin2ascii(f.read())\n",
    "            #open the txtfile and read data\n",
    "            \n",
    "            sentence_list = sent_detector.tokenize(raw_body.strip())\n",
    "            #tokenized sentence\n",
    "            \n",
    "            #Get title dict#####################################\n",
    "            for i in range(len(sentence_list)):\n",
    "                if 'Authored by' in sentence_list[i]:\n",
    "                    title = sentence_list[i][:sentence_list[i].index('Authored by')]\n",
    "                    title = title.strip()\n",
    "                    title = title.replace('\\n','')\n",
    "                    break\n",
    "\n",
    "            title_dict[filepdf]= title.lower()\n",
    "            # store the title in title_dict dictionary in lower case with the key 'filepdf'\n",
    "            ####################################################\n",
    "            \n",
    "            #Extract author dict################################\n",
    "            author = ''\n",
    "            start = 0\n",
    "            stop = 0\n",
    "            for i in range(len(sentence_list)):        \n",
    "                if 'Authored by' in sentence_list[i]:\n",
    "                    #find the line that contains 'Authored by'\n",
    "                    start = i \n",
    "                    #mart the beginning of the block\n",
    "                    \n",
    "                if 'Abstract' in sentence_list[i]:\n",
    "                    #find the line that contains 'Abstract'\n",
    "                    stop = i\n",
    "                    #mark the ending of the block\n",
    "                    break        \n",
    "                    \n",
    "            for i in range(start, stop+1):\n",
    "                temp = sentence_list[i]\n",
    "                if 'Authored by' in temp:\n",
    "                    temp = temp[temp.index('Authored by:'):]\n",
    "                        #Get the data from 'Authored by'\n",
    "                    temp = temp.replace('Authored by:','').strip()\n",
    "                        #Delete the substring 'Authored by'\n",
    "                    \n",
    "                if 'Abstract' in temp:\n",
    "                    temp =  temp[:temp.index('Abstract')]\n",
    "                        #Get the data to 'Abstract'\n",
    "                author += temp + str(' ') \n",
    "                    #add to author string\n",
    "            author_dict[filepdf] = author.strip().split('\\n')\n",
    "            # store the aurhors in author_dict dictionary in lower case with the key 'filepdf' \n",
    "            ####################################################\n",
    "            \n",
    "            \n",
    "            #Get abstract dict##################################\n",
    "            abstract = []\n",
    "            start = 0\n",
    "            stop = 0\n",
    "            #Loop through sentence\n",
    "            for i in range(len(sentence_list)):\n",
    "                if 'Abstract' in sentence_list[i]:\n",
    "                    #Find the sentence containing 'Abstract'\n",
    "                    start = i\n",
    "                        #mart the beginning of the block\n",
    "                if 'Paper Body' in sentence_list[i]:\n",
    "                    #Find the sentence containing 'Paper Body'\n",
    "                    stop = i\n",
    "                        #mart the ending of the block\n",
    "                    break\n",
    "\n",
    "            #This block slicing the block of abstract   \n",
    "            for i in range(start, stop+1):\n",
    "                temp = sentence_list[i]\n",
    "                if 'Abstract' in temp:\n",
    "                    temp = temp[temp.index('Abstract'):]\n",
    "                        #Get the data from 'Abstract'\n",
    "                    temp = temp.replace('Abstract','')\n",
    "                        #Delete term Abstract\n",
    "                    temp = temp.strip()\n",
    "                if 'Paper Body' in sentence_list[i]:\n",
    "                    temp =  temp[:temp.index('1 Paper Body')]\n",
    "                        #Get the data to '1 Paper Body'\n",
    "                abstract.append(selective_lower(temp))             \n",
    "            abstract_dict[filepdf] = \" \".join(abstract)\n",
    "            # use a single space to join the sentences\n",
    "            # store the abstract in abstract_dict dictionary in lower case with the key 'filepdf'\n",
    "            ####################################################\n",
    "            \n",
    "            #extract body######################################\n",
    "            body = []\n",
    "            start = 0\n",
    "            stop = 0\n",
    "            for i in range(len(sentence_list)):\n",
    "                if 'Paper Body' in sentence_list[i]:\n",
    "                    #Find the sentence containing 'Paper Body'\n",
    "                    start = i #mark the start of block   \n",
    "                if '2 References' in sentence_list[i]:\n",
    "                    #Find the sentence containing '2 References'\n",
    "                    stop = i #mark the ending of block\n",
    "                    break\n",
    "            # this is to find the start and stop of Paper body              \n",
    "            for i in range(start, stop+1):\n",
    "                temp = sentence_list[i]\n",
    "                if 'Paper Body' in sentence_list[i]:\n",
    "                    temp = temp.replace('1 Paper Body\\n\\n','')\n",
    "                        #if 'Paper Body' in sentence, then delete the term\n",
    "                if '2 References' in sentence_list[i]:\n",
    "                    temp = temp.replace('2 References\\n\\n','')\n",
    "                        #if '2 References' in sentence, then delete the term\n",
    "                body.append(selective_lower(temp)) \n",
    "                # Normalize the sentence, and then added to body\n",
    "            body_string = \" \".join(body)\n",
    "            body_dict[filepdf] = body_string\n",
    "            # use a single space to join the sentences\n",
    "            # store the body in body_dict dictionary in lower case with the key 'filepdf' \n",
    "            ####################################################          \n",
    "            \n",
    "    return title_dict, author_dict, abstract_dict, body_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a directory text_data to store converted txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caf6fe465d404986a130e05bdd9a096f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#create a folder to store converted files\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "if os.path.exists('text_data'): #if the text_data exist, remove\n",
    "    shutil.rmtree(\"text_data/\") #remove\n",
    "    \n",
    "os.makedirs('text_data') #create new directory text_data\n",
    "\n",
    "for file in tqdm(df['filename']): #convert all the pdf file to txt\n",
    "    pdf_to_text(file)\n",
    "\n",
    "pathtxt = 'text_data/' #define the path linked to text_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Handle the sentence with leftover words that pdfminer doesn't process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix(sentence):\n",
    "    \n",
    "    #########################################################\n",
    "    # Some documents, pdfminer cannot capture the \"The\", and \n",
    "    # some with displayed ﬁ ﬀ that can be displayed in text file\n",
    "    # but cannot be displayed after read by python \n",
    "    #########################################################\n",
    "    sentence = sentence.replace('The','')\n",
    "    sentence = sentence.replace('ﬁ','fi')\n",
    "    sentence = sentence.replace('ﬀ','ff')\n",
    "    sentence = sentence.replace('- ','')\n",
    "    sentence = sentence.replace('-\\n','')\n",
    "    sentence = sentence.replace('\\n',' ') \n",
    "    sentence = sentence.replace('the- orem','theorem')\n",
    "    sentence = sentence.replace(' orem ',' theorem')\n",
    "    sentence = sentence.replace(' oretical ','theoretical')\n",
    "    sentence = sentence.replace(' oretic ','theoretic')\n",
    "    sentence = re.sub('[^A-Za-z-]+', ' ', sentence) \n",
    "\n",
    "    return sentence.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step A - Tokenize\n",
    "\n",
    "### Retrieve title, author, abstract, body \n",
    "* call the User Defined Function get_data function\n",
    "* use UDF fix function to process the __paper body__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb885c15d59c4b009d9098d05b0100ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Four Dictionaries will be used to store data with key being filname without ''.pdf'\n",
    "title_dict, author_dict, abstract_dict, raw_body_dict = get_data(pathtxt)\n",
    "    # extract the corresponding content and store them correspondently\n",
    "body_dict = {}\n",
    "for i in list(raw_body_dict.keys()):\n",
    "    body_dict[i] = fix(raw_body_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step A\n",
    "tokenizer = RegexpTokenizer(\"[A-Za-z]\\w+(?:[-'?]\\w+)?\")\n",
    "    # the required RE expression\n",
    "\n",
    "def tokenize(body):\n",
    "    tokenized_body = tokenizer.tokenize(body_dict[body]) #tokenizing the string\n",
    "    return (body, tokenized_body) # return a tuple of file name and a list of tokens\n",
    "\n",
    "#calling the tokenize method in a loop for all the elements in the dictionary\n",
    "tokenized_body = dict(tokenize(body) for body in body_dict.keys()) \n",
    "    # tokenized_body has file_name as key and tokenized_words as values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 G: Bigrams\n",
    "\n",
    "First 200 meaningful bigrams  (i.e., collocations), based on highest total frequency in the  corpus, must  be  extracted  and  included  in  your  tokenization  process.  Bigrams should not include context-independent stopwords as part of them and they should be separated using double underscore i.e. __ (example: artifical__intelligence)\n",
    "\n",
    "   * initiate nltk.collocations.BigramAssocMeasures()\n",
    "   * use nltk.collocations.BigramCollocationFinder.from_words() to measure all_tokens\n",
    "   * use apply_word_filter() to filter length <3 and lower()\n",
    "   * use nbest() to find top 200 bigrams\n",
    "   * use MWETokenizer() to merge multi words into a single token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step G\n",
    "#Finding the top 200 bigrams\n",
    "all_tokens = list(chain.from_iterable(tokenized_body.values()))\n",
    "    #collect all tokens from tokenized_body\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    #initiate a bigram_measure, \n",
    "    #collocations are expressions of two-words that usually occur at the same time (collocated)\n",
    "bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(all_tokens)\n",
    "    # store all the found bigram_measures in bigram_finder\n",
    "bigram_finder.apply_word_filter(lambda w: len(w) < 3)\n",
    "    # filter the words that are less than 3 in length (that's )\n",
    "bigram_finder.apply_word_filter(lambda w: w.lower() in stopwords)\n",
    "    # and filter the words that are w.lower() in ignored_words)\n",
    "    \n",
    "_list = bigram_finder.ngram_fd.most_common(200) # Top 200 bigrams\n",
    "\n",
    "bigrams_list = [tup for tup, freq in _list] # save to a list\n",
    "\n",
    "#Preserving these bigrams and putting it back in the dictionary, along with the unigrams\n",
    "mwetokenizer = MWETokenizer(bigrams_list,separator='__')\n",
    "\n",
    "#colloc_body is a dictionary that contains both the bigrams as well as the unigrams\n",
    "# the keys are the mwetokenized file_names, the corresponding values are list\n",
    "colloc_body =  dict((body, mwetokenizer.tokenize(data)) for body,data in tokenized_body.items())\n",
    "all_tokens = list(chain.from_iterable(colloc_body.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24954"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check point 1 after tokenizing and obtaining bigrams:\n",
    "len(set(all_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 B Stopwords Removal\n",
    "\n",
    "The context-independent and context-dependent (with the threshold set to %95) stop words must be removed from the vocab. The context-independent stop words list (i.e, stopwords_en.txt) provided in the zip file must be used. In this step, we will remove the context independent stopwords.\n",
    "\n",
    "* using provided stopwords to check whether a token needs to be removed\n",
    "  * use control structure:\n",
    "  if token.lower() not in stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step B #removing stopwords\n",
    "for file in list(colloc_body.keys()):            \n",
    "    colloc_body[file] = [token for token in colloc_body[file] if token.lower() not in stopwords]\n",
    "        # remove stop words from each file\n",
    "all_tokens = list(chain.from_iterable(colloc_body.values()))\n",
    "    # store all the tokens from stopwords removed tokenized_body\n",
    "    # chain.from_iterable is an alternate constructor for chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24322"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check point 2 after removing stopwords:\n",
    "len(set(all_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : D Rare Tokens Removal\n",
    "Rare tokens (with the threshold set to 3%) must be removed from the vocab, and context dependent tokens appearing in at least  95% of documents. The number of documents in which a specific word appear will be calculate, which will help us remove context dependent stopwords with threshold at least 95% as well.\n",
    "\n",
    "To get document frequency, need to:\n",
    " * Obtain unique tokens from colloc_body\n",
    "   * utilize set() to obtain the unique tokens\n",
    " * Obtain word frequency distribution across documents\n",
    "   * use FreqDist() from nltk library\n",
    " * Calculate document frequency\n",
    "   * ${Word Frequency \\over Total Documents}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c80e6aeaff9427cadab825672bc732a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Step D # removing rare token(3%) and tokens appearing in at least 95% of documents\n",
    "words_per_doc = list(chain.from_iterable([set(value) for value in colloc_body.values()]))\n",
    "    # get unique tokens from colloc_body.values(). then list them\n",
    "wpd = FreqDist(words_per_doc)\n",
    "    # get the distribution of the unique tokens (words)\n",
    "word_to_remove = []\n",
    "#create a list of words to remove based on 95% 03% criteria\n",
    "for word, count in wpd.items():\n",
    "    # wpd.items() is a dictionary contains the word distribution.\n",
    "    # the keys are the words, the values are the frequency\n",
    "    \n",
    "    if (count/len(list(colloc_body.keys())) < 0.03) or (count/len(list(colloc_body.keys())) >= 0.95):\n",
    "        # word_appearance: number of documents that contains this word, or the value in wpd.\n",
    "        # document frequency = word_appearance/total_documents (200)\n",
    "        # thus count is the number of documents that contains its corresponding word.\n",
    "        # lent(list(colloc_body.keys())) is a number all the documents (200 documents)\n",
    "        # so count/len(list(colloc_body.keys())) gives the document frequency\n",
    "        \n",
    "        word_to_remove.append(word)\n",
    "#cleaning 95% 03% words from tokenized_body and unitoken\n",
    "for file in tqdm(list(colloc_body.keys())):\n",
    "    colloc_body[file] = [token for token in colloc_body[file] if token not in word_to_remove]\n",
    "        # reform colloc_body dictionary, eliminating the word_to_remove (the 95% 03% words)\n",
    "all_tokens = list(chain.from_iterable(colloc_body.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4513"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check point 4 after removing '9503' tokens\n",
    "len(set(all_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 F Two Letter Words Removal\n",
    "Tokens with the length less than 3 should be removed from the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step F cleaning 2 letter words\n",
    "for file in list(colloc_body.keys()):            \n",
    "    colloc_body[file] = [token for token in colloc_body[file] if len(token) > 2]\n",
    "        # remove tokens that are less than length 2\n",
    "    \n",
    "all_tokens = list(chain.from_iterable(colloc_body.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4188"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check point 5 after removing '2 letter' tokens\n",
    "len(set(all_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 C Unigram Stemming\n",
    "Unigram  tokens  should  be  stemmed  using  the  Porter  stemmer.  (be  careful  that stemming performs lower casing by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3917729419eb460e998e7c1731263ffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e27be44b9e407d8bd14624da354fb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Step C Stemming\n",
    "\n",
    "#Using the porterstemmer method\n",
    "ps = PorterStemmer()\n",
    "\n",
    "#An empty string to store the content of a particular body\n",
    "strcontent=''\n",
    "\n",
    "#An empty dictionary to append the stemmed data back \n",
    "stemmed_dict=dict()\n",
    "\n",
    "#Looping to stem each value in the dictionary\n",
    "for key,body in tqdm(colloc_body.items()):  \n",
    "    # key is the key in collec_body, body is the list of corresponding value to its key\n",
    "    \n",
    "    for word in body:\n",
    "        if '__' in word: # if __ is in the word, it's bigram\n",
    "            strcontent=strcontent+ ' ' + word\n",
    "        else: # if not bigram\n",
    "            if (word[0].isupper() and word[1].islower()):\n",
    "                # words like Thompson, Marry or leading words in sentence\n",
    "                \n",
    "                word = ps.stem(word) # normalize the word to its original form\n",
    "                word = word[0].upper() + word[1:] # turn it back to its original format\n",
    "                strcontent= strcontent + ' ' + word # store and concatenate it with single space\n",
    "                \n",
    "            elif (word[0].isupper() and word[1].isupper()):\n",
    "                # special capital words in the middle of sentence\n",
    "                \n",
    "                word = ps.stem(word) # normalize the word to its original form\n",
    "                word = word.upper() # turn it back to its original format\n",
    "                strcontent=strcontent+ ' ' + word # store and concatenate it with single space\n",
    "            else:\n",
    "                strcontent=strcontent+ ' ' + ps.stem(word)    # store and concatenate it with single space\n",
    "    \n",
    "    #Assigning the string to the respective key\n",
    "    stemmed_dict[key]=strcontent\n",
    "    \n",
    "    #Again emptying the string to store the next body\n",
    "    strcontent=''\n",
    "\n",
    "#Loop to again word tokenize each body in the dictionary and assigning it back to its body number \n",
    "for key,body in tqdm(stemmed_dict.items()):\n",
    "    stemmed_dict[key]=word_tokenize(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2449"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check point 7 after stemming\n",
    "all_tokens = list(chain.from_iterable(stemmed_dict.values()))\n",
    "len(set(all_tokens)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Calculation and Analysis\n",
    "\n",
    "In this step, we aim to derive statistical numbers including `count vector`, `top 10 frequents` \n",
    "\n",
    "To achieve the goals, we need to:\n",
    " * Obtain unique tokens set\n",
    "   * utilize set() to get unique tokens\n",
    " * Obtain vocabulary count\n",
    "   * use vocab library\n",
    "   * use word2index() to derive word index of the document\n",
    " * Generate vocabulary vector output\n",
    "   * use FreqDist() to get the vocabulary distribution\n",
    "   * generate a dictionary to assign the word index and distribution to file_name\n",
    " * Top 10 frequency\n",
    "   * Obtain word distribution across document, FreqDist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Unique tokens \n",
    "\n",
    "Extract the unique tokens from all_tokens in order to perform vector value calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_tokens = []\n",
    "# store and list the stemmed body\n",
    "for file in list(stemmed_dict.keys()):\n",
    "    for word in stemmed_dict[file]:\n",
    "        uni_tokens.append(word)\n",
    "        # each item in uni_tokens is the stemmed word\n",
    "        \n",
    "vocab = list(set(uni_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = [str(tup[0] + str('__') + tup[1]) for tup in bigrams_list]\n",
    "    # adding 200 bigrams back to the vocab\n",
    "for tok in bigrams:            \n",
    "    if tok not in vocab:\n",
    "        vocab.append(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.sort()\n",
    "    # get the unique tokens by using set, and sort them for better access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Vocabulary Count\n",
    "   * use vocab library\n",
    "   * use word2index() to derive word index of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector file output\n",
    "from vocab import Vocab, UnkVocab\n",
    "import collections\n",
    "v = Vocab()\n",
    "vocab_index = v.word2index(vocab, train=True)\n",
    "    # create vocab index for each stemmed words\n",
    "\n",
    "vocab_serial = dict(zip(vocab,vocab_index))\n",
    "    # generate a dictionary that has the stemmed word as key and vocab_index as value\n",
    "\n",
    "vocab_serial = collections.OrderedDict(sorted(vocab_serial.items()))\n",
    "    #  sort the vocab_serial dictionary\n",
    "\n",
    "file = open('Group102_vocab.txt', \"w\", encoding = 'utf')\n",
    "    # write the sorted vocab_serial dictionary into Group102_vocab.txt\n",
    "for k, v in vocab_serial.items():\n",
    "    file.write(str(k) + ':'+ str(v) + '\\n')\n",
    "    # write the file in the format of 'key:value'\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Vocabulary Vector\n",
    "   * use FreqDist() to get the vocabulary distribution\n",
    "   * generate a dictionary to assign the word index and distribution to file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdict = {}\n",
    "for file, body in stemmed_dict.items():\n",
    "    # file_name is the key, body is the stemmed tokens\n",
    "    vdict[file] = FreqDist(body)\n",
    "        # file_name is the key, word distribution is the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to countvector\n",
    "file = open('Group102_count_vectors.txt', \"w\", encoding = 'utf')\n",
    "\n",
    "for filename in vdict.keys():\n",
    "    file.write(str(filename)+str(':'))\n",
    "    vector_list = []\n",
    "    for word in vdict[filename]:\n",
    "        # vdict[filename] gets all the keys\n",
    "      \n",
    "        vector_list.append(str(vocab_serial[word]) + str(':') + str(vdict[filename][word]))\n",
    "            # vocab_serial[word] gets the word count\n",
    "            # vdict[filename][word] gets the distribution value for that word\n",
    "            \n",
    "        vector_string = \",\".join(vector_list)\n",
    "    file.write(vector_string + str('\\n'))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Top 10 Frequency\n",
    "\n",
    "To get the top 10 frequency, need to:\n",
    " * Obtain word frequency distribution across documents\n",
    "   * use FreqDist()\n",
    " * Find top 10 most common \n",
    "   * use most_common() method by nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf0a04d13814e0a8579c1eb6927b4bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#tokenize with regex, same like body\n",
    "def tokenize_abstract(abstract):\n",
    "    tokenized_abstract = tokenizer.tokenize(abstract_dict[abstract]) #tokenizing the string\n",
    "    return (abstract, tokenized_abstract) # return a tuple of file name and a list of tokens\n",
    "\n",
    "tokenized_abstract = dict(tokenize_abstract(filepdf) for filepdf in abstract_dict.keys()) \n",
    "    # the keys are the pdf file names, the values are the tokenized abstract contents\n",
    "\n",
    "# remove stopwords from tokenized_abstract\n",
    "for file in  tqdm((list(tokenized_abstract.keys()))):\n",
    "    tokenized_abstract[file] = [token for token in tokenized_abstract[file] if token.lower() not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('learning', 200),\n",
       " ('data', 159),\n",
       " ('algorithm', 129),\n",
       " ('model', 126),\n",
       " ('show', 122),\n",
       " ('problem', 105),\n",
       " ('models', 105),\n",
       " ('method', 103),\n",
       " ('algorithms', 79),\n",
       " ('results', 73)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_abstract_tokens = list(chain.from_iterable(tokenized_abstract.values()))\n",
    "    # list the tokens in tokenized_abstract\n",
    "abstract_wpd = FreqDist(all_abstract_tokens)\n",
    "    # get the distribution of abstract tokens\n",
    "    \n",
    "top_10_abstract = abstract_wpd.most_common(10) # extract the most common 10 tokens\n",
    "top_10_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5101d8948727439aa3108591a2b483bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('learning', 46),\n",
       " ('models', 15),\n",
       " ('stochastic', 11),\n",
       " ('neural', 10),\n",
       " ('networks', 9),\n",
       " ('bayesian', 8),\n",
       " ('deep', 8),\n",
       " ('model', 7),\n",
       " ('probabilistic', 7),\n",
       " ('linear', 7)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenize with regex, same like body\n",
    "def tokenize_title(title):\n",
    "    tokenized_title = tokenizer.tokenize(title_dict[title]) #tokenizing the string\n",
    "    return (title, tokenized_title) # return a tuple of file name and a list of tokens\n",
    "\n",
    "tokenized_title = dict(tokenize_title(title) for title in title_dict.keys()) \n",
    "    # the keys are the pdf file names, the values are the tokenized title contents\n",
    "    \n",
    "# remove stopwords from tokenized_title    \n",
    "for file in  tqdm((list(tokenized_title.keys()))):\n",
    "    tokenized_title[file] = [token for token in tokenized_title[file] if token not in stopwords]\n",
    "    \n",
    "all_title_tokens = list(chain.from_iterable(tokenized_title.values()))\n",
    "    # list the tokens in tokenized_title\n",
    "title_wpd = FreqDist(all_title_tokens)\n",
    "    # get the distribution of title tokens\n",
    "    \n",
    "top_10_title = title_wpd.most_common(10) # extract the most common 10 tokens\n",
    "top_10_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the authors\n",
    "author_list = []\n",
    "for file, authors in author_dict.items():\n",
    "    # file is the file_names in author_dict's keys\n",
    "    # authors are the corresponding values of its keys\n",
    "    \n",
    "    for author in authors:\n",
    "        author_list.append(author)\n",
    "        # append the authors to the list\n",
    "        \n",
    "author_list = [author for author in author_list if author != '']\n",
    "    # remove empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Prateek Jain', 3),\n",
       " ('Michael I. Jordan', 3),\n",
       " ('Lawrence Carin', 3),\n",
       " ('Kristen Grauman', 3),\n",
       " ('Yee W. Teh', 3),\n",
       " ('Ron Meir', 3),\n",
       " ('Klaus-Robert M?ller', 3),\n",
       " ('Razvan Pascanu', 2),\n",
       " ('Peter Battaglia', 2),\n",
       " ('Sewoong Oh', 2)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_wpd = FreqDist(author_list)\n",
    "    # get the document frequency distribution of the authors\n",
    "top_10_author= author_wpd.most_common(10)\n",
    "    # get the most common 10 authors\n",
    "top_10_author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Write stats into CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat = {}\n",
    "stat['top10_terms_in_titles'] = [key[0] for key in top_10_title]\n",
    "    # top_10_tilte is a list of tuples of (title, number)\n",
    "    # assign the titles as values to the key 'top10_terms_in_titles'\n",
    "    \n",
    "stat['top10_authors'] = [key[0] for key in top_10_author]\n",
    "    # assign the authors as values to the key 'top10_authors'\n",
    "    \n",
    "stat['top10_terms_in_abstracts'] = [key[0] for key in top_10_abstract]\n",
    "    # assign the abstracts as values to the key 'top10_terms_in_abstracts' \n",
    "    \n",
    "stat_table = pd.DataFrame.from_dict(stat)\n",
    "    # generate a dataframe of statistical information\n",
    "stat_table.to_csv('Group102_stats.csv')\n",
    "stat_table.to_csv('Group102_stats.csv',index=False, encoding='utf')\n",
    "    # create and save the dataframe to csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "The main challenge for this assignment was to figure out a proper procedure to tokenize and stem the data. We tried several combinations from the given 7 steps; each time gave us different results.Moreover, Pdfminer, according to the documentation, <i>\"cannot safely concerted to Unicode all the characters.\"</i>. So it depends pretty much on the softwares that created pdf files.\n",
    "\n",
    "https://buildmedia.readthedocs.org/media/pdf/pdfminer-docs/latest/pdfminer-docs.pdf\n",
    "\n",
    "The more challeging is that sample output provided, according to Islam Nassar in Unit Announcement (12/0902019 7:55 AM) that is not correct <i> \"sample output released (sample_vocab, sample_count_vectors, and sample_stats) are only to be used as reference as to how the output should look like. You should not look too much into them and try to infer how to go about text preprocessing as it is not entirely accurate. The files have been generated randomly and therefore they don't really represent any ground truth.\"</i>\n",
    "\n",
    "\n",
    "Main learning:\n",
    " * pdfminer functions such as\n",
    "   * PDFResourceManager()\n",
    "   * io.StringIO()\n",
    "   * TextConverter()\n",
    "   * PDFPageInterpreter()\n",
    " * nltk functions such as\n",
    "   * FreqDist()\n",
    "   * nltk.collocations.BigramAssocMeasures()\n",
    "   * nltk.collocations.BigramCollocationFinder.from_words()\n",
    "   * apply_word_filter()\n",
    "   * nbest()\n",
    "   * MWETokenizer()\n",
    " * Vocab\n",
    "   * word2index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "- Steve B. *NLTK 3.2.5 Documentation* Retrieved from https://buildmedia.readthedocs.org/media/pdf/nltk/latest/nltk.pdf\n",
    "- NLTK Project. (2015). *Collocations*. Retrieved from http://www.nltk.org/howto/collocations.html\n",
    "- Levia3 pdfminer *Release 0.0.1* (2017) Retrieved from https://buildmedia.readthedocs.org/media/pdf/pdfminer-docs/latest/pdfminer-docs.pdf\n",
    "- Vocab 0.0.4 *Project Description* Retrieved from https://pypi.org/project/vocab/\n",
    "- Scikit-learn *Machine learning in python* Retrieved from https://scikit-learn.org/stable/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
