{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PROJ_LIB\"] = \"D:\\Anacoda\\pkgs\\proj4-5.2.0-ha925a31_1\\Library\\share\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.basemap import Basemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point #to find suburb\n",
    "from shapely.geometry.polygon import Polygon #to find suburb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #pandas tables\n",
    "import numpy as np #numpy for linear algrebra solution\n",
    "import matplotlib.pyplot as plt #EDA\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_rows', 500) #display of pandas\n",
    "pd.set_option('display.max_columns', 500)  #display of pandas\n",
    "pd.set_option('display.width', 1000)  #display of pandas\n",
    "\n",
    "import math #math operations\n",
    "from datetime import datetime #datetime manipulation\n",
    "\n",
    "import tqdm #progress bar\n",
    "from tqdm import tqdm_notebook as tqdm #progress bar\n",
    "from tqdm.autonotebook import tqdm #progress bar\n",
    " #progress bar\n",
    "\n",
    "from sklearn import linear_model  #linear regression \n",
    "import statsmodels.api as sm #linear regression \n",
    "\n",
    "import re #regex\n",
    "from contextlib import suppress #suppress error\n",
    "\n",
    "import networkx as nx #networkx to find shortest distance.\n",
    "\n",
    "import datetime as dt #to convert datetime value later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib \n",
    "%matplotlib inline\n",
    "import shapefile\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.collections import PatchCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the columns for the output.\n",
    "ori_col = [\"ID\",\n",
    "         \"Address\",\n",
    "         \"Suburb\",\n",
    "         \"Price\",\n",
    "         \"Type\",\n",
    "         \"Date\",\n",
    "         \"Rooms\",\n",
    "         \"Bathroom\",\n",
    "         \"Car\",\n",
    "         \"LandSize\",\n",
    "         \"Age\",\n",
    "         \"Latitude\",\n",
    "         \"Longitude\",\n",
    "         \"train_station_id\",\n",
    "         \"distance_to_train_station\",\n",
    "         \"travel_min_to_CBD\",\n",
    "         \"over_priced?\",\n",
    "         \"crime_A_average\",\n",
    "         \"crime_B_average\",\n",
    "         \"crime_C_average\",\n",
    "          \"closest_primary_school\",\n",
    "          \"distance_to_closest_primary\",\n",
    "          \"primary_school_ranking\",\n",
    "          \"closest_secondary_school\",\n",
    "          \"distance_to_closest_secondary\",\n",
    "          \"secondary_school_ranking\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = shapefile.Reader(\"VIC_LOCALITY_POLYGON_shp\") # all 3 files are used\n",
    "recs = sf.records()\n",
    "shapes = sf.shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "residents = pd.read_csv('Group102.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 720 row in the data\n"
     ]
    }
   ],
   "source": [
    "print('There are {} row in the data'.format(len(residents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 684 records in the data\n"
     ]
    }
   ],
   "source": [
    "print('There are {} records in the data'.format(len(residents['ID'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are duplicated rows in data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicated values\n",
    "residents = residents.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate the columns for later purposes.\n",
    "residents['Suburb'] = 'not available'\n",
    "residents['train_station_id'] = -1\n",
    "residents['distance_to_train_station'] = -1\n",
    "residents['travel_min_to_CBD'] = -1\n",
    "residents['over_priced?'] = -1\n",
    "residents['crime_A_average'] = -1\n",
    "residents['crime_B_average'] = -1\n",
    "residents['crime_C_average'] = -1\n",
    "residents['closest_primary_school'] = 'not available'\n",
    "residents['distance_to_closest_primary'] = -1\n",
    "residents['primary_school_ranking'] = -1\n",
    "residents['closest_secondary_school'] = 'not available'\n",
    "residents['distance_to_closest_secondary'] = -1\n",
    "residents['secondary_school_ranking'] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the record of the suburbs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the record above, we can get the name of the suburb from the seventh value of each record. Therefore, we will define a function that returns the suburb. The next info we will need is the geolocation of that suburb (long, lat). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import shapefile\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "import re\n",
    "import datetime as dt\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(141.74552399, -35.07228701),\n",
       " (141.74552471, -35.07201624),\n",
       " (141.74748471, -35.06367123),\n",
       " (141.74909525, -35.05681288),\n",
       " (141.74917403, -35.05647197),\n",
       " (141.75887404, -35.05225699),\n",
       " (141.77005396, -35.04914101),\n",
       " (141.77057401, -35.04899703),\n",
       " (141.76823304, -35.04349501),\n",
       " (141.76757096, -35.04270197),\n",
       " (141.76709398, -35.03982199),\n",
       " (141.76544215, -35.03808514),\n",
       " (141.76547768, -35.03782986),\n",
       " (141.76408494, -35.03523684),\n",
       " (141.76398063, -35.03505673),\n",
       " (141.76387633, -35.03487661),\n",
       " (141.75923139, -35.02629546),\n",
       " (141.77479178, -35.02631526),\n",
       " (141.77477947, -35.02789069),\n",
       " (141.79121543, -35.02632192),\n",
       " (141.79143466, -35.02629765),\n",
       " (141.79143596, -35.04874003),\n",
       " (141.79351097, -35.04729598),\n",
       " (141.79428796, -35.04709498),\n",
       " (141.79636183, -35.04970131),\n",
       " (141.79864596, -35.05329199),\n",
       " (141.79782903, -35.05527899),\n",
       " (141.79795996, -35.056872),\n",
       " (141.79605705, -35.05895805),\n",
       " (141.79644599, -35.06340397),\n",
       " (141.79743197, -35.06573301),\n",
       " (141.79758596, -35.06874604),\n",
       " (141.79950797, -35.07147102),\n",
       " (141.79989406, -35.07147891),\n",
       " (141.80991892, -35.07149865),\n",
       " (141.81017497, -35.05904903),\n",
       " (141.82738498, -35.05919797),\n",
       " (141.83315349, -35.05147187),\n",
       " (141.82998858, -35.04741387),\n",
       " (141.82849104, -35.04420226),\n",
       " (141.83461699, -35.04295396),\n",
       " (141.83543016, -35.04278972),\n",
       " (141.83961533, -35.04234615),\n",
       " (141.84461703, -35.04197777),\n",
       " (141.84447435, -35.04287326),\n",
       " (141.84480555, -35.04287488),\n",
       " (141.84513674, -35.0428765),\n",
       " (141.86138828, -35.04284731),\n",
       " (141.86138121, -35.0382533699999),\n",
       " (141.86545095, -35.03872827),\n",
       " (141.86542043, -35.03572918),\n",
       " (141.86563904, -35.035488),\n",
       " (141.86560997, -35.02634496),\n",
       " (141.86585099, -35.02474803),\n",
       " (141.86560299, -35.023568),\n",
       " (141.86559999, -35.02217004),\n",
       " (141.86558821, -35.01697525),\n",
       " (141.865369, -35.01697814),\n",
       " (141.86535192, -35.00768149),\n",
       " (141.86557004, -35.00754897),\n",
       " (141.86549827, -34.9986433),\n",
       " (141.86617501, -34.998637),\n",
       " (141.87719302, -34.9984340399999),\n",
       " (141.89743099, -34.99837598),\n",
       " (141.89747903, -35.00617497),\n",
       " (141.8974042, -35.00676905),\n",
       " (141.89742924, -35.01324666),\n",
       " (141.89751596, -35.01373797),\n",
       " (141.89752796, -35.01514101),\n",
       " (141.89794004, -35.01769102),\n",
       " (141.89755699, -35.019363),\n",
       " (141.89758533, -35.0306481),\n",
       " (141.89757342, -35.03760737),\n",
       " (141.89759403, -35.04766603),\n",
       " (141.89759996, -35.05108097),\n",
       " (141.89760497, -35.05290705),\n",
       " (141.90035203, -35.05713099),\n",
       " (141.90037598, -35.066664),\n",
       " (141.89832698, -35.07077702),\n",
       " (141.89832399, -35.07079598),\n",
       " (141.89778603, -35.07519598),\n",
       " (141.89759199, -35.07609497),\n",
       " (141.89758403, -35.08352398),\n",
       " (141.89756898, -35.09734098),\n",
       " (141.89781301, -35.10252898),\n",
       " (141.89751304, -35.10613097),\n",
       " (141.89734902, -35.10687303),\n",
       " (141.89735598, -35.10941198),\n",
       " (141.89741095, -35.12767702),\n",
       " (141.89630498, -35.12761998),\n",
       " (141.89631997, -35.12935401),\n",
       " (141.89610883, -35.13027697),\n",
       " (141.89606484, -35.14519341),\n",
       " (141.89606503, -35.14537404),\n",
       " (141.89170498, -35.145741),\n",
       " (141.89170567, -35.1459225999999),\n",
       " (141.8917069, -35.15039256),\n",
       " (141.89170976, -35.16055099),\n",
       " (141.89170896, -35.16073302),\n",
       " (141.89872398, -35.15984204),\n",
       " (141.89886595, -35.15982299),\n",
       " (141.89900603, -35.15980101),\n",
       " (141.89914602, -35.15977901),\n",
       " (141.89928496, -35.15975498),\n",
       " (141.89942501, -35.15972902),\n",
       " (141.89956399, -35.15970302),\n",
       " (141.899702, -35.15967499),\n",
       " (141.89984112, -35.15964629),\n",
       " (141.89999803, -35.15960997),\n",
       " (141.8999703, -35.16061859),\n",
       " (141.8999585, -35.1671574),\n",
       " (141.89993371, -35.18090535),\n",
       " (141.89993399, -35.18108397),\n",
       " (141.88913404, -35.18107903),\n",
       " (141.88913533, -35.18126009),\n",
       " (141.88910608, -35.19756353),\n",
       " (141.90040052, -35.19757644),\n",
       " (141.900647, -35.19757702),\n",
       " (141.89456999, -35.20727996),\n",
       " (141.88606504, -35.20966301),\n",
       " (141.88501003, -35.21152304),\n",
       " (141.88988398, -35.21152601),\n",
       " (141.88988353, -35.21179807),\n",
       " (141.88986641, -35.22620832),\n",
       " (141.89118583, -35.22620919),\n",
       " (141.89116523, -35.241798),\n",
       " (141.89116398, -35.24206798),\n",
       " (141.89331402, -35.24207298),\n",
       " (141.89335597, -35.26544597),\n",
       " (141.89357658, -35.26562747),\n",
       " (141.89363202, -35.274266),\n",
       " (141.89363395, -35.27454001),\n",
       " (141.86685904, -35.27838596),\n",
       " (141.85263604, -35.27528102),\n",
       " (141.85269356, -35.27660395),\n",
       " (141.85061834, -35.27734779),\n",
       " (141.83919966, -35.27719931),\n",
       " (141.83886974, -35.2771981),\n",
       " (141.83853984, -35.2771969),\n",
       " (141.83845302, -35.28122592),\n",
       " (141.82501847, -35.28116676),\n",
       " (141.82511046, -35.27982066),\n",
       " (141.80422233, -35.27968568),\n",
       " (141.80425509, -35.27556233),\n",
       " (141.80427126, -35.27352453),\n",
       " (141.80427203, -35.273254),\n",
       " (141.800519, -35.27325104),\n",
       " (141.79469102, -35.27406296),\n",
       " (141.78820802, -35.27143301),\n",
       " (141.78797697, -35.27140899),\n",
       " (141.78738801, -35.27134699),\n",
       " (141.77780324, -35.27062713),\n",
       " (141.76662901, -35.26893202),\n",
       " (141.76293799, -35.26879599),\n",
       " (141.76255202, -35.27035199),\n",
       " (141.75393302, -35.27114399),\n",
       " (141.74995403, -35.27141402),\n",
       " (141.74996796, -35.24597),\n",
       " (141.750212, -35.24594299),\n",
       " (141.75039998, -35.21062504),\n",
       " (141.75502998, -35.210788),\n",
       " (141.75509204, -35.17640702),\n",
       " (141.75438602, -35.17665598),\n",
       " (141.75163496, -35.17646896),\n",
       " (141.74675602, -35.17933195),\n",
       " (141.74422103, -35.18032601),\n",
       " (141.74421963, -35.18002721),\n",
       " (141.74416668, -35.16655838),\n",
       " (141.74416802, -35.165142),\n",
       " (141.74098297, -35.16515202),\n",
       " (141.74097566, -35.16307811),\n",
       " (141.74100421, -35.14677616),\n",
       " (141.74100401, -35.14659503),\n",
       " (141.74529703, -35.14660202),\n",
       " (141.74529773, -35.14642264),\n",
       " (141.74538826, -35.11916598),\n",
       " (141.74538901, -35.11889201),\n",
       " (141.74538898, -35.11889102),\n",
       " (141.74538998, -35.11889098),\n",
       " (141.74970902, -35.11954103),\n",
       " (141.74971012, -35.11926624),\n",
       " (141.749849, -35.1046979),\n",
       " (141.74982964, -35.10412705),\n",
       " (141.74983902, -35.09147111),\n",
       " (141.75019412, -35.09147099),\n",
       " (141.75018956, -35.07288415),\n",
       " (141.75018896, -35.07261203),\n",
       " (141.74552399, -35.07228701)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf.shapes()[0].points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From shape data we have the points, that show the boundary of the suburbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suburb(long, lat):\n",
    "    \n",
    "        \"\"\"\n",
    "        Return the name of the suburb if the geolocation is to be found\n",
    "        within the boundary of the suburbs (calculated using Polygon method)\n",
    "        Input is longitude, lattitude.\n",
    "        Output is name.\n",
    "        \"\"\"\n",
    "        \n",
    "        property_geoloc = Point(long, lat)\n",
    "        # Check if location is within suburb and edit data frame accordingly. \n",
    "        \n",
    "        for index_shape, shape in enumerate(shapes):\n",
    "            # get suburb boundary\n",
    "            suburb_boundary = Polygon(shape.points)\n",
    "                   \n",
    "            if property_geoloc.within(suburb_boundary):\n",
    "                return recs[index_shape][6]\n",
    "                \n",
    "                #if suburb name is found the loop is terminated\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get suburb name\n",
    "residents['Suburb'] = residents[['Longtitude','Lattitude']].apply(lambda x: get_suburb(*x), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time(string):\n",
    "    time = list(map(int, string.split(\":\")))\n",
    "    if time[0]>23:\n",
    "        time[0]  = 0 \n",
    "    return dt.time(*time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ptv():\n",
    "    \n",
    "    def __init__(self, link):\n",
    "        self.agency = pd.read_csv(link+\"/agency.txt\")\n",
    "        self.calendar = pd.read_csv(link+\"/calendar.txt\")\n",
    "        self.calendar_dates = pd.read_csv(link+\"/calendar_dates.txt\")\n",
    "        self.routes = pd.read_csv(link+\"/routes.txt\")\n",
    "        self.shapes = pd.read_csv(link+\"/shapes.txt\")\n",
    "        self.stops = pd.read_csv(link+\"/stops.txt\")\n",
    "        self.stop_times = pd.read_csv(link+\"/stop_times.txt\")\n",
    "        self.trips = pd.read_csv(link+\"/trips.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the GTFS Release http://data.ptv.vic.gov.au/downloads/PTVGTFSReleaseNotes.pdf:\n",
    "\n",
    "The PTV GTFS data has been exported by operational branches listed in the folder numbers below:  \n",
    " \n",
    "1 - Regional Train\n",
    "\n",
    "2 - Metropolitan Train \n",
    "\n",
    "3 - Metropolitan Tram \n",
    "\n",
    "4 - Metropolitan Bus \n",
    "\n",
    "5 - Regional Coach \n",
    "\n",
    "6 - Regional Bus \n",
    "\n",
    "7 - TeleBus \n",
    "\n",
    "8 – Night Bus \n",
    "\n",
    "10 - Interstate \n",
    "\n",
    "11 - SkyBus\n",
    "\n",
    "We only need the Train services information, which is stored in gtfs 1, and 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = ptv(\"gtfs/1/google_transit\")\n",
    "data_2 = ptv(\"gtfs/2/google_transit\")\n",
    "data_list = [data_1,data_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert arrival time and departure time to time value, fixing 24:00 to 00:00\n",
    "for dat in tqdm(data_list):\n",
    "    dat.stop_times[\"arrival_time\"] = dat.stop_times[\"arrival_time\"].apply(convert_time)\n",
    "    dat.stop_times[\"departure_time\"] = dat.stop_times[\"departure_time\"].apply(convert_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#service ids that runs during weekday to Souther Cross Station\n",
    "to_sc_ids = [] #\n",
    "\n",
    "# Loop through each data in data list:\n",
    "for dat in data_list:\n",
    "    # Loop through the calendar, see if trip available each weekday\n",
    "    for i in range(len(dat.calendar)):\n",
    "        if  dat.calendar.loc[i, \"monday\"]    == 1 and\\\n",
    "            dat.calendar.loc[i, \"tuesday\"]   == 1 and\\\n",
    "            dat.calendar.loc[i, \"wednesday\"] == 1 and\\\n",
    "            dat.calendar.loc[i, \"thursday\"]  == 1 and\\\n",
    "            dat.calendar.loc[i, \"friday\"]    == 1:\n",
    "            # If trip available each day, append to list to_sc_ids\n",
    "            to_sc_ids.append(dat.calendar.loc[i, \"service_id\"])\n",
    "            \n",
    "to_sc_ids = list(set(to_sc_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset data to only contain weekday trips\n",
    "weekday_trips = set()\n",
    "\n",
    "for dat in data_list:\n",
    "    trips = set(dat.trips[dat.trips[\"service_id\"].isin(to_sc_ids)][\"trip_id\"])\n",
    "    weekday_trips.update(trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(weekday_trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.stops[dat.stops['stop_id'] == 20043]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store valid stations times to Southern Cross Startion\n",
    "To_SC_times = {22180:0} # Start with Southern Cross as it will be safe, clearly travel time is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sc_time(data_list):\n",
    "\n",
    "    for data in tqdm(data_list, total = len(data_list)):\n",
    "        #loop through each stop\n",
    "        for stop in tqdm(list(data.stops[\"stop_id\"])):\n",
    "\n",
    "            # Find trips for that stop and trips to Southern Cross \n",
    "            # Station, the trip must have the destination to Southern Cross,\n",
    "            # trip available each weekday, and \n",
    "            weekday_sc_trips = data.stop_times[((dat.stop_times[\"trip_id\"].isin(weekday_trips)) &\n",
    "                                               (dat.stop_times[\"departure_time\"] <= dt.time(9,30)) &\n",
    "                                               (dat.stop_times[\"departure_time\"] >= dt.time(7)) &\n",
    "                                               (dat.stop_times[\"stop_id\"]==stop))|\n",
    "                                               ((dat.stop_times[\"stop_id\"].isin([22180, 20043])) &\n",
    "                                               (dat.stop_times[\"trip_id\"].isin(weekday_trips)))]\n",
    "\n",
    "            # Following variables used to track time to CDB\n",
    "            num_trips = 0 # number of trips\n",
    "            time = 0 # total time added\n",
    "\n",
    "            # Loop through the rest of the trip_ids in weekday_sc_trips\n",
    "            # to find out if the trip leads to the destination of\n",
    "            # Southern Cross Station\n",
    "            trip_ids = list(set(weekday_sc_trips[\"trip_id\"]))\n",
    "\n",
    "            # Loop through each trip in trip_ids\n",
    "            for trip in trip_ids:\n",
    "\n",
    "                # Given the trip in trip_ids, check if the trip connected\n",
    "                # to Southern Cross Station\n",
    "                check_trips = weekday_sc_trips[weekday_sc_trips[\"trip_id\"]==trip].reset_index()\n",
    "\n",
    "                # If check_trips has more than one row, which means that \n",
    "                # there is a direct connection to Southern Cross station. \n",
    "                if len(check_trips) > 1:\n",
    "                    # Set dep as the time departed from given station \n",
    "                    # and arr as the time ended at the Southern Cross station\n",
    "                    if check_trips.loc[0, \"stop_id\"] in [22180, 20043]:\n",
    "                        # In case the station before Southern Cross\n",
    "                        dep = check_trips.loc[1, \"departure_time\"]\n",
    "                        arr = check_trips.loc[0, \"arrival_time\"]\n",
    "                    else:\n",
    "                        # In case the station below Southern Cross\n",
    "                        dep = check_trips.loc[0, \"departure_time\"]\n",
    "                        arr = check_trips.loc[1, \"arrival_time\"]\n",
    "\n",
    "                    # Calculate time difference between departure and arrival\n",
    "                    time_delta = dt.datetime.combine(dt.date.today(), arr) -\\\n",
    "                            dt.datetime.combine(dt.date.today(), dep)\n",
    "\n",
    "                    # Only consider the if the time is larger than zero.\n",
    "                    if time_delta.total_seconds() >= 0:\n",
    "                        num_trips += 1                        # Increment counter for number of valid trips\n",
    "                        time += time_delta.total_seconds()/60  # Run tally for total time in minutes\n",
    "\n",
    "            # If there's a valid trip, add station and it's average time to dictionary.\n",
    "            if num_trips > 0:\n",
    "                average_time = time / num_trips\n",
    "                To_SC_times[stop] = average_time\n",
    "                \n",
    "    return To_SC_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "To_SC_times = get_sc_time(data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get closest station from station list to each house\n",
    "\n",
    "Now we can find the closest station from station list to each house sold, and also the time of travelling to CBD in minutes. In order to calculate the distance between two geolocation points, we will need to find thr arc distance. A function will be used to help us calculate the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(geoloc1, geoloc2):\n",
    "    #long, lat of geolocation 1\n",
    "    lat_1, long_1 = map(math.radians, geoloc1)\n",
    "    \n",
    "    #long, lat of geolocation 2\n",
    "    lat_2, long_2 = map(math.radians, geoloc2)\n",
    "    \n",
    "    #calculate arc\n",
    "    arc = math.sin(0.5*(lat_2 - lat_1)) ** 2 + math.cos(lat_1) * math.cos(lat_2) * math.sin(0.5*(long_2 - long_1)) ** 2 \n",
    "    \n",
    "    # calculate distance\n",
    "    dist = 2 * math.atan2(math.sqrt(arc), math.sqrt(1 - arc)) * 6378 * 1000\n",
    "    \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops1 = data_1.stops[data_1.stops[\"stop_id\"].isin(To_SC_times.keys())]\n",
    "stops2 = data_2.stops[data_2.stops[\"stop_id\"].isin(To_SC_times.keys())]\n",
    "\n",
    "stops = pd.concat([stops1,stops2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops.reset_index(drop= True)\n",
    "stops = stops.drop_duplicates()\n",
    "\n",
    "# Subset stops data based on valid stops from dictionaries\n",
    "stops = stops.reset_index(drop= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, loc in tqdm(residents.iterrows(),total = len(residents)):\n",
    "    \n",
    "    point_property = (residents.at[index, \"Lattitude\"], residents.at[index, \"Longtitude\"])  # Position for property\n",
    "\n",
    "    # Set first property in metro data set as benchmark\n",
    "    closest_station_id = stops.loc[0, \"stop_id\"]\n",
    "    station_point = (stops.loc[0, \"stop_lat\"], stops.loc[0, \"stop_lon\"])\n",
    "    shortest_distance = distance(point_property, station_point)\n",
    "    time_to_station = To_SC_times[closest_station_id]\n",
    "\n",
    "    # Iterate over each metro station and compare distances\n",
    "    for i in range(1, len(stops)):\n",
    "        station_point = (stops.loc[i, \"stop_lat\"], stops.loc[i, \"stop_lon\"])\n",
    "        distance_to_station = distance(point_property, station_point)\n",
    "\n",
    "        # Update values if station is closer\n",
    "        if distance_to_station < shortest_distance:\n",
    "            shortest_distance = distance_to_station\n",
    "            closest_station_id = stops.loc[i, \"stop_id\"]\n",
    "            time_to_station = To_SC_times[closest_station_id]\n",
    "    \n",
    "        # Impute values for closest train station, train station ID and travel time to CBD. \n",
    "    residents.at[index, \"train_station_id\"] = closest_station_id\n",
    "    residents.at[index, \"distance_to_train_station\"] = shortest_distance\n",
    "    residents.at[index, \"travel_min_to_CBD\"] = time_to_station"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average of type A,B,C crime in the local government\n",
    "area the property belongs to, in the three years prior to\n",
    "selling the property as the property. For example, if a\n",
    "property is sold in 2016, then you should calculate the\n",
    "average of the crime type A for 2013, 2014 and 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in council data\n",
    "with open(\"councils.txt\", 'r') as file: councils = file.readlines()\n",
    "\n",
    "# strip the new line off\n",
    "councils = [council.strip() for council in councils]\n",
    "\n",
    "# create column year\n",
    "residents[\"year\"] = residents[\"Date\"].apply(lambda x: x[-4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "council_list = {} #each suburb will have corresponding council in dictionary council_list\n",
    "\n",
    "#loop through each line in councils\n",
    "for line in councils:\n",
    "    #store council value\n",
    "    council = line.split(\":\")[0].strip()\n",
    "    \n",
    "    # get suburbs\n",
    "    suburbs = re.findall(r\"'(.+?)'\", line)\n",
    "    \n",
    "    #loop through suburbs\n",
    "    for suburb in suburbs:\n",
    "        council_list[suburb.upper()] = council  \n",
    "residents['council'] = residents['Suburb'].apply(lambda x: council_list[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residents[['Suburb','council']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data of the council and corresponding suburb is not correct, South Yarra do not belong to council Greater Shepparton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read crime data\n",
    "crime = pd.read_excel(\"crimebylocationdatatable-yearending31march2016.xlsx\",\n",
    "                        sheet_name=\"Table 1\", #the Table 1 sheet is where data is stored\n",
    "                        skiprows=19) #The first 19 rows is blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename necessary columns for further reference\n",
    "crime = crime.rename(columns={\"Apr - Mar reference period\":\"year\",\n",
    "                 \"Local Government Area\":\"council\",\n",
    "                 \"CSA Offence Division\":\"type\",\n",
    "                 \"Offence Count\":\"count\"})\n",
    "# Getting the letters for type (A,B,C)\n",
    "crime[\"type\"] = crime[\"type\"].apply(lambda x: x[0])\n",
    "\n",
    "#subset crime type A,B,C\n",
    "crime_A = crime[crime[\"type\"] == \"A\"]\n",
    "crime_B = crime[crime[\"type\"] == \"B\"]\n",
    "crime_C = crime[crime[\"type\"] == \"C\"]\n",
    "\n",
    "#for each crime type, groupby year, council to get total value\n",
    "crime_A = crime_A.groupby(by=[\"year\", \"council\", \"type\"]).sum().reset_index()\n",
    "crime_B = crime_B.groupby(by=[\"year\", \"council\", \"type\"]).sum().reset_index()\n",
    "crime_C = crime_C.groupby(by=[\"year\", \"council\", \"type\"]).sum().reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime['year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each row and find crime average\n",
    "for index, loc in tqdm(residents.iterrows(),total = len(residents)):\n",
    "    \n",
    "    # get year\n",
    "    year = int(residents.at[index, \"year\"])\n",
    "    \n",
    "    # get 3 years prior\n",
    "    years_prior = [str(year - i) for i in range(1,4)]\n",
    "    \n",
    "    residents.at[index,'crime_A_average'] = crime_A.loc[(crime_A[\"year\"].isin(years_prior)) & \n",
    "                                                    (crime_A[\"council\"]==residents.at[index, \"council\"])][\"count\"].mean()\n",
    "    \n",
    "    residents.at[index,'crime_B_average'] = crime_B.loc[(crime_B[\"year\"].isin(years_prior)) & \n",
    "                                                    (crime_B[\"council\"]==residents.at[index, \"council\"])][\"count\"].mean()\n",
    "    \n",
    "    residents.at[index,'crime_C_average'] = crime_C.loc[(crime_C[\"year\"].isin(years_prior)) & \n",
    "                                                    (crime_C[\"council\"]==residents.at[index, \"council\"])][\"count\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Over Priced?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> A boolean feature indicating whether or not the price of the property is higher than the median price of similar properties (with respect to bedrooms, bathrooms, parking_space, and property_type attributes) in the same suburb on the year of selling </i>\n",
    "\n",
    "We will use logistic linear model in this case to obtain value of True and False. If the house is over price (True) if the value is larger than the houses that have same number of bedrooms, bathrooms, car spaces, type, and were sold in the same year from the same suburb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, loc in tqdm(residents.iterrows(),total = len(residents)):\n",
    "    # Find values for comparison\n",
    "    \n",
    "    \n",
    "    # Bedroom\n",
    "    bed = residents.at[index, \"Rooms\"]\n",
    "    # Bathroom\n",
    "    bath = residents.at[index, \"Bathroom\"]\n",
    "    # Carspace\n",
    "    car_space = residents.at[index, \"Car\"]\n",
    "    # property type\n",
    "    ptype = residents.at[index, \"Type\"]\n",
    "    # suburb\n",
    "    sub = residents.at[index, \"Suburb\"]\n",
    "    # year\n",
    "    year = residents.at[index, \"year\"]\n",
    "    \n",
    "    # Filter data set to the rows that have same attributes of\n",
    "    # bedrooms, bathrooms, car_space, type, suburb, and year\n",
    "    check_price = residents[(residents[\"Rooms\"]    == bed) &\\\n",
    "                       (residents[\"Bathroom\"] == bath) &\\\n",
    "                       (residents[\"Car\"]      == car_space) &\\\n",
    "                       (residents[\"Type\"]     == ptype) &\\\n",
    "                       (residents[\"Suburb\"]   == sub) &\\\n",
    "                       (residents[\"year\"]     == year)]\n",
    "    \n",
    "    \n",
    "    # Check if the price is higher than the median price of other\n",
    "    # houses with same attributes\n",
    "    if residents.at[index, \"Price\"] > check_price[\"Price\"].median():\n",
    "        # Overprice\n",
    "        residents.at[index, \"over_priced?\"] = 1 \n",
    "    else:\n",
    "        # Not overprice\n",
    "        residents.at[index, \"over_priced?\"] = 0\n",
    "        \n",
    "# Update to boolean value True False        \n",
    "residents[\"over_priced?\"] = residents[\"over_priced?\"].apply(lambda x: True if x == 1 else False)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schools ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. __Primary__ School Ranking can be found in here: http://www.schoolcatchment.com.au/?p=12301\n",
    "\n",
    "### 2. __Secondary__ School Ranking can be found in https://sites.google.com/a/monash.edu/secondary-school-ranking/\n",
    "\n",
    "### 3. __Integrate__ both Primary and Secondary School information into `Group102.csv` dataframe (named residents)\n",
    "Need to use urllib.request to access website then use BeautifulSoup to scrape the websites. But it's complicated to interact with https://sites.google.com/a/monash.edu/secondary-school-ranking/ so we decide to download the contents as an html file from this url then use BeautifulSoup to parse the contents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Access [Primary School Ranking](http://www.schoolcatchment.com.au/?p=12301) and store the ranking information into a dataframe with pandas\n",
    " * obtain the following information:\n",
    "    * Ranking\n",
    "    * School Name\n",
    "    * State\n",
    "    * School Sector\n",
    "    * Location\n",
    "    * Total Enrolment\n",
    "    * Percentage\n",
    " * store the information into a dataframe\n",
    " * keep the best ranking if the school names are the same (as required by assignment specification)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_rank_url = 'http://www.schoolcatchment.com.au/?p=12301'\n",
    "\n",
    "# send request to access the url\n",
    "primaryClient = uReq(primary_rank_url)\n",
    "page_html = primaryClient.read()\n",
    "primaryClient.close()\n",
    "\n",
    "page_soup = soup(page_html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 We found that, by inspecting the website, the ranking information are under 'td' tag and the following:\n",
    "* column-1 contains rank\n",
    "* column-2 contains school name\n",
    "* column-3 contains state\n",
    "* column-4 contains school sector\n",
    "* column-5 contains location\n",
    "* column-6 contains total enrolment\n",
    "* column-7 contains percentage\n",
    "\n",
    "In order to obtain the information, we need to use findAll() method and store the corresponding information in a 'container', then we loop through that container and append the information to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "containers_1 = page_soup.findAll(\"td\",{\"class\":\"column-1\"})\n",
    "\n",
    "rank_list = []\n",
    "for container in containers_1:\n",
    "    rank_list.append(container.get_text()) # container.get_text() returns the ranking\n",
    "    \n",
    "containers_2 = page_soup.findAll(\"td\",{\"class\":\"column-2\"})\n",
    "\n",
    "primary_school_name_list = []\n",
    "for container in containers_2:\n",
    "    primary_school_name_list.append(container.get_text()) # container.get_text() returns the school_name\n",
    "    \n",
    "    \n",
    "containers_3 = page_soup.findAll(\"td\",{\"class\":\"column-3\"})\n",
    "\n",
    "state_list = []\n",
    "for container in containers_3:\n",
    "    state_list.append(container.get_text()) # container.get_text() returns the state\n",
    "\n",
    "\n",
    "containers_4 = page_soup.findAll(\"td\",{\"class\":\"column-4\"})\n",
    "\n",
    "school_sector_list = []\n",
    "for container in containers_4:\n",
    "    school_sector_list.append(container.get_text()) # container.get_text() returns the school_sector\n",
    "    \n",
    "    \n",
    "containers_5 = page_soup.findAll(\"td\",{\"class\":\"column-5\"})\n",
    "\n",
    "location_list = []\n",
    "for container in containers_5:\n",
    "    location_list.append(container.get_text()) # container.get_text() returns the location\n",
    "    \n",
    "containers_6 = page_soup.findAll(\"td\",{\"class\":\"column-6\"})\n",
    "\n",
    "enrolment_list = []\n",
    "for container in containers_6:\n",
    "    enrolment_list.append(container.get_text()) # container.get_text() returns the total enrolment\n",
    "    \n",
    "\n",
    "containers_7 = page_soup.findAll(\"td\",{\"class\":\"column-7\"})\n",
    "\n",
    "percentage_list = []\n",
    "for container in containers_7:\n",
    "    percentage_list.append(container.get_text()) # container.get_text() returns the percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create primary school dataframe to store primary shcool information\n",
    "column_names = ['Ranking','School Name','State','School Sector','Location','Total Enrolment','Percentage']\n",
    "primary_df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "primary_df.Ranking = rank_list\n",
    "primary_df['School Name'] = primary_school_name_list\n",
    "primary_df['State'] = state_list\n",
    "primary_df['School Sector'] = school_sector_list\n",
    "primary_df['Location'] = location_list\n",
    "primary_df['Total Enrolment'] = enrolment_list\n",
    "primary_df['Percentage'] = percentage_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 There are duplicates in rankings, so we keep the highest ranking and remove the duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_no_duplicate = primary_df.drop_duplicates(subset='School Name',keep='first')\n",
    "primary_no_duplicate = primary_no_duplicate.reset_index(drop= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Access [Secondary School Ranking](https://sites.google.com/a/monash.edu/secondary-school-ranking/) and store the ranking information into a dataframe with pandas\n",
    " * obtain the following information:\n",
    "     * School Name\n",
    "     * Ranking\n",
    "     * Change\n",
    "     * Median VCE Study Score\n",
    "     * 40+ Study Score\n",
    " * store the information into a dataframe\n",
    " * keep the best ranking if the school names are the same (as required by assignment specification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We download the https://sites.google.com/a/monash.edu/secondary-school-ranking/ to process because it would cause lots of troubles if we have to interact with the website using authenticate\n",
    "\n",
    "* Secondary School Ranking is saved as 'secondary-school-ranking.html'\n",
    "\n",
    "* from browsing the contents, we found that the rank information are under 'ul' tag and the contents are seperated by '\\n\\n\\n\\n' four new line characters, also that in each sinlge ranking content, the information is seperated by '\\n' one single new line character. So we need to clean the body before we can extract the desired information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import os\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "chromedriver = \"C:/Users/Caddy'sLenovo/Desktop/chromedriver\"\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "\n",
    "driver.get('https://sites.google.com/a/monash.edu/secondary-school-ranking/')\n",
    "\n",
    "sign_in_link = driver.find_element_by_partial_link_text('Sign in through Monash University')\n",
    "\n",
    "sign_in_link.click()\n",
    "\n",
    "driver.find_element_by_name('username').send_keys('my account')\n",
    "driver.find_element_by_name('password').send_keys('my password')\n",
    "driver.find_element_by_id('okta-signin-submit').click()\n",
    "\n",
    "#driver.find_element_by_name('answer').send_keys('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use beautifulsoup to read the contents in \"secondary-school-ranking.html\"\n",
    "infile_secondary = open(\"secondary-school-ranking.html\",\"r\")\n",
    "contents_secondary = infile_secondary.read()\n",
    "secondary_schools_soup = soup(contents_secondary,'html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate the secondary school information\n",
    "ul = secondary_schools_soup.find_all('ul')\n",
    "\n",
    "# store the school information\n",
    "contents = []\n",
    "for content in ul:\n",
    "    contents.append(content.get_text())\n",
    "\n",
    "# wrangle the contents, remove new lines\n",
    "rank_body = contents[2].strip()\n",
    "split_rank_body = rank_body.split('\\n\\n\\n\\n')\n",
    "\n",
    "# remove new lines and seperate by coma\n",
    "clean_rank_body = []\n",
    "for strings in split_rank_body:\n",
    "    clean_rank_body.append(strings.replace('\\n', ',')) # replace the original seperator <new line character> with coma \n",
    "\n",
    "# display one line of the cleaned body\n",
    "clean_rank_body[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create secondary_ranking dataframe to store secondary school information\n",
    "column_name = ['School Name', 'Ranking', 'Change', 'Median VCE Study Score', '40+ Study Score']\n",
    "\n",
    "secondary_ranking_df = pd.DataFrame(columns = column_name)\n",
    "\n",
    "secondary_school_name = []\n",
    "secondary_ranking = []\n",
    "change = []\n",
    "median_score = []\n",
    "forty_plus = []\n",
    "for item in clean_rank_body:\n",
    "    temp = item.split(',') # seperate the information by coma\n",
    "    secondary_school_name.append(temp[0])\n",
    "    secondary_ranking.append(int(temp[1]))\n",
    "    change.append(temp[2])\n",
    "    median_score.append(temp[3])\n",
    "    forty_plus.append(temp[4])\n",
    "    \n",
    "secondary_ranking_df['School Name'] = secondary_school_name\n",
    "secondary_ranking_df['Ranking'] = secondary_ranking\n",
    "secondary_ranking_df['Change'] = change\n",
    "secondary_ranking_df['Median VCE Study Score'] = median_score\n",
    "secondary_ranking_df['40+ Study Score'] = forty_plus\n",
    "\n",
    "# sort by 'Ranking' so that we can drop the duplicates and keep the first occurrence\n",
    "secondary_ranking_df = secondary_ranking_df.sort_values(by='Ranking')\n",
    "secondary_ranking_df = secondary_ranking_df.reset_index(drop= True)\n",
    "secondary_ranking_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates and keep the highest ranking\n",
    "\n",
    "secondary_no_duplicate = secondary_ranking_df.drop_duplicates(subset='School Name', keep='first')\n",
    "secondary_no_duplicate = secondary_no_duplicate.reset_index(drop= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Integrate Primary and Secondary School Ranking into residents dataframe (Group102.csv)\n",
    "\n",
    "We need to integrate the following:\n",
    "* ranking by primary and secondary schools\n",
    "  * use if-else control structure to match ranking and school names\n",
    "* closest primary and secondary schools\n",
    "  * use priviously defined distance() function to loop through dataframe to find closest distance, keep track with dataframe index\n",
    "  * then use dictionary to match the distance and the index and retrieve the school names respectively\n",
    "* distance from primary and secondary schools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile_schools = open(\"schools.xml\",\"r\")\n",
    "contents_schools = infile_schools.read()\n",
    "schools_soup = soup(contents_schools,'xml')\n",
    "\n",
    "schools_soup.school\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Replace html mark up with their corresponding signs\n",
    "\n",
    "* `&apos;` is actually `'` sign\n",
    "* we found this in `School_Name` and in `Address`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the \" ' \" sign from html mark up\n",
    "for i in schools_soup.findAll('School_Name'):\n",
    "    if i.text == \"&apos;\" in i.text:\n",
    "        i.string = \"'\"\n",
    "        \n",
    "for i in schools_soup.findAll('Address_Line_1'):\n",
    "    if i.text == \"&apos;\" in i.text:\n",
    "        i.string = \"'\"\n",
    "schools_soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Extract school name,  longtitude and latitude\n",
    "* school name is in `School_Name` tag\n",
    "* school type (primary/secondary/etc) is in `School_Type` tag\n",
    "* longtitude is in `X` tag\n",
    "* latitude is in `Y` tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract school name, school type, locations\n",
    "schools_xml_names_container = schools_soup.findAll(\"School_Name\")\n",
    "schools_xml_names_list = []\n",
    "for names in schools_xml_names_container:\n",
    "    schools_xml_names_list.append(names.get_text())\n",
    "\n",
    "schools_xml_type_container = schools_soup.findAll(\"School_Type\")\n",
    "schools_xml_type_list = []\n",
    "for types in schools_xml_type_container:\n",
    "    schools_xml_type_list.append(types.get_text())\n",
    "\n",
    "# X is actually the longtitude\n",
    "schools_xml_x_container = schools_soup.findAll(\"X\")\n",
    "schools_xml_x_list = []\n",
    "for longtitude in schools_xml_x_container:\n",
    "    schools_xml_x_list.append(longtitude.get_text())\n",
    "\n",
    "# Y is latitude\n",
    "schools_xml_y_container = schools_soup.findAll(\"Y\")\n",
    "schools_xml_y_list = []\n",
    "for latitude in schools_xml_y_container:\n",
    "    schools_xml_y_list.append(latitude.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a schools_xml dataframe in order to integrate school information obtained from websites\n",
    "schools_xml_colnames = ['School Name', 'School Type', 'X', 'Y', 'Ranking']\n",
    "schools_xml_df = pd.DataFrame(columns=schools_xml_colnames)\n",
    "schools_xml_df['School Name'] = schools_xml_names_list\n",
    "schools_xml_df['School Type'] = schools_xml_type_list\n",
    "schools_xml_df['X'] = schools_xml_x_list\n",
    "schools_xml_df['Y'] = schools_xml_y_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Integrate the ranking\n",
    "There are many duplicate school names in `schools.xml`, so before we can integrate the school ranking into `residents` dataframe, we need to do some preparation.\n",
    "* use set() to eliminate the duplicated school names\n",
    "* use for loop to assign ranking to corresponding school names if they can be found in primary and secondary school ranking website\n",
    "* pair school name and its ranking in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrate school rankings into schools_xml dataframe\n",
    "schools_names_dictionary = {}\n",
    "unique_school_name = set(schools_xml_names_list)\n",
    "\n",
    "for name in unique_school_name:\n",
    "    # integrate primary school ranking\n",
    "    if name in primary_school_name_list:\n",
    "        ranking = primary_no_duplicate.loc[primary_no_duplicate['School Name']==name].Ranking.values[0]\n",
    "        \n",
    "    # integrate secondary school ranking    \n",
    "    elif name in secondary_school_name:\n",
    "        ranking = secondary_no_duplicate.loc[secondary_no_duplicate['School Name']==name].Ranking.values[0]\n",
    "    \n",
    "    # if the school name can not be found from 'primary_school_name_list' and 'secondary_school_name', set the value to -1 for now\n",
    "    else:\n",
    "        ranking = -1\n",
    "    \n",
    "    # pair the school name and its ranking in a dictionary\n",
    "    schools_names_dictionary[name] = ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set ranking values based on schools_names_dictionary\n",
    "schools_xml_df.Ranking = schools_xml_df['School Name'].apply(lambda x: schools_names_dictionary.get(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Calculate closest distance for primary and secondary schools based on latitude and longtitude using UDF distance() function. Keep track with the dataframe index and create a dictionary based on the distance and index.\n",
    "\n",
    "In order to calculate the closest distance for both primary and secondary based on the residents' location (latitude and longtitude), we need to construct a for loop in a for loop and then perform the distance() function.\n",
    "\n",
    "* The first for loop we use a list (location_1) to store the location(latitude and longtitude) of the current resident, then we keep appending the calculated `primary_school_index and distance_primary_list as well as secondary_school_index and distance_secondary_list` from the second for loop\n",
    "* The second for loop we use a list (location_temp) to temperarily store the primary_school's location if the school type is 'Primary', or the secondary_school's location if the school type is 'Secondary'. After this we append the distance to its corresponding distance_list(stored in the first loop), also we keep track on its corresponding dataframe index and append that to the school_index list(stored in the first loop).\n",
    "* Next step we use dictionary to store the pairs of distance and index\n",
    "* After this we find the closest distance from loop by comparing with previous value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lists to store index and distance for both primary school and secondary school\n",
    "closest_primary_school_index_list = []\n",
    "distance_to_primary_school_list = []\n",
    "\n",
    "closest_secondary_school_index_list = []\n",
    "distance_to_secondary_school_list = []\n",
    "\n",
    "# loop through residents dataframe to find closest school\n",
    "for index, row in tqdm(residents.iterrows(), total = len(residents)):\n",
    "    \n",
    "    # use a list to store the lattitude and longtitude for each individual residents\n",
    "    location_1 = []\n",
    "    location_1.append(row['Lattitude'])\n",
    "    location_1.append(row['Longtitude'])\n",
    "    \n",
    "    # use a list to store the primary school's index and distance  \n",
    "    primary_school_index = []\n",
    "    distance_primary_list = []\n",
    "    \n",
    "    # use a list to store the secondary school's index and distance \n",
    "    secondary_school_index = []\n",
    "    distance_secondary_list = []\n",
    "    \n",
    "    for index_s, row_s in schools_xml_df.iterrows():\n",
    "        location_temp = []\n",
    "        location_temp.append(float(row_s['Y'])) # change the string type to float type\n",
    "        location_temp.append(float(row_s['X'])) # change the string type to float type\n",
    "        \n",
    "        if row_s['School Type'] == 'Primary':\n",
    "            # append distance to distance_primary_list in order to find minimum distance by min()\n",
    "            distance_primary_list.append(distance(location_1, location_temp)) \n",
    "            primary_school_index.append(index_s)\n",
    "            \n",
    "        elif row_s['School Type'] == 'Secondary':\n",
    "            # append distance to distance_secondary_list in order to find minimum distance by min()\n",
    "            distance_secondary_list.append(distance(location_1, location_temp)) \n",
    "            secondary_school_index.append(index_s)\n",
    "    \n",
    "    # create a dictionary having distance as key and school_index as value\n",
    "    # we use distance as key because distance is unique\n",
    "    merged_primary_index_distance = zip(distance_primary_list, primary_school_index)\n",
    "    primary_distance_dictionary = dict(merged_primary_index_distance)\n",
    "    \n",
    "    merged_secondary_index_distance = zip(distance_secondary_list, secondary_school_index)\n",
    "    secondary_distance_dictionary = dict(merged_secondary_index_distance)\n",
    "    \n",
    "    # find minimun distance and locate the school index\n",
    "    closest_primary_distance = min(distance_primary_list)\n",
    "    closest_primary_school_index = primary_distance_dictionary.get(closest_primary_distance)\n",
    "    \n",
    "    # record the minimum distance and its corresponding school index\n",
    "    closest_primary_school_index_list.append(closest_primary_school_index)\n",
    "    distance_to_primary_school_list.append(closest_primary_distance)\n",
    "    \n",
    "    # find minimun distance and locate the school index\n",
    "    closest_secondary_distance = min(distance_secondary_list)\n",
    "    closest_secondary_school_index = secondary_distance_dictionary.get(closest_secondary_distance)\n",
    "    \n",
    "    # record the minimum distance and its corresponding school index\n",
    "    closest_secondary_school_index_list.append(closest_secondary_school_index)\n",
    "    distance_to_secondary_school_list.append(closest_secondary_distance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary having school index as key and distance as value\n",
    "primary_school_index_distance_dictionary = dict(zip(closest_primary_school_index_list, distance_to_primary_school_list))\n",
    "secondary_school_index_distance_dictionary = dict(zip(closest_secondary_school_index_list, distance_to_secondary_school_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Integrate closest distance, school name for each resident.\n",
    "* use school index as a reference to retrieve the corresponding information from `schools_xml_df` dataframe\n",
    "* use iloc[] to look for the corresponding information of the school index\n",
    "* change the default -1 ranking value to 'not ranked' as specified by the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store school index for now in favor of easier processing\n",
    "residents['Closest Primary School Index'] = closest_primary_school_index_list\n",
    "residents['Closest Secondary School Index'] = closest_secondary_school_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up for school name using the index\n",
    "residents['closest_primary_school'] = residents['Closest Primary School Index']\\\n",
    "                            .apply(lambda x: schools_xml_df.iloc[x]['School Name'])\n",
    "\n",
    "residents['closest_secondary_school'] = residents['Closest Secondary School Index']\\\n",
    "                            .apply(lambda x: schools_xml_df.iloc[x]['School Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up for distance using index through index_distance dictionary\n",
    "residents['distance_to_closest_primary'] = residents['Closest Primary School Index']\\\n",
    "                                .apply(lambda x: primary_school_index_distance_dictionary.get(x))\n",
    "\n",
    "residents['distance_to_closest_secondary'] = residents['Closest Secondary School Index']\\\n",
    "                                .apply(lambda x: secondary_school_index_distance_dictionary.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up for ranking using index from schools_xml dataframe\n",
    "residents['primary_school_ranking'] = residents['Closest Primary School Index']\\\n",
    "                                .apply(lambda x: schools_xml_df.iloc[x]['Ranking'])\n",
    "\n",
    "residents['secondary_school_ranking'] = residents['Closest Secondary School Index']\\\n",
    "                                .apply(lambda x: schools_xml_df.iloc[x]['Ranking'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the ranking value to 'not ranked' if the value is default value -1\n",
    "def not_ranked(minusone):\n",
    "    if minusone == -1:\n",
    "        return 'not ranked'\n",
    "residents['primary_school_ranking'] = residents['primary_school_ranking'].apply(lambda x: not_ranked(x)) \n",
    "residents['secondary_school_ranking'] = residents['secondary_school_ranking'].apply(lambda x: not_ranked(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop excess index column\n",
    "residents = residents.drop(columns=['Closest Primary School Index', 'Closest Secondary School Index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: data reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will study the effect of different normalization/transformation methods on Rooms, crime_C_average, travel_min_to_CBD, and property_age attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residents.to_csv('data.csv')\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', usecols = [\"Rooms\", \"crime_C_average\", \"travel_min_to_CBD\", \"Age\", \"Price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[[\"Rooms\", \"crime_C_average\", \"travel_min_to_CBD\", \"Age\"]],\n",
    "                                                    data.Price, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standarisation is also used to bring all the variables to a similar scale. Standarisation means centering the variable at zero, and standarising the variance at 1. The procedure involves subtracting the mean of each observation and then dividing by the standard deviation:\n",
    "\n",
    "z = (x - x_mean) /  std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler #standardization\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new data frame with just columns of interest\n",
    "columns = [\"Rooms\", \"crime_C_average\", \"travel_min_to_CBD\", \"Age\", \"Price\"]\n",
    "modelling = data_final[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's have a look at the scaled training dataset: mean and standard deviation\n",
    "print('means (\"Rooms\", \"crime_C_average\", \"travel_min_to_CBD\", \"Age\"): ', X_train_scaled.mean(axis=0))\n",
    "print('std (\"Rooms\", \"crime_C_average\", \"travel_min_to_CBD\", \"Age\"): ', X_train_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the transformed min and max values\n",
    "\n",
    "print('Min (\"Rooms\", \"crime_C_average\", \"travel_min_to_CBD\", \"Age\"): ', X_train_scaled.min(axis=0))\n",
    "print('Max (\"Rooms\", \"crime_C_average\", \"travel_min_to_CBD\", \"Age\"): ', X_train_scaled.max(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the minimum and maximum values vary according to how spread the variable was to begin with and is highly influenced by the presence of outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the distribution of the transformed variable Rooms\n",
    "plt.hist(X_train_scaled[:,0], bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at how transformed Room looks like compared to the original variable\n",
    "import seaborn as sns\n",
    "sns.jointplot(X_train.Rooms, X_train_scaled[:,0], kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformed variable has a good correlation with the original one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the distribution of the transformed variable crime_C_average\n",
    "plt.hist(X_train_scaled[:,2], bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at how transformed CrimeC looks like compared to the original variable\n",
    "import seaborn as sns\n",
    "sns.jointplot(X_train.crime_C_average, X_train_scaled[:,0], kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(X_train_scaled[:,3], bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at how transformed Travel_min_toCBD looks like compared to the original variable\n",
    "import seaborn as sns\n",
    "sns.jointplot(X_train.travel_min_to_CBD, X_train_scaled[:,0], kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model we plan on creating\n",
    "model = \"Price ~ Rooms + crime_C_average + travel_min_to_CBD + Age\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = smf.ols(model, data=data.loc[X_train]).fit()\n",
    "print(base_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The challenge for School Ranking is to use beautifulSoup to locate the sections that contain desired information. It took us a while to be able to use findAll() method to store the desired information into containers variables. Another challenge in this part is to calculate the closest distance. When constructing the for in for loop to calculate the distance, it was easy to make mistakes. But we managed to use dictionary to keep track with the right distance and its correspongding index."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
